@article{DHAMO2019333,
title = {Peeking behind objects: Layered depth prediction from a single image},
journal = {Pattern Recognition Letters},
volume = {125},
pages = {333-340},
year = {2019},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2019.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167865518307062},
author = {Helisa Dhamo and Keisuke Tateno and Iro Laina and Nassir Navab and Federico Tombari},
keywords = {Layered depth image, RGB-D inpainting, Generative adversarial networks, Occlusion},
abstract = {While conventional depth estimation can infer the geometry of a scene from a single RGB image, it fails to estimate scene regions that are occluded by foreground objects. This limits the use of depth prediction in augmented and virtual reality applications, that aim at scene exploration by synthesizing the scene from a different vantage point, or at diminished reality. To address this issue, we shift the focus from conventional depth map prediction to the regression of a specific data representation called Layered Depth Image (LDI), which contains information about the occluded regions in the reference frame and can fill in occlusion gaps in case of small view changes. We propose a novel approach based on Convolutional Neural Networks (CNNs) to jointly predict depth maps and foreground separation masks used to condition Generative Adversarial Networks (GANs) for hallucinating plausible color and depths in the initially occluded areas. We demonstrate the effectiveness of our approach for novel scene view synthesis from a single image.}
}
@article{RAMEZANI2020119846,
title = {Approaches for resilience and antifragility in collaborative business ecosystems},
journal = {Technological Forecasting and Social Change},
volume = {151},
pages = {119846},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2019.119846},
url = {https://www.sciencedirect.com/science/article/pii/S0040162519304494},
author = {Javaneh Ramezani and Luis M. Camarinha-Matos},
keywords = {Business ecosystem, Collaboration, Disruptions, Resilience, Antifragility},
abstract = {Contemporary business ecosystems are continuously challenged by unexpected disruptive events, which are increasing in their frequency and effects. A critical question is why do some organizations collapse in face of extreme events, while others not? On the other hand, current engineering and socio-technical systems were designed to operate in “mostly stable” situations; sporadic instability and disturbances are at best captured by exception handling mechanisms, focusing on reliability and robustness. Recent and more ambitious design goals, however, aim at building systems that are expected to cope with severe disruptions, and survive or even thrive in a context of volatility and uncertainty. This led to an increasing attention to the concepts of resilience and antifragility. As such, this article introduces the findings of a comprehensive literature survey aimed at shedding light on emerging concepts and approaches to handle disruptions in business ecosystems. Main contributions include a clarification of related concepts, identification and classification of disruption sources and drivers, and extensive lists of strategies and underlying capabilities to cope with disruptions. Related perspectives and approaches developed in multiple knowledge areas are also analysed and synthesized. Finally, a collection of engineered systems implementing promising approaches to increase resilience and antifragility are presented.}
}
@article{CASTILLO2019103,
title = {Recent advances in fabric appearance reproduction},
journal = {Computers & Graphics},
volume = {84},
pages = {103-121},
year = {2019},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2019.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0097849319301256},
author = {Carlos Castillo and Jorge López-Moreno and Carlos Aliaga},
keywords = {Fiber scattering, Fabric modeling, Appearance matching, Fabric rendering, Virtual fabrics},
abstract = {Reproducing the appearance of real world materials has been a long standing problem in computer graphics. Among them, fibrous materials such as cloth, remain as some of the most challenging to recreate. This is due to the intrinsic complexity of fabrics; their overall look is determined by both the anisotropic light scattering behavior exhibited at the fiber level, usually at the micron scale, and the weaving structure that constrains the alignment of those fibers. Despite the increasing research efforts in the different areas involved, from capturing to modeling, rendering and filtering, there is no single survey nowadays that collects and discusses the benefits, drawbacks and practical considerations of the available techniques that aim to reproduce the appearance of fabrics. In this review, we provide a comprehensive survey of the existing techniques involved at each of the different stages of fabric appearance reproduction. We aim to provide guidelines for practitioners to select among existing options in crucial aspects such as scattering models or fabric representations depending on each particular context, also discussing future lines of research and most promising paths in the direction of accurately representing virtual fabrics.}
}
@article{SMITH20186,
title = {The intelligent solution: automation, the skills shortage and cyber-security},
journal = {Computer Fraud & Security},
volume = {2018},
number = {8},
pages = {6-9},
year = {2018},
issn = {1361-3723},
doi = {https://doi.org/10.1016/S1361-3723(18)30073-3},
url = {https://www.sciencedirect.com/science/article/pii/S1361372318300733},
author = {Graham Smith},
abstract = {A Las Vegas hacking event in 2016, the Cyber Grand Challenge, was the ultimate – and only – all-machine hacking competition.1 Each machine identified software vulnerabilities, exploited them and patched their own systems to protect against threats – all without the intervention of a human programmer. Could this be the future of information security? And in particular, could it address the infamous skills shortage? The lack of security skills in the IT industry is in part because professionals in this field work long hours and require patience, resources, knowledge and experience Unfortunately, the cyber-security talent pool simply isn't wide enough to meet these needs. Graham Smith of Curo Talent discusses how AI and automation might relieve some of the pressure, automating the longwinded and repetitive tasks that are currently filling the workflows of IT teams, such as testing, basis threat analysis and data deception tactics.}
}
@article{SPOLAOR2020103557,
title = {A systematic review on content-based video retrieval},
journal = {Engineering Applications of Artificial Intelligence},
volume = {90},
pages = {103557},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.103557},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620300488},
author = {Newton Spolaôr and Huei Diana Lee and Weber Shoity Resende Takaki and Leandro Augusto Ensina and Claudio Saddy Rodrigues Coy and Feng Chung Wu},
keywords = {Color features, Unsupervised learning, Shot boundary detection},
abstract = {Content-based video retrieval and indexing have been associated with intelligent methods in many applications such as education, medicine and agriculture. However, an extensive and replicable review of the recent literature is missing. Moreover, relevant topics that can support video retrieval, such as dimensionality reduction, have not been surveyed. This work designs and conducts a systematic review to find papers able to answer the following research question: “what segmentation, feature extraction, dimensionality reduction and machine learning approaches have been applied for content-based video indexing and retrieval?”. By applying a research protocol proposed by us, 153 papers published from 2011 to 2018 were selected. As a result, it was found that strategies for cut-based segmentation, color-based indexing, k-means based dimensionality reduction and data clustering have been the most frequent choices in recent papers. All the information extracted from these papers can be found in a publicly available spreadsheet. This work also indicates additional findings and future research directions.}
}
@article{EMIL2018840,
title = {Introducing Compressed Mixture Models for Predicting Long-Lasting Brake Events⁎⁎This work has been jointly funded by Volvo Cars and by the research program Fordonsstrategisk Forskning och Innovation (FFI), which is gratefully acknowledged.},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {31},
pages = {840-845},
year = {2018},
note = {5th IFAC Conference on Engine and Powertrain Control, Simulation and Modeling E-COSM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.10.115},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318325795},
author = {Staf Emil and McKelvey Tomas},
keywords = {Machine learning, recursive algorithms, model complexity reduction, prediction methods, probabilistic models, information theory},
abstract = {With tougher restrictions on emissions the automotive industry is in dire need of additional functionality to reduce emissions. We conduct a case study trying to predict long-lasting brake events, to support the decision-making process when the engine can beneficially be put to idle or shut down to achieve emission reduction. We introduce Compressed Mixture Models, a multivariate and mixed variate kernel density model featuring online training and complexity reduction, and use it for prediction purposes. The results show that the proposed method produces comparable prediction results as a Random Forest Classifier and outperform a Support Vector Classifier. On an urban road a prediction accuracy of 87.4 % is obtained, while a prediction accuracy of 76.4 % on a highway segment using the proposed method. Furthermore, it is possible to use a trained Compressed Mixture Model as a tool for statistical inference to study the properties of the observed realization of the underlying random variables.}
}
@article{BARREDOARRIETA202082,
title = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
journal = {Information Fusion},
volume = {58},
pages = {82-115},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
author = {Alejandro {Barredo Arrieta} and Natalia Díaz-Rodríguez and Javier {Del Ser} and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
keywords = {Explainable Artificial Intelligence, Machine Learning, Deep Learning, Data Fusion, Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible Artificial Intelligence},
abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.}
}
@article{SCHMIDT2019167,
title = {Visual perception of shape-transforming processes: ‘Shape Scission’},
journal = {Cognition},
volume = {189},
pages = {167-180},
year = {2019},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010027719300903},
author = {Filipp Schmidt and Flip Phillips and Roland W. Fleming},
keywords = {Vision, Objects, Recognition, Categorization, Perceptual organization, Gestalt},
abstract = {Shape-deforming processes (e.g., squashing, bending, twisting) can radically alter objects’ shapes. After such a transformation, some features are due to the object’s original form, while others are due to the transformation, yet it is challenging to separate the two. We tested whether observers can distinguish the causal origin of different features, teasing apart the characteristics of the original shape from those imposed by transformations, a process we call ‘shape scission’. Using computer graphics, we created 8 unfamiliar objects and subjected each to 8 transformations (e.g., “twisted”, “inflated”, “melted”). One group of participants named transformations consistently. A second group arranged cards depicting the objects into classes according to either (i) the original shape or (ii) the type of transformation. They could do this almost perfectly, suggesting that they readily distinguish the causal origin of shape features. Another group used a digital painting interface to indicate which locations on the objects appeared transformed, with responses suggesting they can localise features caused by transformations. Finally, we parametrically varied the magnitude of the transformations, and asked another group to rate the degree of transformation. Ratings correlated strongly with transformation magnitude with a tendency to overestimate small magnitudes. Responses were predicted by both the magnitude and area affected by the transformation. Together, the findings suggest that observers can scission object shapes into original shape and transformation features and access the resulting representational layers at will.}
}
@article{MINOR201879,
title = {Smokey Bear and the pyropolitics of United States forest governance},
journal = {Political Geography},
volume = {62},
pages = {79-93},
year = {2018},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0962629816301548},
author = {Jesse Minor and Geoffrey A. Boyce},
keywords = {Advertising, Biopolitics, Environmentality, Governmentality, Posthumanism, State theory, Wildfire},
abstract = {Wildfire prevention advertisements featuring Smokey Bear represent the longest-standing and most successful government advertising and branding campaign in U.S. history. As the public face of U.S. fire control policy, Smokey Bear uses mass media to influence the attitudes and behavior of U.S. citizenry in order to accomplish particular outcomes related to wildfire prevention and suppression, forest protection, and resource management. Smokey Bear can therefore be viewed as a governmental instrument that simultaneously targets the behavior of the U.S. public and the biophysical materiality of combustible forests. Examining the evolution of Smokey Bear and related wildfire prevention media, we explore connections between state management of people, territory, and flammable landscapes. Borrowing from Nigel Clark (2011), we use the term pyropolitics to describe the resulting more-than-human assemblage of citizenship, fire suppression and forest ecology. Importantly, this pyropolitical assemblage has substantive and recursive impacts on state practice. Through aggressive wildfire prevention and suppression that include and extend beyond Smokey Bear, the U.S. state has transformed fuel loads, species compositions, and ecosystem dynamics across North America. One result is a heightened propensity toward catastrophic wildfire, requiring additional and sustained state intervention to maintain an imposed and unstable equilibrium. Thus even as the economic, social and cultural realities of U.S. civic life have changed over the course of the 20th and early 21st centuries – and even as knowledge of the ecological benefits of fire to ecosystem health has developed over time – the message of Smokey Bear has remained remarkably consistent, communicating an official imperative to prevent anthropogenic ignition.}
}
@incollection{TERMANINI202039,
title = {Chapter 3 - The miraculous anatomy of the digital immunity ecosystem},
editor = {Rocky Termanini},
booktitle = {Storing Digital Binary Data in Cellular DNA},
publisher = {Academic Press},
pages = {39-77},
year = {2020},
isbn = {978-0-323-85222-7},
doi = {https://doi.org/10.1016/B978-0-12-823295-8.00003-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128232958000033},
author = {Rocky Termanini},
keywords = {Antivirus technology (AVT), Autonomic self-regulating, Causality, Deep battle strategy, Ontology, The Smart Vaccine},
abstract = {In this chapter, we are going to discuss three major points: (1) the structure of one of the digital immunity ecosystem (DIE); (2) scenarios from the battlefield; (3) how to predict an incoming attack. One of the great services to humanity was the discovery of the vaccine. Without adaptive immunity, one-fourth of the human race would have been terminally ill. Dr. Edward Jenner and Louis Pasteur Share this honorable credit. The DIE is the exact replication of the human immune system for the digital world.}
}
@article{WANG2019103850,
title = {Artificial intelligence facilitates drug design in the big data era},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {194},
pages = {103850},
year = {2019},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2019.103850},
url = {https://www.sciencedirect.com/science/article/pii/S0169743919304605},
author = {Liangliang Wang and Junjie Ding and Li Pan and Dongsheng Cao and Hui Jiang and Xiaoqin Ding},
keywords = {Artificial intelligence, Drug design, Big data, Machine learning, Deep learning},
abstract = {With the dramatic development of high-performance computing, the emergence of better algorithms and the accumulation of large amounts of chemical and biological data, computer-aided drug design technology is playing an increasingly prominent role in drug discovery and development with its advantages of fast speed, low cost and high efficiency. In recent years, due to the constant development of machine learning (ML) theory, artificial intelligence (AI), a powerful data mining technology has been widely used in various stages of drug design. More recently, drug design has entered the era of big data, ML methods have gradually evolved into a deep learning (DL) method with stronger generalization ability and more effective big data processing, which further promotes the combination of AI technology and computer-aided drug design technology, thus facilitating the discovery and design of new drugs. This paper mainly summarizes the application progress of AI technology in drug design process, analyses and compares its advantages over traditional methods. Finally, the challenges faced by AI technology and its application prospects in the field of drug design are also discussed.}
}
@article{BOXMANSHABTAI20181,
title = {Reframing the popular: A new approach to parody},
journal = {Poetics},
volume = {67},
pages = {1-12},
year = {2018},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2018.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X17301560},
author = {Lillian Boxman-Shabtai},
keywords = {Copyright, Framing, Gender, Music, Parody, YouTube},
abstract = {The ubiquity of intertextuality in internet culture has ignited long-standing debates about the cultural significance of parody as a device of commentary and as civic speech. It also raises concerns about the legal implications of unprecedented uses of copyrighted material. This paper examines how YouTube videos, self-labeled by their creators as “parody”, reframe the meaning structures of copyrighted material. Focusing on representations of gender in the music industry, it probes 100 music video parodies through a qualitative textual analysis. The paper offers a typology of five interpretive configurations underscoring the relationships between originals and their renditions. While the majority of parodies did not convey the critical commentary that their label promised, most of them did aspire to transform the meaning of the music videos. The typology, which presents a discrepancy between textual and societal forms of critique, is discussed in relation to its contribution to broader evaluations of media audiences and user-generated-content.}
}
@article{2019e296,
title = {Invited Faculty Abstracts from the International Neuromodulation Society’s 14th World Congress},
journal = {Neuromodulation: Technology at the Neural Interface},
volume = {22},
number = {7},
pages = {e296-e584},
year = {2019},
issn = {1094-7159},
doi = {https://doi.org/10.1111/ner.12958},
url = {https://www.sciencedirect.com/science/article/pii/S1094715921020249}
}
@article{NUSKE2019100319,
title = {Vehicle of erotic liberation or instrument of career survival? Japan’s ideologies of English as reflected in conversation school advertisements},
journal = {Discourse, Context & Media},
volume = {31},
pages = {100319},
year = {2019},
issn = {2211-6958},
doi = {https://doi.org/10.1016/j.dcm.2019.100319},
url = {https://www.sciencedirect.com/science/article/pii/S2211695819300078},
author = {Kyle Nuske},
abstract = {Private, for-profit English conversation schools (eikaiwa) are a prominent venue of study among Japanese adults. Thus, eikaiwa marketing strategies are reflective and constitutive of discourses that structure the desire or compulsion for English learning in Japan. Synthesizing critical, multimodal, and Foucauldian discourse analysis techniques, this study examines 28 recent print eikaiwa advertisements to explicate the visual and verbal elements through which particular ideologies of English proficiency and its benefits are constructed. Findings are integrated with data from a small-scale survey of Japanese adults (n = 13) to illuminate how the advertisements are perceived by members of the target audience. Whereas these advertisements have historically exploited images of Caucasian men to incite heterosexual women’s akogare (yearning), texts in the present corpus commonly frame English learning as a rational process of acquiring quantifiable skills, which enhance workers’ self-worth by ensuring their continued value to their employers. Moreover, they often characterize English study as rigorous and inevitable albeit burdensome and demeaning. The study argues that these emerging themes are attributable to the growing influence of neoliberal ideologies and practices, and they mainly serve to propagate fantasies of English proficiency as a reliable means of obtaining stable vocations within volatile labor markets.}
}
@article{HARBINJA2020105403,
title = {Your data will never die, but you will: A comparative analysis of US and UK post-mortem data donation frameworks},
journal = {Computer Law & Security Review},
volume = {36},
pages = {105403},
year = {2020},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2020.105403},
url = {https://www.sciencedirect.com/science/article/pii/S026736492030008X},
author = {Edina Harbinja and Henry Pearce},
keywords = {Health and medical data, International data protection, Organ donation, Patient records, Posthumous medical data donation, Post-mortem privacy},
abstract = {Posthumous medical data donation (PMDD) for the purpose of legitimate, non-commercial and, potentially, very beneficial medical research has been sparsely discussed in legal scholarship to date. Conversely, quite an extensive social science and humanities research establishes benefits of this practice. It also finds that PMDD enables individuals to employ their altruistic motivations and aspirations by helping them participate in ‘citizen's science’ and medical research, thus supporting efforts in finding cures for some of the acutest diseases of today. There appears to be no jurisdiction where a regulatory framework supports and enables PMDD. This paper analyses whether and to what extent law and policy should enable this practice. We take a comparative approach, examining the position under both US and UK law, providing the first comparative legal account of this practice. We do not aim to suggest a detailed legal solution for PMDD, but rather key considerations and principles for legislative/policy reforms, which would support the practice of PMDD. We discuss organ donation and provide a comparative outlook with the aim of drawing lessons from this practice, and applying them to the regulation of PMDD. Our analysis is both normative and black letter since we consider arguments regarding the necessity of organ and data donation, as well as the law that regulates these practices.}
}
@article{MONTAVON20181,
title = {Methods for interpreting and understanding deep neural networks},
journal = {Digital Signal Processing},
volume = {73},
pages = {1-15},
year = {2018},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2017.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S1051200417302385},
author = {Grégoire Montavon and Wojciech Samek and Klaus-Robert Müller},
keywords = {Deep neural networks, Activation maximization, Sensitivity analysis, Taylor decomposition, Layer-wise relevance propagation},
abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.}
}
@article{LIU2018311,
title = {When guanxi meets structural holes: Exploring the guanxi networks of Chinese entrepreneurs on digital platforms},
journal = {The Journal of Strategic Information Systems},
volume = {27},
number = {4},
pages = {311-334},
year = {2018},
note = {Generating Business and Social Value from Digital Entrepreneurship and Innovation},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2018.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0963868717301026},
author = {Jiayuan Liu and Joe Nandhakumar and Markos Zachariadis},
keywords = {Chinese digital entrepreneurs on digital platforms, , Structural holes, Knowledge orchestration},
abstract = {In this exploratory study, we investigate how Chinese entrepreneurs on digital platforms interact and leverage guanxi (a system of relationships and social network) to buffer the negative impacts of structural holes on knowledge orchestration. We develop our research model and formulate ten hypotheses by drawing on the literature. We adopt a mixed-methods research approach in which we use quantitative surveys to test the hypotheses, and qualitative interviews to explain why certain relationships are stronger in one stage of entrepreneurial development than the other. The study contributes to the literature on digital entrepreneurship in two ways. First, this study offers an initial understanding of the dynamics of guanxi networks for knowledge mobilisation and knowledge coordination across start-up and growth stages of Chinese entrepreneurs on digital platforms. Second, by drawing on the relevant literature, our findings extend the current understanding of knowledge orchestration of digital entrepreneurs and contribute to the literatures of structural holes theory and guanxi.}
}
@article{HE202012175,
title = {Continuous Learning of Deep Neural Networks to Improve Forecasts for Regional Energy Markets},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {12175-12182},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1017},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320313823},
author = {Yujiang He and Janosch Henze and Bernhard Sick},
keywords = {Smart grids, Intelligent control of power systems, Modeling, simulation of power systems},
abstract = {Germany generated 54.5% of electricity from renewable energy in March 2019, according to the data collected by the Fraunhofer Institute for Solar Energy Systems. Forecasting power generation and consumption play an essential role in establishing a regional smart energy market. Numerous researches contributed to the field of power forecasting using machine learning and deep learning technologies. However, developing and perfecting energy markets lead to an unavoidable problem of adjusting the architectures of neural networks to adapt to new situations, e.g., new consumers or producers in the power grid. Another critical challenge is to learn new knowledge from the sequentially collected measurements efficiently and how to integrate the new information into the current neural network model. When retrained for a new task with a regular training process, neural network models could perform poorly on the previously learned tasks, which is referred to as the catastrophic forgetting problem. In this article, we design two real-world continuous learning scenarios for those challenges. The scenarios are based on the historical power data, which are obtained from a regional power grid in Germany. The results show that well-known continuous learning algorithms can be used to improve power forecasts with a sequential data stream in such scenarios. We believe that the work is the first step towards establishing an adaptively updating forecast system to assist the highly dynamic intelligent energy markets.}
}
@article{HIELSCHER2018978,
title = {Contested smart and low-carbon energy futures: Media discourses of smart meters in the United Kingdom},
journal = {Journal of Cleaner Production},
volume = {195},
pages = {978-990},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.05.227},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618315749},
author = {Sabine Hielscher and Benjamin K. Sovacool},
keywords = {Smart grid, Media discourses, Futures, Energy infrastructure, Smart meters},
abstract = {The Smart Meter Implementation Programme (SMIP) is arguably one of the most expansive and complex smart meter programmes globally. The UK government regards smart meters to be enablers of a low-carbon energy grid and has set out ambitious consumer-orientated aims within their programme across England, Scotland, and Wales. Despite considerable amount of research on how consumers will (or not) engage with smart meters, media discourses, where some public debates about smart meters are created and reproduced, have received little attention. This paper presents a content analysis of how smart meters are discussed within 11 years of popular print media coverage. A collection of nine discourses are identified: Four of these – “empowered consumers”, “energy conscious world”, “low-carbon grid”, and “future smart innovation” – depict smart meters as a harbinger of positive social change. Five of these – “hacked and vulnerable grid”, “big brother”, “costly disaster”, “astronomical bills”, and “families in turmoil” – represent smart meters as negative forces on society. The results show that discourses and associated storylines mainly represent continuous struggles over particular socio-technical promises linked to smart meters. Somewhat missing are attempts to open up the smart energy debate to broader issues of democracy and energy justice within the print media coverage.}
}
@article{DECLERCQ2019402,
title = {Innovation hotspots in food waste treatment, biogas, and anaerobic digestion technology: A natural language processing approach},
journal = {Science of The Total Environment},
volume = {673},
pages = {402-413},
year = {2019},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2019.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S0048969719315700},
author = {Djavan {De Clercq} and Zongguo Wen and Qingbin Song},
keywords = {Natural language processing, Latent Dirichlet Allocation, TF-IDF, Food waste, Biogas, Anaerobic digestion},
abstract = {The objective of this study is to apply natural language processing to identifying innovative technology trends related to food waste treatment, biogas, and anaerobic digestion. The methodology used involved analyzing large volumes of text data mined from 3186 patents related to these three fields. Latent Dirichlet Allocation and the perplexity method were used to identify the main topics which the patent corpora were comprised of and which technological concepts were most associated with each topic. In addition, term frequency-inverse document frequency (TF-IDF) was used to gauge the “emergingness” of certain technical concepts across the patent corpora in various years. The key results were as follows: (1) perplexity computations showed that a 20 topic models were feasible for these patent corpora; (2) topics were identified, providing an accurate picture of the patenting landscape in the analyzed fields; (3) TF-IDF analysis on unigrams, bigrams, and trigrams, supplemented with network graph analysis, revealed emerging technology trends in each year. This study has important implications for governments who need to decide where to invest resources in anaerobic food waste treatment.}
}
@article{ZHANG2020103200,
title = {How social-media-enabled co-creation between customers and the firm drives business value? The perspective of organizational learning and social Capital},
journal = {Information & Management},
volume = {57},
number = {3},
pages = {103200},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103200},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618305901},
author = {Hong Zhang and Sumeet Gupta and Wei Sun and Yi Zou},
keywords = {Co-creation, Social media, Organizational learning, Social capital, Business value, Organizational performance},
abstract = {Contemporary business organizations are increasingly turning their attention to value co-creation using social media between individual customers and business organizations in the process of new product development (NPD). However, little is known about the mechanisms underlying social-media-based customer-firm co-creation and their implications for business value in NPD. To address this knowledge gap, this study develops a model from the perspective of organizational learning and social capital to examine how the social-media-based customer-firm co-creation mechanism conceptualized as the structural, cognitive, and relational dimension of social capital influences the first-order knowledge outcome (knowledge transfer effectiveness) and second-order dynamic capability outcome (absorptive capacity), and how these co-creation outcomes ultimately influence organizational performance. The model is tested using survey data from 149 Chinese mobile application developers. The results indicate that social-media-based structural, cognitive, and relational linkage, in particular the structural linkage, is an important co-creation mechanism to improve organizational performance. Knowledge transfer effectiveness and absorptive capacity have significant mediating effects in this co-creation mechanism-outcomes-performance framework. Further, the moderating effects of social media use level on the relationships between co-creation mechanism and outcomes are largely supported. The study contributes to theory and practice by shedding light on the social-media-based customer-firm co-creation in NPD at a process level.}
}
@incollection{ABDELMAKSOUD2019209,
title = {Chapter 9 - Medical Images Analysis Based on Multilabel Classification},
editor = {Nilanjan Dey and Surekha Borra and Amira S. Ashour and Fuqian Shi},
booktitle = {Machine Learning in Bio-Signal Analysis and Diagnostic Imaging},
publisher = {Academic Press},
pages = {209-245},
year = {2019},
isbn = {978-0-12-816086-2},
doi = {https://doi.org/10.1016/B978-0-12-816086-2.00009-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128160862000096},
author = {Eman A. {Abdel Maksoud} and Sherif Barakat and Mohammed Elmogy},
keywords = {Multilabel classification, Machine learning, Medical image analysis, CAD system, Classification model},
abstract = {In the last years, a lot of literature has provided considerable support for multilabel classification in machine learning. It means that each sample or instance belongs to more than one class label simultaneously. Therefore, it represents complex objects that have multiple meanings. It helps in capturing more information by labeling some basic and hidden patterns. In this respect, multilabel classification is very useful in medical data analysis. It addresses the problem of diagnosis, surgery, anatomy, disease progress, analysis, and teaching purposes in medicine. There are many patients have many diseases at the same time, maybe in the same organ, such as ocular diseases. On the other side, the multilabel classification is a challenging issue by nature. This is due to high dimensionality, sparseness, and imbalance of available data. Some problems with labels are raised, such as label dependency, locality, interlabel diversity, and similarity. Therefore, our survey introduces significant topics of the multilabel classification in medical image analysis field. Notably, most of the literature did not show how multilabel classification affect the medical image analysis. In this chapter, we presented the different examples of medical image classification by the multilabel methods. We present the detailed analysis and discussions of the literature findings. The performance of the methods is compared on five publicly available data sets such as yeast, scene, genebase, corel5k and BibTex of multilabel classification using famous measures. Moreover, we intend to give the researcher a computer-aided CAD system framework for the existing multilabel classification research.}
}
@article{FORGHANI2019995,
title = {Radiomics and Artificial Intelligence for Biomarker and Prediction Model Development in Oncology},
journal = {Computational and Structural Biotechnology Journal},
volume = {17},
pages = {995-1008},
year = {2019},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2001037019301382},
author = {Reza Forghani and Peter Savadjiev and Avishek Chatterjee and Nikesh Muthukrishnan and Caroline Reinhold and Behzad Forghani},
keywords = {Artificial intelligence, Texture analysis, Radiomics, Machine learning, Precision oncology, Biomarker}
}
@article{HAWES201968,
title = {Relations between numerical, spatial, and executive function skills and mathematics achievement: A latent-variable approach},
journal = {Cognitive Psychology},
volume = {109},
pages = {68-90},
year = {2019},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2018.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010028518302548},
author = {Zachary Hawes and Joan Moss and Beverly Caswell and Jisoo Seo and Daniel Ansari},
keywords = {Spatial skills, Spatial visualization, Numerical skills, Executive functions, Mathematics achievement, Latent-variable analysis (SEM)},
abstract = {Current evidence suggests that numerical, spatial, and executive function (EF) skills each play critical and independent roles in the learning and performance of mathematics. However, these conclusions are largely based on isolated bodies of research and without measurement at the latent variable level. Thus, questions remain regarding the latent structure and potentially shared and unique relations between numerical, spatial, EF, and mathematics abilities. The purpose of the current study was to (i) confirm the latent structure of the hypothesized constructs of numerical, spatial, and EF skills and mathematics achievement, (ii) measure their unique and shared relations with one another, and (iii) test a set of novel hypotheses aimed to more closely reveal the underlying nature of the oft reported space-math association. Our analytical approach involved latent-variable analyses (structural equation modeling) with a sample of 4- to 11-year-old children (N = 316, Mage = 6.68 years). Results of a confirmatory factor analysis demonstrated that numerical, spatial, EF, and mathematics skills are highly related, yet separable, constructs. Follow-up structural analyses revealed that numerical, spatial, and EF latent variables explained 84% of children’s mathematics achievement scores, controlling for age. However, only numerical and spatial performance were unique predictors of mathematics achievement. The observed patterns of relations and developmental trajectories remained stable across age and grade (preschool – 4th grade). Follow-up mediation analyses revealed that numerical skills, but not EF skills, partially mediated the relation between spatial skills and mathematics achievement. Overall, our results point to spatial visualization as a unique and robust predictor of children’s mathematics achievement.}
}
@article{ZHANG2020106296,
title = {Testing and verification of neural-network-based safety-critical control software: A systematic literature review},
journal = {Information and Software Technology},
volume = {123},
pages = {106296},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106296},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300471},
author = {Jin Zhang and Jingyue Li},
keywords = {Software testing and verification, Neural network, Safety-critical control software, Systematic literature review},
abstract = {Context: Neural Network (NN) algorithms have been successfully adopted in a number of Safety-Critical Cyber-Physical Systems (SCCPSs). Testing and Verification (T&V) of NN-based control software in safety-critical domains are gaining interest and attention from both software engineering and safety engineering researchers and practitioners. Objective: With the increase in studies on the T&V of NN-based control software in safety-critical domains, it is important to systematically review the state-of-the-art T&V methodologies, to classify approaches and tools that are invented, and to identify challenges and gaps for future studies. Method: By searching the six most relevant digital libraries, we retrieved 950 papers on the T&V of NN-based Safety-Critical Control Software (SCCS). Then we filtered the papers based on the predefined inclusion and exclusion criteria and applied snowballing to identify new relevant papers. Results: To reach our result, we selected 83 primary papers published between 2011 and 2018, applied the thematic analysis approach for analyzing the data extracted from the selected papers, presented the classification of approaches, and identified challenges. Conclusion: The approaches were categorized into five high-order themes, namely, assuring robustness of NNs, improving the failure resilience of NNs, measuring and ensuring test completeness, assuring safety properties of NN-based control software, and improving the interpretability of NNs. From the industry perspective, improving the interpretability of NNs is a crucial need in safety-critical applications. We also investigated nine safety integrity properties within four major safety lifecycle phases to investigate the achievement level of T&V goals in IEC 61508-3. Results show that correctness, completeness, freedom from intrinsic faults, and fault tolerance have drawn most attention from the research community. However, little effort has been invested in achieving repeatability, and no reviewed study focused on precisely defined testing configuration or defense against common cause failure.}
}
@article{201928,
title = {A Novel Method for High Parameter Multiplex Phenotyping and Analysis of the Tumor Micro-Environment},
journal = {Journal of Pathology Informatics},
volume = {10},
number = {1},
pages = {28},
year = {2019},
issn = {2153-3539},
doi = {https://doi.org/10.4103/2153-3539.266902},
url = {https://www.sciencedirect.com/science/article/pii/S2153353922003911}
}
@article{WEI2019452,
title = {Developing a hierarchical system for energy corporate risk factors based on textual risk disclosures},
journal = {Energy Economics},
volume = {80},
pages = {452-460},
year = {2019},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2019.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0140988319300350},
author = {Lu Wei and Guowen Li and Xiaoqian Zhu and Xiaolei Sun and Jianping Li},
keywords = {Risk management, Energy industry, Risk factor, Text mining, Form 10-K},
abstract = {Selecting risk factors is essential for measuring energy corporate risk. However, the comprehensive identification of energy corporate risk factors is still a difficult issue. This paper innovatively uses the text mining approach to comprehensively identify energy corporate risk factors from textual risk disclosures reported in financial statements. Based on 131,755 risk factor headings from 3707 Form 10-K filings from 840 U.S. energy corporations over the period 2010–2016, 66 types of risk factors that affect energy corporate risks are identified. Furthermore, we develop a hierarchical system for 66 energy corporate risk factors by dividing energy corporations into nine subsectors. Thus, the hierarchical energy corporate risk factor system provides fundamental support for further energy corporate risk measurement. Researchers can comprehensively and effectively select risk factors in measuring risks of the entire energy industry or each of nine energy subsectors.}
}
@article{GARDEZI2019,
title = {Breast Cancer Detection and Diagnosis Using Mammographic Data: Systematic Review},
journal = {Journal of Medical Internet Research},
volume = {21},
number = {7},
year = {2019},
issn = {1438-8871},
doi = {https://doi.org/10.2196/14464},
url = {https://www.sciencedirect.com/science/article/pii/S1438887119003819},
author = {Syed Jamal Safdar Gardezi and Ahmed Elazab and Baiying Lei and Tianfu Wang},
keywords = {breast cancer, lesion classification, malignant tumor, machine learning, convolutional neural networks, deep learning},
abstract = {Background
Machine learning (ML) has become a vital part of medical imaging research. ML methods have evolved over the years from manual seeded inputs to automatic initializations. The advancements in the field of ML have led to more intelligent and self-reliant computer-aided diagnosis (CAD) systems, as the learning ability of ML methods has been constantly improving. More and more automated methods are emerging with deep feature learning and representations. Recent advancements of ML with deeper and extensive representation approaches, commonly known as deep learning (DL) approaches, have made a very significant impact on improving the diagnostics capabilities of the CAD systems.
Objective
This review aimed to survey both traditional ML and DL literature with particular application for breast cancer diagnosis. The review also provided a brief insight into some well-known DL networks.
Methods
In this paper, we present an overview of ML and DL techniques with particular application for breast cancer. Specifically, we search the PubMed, Google Scholar, MEDLINE, ScienceDirect, Springer, and Web of Science databases and retrieve the studies in DL for the past 5 years that have used multiview mammogram datasets.
Results
The analysis of traditional ML reveals the limited usage of the methods, whereas the DL methods have great potential for implementation in clinical analysis and improve the diagnostic capability of existing CAD systems.
Conclusions
From the literature, it can be found that heterogeneous breast densities make masses more challenging to detect and classify compared with calcifications. The traditional ML methods present confined approaches limited to either particular density type or datasets. Although the DL methods show promising improvements in breast cancer diagnosis, there are still issues of data scarcity and computational cost, which have been overcome to a significant extent by applying data augmentation and improved computational power of DL algorithms.}
}
@article{BHAGAT20181,
title = {Image annotation: Then and now},
journal = {Image and Vision Computing},
volume = {80},
pages = {1-23},
year = {2018},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2018.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S0262885618301628},
author = {P.K. Bhagat and P. Choudhary},
keywords = {Image annotation, Automatic image annotation, Multi-label classification, Image labeling, Image tagging, Annotation dataset, Annotation performance evaluation, Image features, Image retrieval},
abstract = {Automatic image annotation (AIA) plays a vital role in dealing with the exponentially growing digital images. Image annotation helps in effective retrieval, organization, classification, auto-illustration, etc. of the image. It started in early 1990. However, in the last three decades, there has been extensive research in AIA, and various new approaches have been advanced. In this article, we review more than 200 references related to image annotation proposed in the last three decades. This paper is an attempt to discuss predominant approaches, its constraints and ways to deal. Each segment of the article exhibits a discourse to expound the finding and future research directions and their hurdles. This paper also presents performance evaluation measures with relevant and influential image annotation database.}
}
@article{GOVINDARAJAN2019100955,
title = {Intelligent collaborative patent mining using excessive topic generation},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100955},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100955},
url = {https://www.sciencedirect.com/science/article/pii/S1474034619300126},
author = {Usharani Hareesh Govindarajan and Amy J.C. Trappey and Charles V. Trappey},
keywords = {Technology mining, Excessive topic generation, Industrial immersive patenting, Patent data visualization},
abstract = {An inevitable consequence of the technology-driven economy has led to the increased importance of intellectual property protection through patents. Recent global pro-patenting shifts have further resulted in high technology overlaps. Technology components are now spread across a huge corpus of patent documents making its interpretation a knowledge-intensive engineering activity. Intelligent collaborative patent mining facilitates the integration of inputs from patented technology components held by diverse stakeholders. Topic generative models are powerful natural language tools used to decompose data corpus topics and associated word bag distributions. This research develops and validates a superior text mining methodology, called Excessive Topic Generation (ETG), as a preprocessing framework for topic analysis and visualization. The presented ETG methodology adapts the topic generation characteristics from Latent Dirichlet Allocation (LDA) with added capability to generate word distance relationships among key terms. The novel ETG approach is used as the core process for intelligent collaborative patent mining. A case study of 741 global Industrial Immersive Technology (IIT) patents covering inventive and novel concepts of Virtual Reality (VR), Augmented Reality (AR), and Brain Machine Interface (BMI) are systematically processed and analyzed using the proposed methodology. Based on the discovered topics of the IIT patents, patent classification (IPC/CPC) predictions are analyzed to validate the superior ETG results.}
}
@article{FENG2019309,
title = {Computer vision algorithms and hardware implementations: A survey},
journal = {Integration},
volume = {69},
pages = {309-320},
year = {2019},
issn = {0167-9260},
doi = {https://doi.org/10.1016/j.vlsi.2019.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167926019301762},
author = {Xin Feng and Youni Jiang and Xuejiao Yang and Ming Du and Xin Li},
keywords = {Computer vision, Hardware accelerator, Deep convolutional neural network, Artificial intelligence},
abstract = {The field of computer vision is experiencing a great-leap-forward development today. This paper aims at providing a comprehensive survey of the recent progress on computer vision algorithms and their corresponding hardware implementations. In particular, the prominent achievements in computer vision tasks such as image classification, object detection and image segmentation brought by deep learning techniques are highlighted. On the other hand, review of techniques for implementing and optimizing deep-learning-based computer vision algorithms on GPU, FPGA and other new generations of hardware accelerators are presented to facilitate real-time and/or energy-efficient operations. Finally, several promising directions for future research are presented to motivate further development in the field.}
}
@article{BERTHET201844,
title = {Organizing collective innovation in support of sustainable agro-ecosystems: The role of network management},
journal = {Agricultural Systems},
volume = {165},
pages = {44-54},
year = {2018},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2018.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X17310144},
author = {Elsa T. Berthet and Gordon M. Hickey},
keywords = {Collective action, Network governance, Design reasoning, Sustainability, Innovation brokering},
abstract = {Designing and managing sustainable agro-ecosystems remains a significant challenge for society. This is largely because their expected functions and values are multiple, and diverse networks of actors and institutions control common pool resources at different scales. Networks are expected to play an important role in facilitating collective innovation in agro-ecosystems, through enabling knowledge acquisition and transfer, resource mobilization for effective governance, and cooperation. However, in order to realize their potential benefit networks require effective management. Drawing on case studies located in the peri-urban agro-ecosystems surrounding Montreal (Quebec, Canada) and Paris (France), we analyze four collective innovation initiatives aiming to reduce the negative impacts of agriculture on the environment. For each case, we assess the contribution of network managers to the core tasks of: “Connecting” (initiating and facilitating interaction processes between actors), “Framing” (guiding interactions through process agreement), “Knowledge brokering” (facilitating knowledge transfer and capitalization) and “Exploring” (searching for goal congruency by creating new content). We then pay particular attention to the activities associated with Exploring across our cases and consider the implications for more collective approaches to designing innovation in agricultural landscapes. Our results suggest that, despite heterogeneity in the activities of network managers in each context, network managers devoted efforts across each of the four tasks. Yet, building a shared vision and engaging diverse stakeholders in a common goal over time were reported as challenging. We identify that the network managers tended to set objectives at the outset, and that design processes were often confined to a limited subgroup of actors. While these strategies were viewed as being efficient in the short term, they likely limited the success of the collective enterprise in the long run.}
}
@article{2020I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {13},
number = {3},
pages = {I-CCLVIII},
year = {2020},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(20)30146-7},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X20301467}
}
@article{SPENCER2019199,
title = {Advances in Computer Vision-Based Civil Infrastructure Inspection and Monitoring},
journal = {Engineering},
volume = {5},
number = {2},
pages = {199-222},
year = {2019},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2018.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S2095809918308130},
author = {Billie F. Spencer and Vedhus Hoskere and Yasutaka Narazaki},
keywords = {Structural inspection and monitoring, Artificial intelligence, Computer vision, Machine learning, Optical flow},
abstract = {Computer vision techniques, in conjunction with acquisition through remote cameras and unmanned aerial vehicles (UAVs), offer promising non-contact solutions to civil infrastructure condition assessment. The ultimate goal of such a system is to automatically and robustly convert the image or video data into actionable information. This paper provides an overview of recent advances in computer vision techniques as they apply to the problem of civil infrastructure condition assessment. In particular, relevant research in the fields of computer vision, machine learning, and structural engineering is presented. The work reviewed is classified into two types: inspection applications and monitoring applications. The inspection applications reviewed include identifying context such as structural components, characterizing local and global visible damage, and detecting changes from a reference image. The monitoring applications discussed include static measurement of strain and displacement, as well as dynamic measurement of displacement for modal analysis. Subsequently, some of the key challenges that persist toward the goal of automated vision-based civil infrastructure and monitoring are presented. The paper concludes with ongoing work aimed at addressing some of these stated challenges.}
}
@article{NAMOANO2020336,
title = {Change detection in streaming data analytics: A comparison of Bayesian online and martingale approaches},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {336-341},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.054},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320302044},
author = {Bernadin Namoano and Christos Emmanouilidis and Cristobal Ruiz-Carcel and Andrew G Starr},
keywords = {streaming analytics, change detection, martingale, Bayesian online detection},
abstract = {On line change detection is a key activity in streaming analytics, which aims to determine whether the current observation in a time series marks a change point in some important characteristic of the data, given the sequence of data observed so far. It can be a challenging task when monitoring complex systems, which are generating streaming data of significant volume and velocity. While applicable to diverse problem domains, it is highly relevant to monitoring high value and critical engineering assets. This paper presents an empirical evaluation of two algorithmic approaches for streaming data change detection. These are a modified martingale and a Bayesian online detection algorithm. Results obtained with both synthetic and real world data sets are presented and relevant advantages and limitations are discussed.}
}
@article{LIU2020100939,
title = {RT-Trust: Automated refactoring for different trusted execution environments under real-time constraints},
journal = {Journal of Computer Languages},
volume = {56},
pages = {100939},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2019.100939},
url = {https://www.sciencedirect.com/science/article/pii/S2590118419300644},
author = {Yin Liu and Kijin An and Eli Tilevich},
keywords = {Trusted execution, Real-time systems, Declarative meta-programming, Software refactoring, Program analyses},
abstract = {Real-time systems must meet strict timeliness requirements. These systems also often need to protect their critical program information (CPI) from adversarial interference and intellectual property theft. Trusted execution environments (TEE) execute CPI tasks on a special-purpose processor, thus providing hardware protection. However, adapting a system written to execute in environments without TEE requires partitioning the code into untrusted and trusted parts. This process involves complex manual program transformations that are not only laborious and intellectually tiresome, but also hard to validate and verify adherence to real-time constraints. To address these problems, this paper presents novel program analyses and transformation techniques, accessible to the developer via a declarative meta-programming model. The developer declaratively specifies the CPI portion of the system. A custom static analysis checks CPI specifications for validity, while probe-based profiling helps identify whether the transformed system would continue to meet the original real-time constraints, with a feedback loop suggesting how to modify the code, so its CPI can be isolated. Finally, an automated refactoring isolates the CPI portion for TEE-based execution, communicated with through generated calls to the TEE API. The reference implementation of our approach profiles and transforms real-time systems to isolate their CPI functions to execute on two different TEE platforms: OP-TEE and SGX. Although these platforms substantially differ in terms of their respective APIs and performance characteristics, our refactoring completely hides these differences from the developer by automatically synthesizing the correct CPI functionality required for these dissimilar TEE implementations. We have evaluated our approach by successfully enabling the trusted execution of the CPI portions of several microbenchmarks and a drone autopilot. Our approach shows the promise of declarative meta-programming in reducing the programmer effort required to adapt systems for trusted execution under real-time constraints.}
}
@article{WANG2019221,
title = {Making sense of blockchain technology: How will it transform supply chains?},
journal = {International Journal of Production Economics},
volume = {211},
pages = {221-236},
year = {2019},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0925527319300507},
author = {Yingli Wang and Meita Singgih and Jingyao Wang and Mihaela Rit},
keywords = {Blockchains, Distributed ledger, Sensemaking, Cognitive mapping, Supply chain, Exploratory study, Expert interview},
abstract = {This research uses sensemaking theory to explore how emerging blockchain technology may transform supply chains. We investigate three research questions (RQs): What are blockchain technology's perceived benefits to supply chains, where are disruptions mostly likely to occur and what are the potential challenges to further blockchain diffusion? We conducted in-depth interviews with 14 supply chain experts. Cognitive mapping and narrative analysis were deployed as the two main data analysis techniques to aid our understanding and evaluation of people's cognitive complexity in making sense of blockchain technology. We found that individual experts developed different cognitive structures within their own sensemaking processes. After merging individual cognitive maps into a strategic map, we identified several themes and central concepts that then allowed us to explore potential answers to the three RQs. Our study is among the very few to date to explicitly explore how blockchains may transform supply chain practices. Using the sensemaking approach afforded a deeper understanding of how senior executives diagnose the symptoms evident from blockchains and develop assumptions, expectations and knowledge of the technology, which will then shape their future actions regarding its utilisation. We demonstrate the usefulness of sensemaking theory as an alternative lens in investigating contemporary supply chain phenomena such as blockchains. Bringing sensemaking theory to this discipline in particular enriches emerging behavioural operations research. Our contributions also lie in extending the theories of prospective sensemaking and adding further insights to the stream of technology adoption studies.}
}
@article{WILLIAMS201832,
title = {Assembling the water factory: Seawater desalination and the techno-politics of water privatisation in the San Diego–Tijuana metropolitan region},
journal = {Geoforum},
volume = {93},
pages = {32-39},
year = {2018},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2018.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0016718518301313},
author = {Joe Williams},
keywords = {Assemblage, Urban political ecology, Infrastructure, Neoliberalism, Water, Desalination},
abstract = {This paper is about the peculiar particularities of the dual trends towards urban water privatization and commodification. It uses as its analytical entry point the extraordinary emergence of large-scale seawater desalination, delivered through public-private partnerships, as an alternative municipal water supply for the San Diego–Tijuana metropolitan region. The paper engages and extends Karen Bakker’s work on water as an ‘uncooperative commodity’. Interrogating the neoliberalization of water through desalination, it is argued, requires reference to the socio-technical relations drawn together under the ‘desalination assemblage’. Such water treatment technologies –and the social relations that flow through them– are, in other words, efficacious in the market-disciplining of water. The paper presents an understanding of privatization and commodification as diffuse, and as unfolding through multiple and contradictory materially heterogeneous relationships. Drawing on both urban political ecology (UPE) and assemblage thinking, the paper calls for a more constructive dialogue between different concepts of socio-material relationality. The empirical case studies of two large seawater desalination plants (one in Southern California, one in Baja California) and the re-configuring relations of public/private water governance associated with these projects, provides a pertinent imperative for greater attention to be paid to contingency and heterogeneity in our understanding of the ecology of capitalism.}
}
@article{MOUZAS2018195,
title = {The mediating role of consent in business marketing},
journal = {Industrial Marketing Management},
volume = {74},
pages = {195-204},
year = {2018},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2018.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0019850117304406},
author = {Stefanos Mouzas and David Ford},
abstract = {The study deepens our understanding of business marketing by looking beyond the individual choices of business actors to the role of consent between interacting actors. Based on an empirical investigation of manufacturers and retailers in Germany and drawing from previous research on business relationships, the paper develops a theoretical structure for the analysis of consent in business marketing. The paper argues for a shift from a view of individual choice as the basis of business marketing towards the idea of choice being part of an evolutionary discursive practice of consent. The study detects the mediating role of consent at four levels: 1) as a stratifying process, 2) as recursive practice, 3) as energizing interaction, and 4) as economizing activities, resources and actors; it elaborates significant theoretical implications and highlights managerial lessons.}
}
@article{RINGBERG2019102,
title = {The technology-mindset interactions: Leading to incremental, radical or revolutionary innovations},
journal = {Industrial Marketing Management},
volume = {79},
pages = {102-113},
year = {2019},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2018.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118304218},
author = {Torsten Ringberg and Markus Reihlen and Pernille Rydén},
keywords = {Innovation, Technological determinism, Managerial mindset determinism, Technology-mindset matrix, B2B marketing},
abstract = {Innovation is an integral part of the major transformation in modern business. Modern managers are increasingly pushed from both in- and outside the organization to innovate their processes including products and services. Research typically investigates innovative processes from either a technology perspective or managerial mindset perspective, but rarely both. We argue that technology and mindset should be analyzed in combination, as they are fundamentally co-constitutive albeit with different levels of interaction. We categorize the levels of interaction in a two-by-two model, with the Y-axis representing levels of innovative technology and the X-axis representing levels of innovative mindset. This categorization leads to a theoretical framework, a Technology-Mindset Matrix that consists of four typical technology-mindset interactions. We show how each type leads to unique innovative outcomes, and label the four types; incremental innovation, radical technological innovation, radical mindset innovation, and revolutionary innovation. We illustrate each square with case examples. Furthermore, we discuss core B2B issues managers face when transforming their organizations by moving up from incremental to higher ranked modes of innovation.}
}
@article{ALINAGHIAN2020110,
title = {A relational embeddedness perspective on dynamic capabilities: A grounded investigation of buyer-supplier routines},
journal = {Industrial Marketing Management},
volume = {85},
pages = {110-125},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0019850119301099},
author = {Leila Alinaghian and Yusoon Kim and Jagjit Srai},
keywords = {Buyer-supplier relationship, Dynamic capabilities, Inter-firm routines, Relational embeddedness, Case studies},
abstract = {Our study extends the emerging inter-firm-level theorization of dynamic capabilities by articulating how firms can develop and adapt their resource bases through supplier relations. Specifically, we aim to explore how different embedded relational aspects function together or separately to induce various inter-firm routines that presumably underpin the buying firm’s dynamic capabilities. The research design is a multiple case study involving 34 buyer-supplier dyad-level innovation events across six product groups of three multinational buying firms in the Pharmaceuticals, Aerospace, and Fast-Moving Consumer Goods sectors. Our inductive analysis suggests that the social, cognitive, and physical aspects of relational embeddedness play roles, in a cumulatively sequential fashion, in inducing three distinctive routine types—unilateral, quasi-unilateral, and bilateral—in the buyer-supplier dyads that underpin the three clusters of dynamic capabilities—sensing, seizing, and transforming, respectively. Furthermore, our study identifies two contingencies that explain variances in the observations and inferences. We therefore investigate the ‘black box’ of dynamic capabilities in inter-firm contexts, elucidating the roles and association of relational embeddedness and patterned activities (routines) in these relationships.}
}
@article{MOUSTAFA201933,
title = {A holistic review of Network Anomaly Detection Systems: A comprehensive survey},
journal = {Journal of Network and Computer Applications},
volume = {128},
pages = {33-55},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518303886},
author = {Nour Moustafa and Jiankun Hu and Jill Slay},
keywords = {Intrusion Detection System (IDS), Network Anomaly Detection Systems (NADS), Data pre-processing, Decision Engine (DE)},
abstract = {Network Anomaly Detection Systems (NADSs) are gaining a more important role in most network defense systems for detecting and preventing potential threats. The paper discusses various aspects of anomaly-based Network Intrusion Detection Systems (NIDSs). The paper explains cyber kill chain models and cyber-attacks that compromise network systems. Moreover, the paper describes various Decision Engine (DE) approaches, including new ensemble learning and deep learning approaches. The paper also provides more details about benchmark datasets for training and validating DE approaches. Most of NADSs’ applications, such as Data Centers, Internet of Things (IoT), as well as Fog and Cloud Computing, are also discussed. Finally, we present several experimental explanations which we follow by revealing various promising research directions.}
}
@article{GAO2020668,
title = {Big data analytics for smart factories of the future},
journal = {CIRP Annals},
volume = {69},
number = {2},
pages = {668-692},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620301359},
author = {Robert X. Gao and Lihui Wang and Moneer Helu and Roberto Teti},
keywords = {Digital manufacturing system, Information, Learning},
abstract = {Continued advancement of sensors has led to an ever-increasing amount of data of various physical nature to be acquired from production lines. As rich information relevant to the machines and processes are embedded within these “big data”, how to effectively and efficiently discover patterns in the big data to enhance productivity and economy has become both a challenge and an opportunity. This paper discusses essential elements of and promising solutions enabled by data science that are critical to processing data of high volume, velocity, variety, and low veracity, towards the creation of added-value in smart factories of the future.}
}
@incollection{TABARESSOTO2020259,
title = {12 - Digital media steganalysis},
editor = {Mahmoud Hassaballah},
booktitle = {Digital Media Steganography},
publisher = {Academic Press},
pages = {259-293},
year = {2020},
isbn = {978-0-12-819438-6},
doi = {https://doi.org/10.1016/B978-0-12-819438-6.00020-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194386000207},
author = {Reinel Tabares-Soto and Raúl Ramos-Pollán and Gustavo Isaza and Simon Orozco-Arias and Mario Alejandro Bravo Ortíz and Harold Brayan {Arteaga Arteaga} and Alejandro Mora Rubio and Jesus Alejandro {Alzate Grisales}},
keywords = {Deep learning, Machine learning, Steganalysis, Steganography, Digital media, Convolutional neural network},
abstract = {Steganography is the process of hiding messages inside an object known as a carrier. The idea is establishing a covert communication channel where messages go unnoticed by observers having access to that channel. Steganalysis is dedicated to the detection of such hidden messages; these messages can be embedded in several different types of digital media, such as images, video, audio, plain text files, and covert channels. Traditional steganalysis schemes are divided into two stages; the first one involves manual extraction of high-end features, and the second one is classification using machine learning (ML) methods. Nowadays, deep learning (DL) can unify both traditional stages into an end-to-end scheme with promising results. In this chapter, we show the most relevant steganalysis techniques using statistical, ML, and DL methods on digital media.}
}
@article{CABARROIHERNANDEZ201995,
title = {The Ganoderma weberianum-resinaceum lineage: multilocus phylogenetic analysis and morphology confirm G. mexicanum and G. parvulum in the Neotropics},
journal = {Mycokeys},
volume = {59},
pages = {95-131},
year = {2019},
issn = {1314-4057},
doi = {https://doi.org/10.3897/mycokeys.59.33182},
url = {https://www.sciencedirect.com/science/article/pii/S1314405719000831},
author = {Milay Cabarroi-Hernández and Alma Rosa Villalobos-Arámbula and  {Mabel Gisela Torres-Torres} and Cony Decock and Laura Guzmán-Dávalos},
keywords = {Caribbean, Chlamydospores,   ,   Ganodermataceae  , Paleotropics, South America},
abstract = {Many species of Ganoderma exhibit a high phenotypic plasticity. Hence, particularly among them, the morphological species concept remains difficult to apply, resulting in a currently confused taxonomy; as a consequence, the geographical distribution range of many species also remains very uncertain. One of the areas with a strong uncertainty, as far as morphological species concept is concerned, is the Neotropics. It is common that names of species described from other regions, mainly from northern temperate areas, have been applied to Neotropical species. The aim of the present study was to determine which species might lay behind the G. weberianum complex in the Neotropics, using morphological studies and phylogenetic inferences based on both single (ITS) and multilocus (ITS, rpb2, and tef1-α) sequences. The results indicated that G. weberianumsensu Steyaert, which is the usually accepted concept for this taxon, was absent from the Neotropics. In this area, G. weberianumsensu Steyaert encompassed at least two phylogenetic species, which are tentatively, for the time being, identified as belonging to G. mexicanum and G. parvulum. These two species could be distinguished morphologically, notably by the ornamentation or its absence on their chlamydospores. The results also showed that additional species from the Neotropics might still exist, including, e.g., G. perzonatum, but their circumscription remains uncertain until now because of the paucity of material available. Furthermore, it was found that the current concept of G. resinaceum embraced a complex of species.}
}
@article{VONBRIEL2018278,
title = {Not all digital venture ideas are created equal: Implications for venture creation processes},
journal = {The Journal of Strategic Information Systems},
volume = {27},
number = {4},
pages = {278-295},
year = {2018},
note = {Generating Business and Social Value from Digital Entrepreneurship and Innovation},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0963868717301002},
author = {Frederik {von Briel} and Jan Recker and Per Davidsson},
keywords = {Digital entrepreneurship, Digital ventures, New venture ideas, Venture creation process, Hardware ventures, Software ventures},
abstract = {Digital ventures are formed around ideas that have digital artifacts at their core. We develop theory that explains how the composition of digital artifacts influences venture creation processes. First, we develop propositions that link differences in the embodiment and coupling of digital artifact components to tensions in venture creation process inputs, behaviors, and outputs. Second, we link compositional differences in digital artifacts to differences in venture creation process initiation, duration, and outcome. Our theorizing establishes a foundation for future research on digital artifacts within and beyond entrepreneurship contexts, and for future research on entrepreneurship within and beyond digital artifact contexts.}
}
@article{JOHNSON2019147,
title = {Treatment-related changes in neural activation vary according to treatment response and extent of spared tissue in patients with chronic aphasia},
journal = {Cortex},
volume = {121},
pages = {147-168},
year = {2019},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2019.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0010945219303053},
author = {Jeffrey P. Johnson and Erin L. Meier and Yue Pan and Swathi Kiran},
keywords = {Aphasia, Language rehabilitation, Picture naming, fMRI, Region of interest (ROI) analysis},
abstract = {Neuroimaging studies of aphasia recovery have linked treatment-related improvements in language processing to changes in functional brain activation in left hemisphere language regions and their right hemisphere homologues. Although there is some consensus that better behavioral outcomes are achieved when activation is restored to the left hemisphere, the circumstances that dictate how and why regions in both hemispheres respond to naming therapy are still unclear. In this study, an fMRI picture-naming task was used to examine 16 regions of interest in 26 patients with chronic aphasia before and after 12 weeks of semantic naming treatment. Ten control patients who did not receive treatment and 17 healthy controls were also scanned. Naming therapy resulted in a significant increase in cortical activation, an effect that was largely driven by patients who responded most favorably to treatment, as patients who responded less favorably (as well as those who did not receive treatment) had little change in activation over time. Relative to healthy controls, patients had higher pre-treatment activation in the bilateral inferior frontal gyri (IFG) and lower activation in the bilateral angular gyri; after treatment, they had higher activation in bilateral IFG, as well as in the right middle frontal gyrus. These results suggest that the predominant effect of beneficial naming treatment was an upregulation of traditional language areas and their right hemisphere homologues and, in particular, regions associated with phonological and semantic/executive semantic processing, as well as broader domain general functions. Additionally, in some left hemisphere regions, post-treatment changes in activation were greater when there was more damage than when there was less damage, indicating that spared tissue in otherwise highly damaged regions can be modulated by treatment.}
}
@article{2019I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {12},
number = {8, Part 1},
pages = {I-CXCVII},
year = {2019},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(19)30664-3},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X19306643}
}
@incollection{CLARK2019245,
title = {Chapter 7 - Finance, Economics, and Sustainability},
editor = {Woodrow W. Clark},
booktitle = {Climate Preservation in Urban Communities Case Studies},
publisher = {Butterworth-Heinemann},
pages = {245-289},
year = {2019},
isbn = {978-0-12-815920-0},
doi = {https://doi.org/10.1016/B978-0-12-815920-0.00007-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128159200000071},
author = {Woodrow W. Clark and Rich Gibson and James Barth and Danilo Bonato}
}
@article{BARBU202099,
title = {Deep video-to-video transformations for accessibility with an application to photosensitivity},
journal = {Pattern Recognition Letters},
volume = {137},
pages = {99-107},
year = {2020},
note = {Learning and Recognition for Assistive Computer Vision},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2019.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167865519300133},
author = {Andrei Barbu and Dalitso Banda and Boris Katz},
keywords = {Photosensitivity, Accessibility, Computer vision, Video-to-video transformation},
abstract = {We demonstrate how to construct a new class of visual assistive technologies that, rather than extract symbolic information, learn to transform the visual environment to make it more accessible. We do so without engineering which transformations are useful allowing for arbitrary modifications of the visual input. As an instantiation of this idea we tackle a problem that affects and hurts millions worldwide: photosensitivity. Any time an affected person opens a website, video, or some other medium that contains an adverse visual stimulus, either intended or unintended, they might experience a seizure with potentially significant consequences. We show how a deep network can learn a video-to-video transformation rendering such stimuli harmless while otherwise preserving the video. This approach uses a specification of the adverse phenomena, the forward transformation, to learn the inverse transformation. We show how such a network generalizes to real-world videos that have triggered numerous seizures, both by mistake and in politically-motivated attacks. A number of complimentary approaches are demonstrated including using a hand-crafted generator and a GAN using a differentiable perceptual metric. Such technology can be deployed offline to protect videos before they are shown or online with assistive glasses or real-time post processing. Other applications of this general technique include helping those with limited vision, attention deficit hyperactivity disorder, and autism.}
}
@article{MONIZ20191,
title = {A review on web content popularity prediction: Issues and open challenges},
journal = {Online Social Networks and Media},
volume = {12},
pages = {1-20},
year = {2019},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2468696418300971},
author = {Nuno Moniz and Luís Torgo},
keywords = {Web content, Predictive modelling, Social media, Popularity, Evaluation},
abstract = {With the profusion of web content, researchers have avidly studied and proposed new approaches to enable the anticipation of its impact on social media, presenting many distinct approaches throughout the last decade. Diverse approaches have been presented to tackle the problem of web content popularity prediction, including standard classification and regression approaches. Furthermore, these approaches have also taken into consideration distinct scenarios of data availability, where one may target the prediction of popularity before or after the publication of the items, which is highly interesting for different objectives from a user standpoint. This work aims at reviewing previous work and discussing open issues and challenges that could foster impactful research on this topic. Five areas are identified that require further research, covering the full spectrum of the problem: social media data, the learning task, recommendation and evaluation.}
}
@article{202045,
title = {Proceedings to the 58th Annual Conference of the Particle Therapy Cooperative Group (PTCOG58)},
journal = {International Journal of Particle Therapy},
volume = {6},
number = {4},
pages = {45-491},
year = {2020},
issn = {2331-5180},
doi = {https://doi.org/10.14338/IJPT.19-PTCOG-6.4},
url = {https://www.sciencedirect.com/science/article/pii/S2331518023001518}
}
@article{BURGUERA202014656,
title = {Visual Loop Detection in Underwater Robotics: an Unsupervised Deep Learning Approach⁎⁎This work is partially supported by Ministry of Economy and Competitiveness under contracts DPI2017-86372-C3-3-R (AEI,FEDER,UE) and TIN2014-58662-R (AEI,FEDER,UE).},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {14656-14661},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1476},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320318899},
author = {Antoni Burguera and Francisco Bonin-Font},
keywords = {Robot vision, underwater robotics, neural networks, loop detection, SLAM},
abstract = {This paper presents a novel Deep Neural Network aimed at fast and robust visual loop detection targeted to underwater images. In order to help the proposed network to learn the features that define loop closings, a global image descriptor built upon clusters of local SIFT descriptors is proposed. Also, a method allowing unsupervised training is presented, eliminating the need for a hand-labelled ground truth. Once trained, the Neural Network builds two descriptors of an image that can be easily compared to other image descriptors to ascertain if they close a loop or not. The experimental results, performed using real data gathered in coastal areas of Mallorca (Spain), show the validity of our proposal and favourably compares it to previously existing methods.}
}
@article{DWIVEDI2021101994,
title = {Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy},
journal = {International Journal of Information Management},
volume = {57},
pages = {101994},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S026840121930917X},
author = {Yogesh K. Dwivedi and Laurie Hughes and Elvira Ismagilova and Gert Aarts and Crispin Coombs and Tom Crick and Yanqing Duan and Rohita Dwivedi and John Edwards and Aled Eirug and Vassilis Galanos and P. Vigneswara Ilavarasan and Marijn Janssen and Paul Jones and Arpan Kumar Kar and Hatice Kizgin and Bianca Kronemann and Banita Lal and Biagio Lucini and Rony Medaglia and Kenneth {Le Meunier-FitzHugh} and Leslie Caroline {Le Meunier-FitzHugh} and Santosh Misra and Emmanuel Mogaji and Sujeet Kumar Sharma and Jang Bahadur Singh and Vishnupriya Raghavan and Ramakrishnan Raman and Nripendra P. Rana and Spyridon Samothrakis and Jak Spencer and Kuttimani Tamilmani and Annie Tubadji and Paul Walton and Michael D. Williams},
keywords = {Artificial intelligence, AI, Cognitive computing, Expert systems, Machine learning, Research agenda},
abstract = {As far back as the industrial revolution, significant development in technical innovation has succeeded in transforming numerous manual tasks and processes that had been in existence for decades where humans had reached the limits of physical capacity. Artificial Intelligence (AI) offers this same transformative potential for the augmentation and potential replacement of human tasks and activities within a wide range of industrial, intellectual and social applications. The pace of change for this new AI technological age is staggering, with new breakthroughs in algorithmic machine learning and autonomous decision-making, engendering new opportunities for continued innovation. The impact of AI could be significant, with industries ranging from: finance, healthcare, manufacturing, retail, supply chain, logistics and utilities, all potentially disrupted by the onset of AI technologies. The study brings together the collective insight from a number of leading expert contributors to highlight the significant opportunities, realistic assessment of impact, challenges and potential research agenda posed by the rapid emergence of AI within a number of domains: business and management, government, public sector, and science and technology. This research offers significant and timely insight to AI technology and its impact on the future of industry and society in general, whilst recognising the societal and industrial influence on pace and direction of AI development.}
}
@article{TANG2018120,
title = {Canadian Association of Radiologists White Paper on Artificial Intelligence in Radiology},
journal = {Canadian Association of Radiologists Journal},
volume = {69},
number = {2},
pages = {120-135},
year = {2018},
issn = {0846-5371},
doi = {https://doi.org/10.1016/j.carj.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0846537118300305},
author = {An Tang and Roger Tam and Alexandre Cadrin-Chênevert and Will Guest and Jaron Chong and Joseph Barfett and Leonid Chepelev and Robyn Cairns and J. Ross Mitchell and Mark D. Cicero and Manuel Gaudreau Poudrette and Jacob L. Jaremko and Caroline Reinhold and Benoit Gallix and Bruce Gray and Raym Geis and Timothy O'Connell and Paul Babyn and David Koff and Darren Ferguson and Sheldon Derkatch and Alexander Bilbily and Wael Shabana},
keywords = {Artificial intelligence, Machine learning, Deep learning, Radiology, Imaging, Medicine, Healthcare, Quality improvement},
abstract = {Artificial intelligence (AI) is rapidly moving from an experimental phase to an implementation phase in many fields, including medicine. The combination of improved availability of large datasets, increasing computing power, and advances in learning algorithms has created major performance breakthroughs in the development of AI applications. In the last 5 years, AI techniques known as deep learning have delivered rapidly improving performance in image recognition, caption generation, and speech recognition. Radiology, in particular, is a prime candidate for early adoption of these techniques. It is anticipated that the implementation of AI in radiology over the next decade will significantly improve the quality, value, and depth of radiology's contribution to patient care and population health, and will revolutionize radiologists' workflows. The Canadian Association of Radiologists (CAR) is the national voice of radiology committed to promoting the highest standards in patient-centered imaging, lifelong learning, and research. The CAR has created an AI working group with the mandate to discuss and deliberate on practice, policy, and patient care issues related to the introduction and implementation of AI in imaging. This white paper provides recommendations for the CAR derived from deliberations between members of the AI working group. This white paper on AI in radiology will inform CAR members and policymakers on key terminology, educational needs of members, research and development, partnerships, potential clinical applications, implementation, structure and governance, role of radiologists, and potential impact of AI on radiology in Canada.
Résumé
L'intelligence artificielle progresse rapidement de la phase expérimentale à la phase de mise en œuvre dans de nombreux domaines, notamment la médecine. L'accès à de grands ensembles de données, la puissance croissante des ordinateurs et les avancées en matière d'algorithmes d'apprentissage ont permis de faire des pas de géant au chapitre du développement des applications d'intelligence artificielle. Au cours des cinq dernières années, des techniques comme l'apprentissage profond ont permis d'améliorer rapidement les capacités de reconnaissance d'images, de production de légendes d'images et de reconnaissance vocale. La radiologie est un domaine tout indiqué pour l'adoption précoce de ces techniques. L'intégration d'applications d'intelligence artificielle en radiologie au cours de la prochaine décennie devrait grandement améliorer la qualité, la valeur et la portée de la contribution de la radiologie aux soins des patients et à la santé de la population, en plus de révolutionner le travail des radiologistes. En sa qualité de porte-parole de la profession au Canada, l’Association canadienne des radiologistes (CAR) défend des normes de pratique élevées en imagerie centrée sur les patients, en apprentissage continu et en recherche. La CAR a mis sur pied un groupe de travail sur l'intelligence artificielle qui a pour mandat de discuter des enjeux liés à la pratique, aux politiques et à la prestation de soins relativement à l'introduction et à la mise en œuvre d'outils d'intelligence artificielle en radiologie. Le présent livre blanc formule à l'intention de la CAR des recommandations issues des délibérations des membres du groupe de travail. Il renseigne les membres de la CAR et les responsables de l’élaboration des politiques sur la terminologie à employer, les besoins en matière de formation, la recherche-développement, les partenariats, les applications cliniques potentielles, la mise en œuvre, la structure et la gouvernance, le rôle des radiologistes et sur les retombées potentielles de l'intelligence artificielle en radiologie au Canada.}
}
@article{XIE2020255,
title = {PolSAR image classification via a novel semi-supervised recurrent complex-valued convolution neural network},
journal = {Neurocomputing},
volume = {388},
pages = {255-268},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220300497},
author = {Wen Xie and Gaini Ma and Feng Zhao and Hanqiang Liu and Lu Zhang},
keywords = {PolSAR terrain classification, Network overfitting, RCV-CNN1, RCV-CNN2},
abstract = {Due to that polarimetric synthetic aperture radar (PolSAR) data suffers from missing labeled samples and complex-valued data, this article presents a novel semi-supervised PolSAR terrain classification method named recurrent complex-valued convolution neural network (RCV-CNN) which combines semi-supervised learning and complex-valued convolution neural network (CV-CNN). The proposed method only needs a small number of labeled samples to achieve good classification results. First, a Wishart classifier is used to select some reliable PolSAR samples. Then, two new semi-supervised deep classification model RCV-CNN1 and RCV-CNN2 have been proposed to improve PolSAR image classification accuracy. Moreover, our proposed methods could solve the problem of network overfitting phenomenon to some extend when the number of training samples is too small. Finally, three real PolSAR dataset are applied to verify the effectiveness of our algorithms. Compared with the other five state-of-the-art methods, the proposed RCV-CNN1 and RCV-CNN2 classification models show good performance in accuracy and generalization.}
}
@article{CHOI2019119737,
title = {Technology opportunity discovery under the dynamic change of focus technology fields: Application of sequential pattern mining to patent classifications},
journal = {Technological Forecasting and Social Change},
volume = {148},
pages = {119737},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2019.119737},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518312745},
author = {Jaewoong Choi and Byeongki Jeong and Janghyeok Yoon},
keywords = {Technology opportunity discovery, Sequential pattern mining, Uncertainty, Technology intelligence},
abstract = {Technology opportunity discovery (TOD), have evolved over time from technology forecasting to an approach based on existing technology capabilities for increased practicality. Unfortunately, TOD studies are still lacking, in that they do not consider the unique direction of the target firm in technology development or the recent trends of technological fields. Consequently, this paper proposes an improved methodology for identifying technology opportunities with less uncertainty by focusing on the target firm's dynamic change of focus technology fields. The proposed approach is as follows: 1) generate a sequence database, containing firms' dynamic change of focus technology fields; 2) explore the frequent sequential patterns from a precedence enterprise (PE) sequence database using the PrefixSpan algorithm to identify the technology candidates from a PE sequence similar with that of the target firm; and 3) evaluate the candidates on technological similarity, business stability, and recency. The results of the proposed approach are expected to help firms discover appropriate technology opportunities by considering both their existing technological capacities and the dynamic change of their focus technology fields. Furthermore, the proposed approach can identify the most appropriate technology opportunities with less uncertainty in real-life business environments by evaluating technological similarity, business stability, and recency.}
}
@incollection{SALHI2019199,
title = {Chapter 8 - Multimodal Localization for Embedded Systems: A Survey},
editor = {Michael Ying Yang and Bodo Rosenhahn and Vittorio Murino},
booktitle = {Multimodal Scene Understanding},
publisher = {Academic Press},
pages = {199-278},
year = {2019},
isbn = {978-0-12-817358-9},
doi = {https://doi.org/10.1016/B978-0-12-817358-9.00014-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128173589000147},
author = {Imane Salhi and Martyna Poreba and Erwan Piriou and Valerie Gouet-Brunet and Maroun Ojail},
keywords = {Localization, Embedded system, Multimodal information, Data fusion, Adequacy algorithm architecture},
abstract = {Localization by jointly exploiting multimodal information, like cameras, inertial measurement units (IMU), and global navigation satellite system (GNSS) data, is an active key research topic for autonomous embedded systems such as smart glasses or drones. These systems have become topical for acquisition, modeling, and interpretation for scene understanding. The exploitation of different sensor types improves the robustness of the localization, e.g. by merging the accuracy of one sensor with the reactivity of another one in a flexible manner. This chapter presents a survey of the existing multimodal techniques dedicated to the localization of autonomous embedded systems. Both the algorithmic and the hardware architecture sides are investigated in order to provide a global overview of the key elements to be considered when designing these embedded systems. Several applications in different domains (e.g. localization for mapping, pedestrian localization, automotive navigation and mixed reality) are presented to illustrate the importance of such systems nowadays in scene understanding.}
}
@article{20202,
title = {Abstracts},
journal = {Fuel and Energy Abstracts},
volume = {61},
number = {1},
pages = {2-103},
year = {2020},
issn = {0140-6701},
doi = {https://doi.org/10.1016/j.fueleneab.2019.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140670119300463}
}
@incollection{CABRERAPONCE201943,
title = {Chapter 3 - Genetic Modifications of Corn},
editor = {Sergio O. Serna-Saldivar},
booktitle = {Corn (Third Edition)},
publisher = {AACC International Press},
edition = {Third Edition},
address = {Oxford},
pages = {43-85},
year = {2019},
isbn = {978-0-12-811971-6},
doi = {https://doi.org/10.1016/B978-0-12-811971-6.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128119716000036},
author = {José Luis Cabrera-Ponce and Eliana Valencia-Lozano and Diana Lilia Trejo-Saavedra},
keywords = {Corn, Mutation, Transgenic plant, , Particle bombardment, Embryogenic calli, Insect resistance, Herbicide tolerance, Genome editing},
abstract = {Corn, Zea mays L., is one of the humankind's earliest innovations. The large productivity gains in corn production have come primarily from advanced plant breeding techniques and improved corn management. Plant breeding has gotten technologically savvy in the last century. Realizing that natural mutants often introduce valuable traits, scientists turned to chemicals and irradiation to speed the creation of mutants. Later, plant tissue culture evolved (1970s), then the use of molecular markers to identify interesting hereditary traits; during the 1980s, genetic engineering by means of making transgenic plants and, more recently, genome editing as a new tool to do more precise specific mutations for specific traits. Since 1996, corn products with biotechnological traits and associated agronomic practices have contributed to the steady increase in corn production by reducing pest and environmental stress on highly productive new corn genetics. The generation of transgenic plants is the crucial step in the development of new biotech trait products. In this chapter, we will review the importance of several methods to create mutants in corn; also, the technology of genetic transformation mainly by particle bombardment and Agrobacterium and the next generation of biotech products; genome editing corn.}
}
@article{BELLINI2018142,
title = {Managing cloud via Smart Cloud Engine and Knowledge Base},
journal = {Future Generation Computer Systems},
volume = {78},
pages = {142-154},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16303867},
author = {Pierfrancesco Bellini and Ivan Bruno and Daniele Cenni and Paolo Nesi},
keywords = {Knwoledge base, Smart cloud, Cloud computing, Service level agreement},
abstract = {Complexity of cloud infrastructures needs models and tools for process management, configuration, scaling, elastic computing and cloud resource health control. This paper presents a Smart Cloud Engine and solution based on a Knowledge Base, KB, with the aim of modeling cloud resources, Service Level Agreements and their evolutions, and enabling the reasoning on structures by implementing strategies of efficient smart cloud management and intelligence. The solution proposed provides formal verification and intelligence tools for cloud control. It can be easily integrated with a large range of cloud configuration manager, cloud orchestrator, and monitoring tools, since the connections with these tools are performed by using REST calls and XML files. The proposed solution has been validated in the context of large ICARO Cloud project and in the cloud facility of a national cloud service provider. Some data resulting from the validation phases have been reported and are referring to the dynamic management of real ECLAP social network http://www.eclap.eu.}
}
@article{DEKHTIAR2018227,
title = {Deep learning for big data applications in CAD and PLM – Research review, opportunities and case study},
journal = {Computers in Industry},
volume = {100},
pages = {227-243},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517305560},
author = {Jonathan Dekhtiar and Alexandre Durupt and Matthieu Bricogne and Benoit Eynard and Harvey Rowson and Dimitris Kiritsis},
keywords = {Deep learning, Machine learning, Computer vision, Product Lifecycle Management, Digital mock-up, Shape retrieval},
abstract = {With the increasing amount of available data, computing power and network speed for a decreasing cost, the manufacturing industry is facing an unprecedented amount of data to process, understand and exploit. Phenomena such as Big Data, the Internet-of-Things, Closed-Loop Product Lifecycle Management, and the advances of Smart Factories tend to produce humanly unmanageable quantities of data. The paper approaches the aforesaid context by assuming that any data processing automation is not only desirable but rather necessary in order to prevent prohibitive data analytics costs. This study focuses on highlighting the major specificities of engineering data and the data-processing difficulties which are inherent to data coming from the manufacturing industry. The artificial intelligence field of research is able to provide methods and tools to address some of the identified issues. A special attention was paid to provide a literature review of the most recent (in 2017) applications, that could present a high potential for the manufacturing industry, in the fields of machine learning and deep learning. In order to illustrate the proposed work, a case study was conducted on the challenging research question of object recognition in heterogeneous formats (3D models, photos and videos) with deep learning techniques. The DICE project – DMU Imagery Comparison Engine – is presented and has been completely open-sourced in order to encourage reuse and improvements of the proposed case-study. This project also leads to the development of an open-source research dataset of 2000 CAD Models, called DMU-Net available at: https://www.dmu-net.org.}
}