@article{PREIKSAITIS2024,
title = {The Role of Large Language Models in Transforming Emergency Medicine: Scoping Review},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/53787},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000516},
author = {Carl Preiksaitis and Nicholas Ashenburg and Gabrielle Bunney and Andrew Chu and Rana Kabeer and Fran Riley and Ryan Ribeira and Christian Rose},
keywords = {large language model, LLM, emergency medicine, clinical decision support, workflow efficiency, medical education, artificial intelligence, AI, natural language processing, NLP, AI literacy, ChatGPT, Bard, Pathways Language Model, Med-PaLM, Bidirectional Encoder Representations from Transformers, BERT, generative pretrained transformer, GPT, United States, US, China, scoping review, Preferred Reporting Items for Systematic Reviews and Meta-Analyses, PRISMA, decision support, workflow efficiency, risk, ethics, education, communication, medical training, physician, health literacy, emergency care},
abstract = {Background
Artificial intelligence (AI), more specifically large language models (LLMs), holds significant potential in revolutionizing emergency care delivery by optimizing clinical workflows and enhancing the quality of decision-making. Although enthusiasm for integrating LLMs into emergency medicine (EM) is growing, the existing literature is characterized by a disparate collection of individual studies, conceptual analyses, and preliminary implementations. Given these complexities and gaps in understanding, a cohesive framework is needed to comprehend the existing body of knowledge on the application of LLMs in EM.
Objective
Given the absence of a comprehensive framework for exploring the roles of LLMs in EM, this scoping review aims to systematically map the existing literature on LLMs’ potential applications within EM and identify directions for future research. Addressing this gap will allow for informed advancements in the field.
Methods
Using PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) criteria, we searched Ovid MEDLINE, Embase, Web of Science, and Google Scholar for papers published between January 2018 and August 2023 that discussed LLMs’ use in EM. We excluded other forms of AI. A total of 1994 unique titles and abstracts were screened, and each full-text paper was independently reviewed by 2 authors. Data were abstracted independently, and 5 authors performed a collaborative quantitative and qualitative synthesis of the data.
Results
A total of 43 papers were included. Studies were predominantly from 2022 to 2023 and conducted in the United States and China. We uncovered four major themes: (1) clinical decision-making and support was highlighted as a pivotal area, with LLMs playing a substantial role in enhancing patient care, notably through their application in real-time triage, allowing early recognition of patient urgency; (2) efficiency, workflow, and information management demonstrated the capacity of LLMs to significantly boost operational efficiency, particularly through the automation of patient record synthesis, which could reduce administrative burden and enhance patient-centric care; (3) risks, ethics, and transparency were identified as areas of concern, especially regarding the reliability of LLMs’ outputs, and specific studies highlighted the challenges of ensuring unbiased decision-making amidst potentially flawed training data sets, stressing the importance of thorough validation and ethical oversight; and (4) education and communication possibilities included LLMs’ capacity to enrich medical training, such as through using simulated patient interactions that enhance communication skills.
Conclusions
LLMs have the potential to fundamentally transform EM, enhancing clinical decision-making, optimizing workflows, and improving patient outcomes. This review sets the stage for future advancements by identifying key research areas: prospective validation of LLM applications, establishing standards for responsible use, understanding provider and patient perceptions, and improving physicians’ AI literacy. Effective integration of LLMs into EM will require collaborative efforts and thorough evaluation to ensure these technologies can be safely and effectively applied.}
}
@article{CAPATINA2024102723,
title = {Elevating students’ lives through immersive learning experiences in a safe metaverse},
journal = {International Journal of Information Management},
volume = {75},
pages = {102723},
year = {2024},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102723},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223001044},
author = {Alexandru Capatina and Nina Jane Patel and Kiril Mitrov and Dragos Sebastian Cristea and Adrian Micu and Angela-Eliza Micu},
keywords = {Safe Metaverse, Immersive education, Web3 technologies, Experiential learning},
abstract = {Metaverse platforms devoted to educational purposes are becoming increasingly popular among educators worldwide due to the benefits they offer in elevating students’ lives. This study explores how immersive platform features, blockchain features, a safe learning environment and new learning models contribute to the educational outcomes. Emerging literature on educational Metaverse platforms has been used to develop a genuine conceptual model. The model was tested using data collected from 100 educators located in different countries from worldwide. The data were analyzed using partial least squares structural equation modeling (PLS-SEM), Random Forest and other different feature importance methods, fuzzy-set qualitative comparative analysis (fsQCA), thematic content analysis and Latent Dirichlet Allocation (LDA). The application of PLS-SEM outlines that immersive platform features and safe learning environments positively affects educational outcomes. Random Forest and feature importance methods investigate the predictive power of items included in the model, fsQCA reveals the combinations of these factors that lead to high educational outcomes, while the qualitative insights reveal the key themes emerged from educators’ perspectives on the use of Metaverse platforms. This study contributes to Metaverse literature, which is relatively understudied in the educational sector.}
}
@article{SINGH2023103628,
title = {Deep learning-based biometric image feature extraction for securing medical images through data hiding and joint encryption–compression},
journal = {Journal of Information Security and Applications},
volume = {79},
pages = {103628},
year = {2023},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2023.103628},
url = {https://www.sciencedirect.com/science/article/pii/S2214212623002120},
author = {Monu Singh and Naman Baranwal and K.N. Singh and A.K. Singh and Huiyu Zhou},
keywords = {Biometric images, Encryption, Data hiding, Feature extraction, Compression, Security, Attacks},
abstract = {Images are promising information carriers when compared to other media documents in the healthcare domain. However, digital data transmission over unprotected wired or wireless networks poses a threat to the security of healthcare systems. As a result, the issue of copyright violation and identity theft can occur due to the unauthorised use of these data. This paper proposes a new secure method under a framework that embeds biometric fingerprint image features in a medical image without any perceptual distortion. This paper uses ResNet152 for biometric image feature extraction in the first stage and features to generate a secret key for embedding in the second stage. The method combines encryption and compression scheme based on a generated key, novel chaotic map and Huffman coding to enhance the security of medical images while reducing the storage consumption or bandwidth requirements if images are transmitted to remote servers. Experimental results show that the proposed method presents superior security with high imperceptibility and compression performance, ensuring its effectiveness as an image protection mechanism for medical applications. Extensive experimental results show that the proposed method achieves an average peak signal-to-noise ratio (PSNR) that is above 54 dB, a structural similarity index measure (SSIM) close to 1, a bit error rate (BER) of 0 and a normalised correlation (NC) of 1. Moreover, this method compresses the images up to 70% when tested on three standard datasets.}
}
@incollection{DONG2024257,
title = {10 - Photonic matrix computing accelerators},
editor = {Min Gu and Elena Goi and Yangyundou Wang and Zhengfen Wan and Yibo Dong and Yuchao Zhang and Haoyi Yu},
booktitle = {Neuromorphic Photonic Devices and Applications},
publisher = {Elsevier},
pages = {257-293},
year = {2024},
series = {Photonic Materials and Applications Series},
isbn = {978-0-323-98829-2},
doi = {https://doi.org/10.1016/B978-0-323-98829-2.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323988292000116},
author = {Jianji Dong and Hailong Zhou and Dexiu Huang},
keywords = {Photonic computing, photonic matrix–vector multiplications, photonic accelerators, photonic artificial intelligence, optical neural networks, mode-division multiplexing, Ising machines},
abstract = {With the growing demand for electronic computing, data centers, and emerging artificial intelligence applications, low latency and high energy efficiency are hard to ensure by traditional electrical methods. Photonic methods can perform high-speed parallel information processing with ultralow energy consumption benefiting from its superior performance. Photonic accelerators are designed to accelerate specific categories of computing in the optical domain, especially matrix computation, to address the growing demand for computing resources and capacity. In this chapter, methods for photonic matrix multiplication are first introduced and compared. Then, the expansions for applications in optical signal processing, optical neural networks, and combinatorial optimization problems with photonic matrix computing accelerators are presented. Finally, general-purpose spatial mode processors, optical neural networks, and Ising machines with photonic accelerators are analyzed in detail.}
}
@article{2024S1,
title = {The NCSBN 2024 Environmental Scan: Every Moment Matters, Realizing Lasting Impact},
journal = {Journal of Nursing Regulation},
volume = {14},
number = {4, Supplement },
pages = {S1-S48},
year = {2024},
note = {The NCSBN 2024 Environmental Scan: Every Moment Matters, Realizing Lasting Impact},
issn = {2155-8256},
doi = {https://doi.org/10.1016/S2155-8256(23)00127-8},
url = {https://www.sciencedirect.com/science/article/pii/S2155825623001278},
keywords = {nursing workforce, nursing education, healthcare delivery, nursing regulation, public policy}
}
@article{HAYAWI2024101533,
title = {Generative AI and large language models: A new frontier in reverse vaccinology},
journal = {Informatics in Medicine Unlocked},
volume = {48},
pages = {101533},
year = {2024},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2024.101533},
url = {https://www.sciencedirect.com/science/article/pii/S2352914824000893},
author = {Kadhim Hayawi and Sakib Shahriar and Hany Alashwal and Mohamed Adel Serhani},
keywords = {Reverse vaccinology, Large language models (LLMs), AI, Generative AI, Vaccine candidate identification, AI ethics, Vaccines},
abstract = {Reverse vaccinology is an emerging concept in the field of vaccine development as it facilitates the identification of potential vaccine candidates. Biomedical research has been revolutionized with the recent innovations in Generative Artificial Intelligence (AI) and Large Language Models (LLMs). The intersection of these two technologies is explored in this study. In this study, the impact of Generative AI and LLMs in the field of vaccinology is explored. Through a comprehensive analysis of existing research, prospective use cases, and an experimental case study, this research highlights that LLMs and Generative AI have the potential to enhance the efficiency and accuracy of vaccine candidate identification. This work also discusses the ethical and privacy challenges, such as data consent and potential biases, raised by such applications that require careful consideration. This study paves the way for experts, researchers, and policymakers to further investigate the role and impact of Generative AI and LLM in vaccinology and medicine.}
}
@incollection{KREISS2024349,
title = {11 - Innovations in signal/image processing and data analysis in optical microscopy},
editor = {Andrea Armani and Tatevik Chalyan and David D. Sampson},
booktitle = {Biophotonics and Biosensing},
publisher = {Elsevier},
pages = {349-389},
year = {2024},
series = {Photonic Materials and Applications Series},
isbn = {978-0-443-18840-4},
doi = {https://doi.org/10.1016/B978-0-44-318840-4.00019-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044318840400019X},
author = {Lucas Kreiss and Kevin C. Zhou and Clare B. Cook and Shiqi Xu and Amey Chaware and Roarke Horstmeyer},
keywords = {signal processing, image processing, data analysis, machine learning, deep learning, image registration, super resolution, explainable AI},
abstract = {Modern optical imaging relies on several computational techniques to address different challenges and fundamental limitations, such as noise, limited space-bandwidth product, unwanted color variability and quantification of relevant image features. A wide range of tools spans from classical image processing all the way to advanced deep learning models are now used to enhance the information content in images and the extraction of meaningful and quantifiable parameters. In many cases, machine learning has become the main method of choice for task-specific applications; however, classical image processing techniques still enjoy wide use as general-purpose tools, especially in low-data instances. Due to well-known challenges and limitations to conventional deep learning, researchers now work on emerging techniques, such as explainable AI, physics-informed or physics-supervised learning, known-operator learning, and others that aim to open the black-box of previous models and promise increased interpretability, incorporation of expert knowledge, and faster convergence for smaller datasets.}
}
@article{ALHAWAWREH2024103809,
title = {Securing the Industrial Internet of Things against ransomware attacks: A comprehensive analysis of the emerging threat landscape and detection mechanisms},
journal = {Journal of Network and Computer Applications},
volume = {223},
pages = {103809},
year = {2024},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103809},
url = {https://www.sciencedirect.com/science/article/pii/S108480452300228X},
author = {Muna Al-Hawawreh and Mamoun Alazab and Mohamed Amine Ferrag and M. Shamim Hossain},
keywords = {IIoT, Ransomware attacks, Artificial intelligence, Detection, IT, OT},
abstract = {Due to the complexity and diversity of Industrial Internet of Things (IIoT) systems, which include heterogeneous devices, legacy and new connectivity protocols and systems, and distributed networks, sophisticated attacks like ransomware will likely target these systems in the near future. Researchers have focused on studying and addressing ransomware attacks against various platforms in recent years. However, to the best of our knowledge, no existing study investigates the new trends of ransomware tactics and techniques and provides a comprehensive analysis of ransomware attacks and their detection techniques for IIoT systems. Therefore, this paper investigates this attack and its associated detection techniques in IIoT systems in various aspects, including recent ransomware tactics, types, infected operating systems, and platforms. Specifically, we initially discuss the evolution of the IIoT system and its common architecture. Then, we provide an in-depth examination of the development of ransomware attacks and their constituent blocks, outline recent tactics and types of ransomware, and provide an extensive overview of the latest research on detection models. We also summarize numerous significant issues that have yet to be addressed and require further research. We conclude that offensive and defensive research is urgently needed to protect IIoT against ransomware attacks.}
}
@incollection{SCHOENHERR2024127,
title = {8 - Building trust with the ethical affordances of education technologies: A sociotechnical systems perspective},
editor = {Prithviraj Dasgupta and James Llinas and Tony Gillespie and Scott Fouse and William Lawless and Ranjeev Mittu and Donald Sofge},
booktitle = {Putting AI in the Critical Loop},
publisher = {Academic Press},
pages = {127-165},
year = {2024},
isbn = {978-0-443-15988-6},
doi = {https://doi.org/10.1016/B978-0-443-15988-6.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443159886000030},
author = {Jordan Richard Schoenherr and Erin Chiou and Maria Goldshtein},
keywords = {Education technology, Learning engineering, Sociotechnical systems, Trust},
abstract = {Learning engineering has the potential to impact society broadly. As we introduce novel AI-enabled technologies into learning environments, we must consider both the qualities of a technology that make it trustworthy (e.g., accuracy, reliability) as well as the qualities of the implementation context (e.g., permissions, involvement) that affect the trust of learners, educators, and administrators in this technology. In this chapter, we consider a broad cross-section of learning technologies and the social and ethical implications of adopting these technologies in higher education. Following a review of values in value-based approaches to psychometrics, we consider how specific formal and informal learning technologies manifest in various learning environments inside and outside of higher education, and the ethical affordances of these systems. We then expand on how these ethical affordances should impact our assessments of technology trustworthiness as well as the need for applications of current trust frameworks to expand their level of analysis beyond traditional evaluations of technology performance (e.g., accuracy and reliability), toward more sociotechnical system level considerations (e.g., social and organizational impacts).}
}
@article{MARTINELLI20244853,
title = {A Method for AI-generated sentence detection through Large Language Models},
journal = {Procedia Computer Science},
volume = {246},
pages = {4853-4862},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.351},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924023767},
author = {Fabio Martinelli and Francesco Mercaldo and Luca Petrillo and Antonella Santone},
keywords = {sentence detection, sentence classification, llm},
abstract = {In recent years, we have seen an impressive expansion of a family of Artificial intelligence models, known as generative AI, that are capable of producing fresh, unique material, including text, images, audio, and even code. Large datasets of previously published information are used to train these models, enabling them to mimic the patterns and structures of the data and produce original output that is stylistically and qualitatively comparable to the training set. While these models have many promising applications, they also carry significant risks and potential dangers that must be carefully considered, such as misinformation, intellectual property violations, or biased information. For these reasons, in this work, we have proposed a method to detect whether a sentence is human-generated or AI-generated. To achieve this goal, we used a labeled dataset to train four different models from the BERT family, achieving an Accuracy of 96%.}
}
@article{BHATTACHARYA2024100194,
title = {ChatGPT’s scorecard after the performance in a series of tests conducted at the multi-country level: A pattern of responses of generative artificial intelligence or large language models},
journal = {Current Research in Biotechnology},
volume = {7},
pages = {100194},
year = {2024},
issn = {2590-2628},
doi = {https://doi.org/10.1016/j.crbiot.2024.100194},
url = {https://www.sciencedirect.com/science/article/pii/S2590262824000200},
author = {Manojit Bhattacharya and Soumen Pal and Srijan Chatterjee and Abdulrahman Alshammari and Thamer H. Albekairi and Supriya Jagga and Elijah {Ige Ohimain} and Hatem Zayed and Siddappa N. Byrareddy and Sang-Soo Lee and Zhi-Hong Wen and Govindasamy Agoramoorthy and Prosun Bhattacharya and Chiranjib Chakraborty},
keywords = {ChatGPT, Accuracy, Reproducibility, Plagiarism, Answer length},
abstract = {Recently, researchers have shown concern about the ChatGPT-derived answers. Here, we conducted a series of tests using ChatGPT by individual researcher at multi-country level to understand the pattern of its answer accuracy, reproducibility, answer length, plagiarism, and in-depth using two questionnaires (the first set with 15 MCQs and the second 15 KBQ). Among 15 MCQ-generated answers, 13 ± 70 were correct (Median : 82.5; Coefficient variance : 4.85), 3 ± 0.77 were incorrect (Median: 3, Coefficient variance: 25.81), and 1 to 10 were reproducible, and 11 to 15 were not. Among 15 KBQ, the length of each question (in words) is about 294.5 ± 97.60 (mean range varies from 138.7 to 438.09), and the mean similarity index (in words) is about 29.53 ± 11.40 (Coefficient variance: 38.62) for each question. The statistical models were also developed using analyzed parameters of answers. The study shows a pattern of ChatGPT-derive answers with correctness and incorrectness and urges for an error-free, next-generation LLM to avoid users’ misguidance.}
}
@article{JAVAHERI2024122697,
title = {Cybersecurity threats in FinTech: A systematic review},
journal = {Expert Systems with Applications},
volume = {241},
pages = {122697},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122697},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423031998},
author = {Danial Javaheri and Mahdi Fahmideh and Hassan Chizari and Pooia Lalbakhsh and Junbeom Hur},
keywords = {Banking trojan, Business sustainability, Cyber-attacks, Data privacy, Financial technology},
abstract = {The rapid evolution of the Smart-everything movement and Artificial Intelligence (AI) advancements have given rise to sophisticated cyber threats that traditional methods cannot counteract. Cyber threats are extremely critical in financial technology (FinTech) as a data-centric sector expected to provide 24/7 services. This paper introduces a novel and refined taxonomy of security threats in FinTech and conducts a comprehensive systematic review of defensive strategies. Through PRISMA methodology applied to 74 selected studies and topic modeling, we identified 11 central cyber threats, with 43 papers detailing them, and pinpointed 9 corresponding defense strategies, as covered in 31 papers. This in-depth analysis offers invaluable insights for stakeholders ranging from banks and enterprises to global governmental bodies, highlighting both the current challenges in FinTech and effective countermeasures, as well as directions for future research.}
}
@article{CHAI2024105877,
title = {SE-NDEND: A novel symmetric watermarking framework with neural network-based chaotic encryption for Internet of Medical Things},
journal = {Biomedical Signal Processing and Control},
volume = {90},
pages = {105877},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.105877},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423013101},
author = {Xiuli Chai and Zongwei Tang and Zhihua Gan and Yang Lu and Binjie Wang and Yushu Zhang},
keywords = {Medical image data, Watermarking, Effectiveness, Moiré distortion, Copyright protection},
abstract = {The development of the Internet of Medical Things heavily relies on big data, and data security based on medical images has become a growing concern in society. Digital watermarking serves as a crucial technique for protecting and tracing medical image data copyright, as well as enabling forensic analysis. However, existing deep watermarking methods often neglect the protection of watermarks after extraction, leading to potential copyright disputes. To address this issue, this paper proposes SE-NDEND, a novel symmetric watermarking framework with neural network-based chaotic encryption for the Internet of Medical Things that significantly enhances the effectiveness and security of watermarking while maintaining robustness. Specifically, the SE-NDEND leverages neural networks to simulate chaotic systems and generate chaotic sequences, mitigating the complexity and high cost of implementing chaotic systems using hardware circuits. Moreover, we introduce a new noise layer with Moiré distortion that interacts with the decoder, forming a symmetric network structure that bolsters the robustness of watermarking. Parameters are jointly trained and shared during the training process to counteract potential interference from the noise layer. Experimental results validate the effectiveness of SE-NDEND in enhancing copyright protection, traceability, and forensic capabilities, surpassing existing deep learning methods in terms of visual quality (with PSNR of 45.8492 dB and SSIM of 0.9874), security, and robustness. The proposed framework can find application in protecting medical image data in the Internet of Medical Things.}
}
@article{ZHAO2024122836,
title = {Autonomous driving system: A comprehensive survey},
journal = {Expert Systems with Applications},
volume = {242},
pages = {122836},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122836},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423033389},
author = {Jingyuan Zhao and Wenyi Zhao and Bo Deng and Zhenghong Wang and Feng Zhang and Wenxiang Zheng and Wanke Cao and Jinrui Nan and Yubo Lian and Andrew F. Burke},
keywords = {Autonomous driving, Deep learning, Scene perception, Localization, Motion planning, Decision-making},
abstract = {Automation is increasingly at the forefront of transportation research, with the potential to bring fully autonomous vehicles to our roads in the coming years. This comprehensive survey provides a holistic look at the essential components and cutting-edge technologies that are driving the development and implementation of autonomous driving. It starts by evaluating two critical system architectures that are fundamental to the operation of autonomous vehicles: the layered and end-to-end structures. It then examines the critical areas of scene perception and localization, emphasizing the importance of sensor technologies. These technologies are vital for tasks such as object detection and semantic segmentation, which allow vehicles to understand and navigate their environment. A special focus is given to the complex topic of object detection, along with suggestions for how it can be enhanced. The survey then proceeds to provide detailed discussions on path planning, trajectory prediction, and decision-making processes. These elements are crucial for the smooth navigation of autonomous vehicles, and the survey highlights the role of artificial intelligence (AI) and machine learning in these processes. Overall, the survey presents the rapid progress in the field of autonomous driving, offering a comprehensive assessment of the technologies and innovations that are essential for moving toward a safe and efficient autonomous future.}
}
@article{CHEN2024,
title = {Generative AI in Medical Practice: In-Depth Exploration of Privacy and Security Challenges},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/53008},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124001055},
author = {Yan Chen and Pouyan Esmaeilzadeh},
keywords = {artificial intelligence, AI, generative artificial intelligence, generative AI, medical practices, potential benefits, security and privacy threats},
abstract = {As advances in artificial intelligence (AI) continue to transform and revolutionize the field of medicine, understanding the potential uses of generative AI in health care becomes increasingly important. Generative AI, including models such as generative adversarial networks and large language models, shows promise in transforming medical diagnostics, research, treatment planning, and patient care. However, these data-intensive systems pose new threats to protected health information. This Viewpoint paper aims to explore various categories of generative AI in health care, including medical diagnostics, drug discovery, virtual health assistants, medical research, and clinical decision support, while identifying security and privacy threats within each phase of the life cycle of such systems (ie, data collection, model development, and implementation phases). The objectives of this study were to analyze the current state of generative AI in health care, identify opportunities and privacy and security challenges posed by integrating these technologies into existing health care infrastructure, and propose strategies for mitigating security and privacy risks. This study highlights the importance of addressing the security and privacy threats associated with generative AI in health care to ensure the safe and effective use of these systems. The findings of this study can inform the development of future generative AI systems in health care and help health care organizations better understand the potential benefits and risks associated with these systems. By examining the use cases and benefits of generative AI across diverse domains within health care, this paper contributes to theoretical discussions surrounding AI ethics, security vulnerabilities, and data privacy regulations. In addition, this study provides practical insights for stakeholders looking to adopt generative AI solutions within their organizations.}
}
@article{HELBERGER2024105915,
title = {FutureNewsCorp, or how the AI Act changed the future of news},
journal = {Computer Law & Security Review},
volume = {52},
pages = {105915},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2023.105915},
url = {https://www.sciencedirect.com/science/article/pii/S0267364923001255},
author = {Natali Helberger},
keywords = {AI Regulation, journalistic AI, standardisation, future scenario writing method, values, legal responsibility},
abstract = {Inspired by scenario writing methods to foster discussion on the societal implications of technology and regulation, the paper develops a ‘legal fiction scenario’ to anticipate the impact of the proposed European AI Act and examine some of the regulatory choices made. The paper tells the story of FutureNewsCorp – the largest news media company in Europe in the year 2043. The story of FutureNewsCorp is used for a critical analysis of the most recent draft of the AI Act and here, in particular, of the role of standardisation bodies and the division of responsibility between providers of AI systems and their professional users. Using the scenario method, the paper demonstrates that regulations like the planned AI Act can result in a shift of the power to decide what responsible use of AI is - from regulators and editors to technology developers and standardisation bodies - and that in doing so it may contribute to changing the structure and workings of an entire sector.}
}
@article{WEERASINGHE2024100052,
title = {Critical inquiry on National Innovation System: Does NIS fit with developing countries?},
journal = {Sustainable Technology and Entrepreneurship},
volume = {3},
number = {1},
pages = {100052},
year = {2024},
issn = {2773-0328},
doi = {https://doi.org/10.1016/j.stae.2023.100052},
url = {https://www.sciencedirect.com/science/article/pii/S2773032823000159},
author = {R.N. Weerasinghe and A.K.W. Jayawardane and Qiubo Huang},
keywords = {National innovation system, Developing counties, Networking relationships, Innovation performance},
abstract = {Innovation is not only a random or accidental process. It requires a deliberate and systematic approach to enhance the innovation performance of a country. The concept of the National Innovation System (NIS) has provided a valuable framework for organizing and fostering innovation efforts across individuals, groups, and organizations within a country. While numerous studies on NIS have been carried out in developed countries, contributing to their socioeconomic progress and shaping effective policies, there is a significant disparity between developed and developing countries. Researchers have recognized the importance of studying NISs in developing countries and have highlighted the need for in-depth investigations into system configurations to generate valuable insights for policy formulation. This paper presents a conceptual model for conducting NIS studies in developing countries, derived from a critical review of existing literature. The proposed model serves as a guide for conducting comprehensive studies within the unique contexts of developing countries, focusing on the central core of NIS. Moreover, this study opens opportunities for future NIS-related research by identifying specific subsections of the overall NIS that warrant attention, while acknowledging the limitations faced by researchers. By adopting this conceptual model, researchers can delve into various aspects of the NIS in developing countries, facilitating a holistic understanding, and enabling the generation of impactful policy recommendations.}
}
@article{ELMESTARI2024103605,
title = {Preserving data privacy in machine learning systems},
journal = {Computers & Security},
volume = {137},
pages = {103605},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103605},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823005151},
author = {Soumia Zohra {El Mestari} and Gabriele Lenzini and Huseyin Demirci},
keywords = {Privacy enhancing technologies, Trustworthy machine learning, Machine learning, Differential privacy, Homomorphic encryption, Functional encryption, Secure multiparty computation, Privacy threats},
abstract = {The wide adoption of Machine Learning to solve a large set of real-life problems came with the need to collect and process large volumes of data, some of which are considered personal and sensitive, raising serious concerns about data protection. Privacy-enhancing technologies (PETs) are often indicated as a solution to protect personal data and to achieve a general trustworthiness as required by current EU regulations on data protection and AI. However, an off-the-shelf application of PETs is insufficient to ensure a high-quality of data protection, which one needs to understand. This work systematically discusses the risks against data protection in modern Machine Learning systems taking the original perspective of the data owners, who are those who hold the various data sets, data models, or both, throughout the machine learning life cycle and considering the different Machine Learning architectures. It argues that the origin of the threats, the risks against the data, and the level of protection offered by PETs depend on the data processing phase, the role of the parties involved, and the architecture where the machine learning systems are deployed. By offering a framework in which to discuss privacy and confidentiality risks for data owners and by identifying and assessing privacy-preserving countermeasures for machine learning, this work could facilitate the discussion about compliance with EU regulations and directives. We discuss current challenges and research questions that are still unsolved in the field. In this respect, this paper provides researchers and developers working on machine learning with a comprehensive body of knowledge to let them advance in the science of data protection in machine learning field as well as in closely related fields such as Artificial Intelligence.}
}
@article{TOMASSINI2023102310,
title = {On-cloud decision-support system for non-small cell lung cancer histology characterization from thorax computed tomography scans},
journal = {Computerized Medical Imaging and Graphics},
volume = {110},
pages = {102310},
year = {2023},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2023.102310},
url = {https://www.sciencedirect.com/science/article/pii/S0895611123001283},
author = {Selene Tomassini and Nicola Falcionelli and Giulia Bruschi and Agnese Sbrollini and Niccolò Marini and Paolo Sernani and Micaela Morettini and Henning Müller and Aldo Franco Dragoni and Laura Burattini},
keywords = {Clinical decision-support system, Cloud computing, Convolutional long short-term memory, Non-small cell lung cancer histology characterization, Supervised deep learning, Thorax computed tomography},
abstract = {Non-Small Cell Lung Cancer (NSCLC) accounts for about 85% of all lung cancers. Developing non-invasive techniques for NSCLC histology characterization may not only help clinicians to make targeted therapeutic treatments but also prevent subjects from undergoing lung biopsy, which is challenging and could lead to clinical implications. The motivation behind the study presented here is to develop an advanced on-cloud decision-support system, named LUCY, for non-small cell LUng Cancer histologY characterization directly from thorax Computed Tomography (CT) scans. This aim was pursued by selecting thorax CT scans of 182 LUng ADenocarcinoma (LUAD) and 186 LUng Squamous Cell carcinoma (LUSC) subjects from four openly accessible data collections (NSCLC-Radiomics, NSCLC-Radiogenomics, NSCLC-Radiomics-Genomics and TCGA-LUAD), in addition to the implementation and comparison of two end-to-end neural networks (the core layer of whom is a convolutional long short-term memory layer), the performance evaluation on test dataset (NSCLC-Radiomics-Genomics) from a subject-level perspective in relation to NSCLC histological subtype location and grade, and the dynamic visual interpretation of the achieved results by producing and analyzing one heatmap video for each scan. LUCY reached test Area Under the receiver operating characteristic Curve (AUC) values above 77% in all NSCLC histological subtype location and grade groups, and a best AUC value of 97% on the entire dataset reserved for testing, proving high generalizability to heterogeneous data and robustness. Thus, LUCY is a clinically-useful decision-support system able to timely, non-invasively and reliably provide visually-understandable predictions on LUAD and LUSC subjects in relation to clinically-relevant information.}
}
@article{KINEBER2023104930,
title = {Modelling the relationship between digital twins implementation barriers and sustainability pillars: Insights from building and construction sector},
journal = {Sustainable Cities and Society},
volume = {99},
pages = {104930},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2023.104930},
url = {https://www.sciencedirect.com/science/article/pii/S2210670723005413},
author = {Ahmed Farouk Kineber and Atul Kumar Singh and Abdulwahed Fazeli and Saeed Reza Mohandes and Clara Cheung and Mehrdad Arashpour and Obuks Ejohwomu and Tarek Zayed},
keywords = {Digital twin, Sustainability, Sustainable construction, Overall sustainable success, Structural equation modelling},
abstract = {A Digital Twin (DT) is a digital copy of a real-world object or process. Although DT has gained traction in construction, its relationship with sustainable success remains insufficiently studied. This research addresses this gap by investigating barriers to implementing DT in sustainable construction. The study employs a hybrid approach involving literature review, expert interviews, and modeling techniques, with data collected from 108 construction experts based on a number of criteria, including the experience, degree, and familiarity of the experts about the Hong Kong building and construction sector Hong Kong. The findings reveal 45 barriers categorized into six clusters, including notable obstacles such as "legacy systems," "data uncertainties," and "connectivity." The key clusters identified are "performance" and "security," while the "social" aspect of sustainable success is least supported. Recognizing these challenges assists decision-makers in navigating obstacles and utilizing DT for environmentally conscious construction, streamlined processes, and positive societal impacts. Future research could delve into integrating sustainability throughout the project lifecycle using technology adoption theories.}
}
@article{GWON2024,
title = {The Use of Generative AI for Scientific Literature Searches for Systematic Reviews: ChatGPT and Microsoft Bing AI Performance Evaluation},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/51187},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000528},
author = {Yong Nam Gwon and Jae Heon Kim and Hyun Soo Chung and Eun Jee Jung and Joey Chun and Serin Lee and Sung Ryul Shim},
keywords = {artificial intelligence, search engine, systematic review, evidence-based medicine, ChatGPT, language model, education, tool, clinical decision support system, decision support, support, treatment},
abstract = {Background
A large language model is a type of artificial intelligence (AI) model that opens up great possibilities for health care practice, research, and education, although scholars have emphasized the need to proactively address the issue of unvalidated and inaccurate information regarding its use. One of the best-known large language models is ChatGPT (OpenAI). It is believed to be of great help to medical research, as it facilitates more efficient data set analysis, code generation, and literature review, allowing researchers to focus on experimental design as well as drug discovery and development.
Objective
This study aims to explore the potential of ChatGPT as a real-time literature search tool for systematic reviews and clinical decision support systems, to enhance their efficiency and accuracy in health care settings.
Methods
The search results of a published systematic review by human experts on the treatment of Peyronie disease were selected as a benchmark, and the literature search formula of the study was applied to ChatGPT and Microsoft Bing AI as a comparison to human researchers. Peyronie disease typically presents with discomfort, curvature, or deformity of the penis in association with palpable plaques and erectile dysfunction. To evaluate the quality of individual studies derived from AI answers, we created a structured rating system based on bibliographic information related to the publications. We classified its answers into 4 grades if the title existed: A, B, C, and F. No grade was given for a fake title or no answer.
Results
From ChatGPT, 7 (0.5%) out of 1287 identified studies were directly relevant, whereas Bing AI resulted in 19 (40%) relevant studies out of 48, compared to the human benchmark of 24 studies. In the qualitative evaluation, ChatGPT had 7 grade A, 18 grade B, 167 grade C, and 211 grade F studies, and Bing AI had 19 grade A and 28 grade C studies.
Conclusions
This is the first study to compare AI and conventional human systematic review methods as a real-time literature collection tool for evidence-based medicine. The results suggest that the use of ChatGPT as a tool for real-time evidence generation is not yet accurate and feasible. Therefore, researchers should be cautious about using such AI. The limitations of this study using the generative pre-trained transformer model are that the search for research topics was not diverse and that it did not prevent the hallucination of generative AI. However, this study will serve as a standard for future studies by providing an index to verify the reliability and consistency of generative AI from a user’s point of view. If the reliability and consistency of AI literature search services are verified, then the use of these technologies will help medical research greatly.}
}
@incollection{MONLEZUN2024133,
title = {5 - Framework part II: artificial intelligence + political economics},
editor = {Dominique J. Monlezun},
booktitle = {Responsible Artificial Intelligence Re-engineering the Global Public Health Ecosystem},
publisher = {Morgan Kaufmann},
pages = {133-184},
year = {2024},
isbn = {978-0-443-21597-1},
doi = {https://doi.org/10.1016/B978-0-443-21597-1.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443215971000056},
author = {Dominique J. Monlezun},
keywords = {Political economics, determinants of health, managed strategic competition, medical diplomacy, supply chain resilience, large language models, clean energy, de-risking, diversification, multilateralism, deterrence, defense, development},
abstract = {This chapter considers the political economic or meta-determinants of health for the global public health ecosystem, critical for the scale, scope, and speed of coordinated actions (including in consensus-based governance and financing) to generate equitable and effective global health solutions to urgent shared challenges. Rising international separation and tensions between democracies and autocracies in addition to the Global North and the Global South undermines the health of these regimes and regions and that of humanity. This chapter thus considers global health and artificial intelligence (AI) in their political economic context in the strategic competition of dominant power players, particularly with the governments, militaries, and corporations of the United States and China which account for most of the global health financing and programs along with that of AI’s development and deployment. Failures in managed strategic competition can not only undermine the cooperation required for the AI-driven global public health ecosystem, but they may even imperil it through accelerated and even catastrophic conflicts. This chapter therefore considers the history and foreseeable future of the global public health ecosystem from the structural perspective of political economics, including the underlying values that may provide a durable foundation for coordinated health action. It additionally considers emergent solutions and advances for the health ecosystem toward this including with human security and data sovereignty within Political Liberalism articulating a bridge between the above blocs, while addressing health determinants integrally and globally: social determinants of health, political determinants of health, economic determinants of health, commercial determinants of health, and digital determinants of health. Specific advances include shared global governance, affordable clean energy transition, and affordable AI digital transformation for sustainable development (with deference and deterrence guardrails maximizing cooperation, managing strategic competition, and minimizing conflict). The chapter additionally considers medical diplomacy, multilateral development, deep medicine, large language models (including ChatGPT), commercial fusion, and digital supply chain resilience (with diversification and de-risking), in the context of moving away from an imperial ideological values-driven ruler-based world order to a more sovereign integral values-driven rules-based world order.}
}
@article{YANG2024155,
title = {MindLLM: Lightweight large language model pre-training, evaluation and domain application},
journal = {AI Open},
volume = {5},
pages = {155-180},
year = {2024},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2024.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666651024000111},
author = {Yizhe Yang and Huashan Sun and Jiawei Li and Runheng Liu and Yinghao Li and Yuhang Liu and Yang Gao and Heyan Huang},
keywords = {Large language model, Light weight, Bilingual},
abstract = {Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is leveraged by developing increasingly large-scale models, there could be another branch to develop lightweight custom models that better serve certain domains, taking into account the high cost of training and deploying LLMs and the scarcity of resources. In this paper, we present MindLLM, a novel series of bilingual lightweight large language models, trained from scratch, alleviating such burdens by offering models with 1.3 billion and 3 billion parameters. A thorough account of experiences accrued during large model development is given, covering every step of the process, including data construction, model architecture, evaluation, and applications. Such insights are hopefully valuable for fellow academics and developers. MindLLM consistently matches or surpasses the performance of other open-source larger models on some public benchmarks. We also introduce an innovative instruction tuning framework tailored for smaller models to enhance their capabilities efficiently. Moreover, we explore the application of MindLLM in specific vertical domains such as law and finance, underscoring the agility and adaptability of our lightweight models.}
}
@article{2023293,
title = {Guide for Authors},
journal = {Intelligent Medicine},
volume = {3},
number = {4},
pages = {293-298},
year = {2023},
issn = {2667-1026},
doi = {https://doi.org/10.1016/S2667-1026(23)00073-6},
url = {https://www.sciencedirect.com/science/article/pii/S2667102623000736}
}
@incollection{FU2024,
title = {Generative artificial intelligence in operations},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-443-28993-4.00057-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443289934000573},
author = {Yingxuan Fu and Hing Kai Chan and Zhao Cai},
keywords = {Generative AI, , Generative adversarial network, Large language model, Transformer, Variational autoencoder},
abstract = {The rise of generative artificial intelligence (AI) may present a significant opportunity for a profound revolution in operations and supply chain management. However, such technological advancement is accompanied by a scholarly discourse that navigates the balance between its promising abilities and challenges. This chapter provides an overview of generative AI in operations and supply chain management. It begins by expositing its fundamental technical concepts and role alongside existing AI technologies. Subsequently, it delves into potential applications and challenges in implementing generative AI in operations. A future research agenda and takeaways for practitioners and Operations Management (OM) researchers are proposed at the end.}
}
@article{DAI2024101606,
title = {SecNLP: An NLP classification model watermarking framework based on multi-task learning},
journal = {Computer Speech & Language},
volume = {86},
pages = {101606},
year = {2024},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2023.101606},
url = {https://www.sciencedirect.com/science/article/pii/S0885230823001250},
author = {Long Dai and Jiarong Mao and Liaoran Xu and Xuefeng Fan and Xiaoyi Zhou},
keywords = {Natural language processing, NLP model security, Black-box watermarking, White-box watermarking},
abstract = {The popularity of ChatGPT demonstrates the immense commercial value of natural language processing (NLP) technology. However, NLP models like ChatGPT are vulnerable to piracy and redistribution, which can harm the economic interests of model owners. Existing NLP model watermarking schemes struggle to balance robustness and covertness. Typically, robust watermarks require embedding more information, which compromises their covertness; conversely, covert watermarks are challenging to embed more information, which affects their robustness. This paper is proposed to use multi-task learning (MTL) to address the conflict between robustness and covertness. Specifically, a covert trigger set is established to implement remote verification of the watermark model, and a covert auxiliary network is designed to enhance the watermark model’s robustness. The proposed watermarking framework is evaluated on two benchmark datasets and three mainstream NLP models. Compared with existing schemes, the framework not only has excellent covertness and robustness but also has a lower false positive rate and can effectively resist fraudulent ownership claims by adversaries.}
}
@article{WAZID2024100040,
title = {A Secure Deepfake Mitigation Framework: Architecture, Issues, Challenges, and Societal Impact},
journal = {Cyber Security and Applications},
volume = {2},
pages = {100040},
year = {2024},
issn = {2772-9184},
doi = {https://doi.org/10.1016/j.csa.2024.100040},
url = {https://www.sciencedirect.com/science/article/pii/S2772918424000067},
author = {Mohammad Wazid and Amit Kumar Mishra and Noor Mohd and Ashok Kumar Das},
keywords = {Deepfake, Artificial Intelligence (AI), Machine learning, Cyber security, Authentication},
abstract = {Deepfake refers to synthetic media generated through artificial intelligence (AI) techniques. It involves creating or altering video, audio, or images to make them appear as though they depict something or someone else. Deepfake technology advances just like the mechanisms that are used to detect them. There’s an ongoing cat-and-mouse game between creators of deepfakes and those developing detection methods. As the technology that underpins deepfakes continues to improve, we are obligated to confront the repercussions that it will have on society. The introduction of educational initiatives, regulatory frameworks, technical solutions, and ethical concerns are all potential avenues via which this matter can be addressed. Multiple approaches need to be combined to identify deepfakes effectively. Detecting deepfakes can be challenging due to their increasingly sophisticated nature, but several methods and techniques are being developed to identify them. Mitigating the negative impact of deepfakes involves a combination of technological advancements, awareness, and policy measures. In this paper, we propose a secure deepfake mitigation framework. We have also provided a security analysis of the proposed framework via the Scyhter tool-based formal security verification. It proves that the proposed framework is secure against various cyber attacks. We also discuss the societal impact of deepfake events along with its detection process. Then some AI models, which are used for creating and detecting the deepfake events, are highlighted. Ultimately, we provide the practical implementation of the proposed framework to observe its functioning in a real-world scenario.}
}
@article{RAY2024117085,
title = {Leveraging ChatGPT and Bard: What does it convey for water treatment/desalination and harvesting sectors?},
journal = {Desalination},
volume = {570},
pages = {117085},
year = {2024},
issn = {0011-9164},
doi = {https://doi.org/10.1016/j.desal.2023.117085},
url = {https://www.sciencedirect.com/science/article/pii/S0011916423007178},
author = {Saikat Sinha Ray and Pranav R.T. Peddinti and Rohit Kumar Verma and Harish Puppala and Byungmin Kim and Ashutosh Singh and Young-Nam Kwon},
keywords = {ChatGPT, Bard, Water treatment, Desalination, Water harvesting and artificial intelligence (AI)},
abstract = {Artificial intelligence (AI) has emerged as a prominent tool in the modern day. The utilization of AI and advanced language models such as chat generative pre-trained transformer (ChatGPT) and Bard is not only innovative but also crucial for handling challenges related to water research. ChatGPT is an AI chatbot that uses natural language processing to create humanlike conversations. ChatGPT has recently gained considerable public interest, owing to its unique ability to simplify tasks from various backgrounds. Similarly, Google introduced Bard, an AI-powered chatbot to simulate human conversations. Herein, we investigated how ChatGPT and Bard (AI powdered chatbots) tools can impact water research through interactive sessions. Typically, ChatGPT and Bard offer significant benefits to various fields, including research, education, scientific publications, and outreach. ChatGPT and Bard simplify complex and challenging tasks. For instance, 50 important questions about water treatment/desalination techniques and 50 questions about water harvesting techniques were provided to both chatbots. Time analytics was performed by ChatGPT 3.5, and Bard was used to generate full responses. In particular, the effectiveness of this emerging tool for research purposes in the field of conventional water treatment techniques, advanced water treatment techniques, membrane technology and seawater desalination has been thoroughly demonstrated. Moreover, potential pitfalls and challenges were also highlighted. Thus, sharing these experiences may encourage the effective and responsible use of Bard and ChatGPT in research purposes. Finally, the responses were compared from the perspective of an expert. Although ChatGPT and Bard possess huge benefits, there are several issues, which are discussed in this study. Based on this study, we can compare the abilities of artificial intelligence and human intelligence in water sector research.}
}
@article{AMIRIAN2023109738,
title = {HexAI-TJAtxt: A textual dataset to advance open scientific research in total joint arthroplasty},
journal = {Data in Brief},
volume = {51},
pages = {109738},
year = {2023},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2023.109738},
url = {https://www.sciencedirect.com/science/article/pii/S2352340923008077},
author = {Soheyla Amirian and Husam Ghazaleh and Luke A. Carlson and Matthew Gong and Logan Finger and Johannes F. Plate and Ahmad P. Tafti},
keywords = {Total joint arthroplasty, Large scale textual dataset, Computational text analytics, ChatGPT},
abstract = {Total joint arthroplasty (TJA) is the most common and fastest inpatient surgical procedure in the elderly, nationwide. Due to the increasing number of TJA patients and advancements in healthcare, there is a growing number of scientific articles being published in a daily basis. These articles offer important insights into TJA, covering aspects like diagnosis, prevention, treatment strategies, and epidemiological factors. However, there has been limited effort to compile a large-scale text dataset from these articles and make it publicly available for open scientific research in TJA. Rapid yet, utilizing computational text analysis on these large columns of scientific literatures holds great potential for uncovering new knowledge to enhance our understanding of joint diseases and improve the quality of TJA care and clinical outcomes. This work aims to build a dataset entitled HexAI-TJAtxt, which includes more than 61,936 scientific abstracts collected from PubMed using MeSH (Medical Subject Headings) terms within “MeSH Subheading” and “MeSH Major Topic,” and Publication Date from 01/01/2000 to 12/31/2022. The current dataset is freely and publicly available at https://github.com/pitthexai/HexAI-TJAtxt, and it will be updated frequently in bi-monthly manner from new abstracts published at PubMed.}
}
@article{NYAMATHI2024,
title = {Establishing the Foundations of Emotional Intelligence in Care Companion Robots to Mitigate Agitation Among High-Risk Patients With Dementia: Protocol for an Empathetic Patient-Robot Interaction Study},
journal = {JMIR Research Protocols},
volume = {13},
year = {2024},
issn = {1929-0748},
doi = {https://doi.org/10.2196/55761},
url = {https://www.sciencedirect.com/science/article/pii/S1929074824004803},
author = {Adeline Nyamathi and Nikil Dutt and Jung-Ah Lee and Amir M Rahmani and Mahkameh Rasouli and Donna Krogh and Erik Krogh and David Sultzer and Humayun Rashid and Hamza Liaqat and Riyam Jawad and Farhan Azhar and Ali Ahmad and Bilal Qamar and Taha Yasin Bhatti and Chet Khay and Jocelyn Ludlow and Lisa Gibbs and Julie Rousseau and Mahyar Abbasian and Yutong Song and Cheonkam Jeong and Sabine Brunswicker},
keywords = {persons with dementia, empathy-based care companion robot, agitation, fall risk, artificial intelligence, AI},
abstract = {Background
An estimated 6.7 million persons are living with dementia in the United States, a number expected to double by 2060. Persons experiencing moderate to severe dementia are 4 to 5 times more likely to fall than those without dementia, due to agitation and unsteady gait. Socially assistive robots fail to address the changing emotional states associated with agitation, and it is unclear how emotional states change, how they impact agitation and gait over time, and how social robots can best respond by showing empathy.
Objective
This study aims to design and validate a foundational model of emotional intelligence for empathetic patient-robot interaction that mitigates agitation among those at the highest risk: persons experiencing moderate to severe dementia.
Methods
A design science approach will be adopted to (1) collect and store granular, personal, and chronological data using Personicle (an open-source software platform developed to automatically collect data from phones and other devices), incorporating real-time visual, audio, and physiological sensing technologies in a simulation laboratory and at board and care facilities; (2) develop statistical models to understand and forecast the emotional state, agitation level, and gait pattern of persons experiencing moderate to severe dementia in real time using machine learning and artificial intelligence and Personicle; (3) design and test an empathy-focused conversation model, focused on storytelling; and (4) test and evaluate this model for a care companion robot (CCR) in the community.
Results
The study was funded in October 2023. For aim 1, architecture development for Personicle data collection began with a search for existing open-source data in January 2024. A community advisory board was formed and met in December 2023 to provide feedback on the use of CCRs and provide personal stories. Full institutional review board approval was received in March 2024 to place cameras and CCRs at the sites. In March 2024, atomic marker development was begun. For aim 2, after a review of open-source data on patients with dementia, the development of an emotional classifier was begun. Data labeling was started in April 2024 and completed in June 2024 with ongoing validation. Moreover, the team established a baseline multimodal model trained and validated on healthy-person data sets, using transformer architecture in a semisupervised manner, and later retrained on the labeled data set of patients experiencing moderate to severe dementia. In April 2024, empathy alignment of large language models was initiated using prompt engineering and reinforcement learning.
Conclusions
This innovative caregiving approach is designed to recognize the signs of agitation and, upon recognition, intervene with empathetic verbal communication. This proposal has the potential to have a significant impact on an emerging field of computational dementia science by reducing unnecessary agitation and falls of persons experiencing moderate to severe dementia, while reducing caregiver burden.
International Registered Report Identifier (IRRID)
PRR1-10.2196/55761}
}
@article{VECCHIARINI2023100879,
title = {Redefining entrepreneurship education in the age of artificial intelligence: An explorative analysis},
journal = {The International Journal of Management Education},
volume = {21},
number = {3},
pages = {100879},
year = {2023},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2023.100879},
url = {https://www.sciencedirect.com/science/article/pii/S1472811723001179},
author = {Mariangela Vecchiarini and Tatiana Somià},
keywords = {Artificial intelligence, ChatGPT, Entrepreneurship education},
abstract = {AI-powered chatbots, such as ChatGPT, have gained significant attention in the education field due to recent advancements and growing popularity. This article investigates the potential uses of ChatGPT in higher education, specifically within entrepreneurship courses, and explores the benefits and challenges associated with its implementation. To address the need for further research on the use of AI in business education, a survey was conducted among undergraduate students enrolled in entrepreneurship courses. The survey focused on students' awareness and usage of ChatGPT, perceived benefits and limitations, and integration strategies for this tool into entrepreneurship courses. As entrepreneurship education evolves alongside AI advancements, AI technologies like ChatGPT can play a transformative role in various activities, from idea generation, to crafting a business model, writing a business plan, or conducting customer interviews. The study's results indicate that ChatGPT has the potential to streamline processes, increase students' efficiency, and support certain types of creativity. The article also addresses concerns regarding ChatGPT accuracy and reliability, emphasizing the importance of using it critically. This research contributes to the understanding of how AI can enable entrepreneurship education and provides valuable insights for educators, students, and institutions seeking to leverage AI in the classroom.}
}
@article{2024A11,
title = {Guide for Authors},
journal = {Journal of the American Society of Echocardiography},
volume = {37},
number = {1},
pages = {A11-A19},
year = {2024},
issn = {0894-7317},
doi = {https://doi.org/10.1016/S0894-7317(23)00603-X},
url = {https://www.sciencedirect.com/science/article/pii/S089473172300603X}
}
@incollection{GRANVILLE2024169,
title = {Chapter 10 - Synthetic tabular data: copulas vs enhanced GANs},
editor = {Vincent Granville},
booktitle = {Synthetic Data and Generative AI},
publisher = {Morgan Kaufmann},
pages = {169-201},
year = {2024},
isbn = {978-0-443-21857-6},
doi = {https://doi.org/10.1016/B978-0-44-321857-6.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044321857600014X},
author = {Vincent Granville},
keywords = {curve fitting, unsupervised regression, logistic regression, constrained optimization, time series, clustering},
abstract = {In this chapter, you will learn how to create your own tabular synthetic data in Python, using two popular techniques: GANs and copulas. One example includes a real-life insurance data set: using copulas, you will be able to create an alternate (synthetic) data set that matches very well the distribution of the observations in your training set, including all the correlations. Another example is the diabetes data set; the goal is to predict cancer, and the context is supervised classification. You will learn how to synthesize this data set using GANs (generative adversarial networks). I also discuss data transformations, how to deal with missing data, and modern tools to assess the quality of the synthesized data, with illustrations. One section is focused on feature clustering to reduce the time required for training a GAN model. This chapter provides you with the skills to generate realistic synthetizations for your applications, and to quickly identify the strengths and weaknesses of each method (GANs, parametric copulas, noise injection), which one to use depending on your data or goal, and how to fine-tune or blend different methods to get the best results or minimize computing time. For instance, I show how to use a different copula for each group, after segmenting your training set. The deep neural networks used here for generative AI also lead to fully replicable experiments, in contrast to many current implementations.}
}
@article{RENAUD2024103877,
title = {VISTA: An inclusive insider threat taxonomy, with mitigation strategies},
journal = {Information & Management},
volume = {61},
number = {1},
pages = {103877},
year = {2024},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2023.103877},
url = {https://www.sciencedirect.com/science/article/pii/S0378720623001258},
author = {Karen Renaud and Merrill Warkentin and Ganna Pogrebna and Karl {van der Schyff}},
keywords = {Insider threats, Taxonomy, Mitigations, Cybersecurity},
abstract = {Insiders have the potential to do a great deal of damage, given their legitimate access to organisational assets and the trust they enjoy. Organisations can only mitigate insider threats if they understand what the different kinds of insider threats are, and what tailored measures can be used to mitigate the threat posed by each of them. Here, we derive VISTA (inclusiVe InSider Threat tAxonomy) based on an extensive literature review and a survey with C-suite executives to ensure that the VISTA taxonomy is not only scientifically grounded, but also meets the needs of organisations and their executives. To this end, we map each VISTA category of insider threat to tailored mitigations that can be deployed to reduce the threat.}
}
@article{GENG2024103595,
title = {A survey of strategy-driven evasion methods for PE malware: Transformation, concealment, and attack},
journal = {Computers & Security},
volume = {137},
pages = {103595},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103595},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823005059},
author = {Jiaxuan Geng and Junfeng Wang and Zhiyang Fang and Yingjie Zhou and Di Wu and Wenhan Ge},
keywords = {Windows portable executable, Malware detection, Malware evasion, Packer, Code obfuscation, Metamorphism, Behavioral obfuscation, Anti-sandbox, Adversarial attack},
abstract = {The continuous proliferation of malware poses a formidable threat to the cyberspace landscape. Researchers have proffered a multitude of sophisticated defense mechanisms aimed at its detection and mitigation. Nevertheless, malware writers persistently pursue pioneering and innovative methods to evade detection by security software, thereby presenting an ever-evolving and dynamic threat to computer systems. Malware evasion refers to the use of certain strategies by malware to evade the detection of security software. Despite numerous surveys on malware evasion techniques, the existing surveys were fragmented and focused on specific types of evasion methods, leading to a lack of systematic and comprehensive research on malware evasion approaches. To fill this gap, this paper proposed a strategy-driven framework from the perspective of malware writers. Based on this framework, we categorize existing evasion detection techniques into transformation (alter the structural and behavioral pattern of the malware), concealment (conceal the behavior of the malware), and attack-based (engage in an attack on the detector to render it inoperable) methods and conduct a comprehensive survey of the relevant research works. In addition, we demonstrate how to integrate existing evasion strategies in the process of generating malware from the perspective of malware writers to subvert the multiple defenses of defenders. Our investigation indicates that: 1) evasion techniques such as packer and code obfuscation remain the foremost selection for attackers, no fewer than 10 off-the-shelf tools provide great assistance to them, 2) environment analysis is the primary concealment-based strategy used by the attacker (48% of the reviewed concealment-based strategy), defenders need greater efforts to counter them, 3) only 3 works discussed techniques for evasion attacks by leveraging fragilities in antivirus engines, meaning that direct attack on the detector is no longer as effective, 4) reinforcement learning algorithm serves as the most popular adversarial attack-based methods and 50% of works based on reinforcement learning are effective against real-world antivirus engines. Furthermore, this paper delves into the development trends in evasive malware and open issues for defenders. The primary objective of this survey is to furnish researchers and practitioners with a thorough comprehension of malware evasion strategies and techniques, thereby fostering the advancement of more potent and efficient approaches to detect and thwart malware.}
}
@article{COHEN2024545,
title = {Digital, Technological and AI Skills for Smart Production Work Environment},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {19},
pages = {545-550},
year = {2024},
note = {18th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.09.269},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324016987},
author = {Yuval Cohen and Hila Chalutz–Ben Gal},
keywords = {Skills, Collaborative Work, Smart Manufacturing, Industry 5.0, Human in the loop, Cobot},
abstract = {This paper analyses the past and anticipated developments in collaborative smart production work environment and points at the required skills to best utilize and flourish in this newly formed work environment. The paper identifies the new work requirements using the job type and its related required technologies and maps the work requirements to the set of skills that may fulfill these requirements. An important notion in this paper is that a shopfloor usually involves several different work environments, each with its unique set of work requirements and associated skills. Thus, tailoring a subset of skills to these set of requirements is the suggested strategy. We use a small example of assembly shopfloor for illustrating the proposed approach. Finally, we propose future research related to this study.}
}
@article{BASOLE2024114133,
title = {Complex business ecosystem intelligence using AI-powered visual analytics},
journal = {Decision Support Systems},
volume = {178},
pages = {114133},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114133},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623002087},
author = {Rahul C. Basole and Hyunwoo Park and C. David Seuss},
keywords = {Business ecosystem, Artificial intelligence, Text mining, Complex networks, Interactive visualization},
abstract = {Business ecosystems are complex, dynamic systems characterized by a multitude of entities, including companies, ventures, and technologies, as well as activities and trends. Understanding the state of business ecosystems is an increasingly critical strategic imperative for many decision makers, but it is a resource-intensive activity as relevant information sources are dispersed, often highly unstructured, and not integrated or curated to deliver actionable insights. In this research, we present the design and implementation of an interactive visual analytic system that integrates artificial intelligence and graph visualization techniques to augment decision makers’ understanding of the complex public narrative associated with business ecosystems entities. Our system is driven by a real-time content engine of 100,000+ global data sources including press releases, news articles, industry reports, analyst blogs in multiple languages organized across several domain-specific repositories. Following a user-specified query, the engine extracts both domain-agnostic and domain-specific entities and concepts for each document in the result set. We then model and visualize the resulting data as a dynamic, multipartite network and implement graph pruning algorithms and interactive data controls to enable users to interactively explore and discover the underlying business ecosystem from multiple perspectives. We illustrate and discuss the value of our system using representative use cases. Our study makes multiple contributions to visual decision support theory and practice, including mining unstructured data, constructing and interacting with knowledge graphs, and designing visual analytic tools for ecosystem intelligence. We conclude the study with implications and future research opportunities.}
}
@incollection{SAMPSON2024417,
title = {13 - AI-driven innovations in signal/image processing and data analysis for optical coherence tomography in clinical applications},
editor = {Andrea Armani and Tatevik Chalyan and David D. Sampson},
booktitle = {Biophotonics and Biosensing},
publisher = {Elsevier},
pages = {417-480},
year = {2024},
series = {Photonic Materials and Applications Series},
isbn = {978-0-443-18840-4},
doi = {https://doi.org/10.1016/B978-0-44-318840-4.00022-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044318840400022X},
author = {Danuta M. Sampson and David D. Sampson},
keywords = {optical coherence tomography, OCT, signal processing, image processing, deep learning, artificial intelligence, machine learning, segmentation, detection, big data, data privacy},
abstract = {Fueled by the explosion of algorithms and computational innovations, optical coherence tomography (OCT) has progressed rapidly in the last decade, towards faster and more accurate imaging and characterization of ocular, systemic, and chronic diseases. This chapter describes recent advances in signal and image processing and data analysis methods responsible for this translational impact of OCT, which has been mainly in the retina, but other applications are included. The tools developed and used to enhance, segment, and extract meaningful and quantifiable parameters from OCT images are described. Traditional image processing methods are briefly outlined and AI-based innovations are reviewed. The importance of open research, protocol harmonization, big data, and patient data privacy in driving further innovation is also discussed. This chapter does not provide an exhaustive review, but rather its purpose is to be illustrative of the ongoing research and translational work and encourage engineers, scientists, and clinicians to work together in this exciting field. Sufficient detail is given to enable newcomers to the field, both engineers and clinicians, to understand the challenges and opportunities.}
}
@article{SCHMIEDEL2023107066,
title = {The New Zealand eating behavior questionnaire – Validation study for a novel assessment tool to describe actionable eating behavior traits},
journal = {Appetite},
volume = {191},
pages = {107066},
year = {2023},
issn = {0195-6663},
doi = {https://doi.org/10.1016/j.appet.2023.107066},
url = {https://www.sciencedirect.com/science/article/pii/S019566632302528X},
author = {Ole Schmiedel and Melissa Ivey and Amy Liu and Rinki Murphy},
keywords = {Obesity, Eating behavior, Questionnaire, Personalized medicine, Pharmacotherapy, Emotional eating},
abstract = {Individualised management of obesity remains challenging and, to date, most treatment is based on clinical judgement. This study aimed to develop and validate a novel questionnaire-based tool to identify three pre-defined eating behavior (EB) traits, emotional eating, reduced satiety (constant hunger) and reduced satiation (feasters) that may predict selective medication response given their targeted actions. We recruited 977 individuals from a tertiary academic diabetes clinic to participate in this two-phase validation study. Participants self-reported weight management activities and were asked to self-assess their EB characteristics. The initial questionnaire included 42 visual analogue scale questions. In Phase I, 729 participants completed the questionnaire, including Māori (11.8%) and Pacific peoples (19.3%). After random division of the study sample, Exploratory Factor Analysis (EFA) confirmed a three-factor model as the best fit. Stepwise removal of items with inadequate factor loading retained 27 of 42 items, which accounted for 96% of the variance. Confirmatory Factor Analysis (CFA), performed on the second half of the sample, demonstrated good model fit with the final 27-item questionnaire. Internal consistency was high for factor (α = 0.82–0.95) and demographic subgroups, and similar to those obtained in the EFA. Test-retest reliability in a subset of 399 participants who repeated the questionnaire after a four-week interval (Phase II) showed moderate to good reliability. Participants classified into one of three EB types based on the highest median score among the factors. Test-retest reliability was robust for emotional eaters (71.25%) and constant hunger (68.9%). The correlation between aggregate EB score (sum of three EB scores) and BMI was significant (Spearman rho = 0.314, P = .0005). The questionnaire reliably identified three distinct EB traits, which may be informative for precision medicine applications for obesity management.}
}
@article{JIANG2023103934,
title = {RDD-net: Robust duplicated-diffusion watermarking based on deep network},
journal = {Journal of Visual Communication and Image Representation},
volume = {96},
pages = {103934},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103934},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323001840},
author = {Guowei Jiang and Zhouyan He and Jiangtao Huang and Ting Luo and Haiyong Xu and Chongchong Jin},
keywords = {Watermarking, Deep network, Robustness, RGB channels, Channel connection},
abstract = {This paper proposes a novel robust duplicated-diffusion watermarking model based on deep network (RDD-net). RDD-net employs the encoder-noise-decoder structure for end-to-end training to obtain high image watermarking invisibility and robustness. Firstly, a duplicated-diffusion strategy is employed in the encoder to replicate the original watermark iteratively until all copies are diffused to the whole image so that the generalized ability in resisting various noises is obtained. Then, a channel connection technique is designed to extract inherent image features for fusing watermark for robustness by mining correlations of RGB three channels of the color image. Meanwhile, each channel is fused with watermark to increase robustness. Another attempt to improve the watermarking performance is the optimizer, which optimizes the watermark distribution by evaluating the similarities between the encoded image and the original image, as well as between the encoded image and the noised image. Our extensive experimental results demonstrate that the proposed RDD-net not only resists different noises, but also obtains better image quality and higher robustness than the existing watermarking models.}
}
@article{HACKER2023105871,
title = {The European AI liability directives – Critique of a half-hearted approach and lessons for the future},
journal = {Computer Law & Security Review},
volume = {51},
pages = {105871},
year = {2023},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2023.105871},
url = {https://www.sciencedirect.com/science/article/pii/S026736492300081X},
author = {Philipp Hacker},
keywords = {Artificial intelligence, ChatGPT, Product liability, EU law, AI act, Sustainability, Innovation, Large generative AI models},
abstract = {The optimal liability framework for AI systems remains an unsolved problem across the globe. With ChatGPT and other large generative models taking the technology to the next level, solutions are urgently needed. In a much-anticipated move, the European Commission advanced two proposals outlining the European approach to AI liability in September 2022: a novel AI Liability Directive (AILD) and a revision of the Product Liability Directive (PLD). They constitute the final cornerstone of AI regulation in the EU. Crucially, the liability proposals and the proposed EU AI Act are inherently intertwined: the latter does not contain any individual rights of affected persons, and the former lack specific, substantive rules on AI development and deployment. Taken together, these acts may well trigger a “Brussels effect” in AI regulation, with significant consequences for the US and other countries. Against this background, this paper makes three novel contributions. First, it examines in detail the liability proposals and shows that, while making steps in the right direction, they ultimately represent a half-hearted approach: if enacted as foreseen, AI liability in the EU will primarily rest on disclosure of evidence mechanisms and a set of narrowly defined presumptions concerning fault, defectiveness and causality. Hence, second, the article suggests amendments to the proposed AI liability framework. They are collected in a concise Annex at the end of the paper. I argue, inter alia, that the dichotomy between the fault-based AILD Proposal and the supposedly strict liability PLD Proposal is fictional and should be abandoned; that an EU framework for AI liability should comprise one fully harmonizing regulation instead of two insufficiently coordinated directives; and that the current proposals unjustifiably collapse fundamental distinctions between social and individual risk by equating high-risk AI systems in the AI Act with those under the liability framework. Third, based on an analysis of the key risks AI poses, the final part of the paper maps out a road for the future of AI liability and regulation, in the EU and beyond. More specifically, I make four key proposals. Effective compensation should be ensured by combining truly strict liability for certain high-risk AI systems with general presumptions of defectiveness, fault and causality in cases involving SMEs or non-high-risk AI systems. The paper introduces a novel distinction between illegitimate- and legitimate-harm models to delineate strict liability's scope. Truly strict liability should be reserved for high-risk AI systems that, from a social perspective, should not cause harm (illegitimate-harm models, e.g., autonomous vehicles or medical AI). Models meant to cause some unavoidable harm by ranking and rejecting individuals (legitimate-harm models, e.g., credit scoring or insurance scoring) may merely face rebuttable presumptions of defectiveness and causality. General-purpose AI systems and Foundation Models should only be subjected to high-risk regulation, including liability for high-risk AI systems, in specific high-risk use cases for which they are deployed. Consumers, in turn, ought to be liable based on regular fault, in general. Furthermore, innovation and legal certainty should be fostered through a comprehensive regime of safe harbours, defined quantitatively to the best extent possible. Moreover, trustworthy AI remains an important goal for AI regulation. Hence, the liability framework must specifically extend to non-discrimination cases and provide for clear rules concerning explainability (XAI). Finally, awareness for the climate effects of AI, and digital technology more broadly, is rapidly growing in computer science. In diametrical opposition to this shift in discourse and understanding, however, EU legislators have long neglected environmental sustainability in both the draft AI Act and the proposed liability regime. To counter this, I propose to jump-start sustainable AI regulation via sustainability impact assessments in the AI Act and sustainable design defects in the liability regime. In this way, the law may help spur not only fair AI and XAI, but also sustainable AI (SAI).}
}
@article{DASSHARMA2024180,
title = {Comparison of Transfer Learning Techniques for Building Energy Forecasting},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {4},
pages = {180-185},
year = {2024},
note = {12th IFAC Symposium on Fault Detection, Supervision and Safety for Technical Processes SAFEPROCESS 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.07.214},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324002982},
author = {Shansita {Das Sharma} and Austin Coursey and Marcos Quinones-Grueiro and Gautam Biswas},
keywords = {Energy systems, Forecasting, Machine learning},
abstract = {The growing demand for building energy efficiency necessitates accurate predictions of normal versus abnormal operations to understand their impact on energy management. However, integrating predictive models into practical applications faces challenges, especially in buildings with limited measurements and data. This paper explores the viability of three widely adopted transfer learning techniques in improving energy consumption models, focusing on real-world data with internal building measurements. The findings suggest that transferring information between buildings is a promising method to provide positive improvements in energy prediction models.}
}
@incollection{BARANSKI2024507,
title = {Chapter 22 - Improving color sources by plant breeding and cultivation},
editor = {Ralf Schweiggert},
booktitle = {Handbook on Natural Pigments in Food and Beverages (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {507-553},
year = {2024},
series = {Woodhead Publishing Series in Food Science, Technology and Nutrition},
isbn = {978-0-323-99608-2},
doi = {https://doi.org/10.1016/B978-0-323-99608-2.00012-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323996082000124},
author = {R. Baranski and I. Goldman and T. Nothnagel and H. Budahn and J.W. Scott},
keywords = {Anthocyanin, beetroot, , betalain, breeding, carrot, cultivation, , lycopene, , tomato},
abstract = {Profitable natural pigment production in the form of pure compounds or concentrates requires high-quality plant sources. Plant breeding is a time-consuming process of crop improvement resulting in new plant cultivars of desired characteristics that are suitable for agricultural production. Demands in the pigment industry for high-quality plant materials must be combined with the needs of farmers for high yield. Progress in cultivar development depends on many factors, including plant reproductive biology, trait heritability, existing genetic variation, agrotechnical practices, and environmental conditions. In this chapter, we discuss these and related aspects of breeding plants for improved sources of natural pigments. We have focused our chapter on three separate case studies of vegetable crops: tomato, beetroot, and carrot, representing the most important industrial sources of the three major pigment classes for carotenoids (lycopene), betalain, and anthocyanin production, respectively. These fruit and root crops differ in their life cycle and reproductive biology, and they exhibit diverse biosynthetic pathways for pigment production. The depth of knowledge on genes and biosynthetic pathways involved in pigment production varies for these crops. In addition, the biennial life cycle of carrot and beetroot makes progress in cultivar development more challenging. Advances in new molecular techniques facilitate conventional breeding for pigment production and new avenues of inquiry have been opened from genome sequencing and related approaches.}
}
@article{GONZALEZ20236079,
title = {Designing Cell Delivery Peptides and SARS-CoV-2-Targeting Small Interfering RNAs: A Comprehensive Bioinformatics Study with Generative Adversarial Network-Based Peptide Design and In Vitro Assays},
journal = {Molecular Pharmaceutics},
volume = {20},
number = {12},
pages = {6079-6089},
year = {2023},
issn = {1543-8392},
doi = {https://doi.org/10.1021/acs.molpharmaceut.3c00444},
url = {https://www.sciencedirect.com/science/article/pii/S1543839223001800},
author = {Ricardo D. González and Susana Simões and Lino Ferreira and Alexandra T. P. Carvalho},
keywords = {SARS-CoV-2, siRNAs, databases, delivery peptides, data analysis, deep learning},
abstract = {Nucleic acid technologies with designed intracellular delivery systems are some of the most promising therapies of the future. Small interfering (si)­RNAs inhibit gene expression and protein synthesis and may complement current vaccines with faster design and production. Although successful delivery remains an issue, delivery peptides may help to fill this gap. Here, we address this issue by applying bioinformatic approaches to design new putative cell delivery peptides and siRNAs for COVID-19 variants and other related viral diseases. Of the 29,880 RNA sequences analyzed, 62 were identified in silico as able to target the virus mRNA sequence, and from the 9,984 peptide sequences analyzed, 10 were selected as delivery peptides. From the latter, we further performed in vitro studies of the two best-ranked peptides and compared them with the broadly used TAT delivery peptide. One of them, seq5, displayed better internalization results with about double intensity signal compared to TAT after a 1 h incubation time in GFP-HeLa cells. This peptide has, thus, the features of a delivery peptide and could be used for cargo intracellular delivery.
}
}
@article{FEI2023107873,
title = {Semi-supervised learning method incorporating structural optimization for shear-wall structure design using small and long-tailed datasets},
journal = {Journal of Building Engineering},
volume = {79},
pages = {107873},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.107873},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223020533},
author = {Yifan Fei and Wenjie Liao and Xinzheng Lu and Ertugrul Taciroglu and Hong Guan},
keywords = {Intelligent structural design, Structural optimization, Semi-supervised learning, Long-tailed learning, Small dataset, Shear-wall structures},
abstract = {Intelligent structural design based on machine learning represents a novel structural design paradigm and has received extensive attention in recent years. However, the performance of the machine learning models is heavily dependent on the quality and quantity of training data, as the underlying approaches are inherently data-driven. Well-recognized data issues ‒ particularly data insufficiencies and long-tailed data distributions ‒ have become critical impediments in this research area. To address these data issues, this study formulates a schematic structural design task as a semi-supervised learning problem. Specifically, a semi-supervised learning method using small, long-tailed datasets is proposed in which a structural optimization method is incorporated into a self-training framework. As a practical application of the proposed method, a shear-wall layout optimization procedure is devised, based on a two-stage evaluation strategy and the previously established empirical design rules. Results of the numerical experiments indicate that the proposed method can improve the design performance on the tail data by 11.6 % and reduce the performance difference between the head and tail data by 21.3 %, compared to the conventional supervised learning method. A typical case study shows that the shear-wall layout created by the proposed method can satisfactorily resemble that by design engineers whilst meeting key code-specified requirements.}
}
@article{NAGARAJAN2024,
title = {Economics and Equity of Large Language Models: Health Care Perspective},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/64226},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124007866},
author = {Radha Nagarajan and Midori Kondo and Franz Salas and Emre Sezgin and Yuan Yao and Vanessa Klotzman and Sandip A Godambe and Naqi Khan and Alfonso Limon and Graham Stephenson and Sharief Taraman and Nephi Walton and Louis Ehwerhemuepha and Jay Pandit and Deepti Pandita and Michael Weiss and Charles Golden and Adam Gold and John Henderson and Angela Shippy and Leo Anthony Celi and William R Hogan and Eric K Oermann and Terence Sanger and Steven Martel},
keywords = {large language model, LLM, health care, economics, equity, cloud service providers, cloud, health outcome, implementation, democratization},
abstract = {Large language models (LLMs) continue to exhibit noteworthy capabilities across a spectrum of areas, including emerging proficiencies across the health care continuum. Successful LLM implementation and adoption depend on digital readiness, modern infrastructure, a trained workforce, privacy, and an ethical regulatory landscape. These factors can vary significantly across health care ecosystems, dictating the choice of a particular LLM implementation pathway. This perspective discusses 3 LLM implementation pathways—training from scratch pathway (TSP), fine-tuned pathway (FTP), and out-of-the-box pathway (OBP)—as potential onboarding points for health systems while facilitating equitable adoption. The choice of a particular pathway is governed by needs as well as affordability. Therefore, the risks, benefits, and economics of these pathways across 4 major cloud service providers (Amazon, Microsoft, Google, and Oracle) are presented. While cost comparisons, such as on-demand and spot pricing across the cloud service providers for the 3 pathways, are presented for completeness, the usefulness of managed services and cloud enterprise tools is elucidated. Managed services can complement the traditional workforce and expertise, while enterprise tools, such as federated learning, can overcome sample size challenges when implementing LLMs using health care data. Of the 3 pathways, TSP is expected to be the most resource-intensive regarding infrastructure and workforce while providing maximum customization, enhanced transparency, and performance. Because TSP trains the LLM using enterprise health care data, it is expected to harness the digital signatures of the population served by the health care system with the potential to impact outcomes. The use of pretrained models in FTP is a limitation. It may impact its performance because the training data used in the pretrained model may have hidden bias and may not necessarily be health care–related. However, FTP provides a balance between customization, cost, and performance. While OBP can be rapidly deployed, it provides minimal customization and transparency without guaranteeing long-term availability. OBP may also present challenges in interfacing seamlessly with downstream applications in health care settings with variations in pricing and use over time. Lack of customization in OBP can significantly limit its ability to impact outcomes. Finally, potential applications of LLMs in health care, including conversational artificial intelligence, chatbots, summarization, and machine translation, are highlighted. While the 3 implementation pathways discussed in this perspective have the potential to facilitate equitable adoption and democratization of LLMs, transitions between them may be necessary as the needs of health systems evolve. Understanding the economics and trade-offs of these onboarding pathways can guide their strategic adoption and demonstrate value while impacting health care outcomes favorably.}
}
@article{SHAH2024215565,
title = {Mxenes for Zn-based energy storage devices: Nano-engineering and machine learning},
journal = {Coordination Chemistry Reviews},
volume = {501},
pages = {215565},
year = {2024},
issn = {0010-8545},
doi = {https://doi.org/10.1016/j.ccr.2023.215565},
url = {https://www.sciencedirect.com/science/article/pii/S0010854523005544},
author = {Syed Shoaib Ahmad Shah and Hafiza Komal Zafar and Muhammad Sufyan Javed and Muhammad Aizaz Ud Din and Saleh S. Alarfaji and Georgia Balkourani and Manzar Sohail and Panagiotis Tsiakaras and Tayyaba Najam},
keywords = {Zn-based energy storage devices, Zn-ion battery, Zn-ion supercapacitor, MXenes, Machine learning, Zn-air battery},
abstract = {Zn-based rechargeable energy devices showed more advantages, including safety, abundance, and high volumetric/gravimetric capacities. MXenes have been evaluated as valuable emerging 2D materials due to their thermal/chemical stabilities, conductivities, flexible mechanical properties, and unique topological features. However, the recent trends in MXenes for Zn-based rechargeable energy devices have rarely been reviewed. This review article presents a comprehensive summary of the latest developments in the design and synthesis of MXene materials intended for utilization as electrodes in Zn-based energy storage devices. Specifically, the focus is on their application in Zn-ion supercapacitors, Zn-ion batteries, Zn-air batteries, and Zn-halide batteries. Firstly, we have deliberately discussed the synthesis of MXenes by summarizing the latest reported techniques but giving the weightage of the initial synthetic methods. Further, the discussion on nano-engineering of active sites revealed that surface termination followed by defect engineering is an emerging strategy to improve the performance of MXenes. The role of machine learning in the synthesis of MXenes is also summarized by establishing the structural activity relationship. In the next section and sub-sections, we have outlined the recent advances in the MXenes as electrode materials for Zn-based energy storage devices. Each section is arranged according to the synthesis strategies to clarify the structural activity relationship in each sub-section and provide a suitable basis for the researchers to design and synthesize targeted materials instead of conventional hit-and-trial methods. Finally, concluding remarks and future perspectives are discussed to offer new directions in targeted MXenes synthesis for energy storage devices.}
}
@article{GUILLAUMET2024131,
title = {The power of generative AI for CRIS systems: a new paradigm for scientific information management},
journal = {Procedia Computer Science},
volume = {249},
pages = {131-149},
year = {2024},
note = {16th International Conference on Current Research Information Systems (CRIS 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.11.057},
url = {https://www.sciencedirect.com/science/article/pii/S187705092403268X},
author = {Anna Guillaumet},
keywords = {CRIS, AI, GenerativeAI, euroCRIS, FECYT, DRIS, OpenAccess, Research, CERIF, FAIR, AI-Act, ENIA, Sandbox, Law, Regulations, Standards, ethics},
abstract = {The paper analyses the implications of the emergence of artificial intelligence (AI), especially generative AI, on current research information systems (CRIS). It reviews the recent European regulations for high-risk AI systems, the Spanish AI strategy, and the IntelComp project as use cases. The study found that the maturity of CRIS systems, coupled with the increasing complexity due to data aggregation, sets the stage for innovative AI applications. The paper proposes key domains where AI can impact and be applied in CRIS, including data management, research assessment, and advanced analytics. It also provides examples of how generative AI can be leveraged to enhance scientific information management within CRIS. The findings highlight the need to ensure the responsible and ethical development of AI technologies in the research domain.}
}
@article{MARULLI20245340,
title = {The Three Sides of the Moon LLMs in Cybersecurity: Guardians, Enablers and Targets},
journal = {Procedia Computer Science},
volume = {246},
pages = {5340-5348},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.653},
url = {https://www.sciencedirect.com/science/article/pii/S187705092402708X},
author = {Fiammetta Marulli and Pierluigi Paganini and Fabio Lancellotti},
keywords = {Large Language Models, Cyber-security, Generative Models Vulnerabilities, Prompt Injection},
abstract = {Large Language Models (LLMs) are rapidly evolving, demonstrating impressive capabilities in multimedia objects generation, ranging from text and image generation from scratch to programming code and efficient conversational agents. From the perspective of cyber-security challenges, LLMs and cyber-security are in a controversial relationship: it can be observed that LLMs, as a type of AI, play a mainfold role: that of security guardians, that of security breachs ”unaware” enablers and that of victims of cyber attacks. In fact, LLMs are able to enhance security of several tasks and applications but they are also attractive for malicious users to be exploited as means to perform novel attacks and, finally they represent challenging assets for targeting attacks. In this work, we discuss this mainfold key reading by providing a brief landscape of both the current defence applications of LLMs against cyber attacks and the currently known LLMs security vulnerabilities along with potential cyber-attacks targeting and involving LLMs. The final aim of study is intended to provide a guideline to further explore specific cyber-security scenarios involving LLMs.}
}
@article{HAKANSSON20245458,
title = {Generative AI and Large Language Models - Benefits, Drawbacks, Future and Recommendations},
journal = {Procedia Computer Science},
volume = {246},
pages = {5458-5468},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.689},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924027492},
author = {Anne Håkansson and Gloria Phillips-Wren},
keywords = {Natural Language Processing, Generative AI, Large Language Models},
abstract = {Natural language processing, with parsing and generation, has a long tradition. Parsing has been easier to perform than a generation but with generative artificial intelligence (a.k.a Gen AI) and large language models (abbr. LLMs), this has changed. Generative artificial intelligence is a type of artificial intelligence that uses a large data set to create something in the genre of that data set. It can generate different outputs ranging from texts, audio, objects, pictures, and paintings to videos, but also synthetic data. LLMs use deep learning and deep neural networks to train on large text corpora for recognizing and generating texts. These models are based on massive data sets, collected from databases and the web. They use transformer models to detect how elements in sequences relate to each other. This provides context support. Two well-known large language models are the Generative Pre-trained Transformer, GPT, used in ChatGPT and Bidirectional Encoder Representations from Transformers, BERT. Although LLMs have advantages, they have problems. This paper presents generative artificial intelligence and LLMs with benefits and drawbacks. Results from applying these models have shown that they can work well for accuracy in specificity, user personalization and human-computer communication but they may not provide acceptable, reliable and truthful results. For example, ethics, hallucinations and incorrect information, or misjudgments, are some major problems. The paper ends with future directions, research questions on LLMs, and recommendations.}
}
@article{TING202314197,
title = {Frontiers in nonviral delivery of small molecule and genetic drugs, driven by polymer chemistry and machine learning for materials informatics},
journal = {Chemical Communications},
volume = {59},
number = {96},
pages = {14197-14209},
year = {2023},
issn = {1359-7345},
doi = {https://doi.org/10.1039/d3cc04705a},
url = {https://www.sciencedirect.com/science/article/pii/S1359734523033189},
author = {Jeffrey M. Ting and Teresa Tamayo-Mendoza and Shannon R. Petersen and Jared {Van Reet} and Usman Ali Ahmed and Nathaniel J. Snell and John D. Fisher and Mitchell Stern and Felipe Oviedo},
abstract = {ABSTRACT
Materials informatics (MI) has immense potential to accelerate the pace of innovation and new product development in biotechnology. Close collaborations between skilled physical and life scientists with data scientists are being established in pursuit of leveraging MI tools in automation and artificial intelligence (AI) to predict material properties in vitro and in vivo. However, the scarcity of large, standardized, and labeled materials data for connecting structure–function relationships represents one of the largest hurdles to overcome. In this Highlight, focus is brought to emerging developments in polymer-based therapeutic delivery platforms, where teams generate large experimental datasets around specific therapeutics and successfully establish a design-to-deployment cycle of specialized nanocarriers. Three select collaborations demonstrate how custom-built polymers protect and deliver small molecules, nucleic acids, and proteins, representing ideal use-cases for machine learning to understand how molecular-level interactions impact drug stabilization and release. We conclude with our perspectives on how MI innovations in automation efficiencies and digitalization of data—coupled with fundamental insight and creativity from the polymer science community—can accelerate translation of more gene therapies into lifesaving medicines.}
}
@article{RAMONEDA2024121776,
title = {Combining piano performance dimensions for score difficulty classification},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121776},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121776},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423022789},
author = {Pedro Ramoneda and Dasaem Jeong and Vsevolod Eremenko and Nazif Can Tamer and Marius Miron and Xavier Serra},
keywords = {Performance difficulty prediction, Education technology, Music complexity, Music difficulty, Difficulty analysis, Performance difficulty, Can I play it, Music playability, Piano fingering, Expressive piano performance, Music information retrieval},
abstract = {Predicting the difficulty of playing a musical score is essential for structuring and exploring score collections. Despite its importance for music education, the automatic difficulty classification of piano scores is not yet solved, mainly due to the lack of annotated data and the subjectiveness of the annotations. This paper aims to advance the state-of-the-art in score difficulty classification with two major contributions. To address the lack of data, we present Can I Play It? (CIPI) dataset, a machine-readable piano score dataset with difficulty annotations obtained from the renowned classical music publisher Henle Verlag. The dataset is created by matching public domain scores with difficulty labels from Henle Verlag, then reviewed and corrected by an expert pianist. As a second contribution, we explore various input representations from score information to pre-trained ML models for piano fingering and expressiveness inspired by the musicology definition of performance. We show that combining the outputs of multiple classifiers performs better than the classifiers on their own, pointing to the fact that the representations capture different aspects of difficulty. In addition, we conduct numerous experiments that lay a foundation for score difficulty classification and create a basis for future research. Our best-performing model reports a 39.5% balanced accuracy and 1.1 median square error across the nine difficulty levels proposed in this study. Code, dataset, and models are made available for reproducibility.}
}
@article{BERNARD202318,
title = {Vision on metal additive manufacturing: Developments, challenges and future trends},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {47},
pages = {18-58},
year = {2023},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2023.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1755581723001256},
author = {Alain Bernard and Jean-Pierre Kruth and Jian Cao and Gisela Lanza and Stefania Bruschi and Marion Merklein and Tom Vaneker and Michael Schmidt and John W. Sutherland and Alkan Donmez and Eraldo J. {da Silva}},
keywords = {Additive Manufacturing, 3D printing, Innovative manufacturing technologies, Metal AM processes},
abstract = {Additive Manufacturing (AM) is one of the innovative technologies to fabricate components, parts, assemblies or tools in various fields of application due to its main characteristics such as direct digital manufacturing, ability to offer both internal and external complex geometries without additional cost, and the potential of varying materials at the voxel level. However, despite high anticipations, AM as a real revolution for serial production of metal components has yet to be seen, mostly due to lacks of fundamental understanding, design engineering tools, and the global robustness of the value chains. This paper aims to provide a vision about the future of metal AM based on the collective knowledge of all ten scientific and technical committees of the International Academy of Production Engineering (CIRP).}
}
@incollection{MAHDIANPARI2024281,
title = {Chapter 13 - Applying GeoAI for effective large-scale wetland monitoring},
editor = {Saurabh Prasad and Jocelyn Chanussot and Jun Li},
booktitle = {Advances in Machine Learning and Image Analysis for GeoAI},
publisher = {Elsevier},
pages = {281-313},
year = {2024},
isbn = {978-0-443-19077-3},
doi = {https://doi.org/10.1016/B978-0-44-319077-3.00018-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190773000183},
author = {Masoud Mahdianpari and Fariba Mohammadimanesh},
keywords = {Wetland mapping, Remote sensing, SAR, Optical, Cloud computing platform},
abstract = {Wetlands are crucial ecosystems, essential for biodiversity conservation and environmental processes. This chapter explores the transformative impact of remote sensing technology on wetland mapping and monitoring. Over the past two decades, satellite sensors have provided valuable data, overcoming the limitations of traditional field campaigns. A multi-source data approach, combining optical, SAR, and elevation data, has emerged as a promising technique for capturing diverse wetland characteristics. However, high data acquisition and management costs historically hindered large-scale studies. The rise of cloud computing platforms, like Google Earth Engine (GEE), has revolutionized remote sensing of wetlands. These platforms offer scalability, data accessibility, storage capabilities, flexibility, collaboration opportunities, and cost-effectiveness. Researchers can now overcome data processing challenges, scale up their studies, and expedite research outcomes. Advanced remote sensing techniques applied in wetland-rich countries like Canada and China have led to significant mapping advancements, including updated inventories and detailed wetland classes.}
}
@article{BECHELLI2024100038,
title = {AI's role in pharmaceuticals: Assisting drug design from protein interactions to drug development},
journal = {Artificial Intelligence Chemistry},
volume = {2},
number = {1},
pages = {100038},
year = {2024},
issn = {2949-7477},
doi = {https://doi.org/10.1016/j.aichem.2023.100038},
url = {https://www.sciencedirect.com/science/article/pii/S2949747723000386},
author = {Solene Bechelli and Jerome Delhommelle},
keywords = {Artificial intelligence, Property prediction, Molecular docking, Drug discovery, Deep learning},
abstract = {Developing new pharmaceutical compounds is a lengthy, costly, and intensive process. In recent years, the development of Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) models has drawn considerable interest in drug discovery. In this review, we discuss recent advances in the field and show how these methods can be leveraged to assist each stage of the drug discovery process. After discussing recent technical progress in the encoding of chemical information via fingerprinting and the emergence of graph-based and generative models, we examine all types of interactions, including drug-target interactions, protein-protein interactions, protein-peptide interactions, and nucleic acid-based interactions. Furthermore, we discuss recent advances enabled by DL models for the prediction of ADMET (Absorption, Distribution, Metabolism, Elimination, Toxicity) properties and of solubility. We also review applications that have emerged in the past two years with the development of models, for instance, on SARS-CoV-2 inhibitors and highlight outstanding challenges.}
}
@article{202356,
title = {16th Clinical Trials on Alzheimer's Disease (CTAD) Boston, MA (USA) October 24–27, 2023: Posters},
journal = {The Journal of Prevention of Alzheimer's Disease},
volume = {10},
pages = {56-240},
year = {2023},
issn = {2274-5807},
doi = {https://doi.org/10.14283/jpad.2022.130},
url = {https://www.sciencedirect.com/science/article/pii/S2274580724001365}
}
@article{WIECHA2024101129,
title = {Deep learning for nano-photonic materials – The solution to everything!?},
journal = {Current Opinion in Solid State and Materials Science},
volume = {28},
pages = {101129},
year = {2024},
issn = {1359-0286},
doi = {https://doi.org/10.1016/j.cossms.2023.101129},
url = {https://www.sciencedirect.com/science/article/pii/S1359028623000748},
author = {Peter R. Wiecha},
abstract = {Deep learning is currently being hyped as an almost magical tool for solving all kinds of difficult problems that computers have not been able to solve in the past. Particularly in the fields of computer vision and natural language processing, spectacular results have been achieved. The hype has now infiltrated several scientific communities. In (nano-) photonics, researchers are trying to apply deep learning to all kinds of forward and inverse problems. A particularly challenging problem is for instance the rational design of nanophotonic materials and devices. In this opinion article, I will first discuss the public expectations of deep learning and give an overview of the quite different scales at which actors from industry and research are operating their deep learning models. I then examine the weaknesses and dangers associated with deep learning. Finally, I’ll discuss the key strengths that make this new set of statistical methods so attractive, and review a personal selection of opportunities that shouldn’t be missed in the current developments.}
}
@article{CHEN2023103504,
title = {FedRight: An effective model copyright protection for federated learning},
journal = {Computers & Security},
volume = {135},
pages = {103504},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103504},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823004145},
author = {Jinyin Chen and Mingjun Li and Yao Cheng and Haibin Zheng},
keywords = {Copyright protection, Federated learning, Model fingerprints, Robustness, Black-box fingerprints},
abstract = {Federated learning (FL), an effective distributed machine learning framework, implements model training and meanwhile protects local data privacy. It has been applied to a broad variety of practical areas due to their great performance and appreciable profits. Who really owns the model, and how to protect the copyright has become a real problem. Intuitively, the existing property rights protection methods in centralized scenarios (e.g., watermark embedding and model fingerprints) are possible solutions for FL. But they are still challenged by the distributed nature of FL in aspects of the no data sharing, parameter aggregation, and federated training settings. For the first time we formalize the problem of copyright protection for FL, and propose FedRight to protect model copyright based on model fingerprints, i.e., extracting model features by generating adversarial examples as model fingerprints. FedRight outperforms previous works in four key aspects: (i) Validity - it extracts model features to generate transferable fingerprints to train a detector to verify the copyright of the model. (ii) Fidelity - it is with imperceptible impact on the federated training, thus promises good main task performance. (iii) Robustness - it is empirically robust against malicious attack on copyright protection, i.e., fine-tuning, model pruning and adaptive attacks. (iv) Black-box - it is valid in black-box forensic scenario where only application programming interface calls to the model are available. Extensive evaluations across 3 datasets and 9 model structures demonstrate FedRight's superior fidelity, validity and robustness.}
}
@article{CAIN202461,
title = {Low-dimensional representations of genome-scale metabolism},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {23},
pages = {61-66},
year = {2024},
note = {10th IFAC Conference on Foundations of Systems Biology in Engineering FOSBE 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324017579},
author = {Samuel Cain and Charlotte Merzbacher and Diego A. Oyarzún},
keywords = {Variational autoencoders, deep learning, genome-scale metabolic models},
abstract = {Cellular metabolism is a highly interconnected network with thousands of reactions that convert nutrients into the molecular building blocks of life. Metabolic connectivity varies greatly with cellular context and environmental conditions, and it remains a challenge to compare genome-scale metabolism across cell types because of the high dimensionality of the reaction flux space. Here, we employ self-supervised learning and genome-scale metabolic models to compress the flux space into low-dimensional representations that preserve structure across cell types. We trained variational autoencoders (VAEs) on large fluxomic data (N = 800, 000) sampled from patient-derived models for various cancer cell types. The VAE embeddings have an improved ability to distinguish cell types than the uncompressed fluxomic data, and sufficient predictive power to classify cell types with high accuracy. We tested the ability of these classifiers to assign cell type identities to unlabelled patient-derived metabolic models not employed during VAE training. We further employed the pre-trained VAE to embed another 38 cell types and trained multilabel classifiers that display promising generalization performance. Our approach distils the metabolic space into a semantically rich vector that can be used as a foundation for predictive modelling, clustering or comparing metabolic capabilities across organisms.}
}
@incollection{2024I1,
title = {Index},
editor = {John R. Vacca},
booktitle = {Computer and Information Security Handbook (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {I1-I32},
year = {2024},
isbn = {978-0-443-13223-0},
doi = {https://doi.org/10.1016/B978-0-443-13223-0.20001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443132230200015}
}
@article{VARMA2024101037,
title = {Artificial intelligence and performance management},
journal = {Organizational Dynamics},
volume = {53},
number = {1},
pages = {101037},
year = {2024},
issn = {0090-2616},
doi = {https://doi.org/10.1016/j.orgdyn.2024.101037},
url = {https://www.sciencedirect.com/science/article/pii/S009026162400010X},
author = {Arup Varma and Vijay Pereira and Parth Patel},
keywords = {Artificial Intelligence, AI, Performance management systems, PMS},
abstract = {Artificial Intelligence (AI) enabled tools have increasingly becoming popular in our societies and are increasingly being used by students and practitioners, among others. Within corporations, numerous different applications have been identified where AI-enabled tools have been applied with different levels of success. In this article, we explore the pros and cons of using AI in performance management (PM). We draw upon the practitioner literature to summarize the current status of AI and AI-enabled tools. We also interviewed 8 HR professionals from around the world to learn about their experience(s) with the tools and to gain an insight into the future. In doing so, we explore the various components of performance management systems (PMS) and discuss how each might be impacted by the use of AI. Finally, we discuss the pros and cons of such usage and make recommendations for organizations that are considering using AI or AI enabled tools in their PMSs.}
}
@article{HE2024111277,
title = {Progressive normalizing flow with learnable spectrum transform for style transfer},
journal = {Knowledge-Based Systems},
volume = {284},
pages = {111277},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111277},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123010250},
author = {Zixuan He and Guoheng Huang and Xiaochen Yuan and Guo Zhong and Chi-Man Pun and Yiwen Zeng},
keywords = {Style transfer, Reversible neural network, Neural flow, Feature spectrum, Progressive stylization, Feature decomposition},
abstract = {Most current style transfer models are designed as encoder–decoder structures. Some encoding operations, such as downsampling and pooling, cause a loss of image details. If the encoder and decoder are not compatible, it can also introduce distortion. Reversible neural networks have demonstrated their superior power in lossless projection. However, since the inputs and outputs of neural flows are holistic features, merely the high-level features can be utilized for image generation through reverse inference. These high-level features emphasize the image style more, leading to the generated results easily losing content details and producing abstract colors. To address the above issues, we propose LSTFlow, the first progressive reversible neural network capable of feature decomposition. First, LSTFlow incorporates our proposed reversible Learnable Spectrum Transform (LST), which can dynamically decompose the feature into feature spectrum and recover them losslessly. LSTFlow can retain more details by enabling multi-level features to be fused in backward inference. Second, we propose a Progressive Flow Stylization Strategy (PFSS) to balance the model’s emphasis between content and style and enhance the color perception. Forward inference based PFSS is carried out progressively, while the backward inference focuses on progressive generation. To demonstrate the effectiveness of our proposed method, we conducted comparative experiments with seven other state-of-the-art algorithms. The stylized effects are evaluated in terms of visual effects and quantitative indicators. The experiments show that the lightest LSTFlow performs the best in SSIM, Color Entropy, Color Uniformity and FID indicators and outperforms state-of-the-art methods.}
}
@article{2024A15,
title = {Instructions for authors},
journal = {Gastrointestinal Endoscopy},
volume = {99},
number = {1},
pages = {A15-A21},
year = {2024},
issn = {0016-5107},
doi = {https://doi.org/10.1016/j.gie.2023.11.046},
url = {https://www.sciencedirect.com/science/article/pii/S001651072303122X}
}
@article{LI2023104531,
title = {Technical/Algorithm, Stakeholder, and Society (TASS) barriers to the application of artificial intelligence in medicine: A systematic review},
journal = {Journal of Biomedical Informatics},
volume = {147},
pages = {104531},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104531},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423002526},
author = {Linda T. Li and Lauren C. Haley and Alexandra K. Boyd and Elmer V. Bernstam},
keywords = {Artificial intelligence, Augmented intelligence, Machine learning, Predictive analytics, Barriers, Implementation},
abstract = {Introduction
The use of artificial intelligence (AI), particularly machine learning and predictive analytics, has shown great promise in health care. Despite its strong potential, there has been limited use in health care settings. In this systematic review, we aim to determine the main barriers to successful implementation of AI in healthcare and discuss potential ways to overcome these challenges.
Methods
We conducted a literature search in PubMed (1/1/2001–1/1/2023). The search was restricted to publications in the English language, and human study subjects. We excluded articles that did not discuss AI, machine learning, predictive analytics, and barriers to the use of these techniques in health care. Using grounded theory methodology, we abstracted concepts to identify major barriers to AI use in medicine.
Results
We identified a total of 2,382 articles. After reviewing the 306 included papers, we developed 19 major themes, which we categorized into three levels: the Technical/Algorithm, Stakeholder, and Social levels (TASS). These themes included: Lack of Explainability, Need for Validation Protocols, Need for Standards for Interoperability, Need for Reporting Guidelines, Need for Standardization of Performance Metrics, Lack of Plan for Updating Algorithm, Job Loss, Skills Loss, Workflow Challenges, Loss of Patient Autonomy and Consent, Disturbing the Patient-Clinician Relationship, Lack of Trust in AI, Logistical Challenges, Lack of strategic plan, Lack of Cost-effectiveness Analysis and Proof of Efficacy, Privacy, Liability, Bias and Social Justice, and Education.
Conclusion
We identified 19 major barriers to the use of AI in healthcare and categorized them into three levels: the Technical/Algorithm, Stakeholder, and Social levels (TASS). Future studies should expand on barriers in pediatric care and focus on developing clearly defined protocols to overcome these barriers.}
}
@article{LIU2023102872,
title = {Technology opportunity analysis using hierarchical semantic networks and dual link prediction},
journal = {Technovation},
volume = {128},
pages = {102872},
year = {2023},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2023.102872},
url = {https://www.sciencedirect.com/science/article/pii/S0166497223001839},
author = {Zhenfeng Liu and Jian Feng and Lorna Uden},
keywords = {Technology opportunity analysis, Patent analysis, Hierarchical semantic network, Link prediction},
abstract = {Technology opportunity analysis using network analysis and link prediction has attracted the interest of both academia and industry. However, there are several unresolved issues with existing research, such as a lack of semantic relationships between nodes in a single-layer network, analyzing current technology trends rather than predicting future technology developments based on existing edges in a semantic network, and ignoring evaluation criteria for technology opportunity identification based on a single link prediction. This study proposes a new systematic methodology to address these issues and identify technology opportunities using a hierarchical semantic network and dual link prediction. The proposed methodology consists of three modules: 1) constructing the hierarchical semantic network based on SAO structures extracted from patents; 2) identifying technology opportunities in this semantic network through probabilistic-based link prediction; and 3) evaluating these opportunities via similarity-based link prediction. The viability and usefulness of the proposed methodology is proved by empirical analysis of the exploitation technology in the coal seam gas (CSG) industry. The results show that the hierarchical semantic network, including semantic and co-word relationships, can improve prediction accuracy. The dual link prediction can not only automatically identify technology opportunities with semantics, but also evaluate them to narrow down the problem-solving according to the novelty criteria. This study represents a contribution to existing research on technology opportunity analysis by integrating the hierarchical semantic network and dual link prediction.}
}
@article{BARGHOUT2023103007,
title = {Advances in generative modeling methods and datasets to design novel enzymes for renewable chemicals and fuels},
journal = {Current Opinion in Biotechnology},
volume = {84},
pages = {103007},
year = {2023},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2023.103007},
url = {https://www.sciencedirect.com/science/article/pii/S0958166923001179},
author = {Rana A Barghout and Zhiqing Xu and Siddharth Betala and Radhakrishnan Mahadevan},
abstract = {Biotechnology has revolutionized the development of sustainable energy sources by harnessing biomass as a feedstock for energy production. However, challenges such as recalcitrant feedstocks and inefficient metabolic pathways hinder the large-scale integration of renewable energy systems. Enzyme engineering has emerged as a powerful tool to address these challenges by enhancing enzyme activity, specificity, and stability. Generative machine learning (ML) models have shown great promise in accelerating protein design, allowing for the generation of novel protein sequences with desired properties by navigating vast spaces. This review paper aims to summarize the state of the art in generative models for protein design and how they can be applied to bioenergy applications, including the underlying architectures and training strategies. Additionally, it highlights the importance of high-quality datasets for training and evaluating generative models, organizes available datasets for generative protein design, and discusses the potential of applying generative models to strain design for bioenergy production.}
}
@article{PRICOP20241855,
title = {Music Generation with Machine Learning and Deep Neural Networks},
journal = {Procedia Computer Science},
volume = {246},
pages = {1855-1864},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.692},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924027522},
author = {Tudor-Constantin Pricop and Adrian Iftene},
keywords = {Deep neural networks, machine learning algorithms, variational autoencoders, long short-term memory networks, transformer},
abstract = {This paper explores advanced music generation through hybrid models combining deep neural networks, machine learning algorithms, variational autoencoders (VAEs), long short-term memory (LSTM) networks, and Transformers to create diverse and engaging musical experiences. Our research aims to advance the understanding of music’s impact on our lives and develop methodologies to create diverse and engaging musical experiences tailored to individual preferences. We begin by extracting relevant features from a large and diverse collection of music samples from different genres. These features, encompassing spectral properties, rhythmic patterns, and tonal characteristics, serve as the foundation for our generation models. To generate music, we explore the potential of VAEs, LSTMs, and Transformers, each offering unique capabilities for handling different aspects of the task. VAEs are employed to learn a continuous latent space representation of the music samples, enabling the generation of novel compositions within a specified genre. LSTMs and Transformers, on the other hand, are used to model the temporal dependencies and intricate patterns inherent in music. While not claiming state-of-the-art performance, our approach demonstrates promising outcomes in generation tasks, showcasing its potential to enhance music-related applications such as recommendation systems and creative tools for composers.}
}
@article{WALKER20231768,
title = {Extracting structured seed-mediated gold nanorod growth procedures from scientific text with LLMs††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d3dd00019b},
journal = {Digital Discovery},
volume = {2},
number = {6},
pages = {1768-1782},
year = {2023},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00019b},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X23001249},
author = {Nicholas Walker and Sanghoon Lee and John Dagdelen and Kevin Cruse and Samuel Gleason and Alexander Dunn and Gerbrand Ceder and A. Paul Alivisatos and Kristin A. Persson and Anubhav Jain},
abstract = {ABSTRACT
Although gold nanorods have been the subject of much research, the pathways for controlling their shape and thereby their optical properties remain largely heuristically understood. Although it is apparent that the simultaneous presence of and interaction between various reagents during synthesis control these properties, computational and experimental approaches for exploring the synthesis space can be either intractable or too time-consuming in practice. This motivates an alternative approach leveraging the wealth of synthesis information already embedded in the body of scientific literature by developing tools to extract relevant structured data in an automated, high-throughput manner. To that end, we present an approach using the powerful GPT-3 language model to extract structured multi-step seed-mediated growth procedures and outcomes for gold nanorods from unstructured scientific text. GPT-3 prompt completions are fine-tuned to predict synthesis templates in the form of JSON documents from unstructured text input with an overall accuracy of 86% aggregated by entities and 76% aggregated by papers. The performance is notable, considering the model is performing simultaneous entity recognition and relation extraction. We present a dataset of 11 644 entities extracted from 1137 papers, resulting in 268 papers with at least one complete seed-mediated gold nanorod growth procedure and outcome for a total of 332 complete procedures.}
}
@article{YAN2024108062,
title = {Optimal design feature of computer-assisted reading instruction for students with reading difficulties? A Bayesian network meta-analysis},
journal = {Computers in Human Behavior},
volume = {152},
pages = {108062},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.108062},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223004132},
author = {Xueye Yan and Peng Peng and Yuting Liu},
keywords = {Computer-assisted instruction, Design feature, Reading difficulty, Bayesian network meta-analysis},
abstract = {Mayer (2017, 2020) proposed three major design features of computer-assisted instructions (CAI) within the Cognitive Theory of Multimedia Learning: reducing extraneous processing (i.e., excluding irrelevant content), managing essential processing (i.e., focusing on the complex but essential learning materials), and fostering generative processing (i.e., maximizing learner motivation with multimedia features). No study so far has systematically evaluated each design feature, or their combinations for students with reading difficulties. The present study is the first meta-analysis to investigate the optimal design features of CAIs for students with reading difficulties on their decoding/word reading and reading comprehension performance. A total of 49 experimental studies were reviewed with Bayesian network meta-analysis. Results showed that CAI programs with features that reduced extraneous processing were the most effective (g = 0.65) followed by programs that combined reducing extraneous processing, managing essential processing, and fostering generative processing (g = 0.29) as well as programs that combined reducing extraneous processing and managing essential processing (g = 0.27). CAI programs yielded larger effects when they were designed for younger learners with reading difficulties compared to older learners. No significant moderation effects were observed for students' reading difficulty status, reading content, reading outcomes, instruction dosages, control group types, measures, and fidelity checks. These findings suggest that different combinations of design features of CAI programs may generate different effects. Lowering students’ cognitive loads by excluding irrelevant content may be the foundation for designing effective computer instructions for students with reading difficulties.}
}
@article{HEMMATIAN202469,
title = {The utilitarian brain: Moving beyond the Free Energy Principle},
journal = {Cortex},
volume = {170},
pages = {69-79},
year = {2024},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2023.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0010945223003076},
author = {Babak Hemmatian and Lav R. Varshney and Frederick Pi and Aron K. Barbey},
keywords = {Free Energy Principle, Subjective utility, Extended cognition, Decision-making, Cognitive neuroscience, Bayesian Brain Hypothesis},
abstract = {The Free Energy Principle (FEP) is a normative computational framework for iterative reduction of prediction error and uncertainty through perception–intervention cycles that has been presented as a potential unifying theory of all brain functions (Friston, 2006). Any theory hoping to unify the brain sciences must be able to explain the mechanisms of decision-making, an important cognitive faculty, without the addition of independent, irreducible notions. This challenge has been accepted by several proponents of the FEP (Friston, 2010; Gershman, 2019). We evaluate attempts to reduce decision-making to the FEP, using Lucas' (2005) meta-theory of the brain's contextual constraints as a guidepost. We find reductive variants of the FEP for decision-making unable to explain behavior in certain types of diagnostic, predictive, and multi-armed bandit tasks. We trace the shortcomings to the core theory's lack of an adequate notion of subjective preference or “utility”, a concept central to decision-making and grounded in the brain's biological reality. We argue that any attempts to fully reduce utility to the FEP would require unrealistic assumptions, making the principle an unlikely candidate for unifying brain science. We suggest that researchers instead attempt to identify contexts in which either informational or independent reward constraints predominate, delimiting the FEP's area of applicability. To encourage this type of research, we propose a two-factor formal framework that can subsume any FEP model and allows experimenters to compare the contributions of informational versus reward constraints to behavior.}
}
@article{LIU2024509,
title = {Integration of data science with product design towards data-driven design},
journal = {CIRP Annals},
volume = {73},
number = {2},
pages = {509-532},
year = {2024},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2024.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0007850624001252},
author = {Ang Liu and Stephen Lu and Fei Tao and Nabil Anwer},
keywords = {Product design, Data science, Data-driven design},
abstract = {This paper aims to investigate the scientific integration of data science with product design towards data-driven design (D3). Data science has potential to facilitate design decision-making through insight extraction, predictive analytics, and automatic decisions. A systematic scoping review is conduced to converge various D3 applications in four dimensions: the design dimension about design operations, the data dimension about popular data sources and common data-related challenges, the method dimension about the methodological foundations, and the social/ethical dimension about social/ethical considerations and implications. Based on the state-of-the-art, this paper also highlights potential future research avenues in this dynamic field.}
}
@article{CORREA2023100857,
title = {Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance},
journal = {Patterns},
volume = {4},
number = {10},
pages = {100857},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100857},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923002416},
author = {Nicholas Kluge Corrêa and Camila Galvão and James William Santos and Carolina {Del Pino} and Edson Pontes Pinto and Camila Barbosa and Diogo Massmann and Rodrigo Mambrini and Luiza Galvão and Edmund Terem and Nythamar {de Oliveira}},
keywords = {artificial intelligence, machine learning, ethics, ethics of artificial intelligence, guidelines},
abstract = {Summary
The utilization of artificial intelligence (AI) applications has experienced tremendous growth in recent years, bringing forth numerous benefits and conveniences. However, this expansion has also provoked ethical concerns, such as privacy breaches, algorithmic discrimination, security and reliability issues, transparency, and other unintended consequences. To determine whether a global consensus exists regarding the ethical principles that should govern AI applications and to contribute to the formation of future regulations, this paper conducts a meta-analysis of 200 governance policies and ethical guidelines for AI usage published by public bodies, academic institutions, private companies, and civil society organizations worldwide. We identified at least 17 resonating principles prevalent in the policies and guidelines of our dataset, released as an open source database and tool. We present the limitations of performing a global-scale analysis study paired with a critical analysis of our findings, presenting areas of consensus that should be incorporated into future regulatory efforts.}
}
@article{KEEGAN2023228,
title = {Examining the dark force consequences of AI as a new actor in B2B relationships},
journal = {Industrial Marketing Management},
volume = {115},
pages = {228-239},
year = {2023},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2023.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0019850123001918},
author = {Brendan James Keegan and Sophie Iredale and Peter Naudé},
keywords = {Artificial intelligence, B2B relationships, Dark forces, Tensions, Dehumanization},
abstract = {Artificial intelligence (AI) in industrial marketing has seen significant research attention through various theoretical lenses with an emerging thread examining the dark side effects of AI. Thirty-four semi-structured interviews were conducted with buyers and suppliers of AI marketing solutions to investigate the consequences of AI ‘dark forces’ on B2B relationships. We posit AI as a new actor that has blurred the lines of the actors-resources-activities model. Findings show AI is now considered a new actor within B2B networks wielding dark force consequences such as algorithmic gatekeeping, which initiates dehumanization effects. In addition, AI is reliant on access to datasets which drives up resource costs. A lack of accountability of AI marketing solutions leads to opportunistic behaviours compromising actor relationships. Our conceptual model maps our understanding of the dark force consequences underpinning theoretical and managerial implications and recommendations for increased awareness and mitigation of dark forces.}
}
@article{ALI2024123076,
title = {The effects of artificial intelligence applications in educational settings: Challenges and strategies},
journal = {Technological Forecasting and Social Change},
volume = {199},
pages = {123076},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.123076},
url = {https://www.sciencedirect.com/science/article/pii/S0040162523007618},
author = {Omar Ali and Peter A. Murray and Mujtaba Momin and Yogesh K. Dwivedi and Tegwen Malik},
keywords = {ChatGPT, Artificial intelligence, Challenges, Strategies, Education sector},
abstract = {With the continuous intervention of AI tools in the education sector, new research is required to evaluate the viability and feasibility of extant AI platforms to inform various pedagogical methods of instruction. The current manuscript explores the cumulative published literature to date in order to evaluate the key challenges that influence the implications of adopting AI models in the Education Sector. The researchers' present works both in favour and against AI-based applications within the Academic milieu. A total of 69 articles from a 618-article population was selected from diverse academic journals between 2018 and 2023. After a careful review of selected articles, the manuscript presents a classification structure based on five distinct dimensions: user, operational, environmental, technological, and ethical challenges. The current review recommends the use of ChatGPT as a complementary teaching-learning aid including the need to afford customized and optimized versions of the tool for the teaching fraternity. The study addresses an important knowledge gap as to how AI models enhance knowledge within educational settings. For instance, the review discusses interalia a range of AI-related effects on learning from the need for creative prompts, training on diverse datasets and genres, incorporation of human input and data confidentiality and elimination of bias. The study concludes by recommending strategic solutions to the emerging challenges identified while summarizing ways to encourage wider adoption of ChatGPT and other AI tools within the education sector. The insights presented in this review can act as a reference for policymakers, teachers, technology experts and stakeholders, and facilitate the means for wider adoption of ChatGPT in the Education sector more generally. Moreover, the review provides an important foundation for future research.}
}
@article{REITER2024104905,
title = {Managing multi-tiered innovation ecosystems},
journal = {Research Policy},
volume = {53},
number = {1},
pages = {104905},
year = {2024},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2023.104905},
url = {https://www.sciencedirect.com/science/article/pii/S0048733323001890},
author = {Andreas Reiter and Joachim Stonig and Karolin Frankenberger},
keywords = {Ecosystems, Governance, Open innovation, Complementors, Multiple-case study},
abstract = {We study how orchestrators of innovation ecosystems govern their relationships with multiple heterogeneous complementors that differ in their positions and activities that contribute to an integrated value proposition. Prior research has focused on ecosystem-wide and complementor-specific governance, each not suited to adequately address the challenges of this context. Building on a multiple-case study in the financial services sector, our findings demonstrate that orchestrators create distinctly governed tiers of complementors based on the domains of uncertainty underlying their ecosystem blueprint. The organizational form and coordination mechanisms of these tiers reflect the convergent or explorative role of complementors in the core or the periphery of the blueprint. We contribute to ecosystem governance by introducing complementor tiers as a governance approach that blends elements of ecosystem-wide and complementor-specific governance and by linking governance choices to uncertainty and complementor roles. Furthermore, we discuss the potential for future research at the intersection of ecosystem and open innovation governance.}
}
@article{XIE2024231,
title = {Highly Interactive Self-Supervised Learning for Multi-Modal Trajectory Prediction⁎⁎This work was supported by the National Key R&D Program of China under Grant 2022ZD0162200, the National Natural Science Foundation of China under Grant 62271485, and the Fundamental Research Funds for the Central Universities, CHD, under Grant 300102343513},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {10},
pages = {231-236},
year = {2024},
note = {17th IFAC Symposium on Control of Transportation Systems CTS 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.07.345},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324004294},
author = {Wenda Xie and Yahui Liu and Hongxia Zhao and Chao Guo and Xingyuan Dai and Yisheng Lv},
keywords = {Automatic driving, Self-supervised learning, Trajectory prediction, Deep learning, Intelligent Transportation},
abstract = {To ensure the safety of autonomous vehicles, trajectory prediction is critical as it enables vehicles to anticipate the movements of surrounding agents, thereby facilitating the planning of secure and strategic driving routes. However, striking a trade-of between predictive accuracy and training costs has always been an intricate challenge. This paper introduces a groundbreaking framework for trajectory prediction known as Highly Interactive Self-Supervised Learning (HI-SSL), a methodology based on self-supervised learning (SSL) that has yet to be thoroughly investigated in the realm of trajectory prediction. The cornerstone of the aforementioned framework is Interactive Masking, which leverages a novel trajectory masking strategy facilitating self-supervised learning tasks that not only enhance prediction accuracy but also eliminate the need for manual annotations. Experiments conducted on the Argoverse motion forecasting dataset demonstrate that our approach achieves competitive performance to prior methods that depend on supervised learning without additional annotation costs.}
}
@article{PULADI202478,
title = {The impact and opportunities of large language models like ChatGPT in oral and maxillofacial surgery: a narrative review},
journal = {International Journal of Oral and Maxillofacial Surgery},
volume = {53},
number = {1},
pages = {78-88},
year = {2024},
issn = {0901-5027},
doi = {https://doi.org/10.1016/j.ijom.2023.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0901502723002163},
author = {B. Puladi and C. Gsaxner and J. Kleesiek and F. Hölzle and R. Röhrig and J. Egger},
keywords = {ChatGPT, Artificial intelligence, Oral and Maxillofacial Surgery, Review},
abstract = {Since its release at the end of 2022, the social response to ChatGPT, a large language model (LLM), has been huge, as it has revolutionized the way we communicate with computers. This review was performed to describe the technical background of LLMs and to provide a review of the current literature on LLMs in the field of oral and maxillofacial surgery (OMS). The PubMed, Scopus, and Web of Science databases were searched for LLMs and OMS. Adjacent surgical disciplines were included to cover the entire literature, and records from Google Scholar and medRxiv were added. Out of the 57 records identified, 37 were included; 31 (84%) were related to GPT-3.5, four (11%) to GPT-4, and two (5%) to both. Current research on LLMs is mainly limited to research and scientific writing, patient information/communication, and medical education. Classic OMS diseases are underrepresented. The current literature related to LLMs in OMS has a limited evidence level. There is a need to investigate the use of LLMs scientifically and systematically in the core areas of OMS. Although LLMs are likely to add value outside the operating room, the use of LLMs raises ethical and medical regulatory issues that must first be addressed.}
}
@incollection{SAGGI2024269,
title = {Chapter Eight - Federated quantum machine learning for drug discovery and healthcare},
editor = {Angela K. Wilson},
series = {Annual Reports in Computational Chemistry},
publisher = {Elsevier},
volume = {20},
pages = {269-322},
year = {2024},
issn = {1574-1400},
doi = {https://doi.org/10.1016/bs.arcc.2024.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1574140024000124},
author = {Mandeep Kaur Saggi and Amandeep Singh Bhatia and Sabre Kais},
keywords = {Index Terms—Quantum Computing, Quantum Computer, Quantum Machine Learning, Quantum Neural Networks, Quantum Boltzmann Machine, Machine Learning, Neural Networks, Quantum Optimization, Quantum Federated Learning, Qubit, Superposition, Quantum Correlation, Entanglement, Quantum Gate, Quantum Circuit, Quantum Noise, Noisy Intermediate-Scale Quantum, Fault-Tolerant Quantum Computing, Parametrized Quantum Circuits, Quantum Annealing, Quantum Kernels},
abstract = {Integrating quantum computing with traditional high-performance systems holds great promise for revolutionizing preclinical drug discovery and healthcare. This combination can improve data processing capabilities, accelerate research timelines, and optimize the analysis of complex biological data, ultimately leading to more efficient and effective drug development processes. Quantum advancements can help streamline the drug development process, ultimately lowering both the time and costs involved. This chapter provides an overview of the recent advancements in Quantum Machine Learning (QML) and Federated Learning (FL), highlighting their transformative potential in preclinical drug discovery and healthcare. As the demand for innovative solutions in drug development and patient care increases, integrating quantum computing capabilities offers a new paradigm for tackling complex biomedical challenges. Additionally, the chapter discusses the role of FL and Quantum Federated Learning (QFL) in facilitating collaborative research while preserving data privacy, allowing multiple pharmaceutical and healthcare sectors to contribute to model training without sharing sensitive patient information. In the pursuit of innovative solutions for quality assurance within the pharmaceutical sector, we investigate the implementation of quantum federated learning utilizing variational quantum circuits for pill classification. Our focus is on effectively distinguishing between defective and non-defective pills, particularly in the context of unbalanced data distributions. Additionally, we explore the potential of integrating genomic sequencing within quantum federated settings to enhance classification tasks in healthcare, aiming to leverage the strengths of quantum computing for improved accuracy and efficiency in critical quality assurance processes. In addition, we provide a review of existing frameworks for current federated learning research. This chapter highlights the synergy between quantum technologies and federated learning approaches, showcasing key studies and methodologies that advance preclinical research, particularly in clinical trial outcome prediction, trial matching, and site selection, ultimately enhancing healthcare outcomes. By providing a thorough analysis of the current research landscape, this chapter aims to illuminate the challenges and opportunities within this promising field, fostering a deeper understanding of how these innovative techniques can revolutionize drug discovery and healthcare practices.}
}
@article{SHIVAMURTHY20241,
title = {Natural Language Processing based Auto Generation of Proof Obligations for Formal Verification of Control Requirements in Safety-Critical Systems},
journal = {IFAC-PapersOnLine},
volume = {57},
pages = {1-6},
year = {2024},
note = {8th Conference on Advances in Control and Optimization of Dynamical Systems ACODS 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324000016},
author = {Jagadish Shivamurthy and Deepti Vidyarthi and Tarun Uppal},
keywords = {Safety-critical systems, flight control system, formal verification, proof obligations, NLP, Propositional logic, LTL},
abstract = {Formal verification uses mathematically rigorous techniques to establish the correctness of an algorithm or model. While traditional testing shows the presence of defects, it cannot guarantee the absence of defects in a design. Formal verification, on the other hand, can guarantee the absence of defects concerning a set of desirable properties, or provide counter-examples where the properties do not hold. Despite its value, it is not commonly used due to various reasons. This paper discusses two major reasons and proposes solutions for them. The first reason is the difficulty in deriving the proof obligations, the properties to be proved, from the textual requirements. The second hindrance is the additional effort in developing the infrastructure for formal verification. The paper proposes a Natural Language Processing (NLP) based approach to automatically suggest the proof obligations from the textual requirements to remove the first hindrance. They are expressed in propositional, Linear-time Temporal Logic (LTL), and a few customized expressions. The paper also provides methods for converting these obligations into verification subsystems which enable model checking, a method of formal verification to be invoked on the design model, thereby alleviating the second hindrance. The approach and methods are explained in the context of a flight control system's fault handling and safety requirements.}
}
@article{SACHAN2024107666,
title = {Blockchain-based auditing of legal decisions supported by explainable AI and generative AI tools},
journal = {Engineering Applications of Artificial Intelligence},
volume = {129},
pages = {107666},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107666},
url = {https://www.sciencedirect.com/science/article/pii/S095219762301850X},
author = {Swati Sachan and Xi {Liu (Lisa)}},
keywords = {Legal, Law, Explainable AI, Blockchain, Generative AI, Responsible AI},
abstract = {Generative AI tools powered by Large Language Models (LLMs) have demonstrated advanced capabilities in understanding and articulating legal facts closer to the level of legal practitioners. However, scholars hold contrasting views on the reliability of the reasoning behind a decision derived from LLMs due to its black-box nature. Law firms are vigilant in recognizing the potential risks of violating confidentiality and inappropriate exposure of sensitive legal data through the prompt sent to Generative AI. This research attempts to find an equilibrium between responsible usage and control of human legal professionals over content produced by Generative AI through regular audits. It investigates the potential of Generative AI in drafting correspondence for pre-litigation decisions derived from an eXplainable AI (XAI) algorithm. This research presents an end-to-end process of designing the architecture and methodology for a blockchain-based auditing system. It detects unauthorized alterations of data repositories containing the decisions by an XAI model and automated textual explanation by Generative AI. The automated auditing by blockchain facilitates responsible usage of AI technologies and reduces discrepancies in tracing the accountability of adversarial decisions. It conceptualizes the two algorithms. First, strategic on-chain (within blockchain) and off-chain (outside blockchain) data storage in compliance with the data protection laws and critical requirements of stakeholders in a legal firm. Second, auditing by comparison of the unique signature as Merkle roots of files stored off-chain with their immutable blockchain counterpart. A case study on liability cases under tort law demonstrates the system implementation results.}
}
@incollection{ZAHM2024241,
title = {Chapter six - Macrosystems in motion, representation, value, emotion, and neuropsychiatric illness},
editor = {Daniel S. Zahm},
booktitle = {Anatomy of Neuropsychiatry (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {241-301},
year = {2024},
isbn = {978-0-443-15596-3},
doi = {https://doi.org/10.1016/B978-0-443-15596-3.00006-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443155963000060},
author = {Daniel S. Zahm},
keywords = {Active inference, Categorization, Image, Inferential processing, Locomotion, Movement, Predictive coding, Predictive processing, Representation, Valuation, Value assessment},
abstract = {Neuroanatomical organization of basal forebrain macrosystems as developed in the preceding chapters is integrated with current models of mechanisms thought to underlie locomotion, intentional movements. A brief epistemological and historical background is then given as an introduction to the sensorimotor origins, substrates, and manipulation of CNS images, a.k.a., representations. The unsatisfactorily introspective nature of much of the inquiry into mental representations but general inadequacy of conventional physiological approaches to address them experimentally are discussed. Contributions of macrosystems to subjective valuation and consequent macrosystem interactions with cerebral cortex in the genesis of emotion are then considered followed by an analysis of vulnerabilities in these neuroanatomical and functional relationships that contribute to neuropsychiatric illness.}
}
@article{HUANG20231710,
title = {ChemDataWriter: a transformer-based toolkit for auto-generating books that summarise research††Electronic supplementary information (ESI) available: An example book that ChemDataWriter auto-generates: literature summary of recent research about Na-ion, Li–S, and Li–O2 battery materials. Results from applying a plagiarism checker to chapter 4 of this example book. See DOI: https://doi.org/10.1039/d3dd00159h},
journal = {Digital Discovery},
volume = {2},
number = {6},
pages = {1710-1720},
year = {2023},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00159h},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X23001237},
author = {Shu Huang and Jacqueline M. Cole},
abstract = {ABSTRACT
Since the number of scientific papers has grown substantially over recent years, scientists spend much time searching, screening, and reading papers to follow the latest research trends. With the development of advanced natural-language-processing (NLP) models, transformer-based text-generation algorithms have the potential to summarise scientific papers and automatically write a literature review from numerous scientific publications. In this paper, we introduce a Python-based toolkit, ChemDataWriter, which auto-generates books about research in a completely unsupervised fashion. ChemDataWriter adopts a conservative book-generation pipeline to automatically write the book by suggesting potential book content, retrieving and re-ranking the relevant papers, and then summarising and paraphrasing the text within the paper. To the best of our knowledge, ChemDataWriter is the first open-source toolkit in the area of chemistry to be able to compose a literature review entirely via artificial intelligence once one has suggested a broad topic. We also provide an example of a book that ChemDataWriter has auto-generated about battery-materials research. To aid the use of ChemDataWriter, its code is provided with associated documentation to serve as a user guide.}
}
@article{2023I,
title = {Guide for Authors},
journal = {Physiotherapy},
volume = {121},
pages = {I-VII},
year = {2023},
issn = {0031-9406},
doi = {https://doi.org/10.1016/S0031-9406(23)00074-3},
url = {https://www.sciencedirect.com/science/article/pii/S0031940623000743}
}
@article{IGLESIAS2024100792,
title = {A practical guide to (successfully) collect and process images through online surveys},
journal = {Social Sciences & Humanities Open},
volume = {9},
pages = {100792},
year = {2024},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2023.100792},
url = {https://www.sciencedirect.com/science/article/pii/S2590291123003972},
author = {Patricia A. Iglesias and Carlos Ochoa and Melanie Revilla},
keywords = {Images, Manual classification, Automatic classification, Visual data},
abstract = {Asking online survey respondents to share images is a practice that has gained notoriety recently. Although this collecting strategy may offer many advantages, it requires researchers to know how to operationalize, collect, process, and analyze this type of data, which is not yet an extended expertise among survey practitioners. This paper aims to guide researchers inexperienced in image analysis by presenting the main steps involved in the process of using images as a new data source: 1) operationalization, 2) definition of the labels, 3) choice of the most suitable classification method(s), 4) collection, 5) enhancement, and 6) classification of the images, 7) verification of the classification outcomes, and 8) data analysis. Following this eight-step process can help practitioners assess whether image collection is appropriate for their research problem and, if so, plan their image-based research, by providing them with the key considerations and decisions to address throughout their implementation.}
}
@article{MOENCK2024250,
title = {Industrial Language-Image Dataset (ILID): Adapting Vision Foundation Models for Industrial Settings},
journal = {Procedia CIRP},
volume = {130},
pages = {250-263},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.084},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124012411},
author = {Keno Moenck and Duc Trung Thieu and Julian Koch and Thorsten Schüppstuhl},
keywords = {industrial dataset, self-supervised, CLIP, vision foundation model},
abstract = {In recent years, the upstream of Large Language Models (LLM) has also encouraged the computer vision community to work on substantial multimodal datasets and train models on a scale in a self-/semi-supervised manner, resulting in Vision Foundation Models (VFM), as, e.g., Contrastive Language–Image Pre-training (CLIP). The models generalize well and perform outstandingly on everyday objects or scenes, even on downstream tasks, tasks the model has not been trained on, while the application in specialized domains, as in an industrial context, is still an open research question. Here, fine-tuning the models or transfer learning on domain-specific data is unavoidable when objecting to adequate performance. In this work, we, on the one hand, introduce a pipeline to generate the Industrial Language-Image Dataset (ILID) based on web-crawled data; on the other hand, we demonstrate effective self-supervised transfer learning and discussing downstream tasks after training on the cheaply acquired ILID, which does not necessitate human labeling or intervention. With the proposed approach, we contribute by transferring approaches from state-of-the-art research around foundation models, transfer learning strategies, and applications to the industrial domain.}
}
@article{JUST2024102883,
title = {Natural language processing for innovation search – Reviewing an emerging non-human innovation intermediary},
journal = {Technovation},
volume = {129},
pages = {102883},
year = {2024},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2023.102883},
url = {https://www.sciencedirect.com/science/article/pii/S0166497223001943},
author = {Julian Just},
keywords = {Natural language processing, Innovation search, Innovation intermediation, Front-end of innovation, AI-based innovation management, Systematic literature review},
abstract = {Applying artificial intelligence (AI), especially natural language processing (NLP), to harness large amounts of information from patent databases, online communities, social media, or crowdsourcing platforms is becoming increasingly popular to help organizations find promising solutions. In the era of non-human innovation intermediaries, we should begin to view NLP not only as a useful technology applied in different innovation practices but also as an intermediary orchestrating valuable information. Previous research has not taken this perspective, and knowledge about its intermediation activities and functions is limited. This study reviews 167 academic articles to better understand how NLP approaches can enrich intermediation in early-stage innovation search. It identifies 18 distinctive innovation practices taking over activities like forecasting trends, illustrating technology and idea landscapes, filtering out distinctive contributions, recombining domain-specific and analogous knowledge, or matching problems with solutions. While certain NLP capabilities complement each other, the analysis shows that the choice of the most appropriate approach depends on the characteristics of the innovation practice. Innovation researchers and practitioners should rethink current roles and responsibilities in AI-based innovation processes. As seen in the recent emergence of large language models (LLMs), the rapidly evolving field offers many future research opportunities and practical benefits.}
}
@article{ALANAZI20247,
title = {Pattern recognition tools for output-based classification of synchronised Kuramoto states},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {17},
pages = {7-12},
year = {2024},
note = {26th International Symposium on Mathematical Theory of Networks and Systems MTNS 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.10.105},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324018603},
author = {Faizah Alanazi and Markus Mueller and Stuart Townley},
keywords = {Kuramoto Networks, Synchrony, Observability, Artificial Neural Networks, Pattern recognition},
abstract = {Kuramoto oscillators are known to exhibit multiple synchrony where the states of individual oscillators synchronise in groups. We present a method for output-based classification of synchronised states in networks of Kuramoto oscillators using an artificial neural network for pattern recognition. Outputs of synchronised states are represented by spectrograms, in other words “fingerprint”, on which an artificial neural network of stacked autoencoders is then trained to classify these fingerprints and thus the different types of synchrony. We illustrate the approach for a Kuramoto model with N = 4 oscillators which exhibits synchrony of five types. We provide performance metrics for learning and training data which demonstrat that the approach reaches high levels of reliability.}
}
@article{CHO2024187,
title = {Development of Fine-Tuned Retrieval Augmented Language Model specialized to manual books on machine tools⁎⁎This work is supported by Korea Evaluation Institute of Industrial Technology (KEIT) grant funded by the Ministry of Trade, Industry and Energy(No. 20026431) and Institute of information communications Technology Planning Evaluation (IITP) grant funded by the Korea government(MSIT) (No.RS-2022-00155911, Artificial Intelligence Convergence Innovation Human) Resources Development(Kyung Hee University)},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {19},
pages = {187-192},
year = {2024},
note = {18th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.09.157},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324015696},
author = {Seongwoo Cho and Jongsu Park and Jumyung Um},
keywords = {Fine-tuning, Retrieval Augmented Generation, Prompt Engineering, Large Language Models, Artificial Intelligence, Machine tools},
abstract = {This paper aims to reduce potential human errors and loads that arise from the lack of novice human operators and different interfaces in advanced machine tool industries. A digital assistant using generative artificial intelligence is designed to answer questions about machine terminologies, and operation sequences happening in an alarm state. Combining Fine-Tuning, Retrieval Augmented Generation, and Prompt Engineering, it solves problems of common-purpose large language models. The proposed system is implemented on a local server and connected to a mobile device. It shows increasing quantitative accuracy from 51% to 79% and 84% in the fine-tuned and retriever model, and the qualitative score increases from 21 to 25 in the retriever model.}
}
@article{GHANE2024e23775,
title = {Semantic TRIZ feasibility in technology development, innovation, and production: A systematic review},
journal = {Heliyon},
volume = {10},
number = {1},
pages = {e23775},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e23775},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023109832},
author = {Mostafa Ghane and Mei Choo Ang and Denis Cavallucci and Rabiah {Abdul Kadir} and Kok Weng Ng and Shahryar Sorooshian},
keywords = {Semantic TRIZ, Data analytics, Artificial intelligence, Automate innovation},
abstract = {The study unfolds with an acknowledgment of the extensive exploration of TRIZ components, spanning a solid philosophy, quantitative and inductive methods, and practical tools, over the years. While the adoption of Semantic TRIZ (S-TRIZ) in high-tech industries for system development, innovation, and production has increased, the application of AI technologies to specific TRIZ components remains unexplored. This systematic literature review is conducted to delve into the detailed integration of AI with TRIZ, particularly S-TRIZ. The results elucidate the current state of AI applications within TRIZ, identifying focal TRIZ components and areas requiring further study. Additionally, the study highlights the trending AI technologies in this context. This exploration serves as a foundational resource for researchers, developers, and inventors, providing valuable insights into the integration of AI technologies with TRIZ concepts. The study not only paves the way for the development and automation of S-TRIZ but also outlines limitations for future research, guiding the trajectory of advancements in this interdisciplinary field.}
}
@article{GHAZINOORI2023100496,
title = {Bursting into the Public Eye: Analyzing the Development of Renewable Energy Research Interests},
journal = {Renewable Energy Focus},
volume = {47},
pages = {100496},
year = {2023},
issn = {1755-0084},
doi = {https://doi.org/10.1016/j.ref.2023.100496},
url = {https://www.sciencedirect.com/science/article/pii/S1755008423000923},
author = {Soroush Ghazinoori and Saeed Roshani and Reza Hafezi and David A. Wood},
keywords = {Renewable energy, Technology evolution, Burst detection, Entity linking, Future energy orientations, Political decisions},
abstract = {Renewable energies (RE) are rapidly reforming the power sector because they have become more economically competitive, thereby enabling nations to overcome high dependence on energy imports (i.e. reducing energy security concerns), as well as reducing their negative environmental footprints. Therefore, both the public and private sectors need to uncover the future technological trajectory of renewable energy. As part of an entity-linking approach, this paper examines REs developments from 2000 to 2021 and explores high-potential topics by analyzing hot topics during the period to identify evolutionary trends of relevant research interests (using published research papers). The results are analyzed and classified to determine how and when RE development issues “burst” into the public eye. The article tells the story of RE development from the past, the present, and the likely future hot topics for research. We analyze the results and cluster them into five categories: (1) energy sources, (2) sustainable development, (3) technological developments, (4) supply chains, and (5) energy management systems.se Findings reveal signals about the potential future energy market shifts and how REs evolve and contribute to the market. Signals are converted to policies and future market interpretations using an expert-based process. Analysis and methodology outcomes reveal that digital technologies have become an important factor in market changes. The findings of the study highlight how sustainable development actions and digital technologies will likely transform RE supply and management.}
}
@incollection{SCHMID202453,
title = {3 - The Laser Sintering Process},
editor = {Manfred Schmid},
booktitle = {Laser Sintering with Plastics (Second Edition)},
publisher = {Hanser},
edition = {Second Edition},
pages = {53-102},
year = {2024},
isbn = {978-1-56990-921-8},
doi = {https://doi.org/10.3139/9781569909294.003},
url = {https://www.sciencedirect.com/science/article/pii/B9781569909218500047},
author = {Manfred Schmid}
}
@article{LIAN2024102442,
title = {Public attitudes and sentiments toward ChatGPT in China: A text mining analysis based on social media},
journal = {Technology in Society},
volume = {76},
pages = {102442},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102442},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23002476},
author = {Ying Lian and Huiting Tang and Mengting Xiang and Xuefan Dong},
keywords = {ChatGPT, Artificial intelligence, Social media, Public perception, Text mining, Online public opinion},
abstract = {ChatGPT, an innovative artificial intelligence language model, is attracted significant attention around the world, sparking both enthusiasm and controversy, but identifying its societal impact and addressing its potential concerns necessitate an understanding of the prevailing public's attitudes toward the tool. In this study, we leverage text mining techniques to analyze the sentiments and themes prevalent among Chinese social media discussions of ChatGPT. In total, 96,435 comment data and 55,186 repost data were used, and the results show that public discussions mainly focused on ChatGPT's technical support, AI-related effectiveness, impact on human work, and effects on education and technology. Concerns were related to disinformation risks, technological unemployment, and the human–computer relationship. In addition, we found that social media played a prominent role in information dissemination, while official media and government units demonstrated a limited influence. The insights obtained through this study can inform policymakers, industry stakeholders, and the public of the public's prevailing attitude toward AI technologies, and they can facilitate informed decision-making.}
}
@incollection{STRASSER2024195,
title = {Chapter 10 - Pitfalls (and advantages) of sophisticated large language models},
editor = {Santi Caballé and Joan Casas-Roma and Jordi Conesa},
booktitle = {Ethics in Online AI-based Systems},
publisher = {Academic Press},
pages = {195-210},
year = {2024},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-443-18851-0},
doi = {https://doi.org/10.1016/B978-0-443-18851-0.00007-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044318851000007X},
author = {Anna Strasser},
keywords = {Large language models, human-machine discrimination abilities, ethical consequences, privacy rights, human counterfeits, overreliance, misinformation},
abstract = {Natural language processing (NLP) based on large language models (LLMs) is a booming field of AI research. After neural networks have proven to outperform humans in games and practical domains based on pattern recognition, we might now stand at a road junction where artificial entities might eventually enter the realm of human communication. However, this comes with serious risks. Due to the inherent limitations regarding the reliability of neural networks, overreliance on LLMs can have disruptive consequences. And since it will be increasingly difficult to distinguish between human-written and machine-generated text, one is confronted with new ethical challenges. This begins with the no longer undoubtedly verifiable human authorship and continues with various types of fraud, such as a new form of plagiarism. This also concerns the violation of privacy rights, the possibility of circulating counterfeits of humans, and, last but not least, it makes a massive spread of misinformation possible.}
}
@article{ALI2023102402,
title = {The knowledge and innovation challenges of ChatGPT: A scoping review},
journal = {Technology in Society},
volume = {75},
pages = {102402},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102402},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23002075},
author = {Omar Ali and Peter A. Murray and Mujtaba Momin and Fawaz S. Al-Anzi},
keywords = {ChatGPT, Artificial intelligence, Challenges, Strategies, Education sector},
abstract = {This study has several objectives. Firstly, the authors adopt a scoping review of extant research to identify common emerging themes in the ChatGPT model. Secondly, the manuscript explores the emerging innovation and knowledge management challenges by problematizing gaps that need to be explored. This study setting comprises a comprehensive scoping review and analysis of the ChatGPT and related literature. Based on a substantive content analysis of 652 articles between the years 2018 and 2023, four themes emerged from the literature that present gaps to be explored in future research. The technology acceptance model (TAM) grounds the theoretical framework to explore the emerging themes and potential user satisfaction criteria. In addition, this is the first study to broaden the TAM innovation literature by applying knowledge generation criteria to assess the extent to which ChatGPT would satisfy expected user sensemaking represented by know-that, know-how, know-why, and care-why knowledge. Two research questions assessed the findings against TAM and knowledge generation criteria. Based on the emerging gaps identified, the review found that by applying TAM, user satisfaction is not expected to be high especially when applied in educational settings given specific learner needs in complex learning situations. While aspects of user knowledge is expected to increase exponentially, the quality of the information generated by ChatGPT is not expected to result in high user know-why knowledge and complex system understanding. At a time when generative AI models are challenging traditional scientific means of delivering education pedagogy in driving student and user learning, this study has important implications for the future of the ChatGPT model application within educational and broader settings.}
}
@incollection{20241731,
title = {Appendix G - Answers to Review Questions/Exercises, Hands-on Projects, Case Projects, and Optional Team Case Project by Chapter},
editor = {John R. Vacca},
booktitle = {Computer and Information Security Handbook (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {1731-1829},
year = {2024},
isbn = {978-0-443-13223-0},
doi = {https://doi.org/10.1016/B978-0-443-13223-0.15007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443132230150076}
}
@article{NEGRI2024258,
title = {A Transfer Learning approach for Anomaly Detection within a Collaborative Prognostic Framework for advanced maintenance services},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {8},
pages = {258-263},
year = {2024},
note = {6th IFAC Workshop on Advanced Maintenance Engineering, Services and Technology AMEST 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.08.130},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324008528},
author = {M. Negri and L. Pavan and M. Macchi and A. Polenghi and A. Ruberti},
keywords = {transfer learning, collaborative prognostics, anomaly detection, servitization, predictive maintenance, Original Equipment Manufacturer, manufacturing},
abstract = {Original Equipment Manufacturers of industrial machineries are shifting toward a servitization strategy aimed at proposing various maintenance solutions delivered as a Service leveraging on advanced digitalised technologies. Given this trend, the present research aims at studying the feasibility of adopting a transfer learning approach, within a collaborative prognostic framework, to support the maintenance servitization strategy of an Original Equipment Manufacturer. The maintenance task that the research focuses on is anomaly detection. The research is carried out considering the need for a cost-effective maintenance management solution delivered to support a fleet of assets. The application of the approach to an industrial case is correspondingly developed, allowing to validate the transfer learning approach in the context of an Original Equipment Manufacturer that provides an advanced maintenance service offering, and can leverage the proposed solution to create business-grade anomaly detection models, with limited effort in terms of resources and time. The validation allows to derive different managerial implications.}
}
@article{XIONG2024122088,
title = {Efficient reinforcement learning-based method for plagiarism detection boosted by a population-based algorithm for pretraining weights},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122088},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122088},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423025903},
author = {Jiale Xiong and Jing Yang and Lei Yan and Muhammad Awais and Abdullah Ayub Khan and Roohallah Alizadehsani and U. Rajendra Acharya},
keywords = {Plagiarism detection, Unbalanced classification, Bidirectional encoder representations from transformers, Artificial bee colony, Reinforcement learning},
abstract = {Plagiarism detection (PD) in natural language processing involves locating similar words in two distinct sources. The paper introduces a new approach to plagiarism detection utilizing bidirectional encoder representations from transformers (BERT)-generated embedding, an enhanced artificial bee colony (ABC) optimization algorithm for pre-training, and a training process based on reinforcement learning (RL). The BERT model can be incorporated into a subsequent task and meticulously refined to function as a model, enabling it to apprehend a variety of linguistic characteristics. Imbalanced classification is one of the fundamental obstacles to PD. To handle this predicament, we present a novel methodology utilizing RL, in which the problem is framed as a series of sequential decisions in which an agent receives a reward at each level for classifying a received instance. To address the disparity between classes, it is determined that the majority class will receive a lower reward than the minority class. We also focus on the training stage, which often utilizes gradient-based learning techniques like backpropagation (BP), leading to certain drawbacks such as sensitivity to initialization. In our proposed model, we utilize a mutual learning-based ABC (ML-ABC) approach that adjusts the food source with the most beneficial results for the candidate by considering a mutual learning factor that incorporates the initial weight. We evaluated the efficacy of our novel approach by contrasting its results with those of population-based techniques using three standard datasets, namely Stanford Natural Language Inference (SNLI), Microsoft Research Paraphrase Corpus (MSRP), and Semantic Evaluation Database (SemEval2014). Our model attained excellent results that outperformed state-of-the-art models. Optimal values for important parameters, including reward function are identified for the model based on experiments on the study dataset. Ablation studies that exclude the proposed ML-ABC and reinforcement learning from the model confirm the independent positive incremental impact of these components on model performance.}
}
@incollection{AURET2024,
title = {Machine Learning for Industrial Process Monitoring},
booktitle = {Reference Module in Materials Science and Materials Engineering},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-803581-8},
doi = {https://doi.org/10.1016/B978-0-443-14081-5.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443140815000143},
author = {Lidia Auret},
keywords = {Autoencoders, Chemical process control, Fault detection, Fault detection, supervision and safety of technical processes, Fault diagnosis, Fault identification, Kernel methods, Latent variable methods, Machine learning, Mining, mineral and metal processing, Power and energy systems, Process automation, Process monitoring, Process recovery, Tree methods},
abstract = {The safe and efficient operation of industrial processes is a growing concern. As industrial processes have become increasingly automated, more process data is being collected, presenting an opportunity for data-driven process monitoring, albeit in the presence of challenging process characteristics. Statistical and machine learning methods have been applied to process monitoring with general success in the task of fault detection. However, challenges remain in locating faults and providing timely and specific suggestions for interventions to return the process to optimal operations. Practical considerations in machine learning for process monitoring also includes the motivation, design, and maintenance of deployed solutions.}
}
@article{2024e1,
title = {Full Issue PDF},
journal = {JACC: Heart Failure},
volume = {12},
number = {1},
pages = {e1-e242},
year = {2024},
issn = {2213-1779},
doi = {https://doi.org/10.1016/S2213-1779(23)00780-1},
url = {https://www.sciencedirect.com/science/article/pii/S2213177923007801}
}
@article{HEIKKILA2023122853,
title = {The changing work of IPR attorneys: 30 years of institutional transitions},
journal = {Technological Forecasting and Social Change},
volume = {197},
pages = {122853},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122853},
url = {https://www.sciencedirect.com/science/article/pii/S0040162523005383},
author = {Jussi T.S. Heikkilä and Mirva Peltoniemi},
keywords = {Patent attorney, IPR, Small open economy, European integration, Industry dynamics, Institutional change},
abstract = {Intellectual property rights (IPR) are at the core of innovation studies. Patent attorneys and other IPR experts play an important role in drafting and filing processes yet we know little of their work. We conduct an exploratory case study to shed light on how IPR attorneys adapt to changes in institutions and competitive environment that overturn the fundamentals of their business. We focus on the sector's evolution in Finland from 1990 to 2020, and analyse the impacts of globalization, European integration, and digitalization. EPC, EUTM, RCD and the London Agreement are identified as significant changes for the industry. IPR register data and expert interviews show that the business has shifted from serving foreign clients filing in Finland to serving Finnish clients filing internationally, increasing the knowledge requirements of local experts. The filing volume has increased due to globalization while billing per filing has decreased. This has triggered the development of consulting services relating to technology strategy. We contribute by analysing the sector's evolution in a small open economy where start-ups typically aim at the global market from the start. Our study also highlights the need to integrate IPR attorneys into the literatures on appropriability and propensity to file.}
}