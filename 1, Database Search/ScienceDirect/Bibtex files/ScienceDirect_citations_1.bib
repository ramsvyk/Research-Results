@article{FARAHANI2025103010,
title = {Time-series forecasting in smart manufacturing systems: An experimental evaluation of the state-of-the-art algorithms},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {95},
pages = {103010},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2025.103010},
url = {https://www.sciencedirect.com/science/article/pii/S073658452500064X},
author = {Mojtaba A. Farahani and Fadi El Kalach and Austin Harper and M.R. McCormick and Ramy Harik and Thorsten Wuest},
keywords = {Smart manufacturing, Artificial intelligence, Time-series, Forecasting, Machine learning, Datasets},
abstract = {Time-Series Forecasting (TSF) is a growing research area across various domains including manufacturing. Manufacturing can benefit from Artificial Intelligence (AI) and Machine Learning (ML) innovations for TSF tasks. Although numerous TSF algorithms have been developed and proposed over the past decades, the critical validation and experimental evaluation of the algorithms hold substantial value for researchers and practitioners and are missing to date. This study aims to fill this research gap by providing a rigorous experimental evaluation of the state-of-the-art TSF algorithms on thirteen manufacturing-related datasets with a focus on their applicability in smart manufacturing environments. Each algorithm was selected based on the defined TSF categories to ensure a representative set of state-of-the-art algorithms. The evaluation includes different scenarios to evaluate the models using combinations of two problem categories (univariate and multivariate) and two forecasting horizons (short- and long-term). To evaluate the performance of the algorithms, the weighted average percent error was calculated for each application, and additional post hoc statistical analyses were conducted to assess the significance of observed differences. Only algorithms with accessible codes from open-source libraries were utilized, and no hyperparameter tuning was conducted. This approach allowed us to evaluate the algorithms as "out-of-the-box" solutions that can be easily implemented, ensuring their usability within the manufacturing sector by practitioners with limited technical knowledge of ML algorithms. This aligns with the objective of facilitating the adoption of these techniques in Industry 4.0 and smart manufacturing systems. Based on the results, transformer- and MLP-based architectures demonstrated the best performance across different scenarios with MLP-based architecture winning the most scenarios. For univariate TSF, PatchTST emerged as the most robust algorithm, particularly for long-term horizons, while for multivariate problems, MLP-based architectures like N-HITS and TiDE showed superior results. The study revealed that simpler algorithms like XGBoost could outperform more complex transformer-based in certain tasks. These findings challenge the assumption that more sophisticated models inherently produce better results. Additionally, the research highlighted the importance of computational resource considerations, showing significant variations in runtime and memory usage across different algorithms.}
}
@article{CHARLES2025177508,
title = {AI in action: Changes to student perceptions when using generative artificial intelligence for the creation of a multimedia project-based assessment},
journal = {European Journal of Pharmacology},
volume = {998},
pages = {177508},
year = {2025},
issn = {0014-2999},
doi = {https://doi.org/10.1016/j.ejphar.2025.177508},
url = {https://www.sciencedirect.com/science/article/pii/S0014299925002626},
author = {Kellie A. Charles and Arsalan Yousuf and Han Chow Chua and Slade Matthews and Joanna Harnett and Tina Hinton},
keywords = {Science education, Pharmacology education, Artificial intelligence, AI, ChatGPT},
abstract = {Introduction
New modes of assessments are needed to evaluate of the authenticity of student learning in an artificial intelligence (AI) world. In mid-2023, we piloted a new assessment type; a collaborative group multimedia assessment with AI allowance. The aim of the research study was to explore the experiences of students using AI in a multimedia assessment. We further aimed to determine whether these use cases changed student perceptions of the ways AI can be used in learning and assessment.
Methods
Students enrolled in a capstone Pharmacology interdisciplinary unit (n = 40) were included in an exploratory, qualitative case study methodology. Thematic analysis using an AI role-based conceptual framework was used to explore student perceptions of AI use prior to and during their projects from logbooks documenting the assessment process.
Results
AI was initially perceived by students as having a personal tutor-style role, which aligned with the taxonomy with AI acting as an Arbiter (49 %), Oracle (41 %) and Quant (10 %). In contrast to their earlier perceptions, AI was only used in a limited manner in the early stages of assessment in the idea generation in the role as an Oracle (86 %) or in data analytic purposes as a Quant (14 %), (n = 14 cases in 5 groups). No student group used AI to generate written text for the final assessment.
Discussion
Tension between perceived and actual use of AI is indicative of the uncertainty faced by students with the allowance of AI within assessments. Clear guidance for educators and students about how to assess the AI-supported learning process is needed to ensure the integrity of the assessment system.}
}
@article{KISS20251467,
title = {UbiREAD deciphers proteasomal degradation code of homotypic and branched K48 and K63 ubiquitin chains},
journal = {Molecular Cell},
volume = {85},
number = {7},
pages = {1467-1476.e6},
year = {2025},
issn = {1097-2765},
doi = {https://doi.org/10.1016/j.molcel.2025.02.021},
url = {https://www.sciencedirect.com/science/article/pii/S1097276525001522},
author = {Leo Kiss and Leo C. James and Brenda A. Schulman},
keywords = {ubiquitin, ubiquitin code, protein degradation, K48, K63, branched ubiquitin chains, proteasome, deubiquitination, electroporation, proteostasis},
abstract = {Summary
Ubiquitin chains define the fates of their modified proteins, often mediating proteasomal degradation in eukaryotes. Yet heterogeneity of intracellular ubiquitination has precluded systematically comparing the degradation capacities of different ubiquitin chains. We developed ubiquitinated reporter evaluation after intracellular delivery (UbiREAD), a technology that monitors cellular degradation and deubiquitination at high temporal resolution after bespoke ubiquitinated proteins are delivered into human cells. Comparing the degradation of a model substrate modified with various K48, K63, or K48/K63-branched ubiquitin chains revealed fundamental differences in their intracellular degradation capacities. K48 chains with three or more ubiquitins triggered degradation within minutes. K63-ubiquitinated substrate was rapidly deubiquitinated rather than degraded. Surprisingly, in K48/K63-branched chains, substrate-anchored chain identity determined the degradation and deubiquitination behavior, establishing that branched chains are not the sum of their parts. UbiREAD reveals a degradation code for ubiquitin chains varying by linkage, length, and topology and a functional hierarchy within branched ubiquitin chains.}
}
@article{PENG2025126684,
title = {MeshPAD: Payload-aware mesh distortion for 3D steganography based on geometric deep learning},
journal = {Expert Systems with Applications},
volume = {272},
pages = {126684},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126684},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425003069},
author = {Weilong Peng and Keke Tang and Weixuan Tang and Yong Su and Meie Fang and Ping Li},
keywords = {Mesh steganography, Mesh distortion, Graph neural networks},
abstract = {Minimizing distortion while embedding specific payloads is a critical challenge in 3D steganography task. The traditional methods usually involve two steps: first, calculating embedding change probabilities for each vertex using a heuristic distortion formula, and then embedding the secret data according to these probabilities. This paper introduces a novel approach called Payload-Aware Mesh Distortion (MeshPAD), which utilizes a geometric deep learning framework tailored for 3D steganography. MeshPAD directly learns embedding change probabilities while maintaining the minimal distribution distance. The framework is built on three main components: (1) a graph auto-encoder that captures edgewise sensitivity based on topological data; (2) a mechanism that using the edge sensitivity and different payload rates to predict embedding change probabilities, creating a payload-aware distortion process; and (3) a combination consisting of a trainable data embedding mechanism and a discriminator, which work in an adversarial manner to refine the distortion process and enhance security. Experimental results show that MeshPAD achieves superior undetectability compared to heuristic methods. For instance, MeshPAD improves undetectability by about 3% at a payload rate of 0.5 over heuristic methods on the Manifold40 dataset. Across various payload settings, it consistently matches or surpasses existing methods in mesh steganography scenarios, demonstrating its effectiveness and improved security. This development not only enhances the robustness of data protection in 3D environments but also suggests potential applications in areas such as virtual reality security and digital asset management within the information security field.}
}
@article{FENG2025100488,
title = {Identifying potential technology opportunities for coal bed methane exploitation via patent analysis},
journal = {Sustainable Futures},
volume = {9},
pages = {100488},
year = {2025},
issn = {2666-1888},
doi = {https://doi.org/10.1016/j.sftr.2025.100488},
url = {https://www.sciencedirect.com/science/article/pii/S2666188825000589},
author = {Jian Feng and Zhenfeng Liu},
keywords = {Coal bed methane, Patent analysis, Vacant technology, Technology opportunity analysis},
abstract = {To find the unexplored technology opportunities and indicate the potential breakthroughs for the technology development of coal bed methane (CBM) exploitation, this study proposes an automated identification approach by combining the patent classification codes and generative topographic mapping (GTM) based on patent analysis. The experimental findings reveal that the proposed model outperforms nine traditional link prediction benchmarks in identifying 15 vacant technologies. This automated identification approach not only saves R&D time to discover vacant technologies for CBM exploitation but also generates actionable policy insights for governments and enterprises to optimize strategic resource allocation and foster cross-sector innovation.}
}
@article{CHEN2025100481,
title = {Advancing oil and gas emissions assessment through large language model data extraction},
journal = {Energy and AI},
volume = {20},
pages = {100481},
year = {2025},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2025.100481},
url = {https://www.sciencedirect.com/science/article/pii/S2666546825000138},
author = {Zhenlin Chen and Roujia Zhong and Wennan Long and Haoyu Tang and Anjing Wang and Zemin Liu and Xuelin Yang and Bo Ren and James Littlefield and Sanmi Koyejo and Mohammad S. Masnadi and Adam R. Brandt},
keywords = {Artificial intelligence, Large language models, Carbon intensity, Supervised learning, Oil and gas field, Greenhouse gas emissions},
abstract = {The oil and gas industry strives to improve environmental stewardship and reduce its carbon footprint, but lacks comprehensive global operational data for accurate environmental assessment and decision-making. This challenge is compounded by dispersed information sources and the high costs of accessing proprietary databases. This paper presents an innovative framework using Large Language Models (LLMs) – specifically GPT-4 and GPT-4o – to extract critical oil and gas asset information from diverse literature sources. Our framework employs iterative comparisons between GPT-4’s output and a dataset of 129 ground truth documents labeled by domain experts. Through 11 training and testing iterations, we fine-tuned prompts to optimize information extraction. The evaluation process assessed performance using true positive rate, precision, and F1 score metrics. The framework achieved strong results, with a true positive rate of 83.74% and an F1 score of 78.16% on the testing dataset. The system demonstrated remarkable efficiency, processing 32 documents in 61.41 min with GPT-4o, averaging 7.09 s per extraction - a substantial improvement over the manual method. Cost-effectiveness was also achieved, with GPT-4o reducing extraction costs by a factor of 10 compared to GPT-4. This research has significant implications for the oil and gas industry. By creating an organized, transparent, and accessible database, we aim to democratize access to critical information. The framework supports more accurate climate modeling efforts, enhances decision-making processes for operations and investments, and contributes to the sector’s ability to meet environmental commitments. These improvements particularly impact emissions reduction and energy transition strategies, potentially transforming how data is extracted and utilized in this field and beyond.}
}
@article{MORADI2025101004,
title = {Sport destination competitiveness and attractiveness: Scale development and validation},
journal = {Journal of Destination Marketing & Management},
volume = {37},
pages = {101004},
year = {2025},
issn = {2212-571X},
doi = {https://doi.org/10.1016/j.jdmm.2025.101004},
url = {https://www.sciencedirect.com/science/article/pii/S2212571X25000162},
author = {Erfan Moradi and Rasool {Norouzi Seyed Hossini}},
keywords = {Attractiveness destination, Competitiveness destination, Destination development, Scale development, Sport tourism},
abstract = {Without a standardized scale, evaluating different sport destinations and identifying contexts for improvement or growth becomes challenging. This hinders the ability of researchers and policymakers to make informed decisions and implement effective strategies for developing and promoting destinations. As far as we know, no such scale is currently available. To bridge this gap, we use a rigorous multi-step scale development procedure to create a scale focusing on destination competitiveness and attractiveness. Initially, we defined the construct's scope. Next, we created items through a literature review, semi-structured interviews, and fuzzy Delphi, followed by a content validity assessment. We then gathered data for testing, involving 237 participants in item refining, 477 in scale validation, and 355 in nomological validity assessment. The results confirmed a six-dimensional and twenty-one-item scale with acceptable reliability and validity levels. The scale offers valuable insights from theoretical, methodological, and managerial perspectives for researchers and practitioners involved in developing sport tourism destinations.}
}
@article{ANCONA2025356,
title = {Technical, legal and policy aspects of an international space traffic management framework},
journal = {Acta Astronautica},
volume = {232},
pages = {356-363},
year = {2025},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2025.02.028},
url = {https://www.sciencedirect.com/science/article/pii/S0094576525001092},
author = {Elena Ancona and Diego Guerra and Steven Armstrong and Henrique {Oliveira da Mata} and Claudia Medeiros and Jaime Silva and Eric Dahlstrom},
keywords = {Space traffic management, Collision avoidance, International framework, Space situational awareness},
abstract = {As part of the International Space University (ISU) Space Studies Program (SSP) 2023 Team Project on Space Situational Awareness (SSA), a group of students, researchers, and professionals with a very diverse background, focused on proposing a global framework for Space Traffic Management (STM) [1, 2]. Present statistics indicate that more than 7000 operational satellites coexist within the same orbital regime, together with more than 36,500 medium-sized trackable fragments of space debris. Following the last few years big constellations' deployment, and considering the ones that are scheduled for launch soon, the way satellites operations were carried out is rapidly becoming obsolete. This situation substantially increases the threat of potential collisions, emphasizing the necessity for an open and internationally coordinated STM framework. This paper introduces an international framework derived from the research conducted by Murakami et al., in 2019 [3]. It offers a comprehensive approach to addressing the STM challenges, with a particular emphasis on fostering precise data exchange, establishing channels for clear and direct communication, and encouraging international collaboration. In the current work, after some considerations on the policy and legal aspects of our proposed international framework, we present the technical features and provide a broader perspective on the reasons that led to its definition. In our analysis, we considered the vibrant scenario involving new commercial actors offering Space Surveillance and Tracking (SST) services, collision risk prediction, recommendations for Collision Avoidance (CA) manoeuvres, and autonomous CA systems. The authors of this work believe that the variety and complexity of this ecosystem can only be beneficial, and by no means is the proposed framework intended to substitute those players. Instead, its goal is to fill the gaps where the current system would be incomplete. As multiple entities offer insights into space traffic and predict potential collisions among space objects, determining the most dependable source becomes a challenge when significant inconsistencies arise. Still being aware of these challenges, we want to foster collaboration within the evolving New Space landscape. Our proposed framework is designed to integrate existing suppliers rather than replace them, allowing for the seamless inclusion of their data within our system. These suppliers can continue to run their independent analysis and operations while also be contributing to, and benefiting from, the enhanced functionality within our unified system.}
}
@article{MUSTOFA2025100379,
title = {Extending the technology acceptance model: The role of subjective norms, ethics, and trust in AI tool adoption among students},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100379},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100379},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000190},
author = {Rochman Hadi Mustofa and Trian Gigih Kuncoro and Dwi Atmono and Hardika Dwi Hermawan and  Sukirman},
keywords = {Technology acceptance model, Ethics, Trust, Subjective norms, Artificial intelligence},
abstract = {This study extends the Technology Acceptance Model (TAM) to investigate the adoption of AI tools among university students, incorporating Ethics and Trust as moderating variables and Subjective Norms as a quadratic variable. Structural Equation Modeling (SEM) on a sample of 437 students’ reveals that Perceived Usefulness (PU) significantly influences Attitude Toward Using (ATU), while Perceived Ease of Use (PU) significantly influences Attitude Toward Using (ATU), while Perceived Ease of Use (PEoU) does not, suggesting familiarity with technology reduces the role of ease of use. Ethics positively impacts ATU, highlighting its importance in shaping attitudes. However, Ethics and Trust do not moderate the ATU-Actual Use (AU) relationship, and the hypothesized quadratic effect of Subjective Norms is unsupported, confirming a linear relationship. These findings underscore the direct influence of Ethics and Trust in AI adoption and suggest that educational policies should prioritize ethical AI usage and trust-building to enhance acceptance.}
}
@article{LEITE2025124115,
title = {Artificial intelligence in higher education: Research notes from a longitudinal study},
journal = {Technological Forecasting and Social Change},
volume = {215},
pages = {124115},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2025.124115},
url = {https://www.sciencedirect.com/science/article/pii/S0040162525001465},
author = {Higor Leite},
keywords = {Generative artificial intelligence, Higher education, Innovation, Technology, Transformative service research},
abstract = {Generative artificial intelligence (GenAI) has disrupted traditional educational approaches. Students are applying GenAI tools to access and create new content. However, the emergence of GenAI in higher education comes with caveats and academics and university administrators are learning to navigate this uncharted territory. GenAI is treated as a double-edged sword, with several benefits, such as innovation and productivity, but also drawbacks regarding ethics and academic misconduct. Therefore, our study aims to understand the impact of GenAI on students' experiences in the higher education ecosystem as students move to a new AI-enhanced job market. This research note article presents preliminary results from a 12-month longitudinal study with students interacting with GenAI. We conducted 35 semi-structured interviews and collected private diary entries (n = 108). Our results show six meaningful themes: Harnessing AI for Enhanced Academic Performance, AI Ethics and Trust Impact on Learning, GenAI as a Supplement to Human Work, Integration and Versatility of GenAI Tools, Balancing GenAI Limitations, and Navigating the AI Adoption Journey. The study also uses the transformative service research lens to present the transformative impact of GenAI in higher education. To contribute to practice and policymakers, we designed a research agenda to inform future studies on GenAI.}
}
@article{MORO2025100016,
title = {Multimodal foundation models for material property prediction and discovery},
journal = {Newton},
volume = {1},
number = {1},
pages = {100016},
year = {2025},
issn = {2950-6360},
doi = {https://doi.org/10.1016/j.newton.2025.100016},
url = {https://www.sciencedirect.com/science/article/pii/S2950636025000088},
author = {Viggo Moro and Charlotte Loh and Rumen Dangovski and Ali Ghorashi and Andrew Ma and Zhuo Chen and Samuel Kim and Peter Y. Lu and Thomas Christensen and Marin Soljačić},
keywords = {multimodal learning, foundation models, machine learning for materials science, material property prediction, material discovery, contrastive learning, representation learning, self-supervised learning},
abstract = {Summary
Artificial intelligence is transforming computational materials science by improving property prediction and accelerating the discovery of novel materials. Recently, publicly available material data repositories have grown rapidly, encompassing not only more materials but also a greater variety and quantity of their associated properties. Existing machine-learning efforts in materials science focus primarily on single-modality tasks, i.e., relationships between materials and a single physical property, thus not taking advantage of the rich multimodal data available. Here, we introduce multimodal learning for materials (MultiMat), a framework enabling self-supervised multimodal training of foundation models for materials. Using the Materials Project database, we demonstrate the potential of MultiMat by: (1) achieving state-of-the-art performance for challenging material property prediction tasks; (2) enabling novel and accurate material discovery via latent-space similarity, allowing screening for stable materials with desired properties; and (3) encoding emergent features that correlate with material properties and may provide novel scientific insights.}
}
@article{MONDAL2025100679,
title = {Investigating the effect of state support on innovation pathways by tracking the legacy performance of firms involved in academic co-operations},
journal = {Journal of Innovation & Knowledge},
volume = {10},
number = {2},
pages = {100679},
year = {2025},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2025.100679},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X25000290},
author = {Charles Mondal and Robert B. Mellor},
keywords = {Entrepreneurial universities, Geographical distribution, Knowledge spillovers, State funding, Technology transfer},
abstract = {The performance of firms involved in projects from 2 UK research councils was investigated; firms in Innovate UK projects receive co-funding while firms in Arts & Humanities Research Council (AHRC) projects do not. Firms in 266 projects 2009–2012 were tracked for Standard Industrial Code (SIC), location and year-on-year financial performance 2012–22. The results show that firms (un- and co-funded) were mainly not local to universities. The growth performance of non-funded firms was steady in the majority of SIC codes, but some SIC codes performed very well, while for co-funded firms, many SICs performed under control but losses were made up for on average by exceptionally high performance in other SIC codes. Overall, non-funded firms achieved average growth of ∼29 % above control while co-funded firms only achieved an average growth of ∼18 % above control. Firms (both co- and un-funded) associated with 21 universities perform consistently well, while other firms (co- and un-funded) associated with 24 other universities perform consistently poorly. This difference in performance was better correlated to degree of business ambidexterity in the tech transfer function, rather than with university reputation.}
}
@article{QIN2025104459,
title = {Leveraging social media for new energy vehicle policy diffusion in China: A central-local government interaction analysis},
journal = {Transportation Research Part A: Policy and Practice},
volume = {195},
pages = {104459},
year = {2025},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2025.104459},
url = {https://www.sciencedirect.com/science/article/pii/S0965856425000874},
author = {Quande Qin and Zhibin Wen and Zhihao Zhou and Bi Fan},
keywords = {New Energy Vehicle, Policy Diffusion, Central-Local Government Interaction, Social Media Analysis, Dynamic Topic Model, Net Directional Connectedness},
abstract = {Social media is increasingly associated with new energy vehicle (NEV) policy diffusion in China. One way to explore how social media and policy diffusion are linked is to review the government posts on different topics of the policy over time. To investigate the diffusion dynamics of NEV policies, this study proposes a central-local government interaction analysis, leveraging the government posts on social media. Dynamic topic model is deployed to unveil the evolving nature and intergovernmental disparities of policy topics. Subsequently, a net directional connectedness model is employed to construct information overflow networks of different policy topics among governments. A comprehensive examination of 5386 text posts is conducted using the proposed method. It is identified that the central government prioritizes guidance policies, emphasizing industry support and global cooperation, while local governments concentrate on specific implementation measures, following a trajectory of infrastructure development, research and development, and tax exemptions. In the policy diffusion network, central government serves as the primary driver of promotional signals, followed by Shanghai, Guangdong and Beijing, while Chongqing acts as a key recipient. Simultaneously, successful local implementation can feed back experiential knowledge to central policy, constituting a dual-directional diffusion mechanism of NEV policies between the central and local governments. These findings offer valuable insights into the understanding and diffusion of NEV policy in the digital era, revealing the complex interactions across diverse regions and administrative levels.}
}
@article{HELBERGER2025106118,
title = {The rise of technology courts, or: How technology companies re-invent adjudication for a digital world},
journal = {Computer Law & Security Review},
volume = {56},
pages = {106118},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106118},
url = {https://www.sciencedirect.com/science/article/pii/S0267364925000135},
author = {Natali Helberger},
keywords = {Digitisation, AI, Justice, Courts, Big Tech, Digital transformation, Values},
abstract = {The article “The Rise of Technology Courts” explores the evolving role of courts in the digital world, where technological advancements and artificial intelligence (AI) are transforming traditional adjudication processes. It argues that traditional courts are undergoing a significant transition due to digitization and the increasing influence of technology companies. The paper frames this transformation through the concept of the “sphere of the digital,” which explains how digital technology and AI redefine societal expectations of what courts should be and how they function. The article highlights that technology is not only changing the materiality of courts—moving from physical buildings to digital portals—but also affecting their symbolic function as public institutions. It discusses the emergence of AI-powered judicial services, online dispute resolution (ODR), and technology-driven alternative adjudication bodies like the Meta Oversight Board. These developments challenge the traditional notions of judicial authority, jurisdiction, and legal expertise. The paper concludes that while these technology-driven solutions offer increased efficiency and accessibility, they also raise fundamental questions about the legitimacy, transparency, and independence of adjudicatory bodies. As technology companies continue to shape digital justice, the article also argues that there are lessons to learn for the role and structure of traditional courts to ensure that human rights and public values are upheld.}
}
@article{YE2025112199,
title = {Brain-like border ownership signals support prediction of natural videos},
journal = {iScience},
volume = {28},
number = {4},
pages = {112199},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.112199},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225004602},
author = {Zeyuan Ye and Ralf Wessel and Tom P. Franken},
keywords = {Neuroscience, Behavioral neuroscience, Social sciences},
abstract = {Summary
To make sense of visual scenes, the brain must segment foreground from background. This is thought to be facilitated by neurons that signal border ownership (BOS), which indicate which side of a border in their receptive field is owned by an object. How these signals emerge without a teaching signal of what is foreground remains unclear. Here we find that many units in PredNet, a self-supervised deep neural network trained to predict future frames in natural videos, are selective for BOS. They share key properties with BOS neurons in the brain, including robustness to object transformations and hysteresis. Ablation revealed that BOS units contribute more to prediction than other units for videos with moving objects. Our findings suggest that BOS neurons might emerge due to an evolutionary or developmental pressure to predict future input in natural, complex dynamic environments, even without an explicit requirement to segment foreground from background.}
}
@article{LYU2025100383,
title = {Understanding the practices, perceptions, and (dis)trust of generative AI among instructors: A mixed-methods study in the U.S. higher education},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100383},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100383},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000232},
author = {Wenhan Lyu and Shuang Zhang and Tingting Chung and Yifan Sun and Yixuan Zhang},
keywords = {Generative AI, Trust, Distrust, Survey study, Teaching and learning, Higher education},
abstract = {Generative AI (GenAI) has brought opportunities and challenges for higher education as it integrates into teaching and learning environments. As instructors navigate this new landscape, understanding their engagement with and attitudes toward GenAI is crucial. We surveyed 178 instructors from a single U.S. university to examine their current practices, perceptions, trust, and distrust of GenAI in higher education in March 2024. While most surveyed instructors reported moderate to high familiarity with GenAI-related concepts, their actual use of GenAI tools for direct instructional tasks remained limited. Our quantitative results show that trust and distrust in GenAI are related yet distinct; high trust does not necessarily imply low distrust, and vice versa. We also found significant differences in surveyed instructors' familiarity with GenAI across different trust and distrust groups. Our qualitative results show nuanced manifestations of trust and distrust among surveyed instructors and various approaches to support calibrated trust in GenAI. We discuss practical implications focused on (dis)trust calibration among instructors.}
}
@article{SUK2025102261,
title = {Communicative AI in the scientific public sphere: An analysis of Twitter discourse on generative AI tools},
journal = {Telematics and Informatics},
volume = {98},
pages = {102261},
year = {2025},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2025.102261},
url = {https://www.sciencedirect.com/science/article/pii/S0736585325000231},
author = {Jiyoun Suk and Yini Zhang and Jiawei Liu and Yukyung Yang},
keywords = {Generative artificial intelligence, Scientific public sphere, Social media, Diffusion of innovation},
abstract = {Drawing on the concept of the scientific public sphere, this study examines the public sense-making of communicative AI (e.g., generative AI) on social media. Advancing a framework encompassing cognitive (technology vs. use) and affective (positive vs. negative) dimensions of the public discourse on communicative AI, we analyzed global Twitter (now X) conversations about generative AI tools. Findings showed that the text generator (ChatGPT) discussions centered more on the technology-centered themes, whereas the image generator discussions emphasized their uses. ChatGPT received mixed sentiments in technology-related discussions, while there was more positive sentiment about the its uses. Theoretical and practical implications are discussed.}
}
@article{BANERJEE2025104815,
title = {CEO-board directional age difference and R&D of firms: Examining the moderation effects of strategic orientation},
journal = {Acta Psychologica},
volume = {254},
pages = {104815},
year = {2025},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2025.104815},
url = {https://www.sciencedirect.com/science/article/pii/S0001691825001283},
author = {Saikat Banerjee},
keywords = {R&d, CEO, Knowledge based view, Upper echelon theory, Strategic orientation, Innovation},
abstract = {This study explores the influence of CEO-board age differences on a firm's R&D intensity through the lens of the Knowledge-Based View, considering how strategic orientations such as digital, entrepreneurial, and long-term focus moderate this relationship. Drawing on the KBV, the research posits that age diversity between CEOs and board members enhances the firm's knowledge base by combining diverse experiences, insights, and cognitive styles, thereby fostering innovation. Using data from 309 Indian firms listed on the NSE 500 index between 2007 and 2016, the findings demonstrate that a digital orientation strengthens the positive impact of CEO-board age differences on R&D intensity by facilitating knowledge creation and sharing. Conversely, an entrepreneurial orientation may weaken this relationship by focusing more on immediate market opportunities than on sustained knowledge development. A long-term orientation further amplifies the positive linkage by aligning diverse leadership knowledge with sustained innovation objectives. This study contributes to the strategic management literature by clarifying how leadership demographic diversity, viewed through a knowledge-based perspective, influences a firm's ability to generate and leverage knowledge for innovation, thereby optimizing R&D outcomes.}
}
@article{HUANG2025109389,
title = {Artificial intelligence: A key fulcrum for addressing complex environmental health issues},
journal = {Environment International},
volume = {198},
pages = {109389},
year = {2025},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2025.109389},
url = {https://www.sciencedirect.com/science/article/pii/S0160412025001400},
author = {Lei Huang and Qiannan Duan and Yuxin Liu and Yangyang Wu and Zenghui Li and Zhao Guo and Mingliang Liu and Xiaowei Lu and Peng Wang and Fan Liu and Futian Ren and Chen Li and Jiaming Wang and Yujia Huang and Beizhan Yan and Marianthi-Anna Kioumourtzoglou and Patrick L. Kinney},
keywords = {Artificial intelligence, Environmental health, Combined pollution, Complex exposures, Machine learning},
abstract = {Environmental health (EH) is a complex and interdisciplinary field dedicated to the examination of environmental behaviours, toxicological effects, health risks, and strategies for mitigating harmful environmental factors. Traditional EH research investigates correlations between risk factors and health outcomes through control variables, but this route is difficult to address complex EH issue. Artificial intelligence (AI) technology not only has accelerated the innovation of the scientific research paradigm but also has become an important tool for solving complex EH problems. However, the in-depth and comprehensive implementation of AI in the field of EH still faces many barriers, such as model generalizability, data privacy protection, algorithm transparency, and regulatory and ethical issues. This review focuses on the compound exposures of EH and explores the potential, challenges, and development directions of AI in four key phases of EH research: (1) data collection, fusion, and management, (2) hazard identification and screening, (3) risk modeling and assessment and (4) EH management. It is not difficult to see that in the future, artificial intelligence technology will inevitably carry out multidimensional simulation of complex exposure factors through multi-mode data fusion, so as to achieve accurate identification of environmental health risks, and eventually become an efficient tool for global environmental health management. This review will help researchers re-examine this strategy and provide a reference for AI to solve complex exposure problems.}
}
@article{HAO2025104160,
title = {Disrupting political ties, enhancing transparency: China's anti-corruption campaign and corporate R&D disclosure},
journal = {International Review of Financial Analysis},
volume = {103},
pages = {104160},
year = {2025},
issn = {1057-5219},
doi = {https://doi.org/10.1016/j.irfa.2025.104160},
url = {https://www.sciencedirect.com/science/article/pii/S1057521925002479},
author = {Mengshu Hao and Jieying Hong and Yining Zhang},
keywords = {Anti-corruption campaign, R&D disclosure transparency, Political connections, Topic modeling},
abstract = {This paper examines the impact of China's anti-corruption campaign on firms' R&D disclosure practices. Utilizing a novel measure of R&D disclosure transparency, constructed via the Latent Dirichlet Allocation (LDA) approach applied to question-and-answer texts from Earnings Communication Conferences (ECCs), we find that firms with high levels of corruption prior to the campaign significantly increase the transparency of their R&D disclosures following its implementation. These findings remain robust across a range of robustness and endogeneity tests. Further analysis reveals that firms receiving greater R&D subsidies or benefiting from lower debt costs demonstrate more pronounced improvements in R&D disclosure transparency. Moreover, firms exhibiting larger increases in disclosure transparency secure higher levels of R&D subsidies and long-term loans post-campaign. These results suggest that the enhanced R&D disclosure likely represents a strategic response by firms to sustain government support and obtain favorable financing conditions amid weakened political connections. Thus, our study illuminates a novel role for R&D disclosure in enabling firms to adapt to the loss of political ties and highlights an unintended benefit of government anti-corruption efforts in enhancing corporate information transparency.}
}
@article{PHIPPS2025101353,
title = {AI image generation technology in ophthalmology: Use, misuse and future applications},
journal = {Progress in Retinal and Eye Research},
volume = {106},
pages = {101353},
year = {2025},
issn = {1350-9462},
doi = {https://doi.org/10.1016/j.preteyeres.2025.101353},
url = {https://www.sciencedirect.com/science/article/pii/S1350946225000266},
author = {Benjamin Phipps and Xavier Hadoux and Bin Sheng and J. Peter Campbell and T.Y. Alvin Liu and Pearse A. Keane and Carol Y. Cheung and Tham Yih Chung and Tien Y. Wong and Peter {van Wijngaarden}},
keywords = {Generative AI, Artificial intelligence, Deep learning, Ophthalmology, Image generation, Generative adversarial networks, GAN, Autoencoders, Diffusion models, AI diagnostic models, Data augmentation, Image denoising, Bias in AI, Patient data security, Deepfake, Blockchain, Multimodal imaging, Foundation model},
abstract = {Background
AI-powered image generation technology holds the potential to reshape medical practice, yet it remains an unfamiliar technology for both medical researchers and clinicians alike. Given the adoption of this technology relies on clinician understanding and acceptance, we sought to demystify its use in ophthalmology. To this end, we present a literature review on image generation technology in ophthalmology, examining both its theoretical applications and future role in clinical practice.
Methods
First, we consider the key model designs used for image synthesis, including generative adversarial networks, autoencoders, and diffusion models. We then perform a survey of the literature for image generation technology in ophthalmology prior to September 2024, presenting both the type of model used and its clinical application. Finally, we discuss the limitations of this technology, the risks of its misuse and the future directions of research in this field.
Results
Applications of this technology include improving AI diagnostic models, inter-modality image transformation, more accurate treatment and disease prognostication, image denoising, and individualised education. Key barriers to its adoption include bias in generative models, risks to patient data security, computational and logistical barriers to development, challenges with model explainability, inconsistent use of validation metrics between studies and misuse of synthetic images. Looking forward, researchers are placing a further emphasis on clinically grounded metrics, the development of image generation foundation models and the implementation of methods to ensure data provenance.
Conclusion
Compared to other medical applications of AI, image generation is still in its infancy. Yet, it holds the potential to revolutionise ophthalmology across research, education and clinical practice. This review aims to guide ophthalmic researchers wanting to leverage this technology, while also providing an insight for clinicians on how it may change ophthalmic practice in the future.}
}
@article{KELLOGG2025100559,
title = {Novice risk work: How juniors coaching seniors on emerging technologies such as generative AI can lead to learning failures},
journal = {Information and Organization},
volume = {35},
number = {1},
pages = {100559},
year = {2025},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2025.100559},
url = {https://www.sciencedirect.com/science/article/pii/S1471772725000053},
author = {Katherine C. Kellogg and Hila Lifshitz and Steven Randazzo and Ethan Mollick and Fabrizio Dell'Acqua and Edward McFowland and François Candelon and Karim R. Lakhani},
keywords = {Artificial intelligence, Experts, Generative AI, Risk work, Novices, Professional organizations, Professionals, HCI, Communities of practice, Work and occupations, Technology and organizations},
abstract = {Historically, junior professionals have mentored senior professionals around new technologies, because juniors are typically more willing than seniors to perform lower-level tasks to learn new skills, better able than seniors to engage in real-time experimentation close to the work itself, and more willing than seniors to learn innovative methods that conflict with traditional identities and norms. However, we know little about what happens when emerging technologies have a high level of uncertainty in their use, because they have wide-ranging capabilities and are exponentially changing. With the rise of Artificial Intelligence, specifically learning algorithms and LLMs, such contexts may be increasingly common. In our study conducted with the Boston Consulting Group, a global management consulting firm, we interviewed 78 junior consultants in July–August 2023 who had recently participated in a field experiment that gave them access for the first time to generative AI (GPT-4) for a strategic business problem solving task. Drawing from junior professionals' in situ reflections soon after the experiment, we found that junior professionals may fail to manage risks around uncertain emerging technologies because juniors are likely to recommend three kinds of novice risk work tactics that: 1) are grounded in a lack of deep understanding of technologies that have uncertain and wide-ranging capabilities and are changing exponentially, 2) focus on change to human routines rather than system design, and 3) focus on interventions at the project-level rather than system deployer- or ecosystem-level. The implications of novice risk work are that, when junior professionals are expected to be a source of expertise in the use of uncertain, emerging technologies, this can lead to learning failures. This study contributes to our understanding of occupational learning around emerging technologies, risk work in organizations, and human-computer interaction.}
}
@article{COLAHAN2025103048,
title = {Designing effective library treasure hunts: Theory, practice, and framework alignment},
journal = {The Journal of Academic Librarianship},
volume = {51},
number = {3},
pages = {103048},
year = {2025},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2025.103048},
url = {https://www.sciencedirect.com/science/article/pii/S0099133325000448},
author = {Ellwood Colahan},
keywords = {Treasure hunts, Scavenger hunts, Information literacy, ACRL framework, Piaget, Vygotsky, Montessori},
abstract = {Treasure hunts have long been employed in academic library instruction as an interactive method for fostering information literacy skills through active learning, exploration, and adventure. This essay offers a comprehensive analysis of the theory and practice of library treasure hunts. It begins by distinguishing treasure hunts from scavenger hunts and emphasizing the importance of accurate terminology. The pedagogical principles underlying their effectiveness are then examined, followed by a review of the relevant literature. The essay also addresses the challenges associated with the labor-intensive process of designing and implementing these activities. A separate analysis course alignment, learning outcomes, and how the tasks in treasure hunts align with the ACRL Framework for Information Literacy in Higher Education, and offers strategies for enhancing or adapting these activities to support the Framework. Two case studies are presented: one focusing on a team-based, in-person treasure hunt and the other one an asynchronous, fully virtual activity for individual use. Suitable customized to a particular library environment, these may be used to build and reinforce information literacy through active learning.}
}
@article{SINGH2025100309,
title = {A one health approach addressing poultry-associated antimicrobial resistance: Human, animal and environmental perspectives},
journal = {The Microbe},
volume = {7},
pages = {100309},
year = {2025},
issn = {2950-1946},
doi = {https://doi.org/10.1016/j.microb.2025.100309},
url = {https://www.sciencedirect.com/science/article/pii/S2950194625000779},
author = {Samradhi Singh and Mona Kriti and Anamika K.S and Poonam Sharma and Namrata Pal and Devojit Kumar Sarma and Rajnarayan Tiwari and Manoj Kumar},
keywords = {Poultry, Antibiotics, Antibiotic Resistance, ARG, Innovations, AMR, Environmental Health, Probiotics, One health},
abstract = {Antibiotics, often viewed as a solution, are now recognized as a double-edged sword due to their widespread and improper use. As the global demand for poultry products rises, antibiotics have become a seemingly indispensable tool to meet this need. However, while this practice addresses production demands, it leaves significant health concerns. The emergence of antibiotic-resistant bacteria (ARBs) and antibiotic-resistant genes (ARGs) marks the beginning of a series of unpredictable and potentially undefendable diseases. The topic of zoonosis has gained considerable attention recently, highlighting the intricate connections between humans, animals, and the environment. This review explores the specific impacts of antibiotics—particularly ARGs and ARBs—within the poultry industry, examining the driving factors behind their rise. By delving into the One Health concept, which underscores the interconnectedness of these three domains, the review also discusses innovative strategies to minimize antibiotic use in poultry farming which are vital for preventing and controlling zoonotic diseases, ensuring a healthier environment, and ultimately achieving optimal public health across all tiers of One Health.}
}
@article{2025A10,
title = {Guidelines for Authors},
journal = {Journal of Endodontics},
volume = {51},
number = {3},
pages = {A10-A19},
year = {2025},
issn = {0099-2399},
doi = {https://doi.org/10.1016/S0099-2399(25)00075-5},
url = {https://www.sciencedirect.com/science/article/pii/S0099239925000755}
}
@article{AO2025105207,
title = {TMN: Transformer in matrix network for single image super-resolution with enhanced shallow feature preservation},
journal = {Digital Signal Processing},
volume = {162},
pages = {105207},
year = {2025},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2025.105207},
url = {https://www.sciencedirect.com/science/article/pii/S1051200425002295},
author = {Ou Ao and Zhenhong Shang},
keywords = {Image super-resolution, Deep learning, Transformer, Lightweight},
abstract = {Transformer-based image super-resolution has witnessed remarkable advancements in recent years. However, as transformer networks grow in depth, numerous existing super-resolution methods encounter challenges in effectively preserving shallow features, which play a crucial role in single image super-resolution. The low-resolution input image contains crucial structural and contextual information, and the shallow features serve as the carriers of this information. To address the challenge of preserving shallow features, we propose the Transformer in Matrix Network (TMN), a novel architecture specifically tailored for single image super-resolution. TMN incorporates a redesigned and optimized matrix mapping module, which arranges transformer blocks in a matrix structure to preserve and effectively exploit shallow features while facilitating the efficient reuse of hierarchical feature representations across the network. Additionally, TMN refines the efficient transformer to augment its capacity for modelling long-range dependencies, thereby enabling enhanced integration of information from spatially correlated regions within the image. To further enhance the reconstruction performance, TMN incorporates the structural loss into the loss function. By constraining the relevant statistical quantities, it improves the perceptual fidelity and preserves the intricate details. Experimental results show that TMN achieves competitive performance, with a reduction in computational costs by approximately one-third compared to leading methods like SwinIR. TMN's efficient design and high-quality reconstruction make it particularly suitable for deployment on resource-constrained devices, addressing a critical need in practical applications. The implementation code is publicly available at https://github.com/13752849314/TMN.}
}
@article{ZHOU2025103953,
title = {Government adoption of generative artificial intelligence and ambidextrous innovation},
journal = {International Review of Economics & Finance},
volume = {98},
pages = {103953},
year = {2025},
issn = {1059-0560},
doi = {https://doi.org/10.1016/j.iref.2025.103953},
url = {https://www.sciencedirect.com/science/article/pii/S1059056025001169},
author = {Zhikai Zhou and Dewen Liu and Zhongjie Chen and Martin Pancho},
keywords = {Generative artificial intelligence, TOE framework, Technology adoption, Organizational ambidextrous innovation},
abstract = {Every information technological revolution has brought about new possibilities for governmental organizational innovation, and the rapid development of Generative artificial intelligence (Gen-AI) is poised to profoundly impact government governance models and public service supply methods. Understanding the factors influencing government adoption of Gen-AI, and analyzing the impact of such adoption on governmental organizational innovation behavior, have emerged as urgent and cutting-edge topics. Based on the Technology-Organization-Environment (TOE) framework and the ambidextrous organization theory, this study systematically analyzes the three-layered driving factors that influence government organizations' adoption of Gen-AI, and examines the impact of Gen-AI on exploratory and exploitative innovation within government organizations. Furthermore, it delves into the influence mechanisms of technology adoption on different innovation behaviors from the meso-institutional and micro-implementation perspectives. At the theoretical level, this study constructs a conceptual framework for understanding the adoption of Gen-AI technology, extends the application scope of the TOE theory and enhances its explanatory power, while also providing new insights into the complexity of technology-enabled organizational innovation. At the practical level, it offers a more strategic perspective and profound implications for government organizations to maintain innovative vitality and achieve sustainable development amidst the wave of intelligent transformation.}
}
@article{ZHANG2025117325,
title = {A review of eye-tracking technology and its application in stroke diagnosis and assessment},
journal = {Measurement},
volume = {252},
pages = {117325},
year = {2025},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2025.117325},
url = {https://www.sciencedirect.com/science/article/pii/S0263224125006840},
author = {Jun Zhang and Wei Kong and Ming Ma and Xi Yang and Weifeng Li and Aiguo Song},
keywords = {Eye tracking, Stroke assessment, Neurological disorder diagnosis, Eye-tracking devices, Video-oculography, Active perception, Human-computer interaction},
abstract = {The eyes are the windows of the soul, providing essential information through eye movement. With the rapid development of eye-tracking technology (ETT), its application in health assessment, including diagnosing and treating neurological diseases, has expanded significantly. Stroke is a leading cause of adult death and disability worldwide, and studies have shown that eye movement information can serve as a quantitative indicator for stroke diagnosis and assessment. Despite significant research on ETT-based stroke diagnosis, a comprehensive review is still lacking. This paper reviews 238 papers from the past twenty years, focusing on recent advancements in ETT and its application in stroke diagnosis. The studies were selected through a systematic review process following PRISMA-ScR guidelines. This paper provides a systematic overview of ETT principles, methods, and systems, detailing the entire application process in stroke diagnosis and assessment, from data acquisition to symptom evaluation. It also compares various methods and discusses the latest advancements. Statistical analysis methods remain the majority in ETT-based stroke research, while machine learning methods are increasingly attracting attention as alternative approaches. Appearance-based deep learning methods show potential for future stroke diagnosis but require accuracy improvements. Future directions in stroke diagnosis and clinical applications mainly include synthetic data generation, VR integration, wearable ETT devices, multimodal data fusion, and expanding application scenarios. Finally, we propose an active perception strategy and a stroke medical system framework for ETT application. This paper aims to provide researchers with a rapid understanding of core technologies and comprehensive knowledge.}
}
@article{ZHAO2025101014,
title = {The use of generative AI by students with disabilities in higher education},
journal = {The Internet and Higher Education},
volume = {66},
pages = {101014},
year = {2025},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2025.101014},
url = {https://www.sciencedirect.com/science/article/pii/S1096751625000235},
author = {Xin Zhao and Andrew Cox and Xuanning Chen},
keywords = {ChatGPT, Artificial intelligence (AI), Generative AI, Students with disabilities, Academic writing, AI literacy},
abstract = {The use of generative AI is controversial in education largely because of its potential impact on academic integrity. Yet some scholars have suggested it could be particularly beneficial for students with disabilities. To date there has been no empirical research to discover how these students use generative AI in academic writing. Informed by a prior interview study and AI-literacy model, we surveyed students regarding their use of generative AI, and gained 124 valid responses from students with disabilities. We identified primary conditions affecting writing such as ADHD, dyslexia, dyspraxia, and autism. The main generative AI used were chatbots, particularly ChatGPT, and rewriting applications. They were used in a wide range of academic writing tasks. Key concerns students with disabilities had included the inaccuracy of AI answers, risks to academic integrity, and subscription cost barriers. Students expressed a strong desire to participate in AI policymaking and for universities to provide generative AI training. The paper concludes with recommendations to address educational disparities and foster inclusivity.}
}
@article{WANG2025112217,
title = {Capsule neural network and its applications in drug discovery},
journal = {iScience},
volume = {28},
number = {4},
pages = {112217},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.112217},
url = {https://www.sciencedirect.com/science/article/pii/S258900422500478X},
author = {Yiwei Wang and Binyou Wang and Jun Zou and Anguo Wu and Yuan Liu and Ying Wan and Jiesi Luo and Jianming Wu},
keywords = {Health sciences, Medicine, Computer science, Artificial intelligence applications},
abstract = {Summary
Deep learning holds great promise in drug discovery, yet its application is hindered by high labeling costs and limited datasets. Developing algorithms that effectively learn from sparsely labeled data is crucial. Capsule networks (CapsNet), introduced in 2017, solve the spatial information loss in traditional neural networks and excel in handling small datasets by capturing spatial hierarchical relationships among features. This capability makes CapsNet particularly promising for drug discovery, where data scarcity is a common challenge. Various modified CapsNet architectures have been successfully applied to drug design and discovery tasks. This review provides a comprehensive analysis of CapsNet’s theoretical foundations, its current applications in drug discovery, and its performance in addressing key challenges in the field. Additionally, the study highlights the limitations of CapsNet and outlines potential future research directions to further enhance its utility in drug discovery, offering valuable insights for researchers in both computational and pharmaceutical sciences.}
}
@article{BUCZYNSKI2025106111,
title = {Future themes in regulating artificial intelligence in investment management},
journal = {Computer Law & Security Review},
volume = {56},
pages = {106111},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106111},
url = {https://www.sciencedirect.com/science/article/pii/S0267364925000068},
author = {Wojtek Buczynski and Felix Steffek and Mateja Jamnik and Fabio Cuzzolin and Barbara Sahakian},
keywords = {AI, Artificial intellogence, Investments, investment management, Finance, Financial services, Regulations, law, Laws, Regulation},
abstract = {We are witnessing the emergence of the “first generation” of AI and AI-adjacent soft and hard laws such as the EU AI Act or South Korea's Basic Act on AI. In parallel, existing industry regulations, such as GDPR, MIFID II or SM&CR, are being “retrofitted” and reinterpreted from the perspective of AI. In this paper we identify and analyze ten novel, “second generation” themes which are likely to become regulatory considerations in the near future: non-personal data, managerial accountability, robo-advisory, generative AI, privacy enhancing techniques (PETs), profiling, emergent behaviours, smart contracts, ESG and algorithm management. The themes have been identified on the basis of ongoing developments in AI, existing regulations and industry discussions. Prior to making any new regulatory recommendations we explore whether novel issues can be solved by existing regulations. The contribution of this paper is a comprehensive picture of emerging regulatory considerations for AI in investment management, as well as broader financial services, and the ways they might be addressed by regulations – future or existing ones.}
}
@article{SCUOTTO2025115217,
title = {Growth hacking: Leveraging hyper-scalability, hyper-specialization, and human-centric strategies for competitive advantage},
journal = {Journal of Business Research},
volume = {190},
pages = {115217},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2025.115217},
url = {https://www.sciencedirect.com/science/article/pii/S0148296325000402},
author = {Veronica Scuotto and Theofilos Tzanidis and Alan Murray and Del Giudice Manlio},
keywords = {Growth hacking, Digital strategy, Digital adoption, Digital capability, Hyper-scalability, Hyprspecialization, Human-based competitive advantage},
abstract = {The Growth Hacking process brings new opportunities for business which are achieved through new strategies such as hyper-scalability, hyperspecialization and human- based competitive advantage. However this is a field which has not been deeply studied. In order to bridge this gap, by applying the lens of resource-based theory of the digital firm, this study is based on the findings from 20 semi-structured interviews with growth orientated entrepreneurs from a diverse range of sectors based in the UK. The study applies a content analysis and inductive approach and the results show that in the competitive marketplace new digital skills are needed to grow a successful business and these skills should be balanced with the introduction of any new technologies. The requirement for human skills and engagement is necessity for the development and leverage of unique capabilities and competencies to drive growth hacking strategies. As a result, these new strategies allow entrepreneurs to exploit new opportunities and overcome business challenges.}
}
@article{ZHAO2025,
title = {High-throughput and intelligent design of potential GRK2 inhibitor candidates using deep learning and mathematical programming methods},
journal = {Chinese Journal of Chemical Engineering},
year = {2025},
issn = {1004-9541},
doi = {https://doi.org/10.1016/j.cjche.2025.02.024},
url = {https://www.sciencedirect.com/science/article/pii/S1004954125001338},
author = {Yujing Zhao and Qilei Liu and Jian Du and Qingwei Meng and Liang Sun and Lei Zhang},
keywords = {mathematical modeling, optimal design, product design, GRK2, mathematical programming method, binding affinity},
abstract = {G protein coupled receptor kinase 2 (GRK2) is a kinase that regulates cardiac signaling activity. Inhibiting GRK2 is a promising mechanism for the treatment of heart failure (HF). Further development and optimization of inhibitors targeting GRK2 are highly meaningful. Therefore, in order to design GRK2 inhibitors with better performance, the most active molecule was selected as a reference compound from a data set containing 4-pyridylhydrazone derivatives and triazole derivatives, and its scaffold was extracted as the initial scaffold. Then, a powerful optimization-based framework for de novo drug design, guided by binding affinity, was used to generate a virtual molecular library targeting GRK2. The binding affinity of each virtual compound in this dataset was predicted by our developed deep learning model, and the designed potential compound with high binding affinity was selected for molecular docking and molecular dynamics simulation. It was found that the designed potential molecule binds to the ATP site of GRK2, which consists of key amino acids including Gly198, Arg199, Gly200, Phe202, Val205, Lys220, Met274 and Asp335. The scaffold of the molecule is stabilized mainly by H-bonding and hydrophobic contacts. Concurrently, the reference compound in the dataset was also simulated by docking. It was found that this molecule also binds to the ATP site of GRK2. In addition, its scaffold is stabilized mainly by H-bonding and π-cation stacking interactions with Lys220, as well as hydrophobic contacts. The above results show that the designed potential molecule has similar binding modes to the reference compound, supporting the effectiveness of our framework for activity-focused molecular design. Finally, we summarized the interaction characteristics of general GRK2 inhibitors and gained insight into their molecule-target binding mechanisms, thereby facilitating the expansion of lead to hit compound.}
}
@article{ITO2025102283,
title = {ZenSVI: An open-source software for the integrated acquisition, processing and analysis of street view imagery towards scalable urban science},
journal = {Computers, Environment and Urban Systems},
volume = {119},
pages = {102283},
year = {2025},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2025.102283},
url = {https://www.sciencedirect.com/science/article/pii/S0198971525000365},
author = {Koichi Ito and Yihan Zhu and Mahmoud Abdelrahman and Xiucheng Liang and Zicheng Fan and Yujun Hou and Tianhong Zhao and Rui Ma and Kunihiko Fujiwara and Jiani Ouyang and Matias Quintana and Filip Biljecki},
keywords = {Street-level imagery, Python package, Computer vision, FAIR, Reproducibility},
abstract = {Street view imagery (SVI) has been instrumental in many studies in the past decade to understand and characterize street features and the built environment. Researchers across a variety of domains, such as transportation, health, architecture, human perception, and infrastructure have employed different methods to analyze SVI. However, these applications and image-processing procedures have not been standardized, and solutions have been implemented in isolation, often making it difficult for others to reproduce existing work and carry out new research. Using SVI for research requires multiple technical steps: accessing APIs for scalable data collection, preprocessing images to standardize formats, implementing computer vision models for feature extraction, and conducting spatial analysis. These technical requirements create barriers for researchers in urban studies, particularly those without extensive programming experience. We developed ZenSVI, a free and open-source Python package that integrates and implements the entire process of SVI analysis, supporting a wide range of use cases. Its end-to-end pipeline includes downloading SVI from multiple platforms (e.g., Mapillary and KartaView) efficiently, analyzing metadata of SVI, applying computer vision models to extract target features, transforming SVI into different projections (e.g., fish-eye and perspective) and different formats (e.g., depth map and point cloud), visualizing analyses with maps and plots, and exporting outputs to other software tools. We demonstrated its use in Singapore through a case study of data quality assessment and clustering analysis in a streamlined manner. Our software improves the transparency, reproducibility, and scalability of research relying on SVI and supports researchers in conducting urban analyses efficiently. Its modular design facilitates extensions of the package for new use cases. This package is openly available at https://github.com/koito19960406/ZenSVI, and it is supported by documentation including tutorials (https://zensvi.readthedocs.io/en/latest/examples/index.html).}
}
@article{ZHANG2025108665,
title = {“Their Privacy Matters Too!” A Self-other Calculus Perspective on Disclosure of Others’ Information on Social Media: The Influence of Relationship Closeness and Affect},
journal = {Computers in Human Behavior},
pages = {108665},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2025.108665},
url = {https://www.sciencedirect.com/science/article/pii/S0747563225001128},
author = {Mingxin Zhang and Hou Zhu},
keywords = {Disclosure of Information about Others, Peer Privacy, Interdependent Privacy, Co-owned Privacy, Privacy Calculus, Relationship Closeness, Affect},
abstract = {This study addresses the often-overlooked behavioral mechanisms underlying interdependent privacy violations on social media, where individuals (i.e., data senders) post content about other people (i.e., data subjects). In response, we introduce a self-other calculus framework to consider both personal and others’ benefits and risks in such sharing decisions. It hypothesizes that data senders weigh these factors, influenced by affect (intrapersonal influence) and their relationship closeness with data subjects (interpersonal influence). Using 2×2 between-subjects design, we performed a scenario-based experiment with 1007 participants. Data analysis through multiple linear regression and Hayes’s PROCESS macro confirmed our main hypotheses. We identified two important conditions under which people tend to disclose others’ information: (1) when data senders and data subjects have a closer interpersonal relationship and (2) when data senders feel emotionally positive. This study shifts extant behavioral research focus from personal privacy to the interpersonal context, as well as examines the context-dependent and bounded-rationality nature of privacy decisions involving multiple stakeholders.}
}
@article{SALAHSHOORI2025216580,
title = {Comprehensive insights into molecular simulation-driven advances in functional materials for pollutant mitigation},
journal = {Coordination Chemistry Reviews},
volume = {534},
pages = {216580},
year = {2025},
issn = {0010-8545},
doi = {https://doi.org/10.1016/j.ccr.2025.216580},
url = {https://www.sciencedirect.com/science/article/pii/S001085452500150X},
author = {Iman Salahshoori and Majid {Namayandeh Jorabchi} and Morteza Asghari and Sebastian Wohlrab and Mehdi Golriz and Hossein Ali Khonakdar},
keywords = {Air pollutants, Advanced functional materials, Computational methods, Gas separation, Materials design, Molecular simulations},
abstract = {Removing gaseous pollutants from the environment is a pressing global concern due to their detrimental effects on human health and ecosystems. Adsorbents, materials capable of capturing and retaining gaseous molecules, play a crucial role in addressing this issue. While laboratory experiments are indispensable for adsorbent development, they can be time-consuming and resource-intensive. On the other hand, molecular simulation methods offer a powerful alternative by providing insights into the adsorption process at the molecular level. This review article explores the application of molecular simulation in designing and optimizing functional materials for gaseous pollutant mitigation. It discusses the fundamental principles of molecular simulation techniques and their advantages over traditional laboratory methods. A wide range of adsorbent materials, including polymers, carbon nanotubes, graphene oxide, zeolites, metal-organic frameworks, zeolitic imidazolate frameworks, and covalent organic frameworks, are examined in detail. Numerous practical examples illustrate how molecular simulation can predict adsorption capacities, selectivity, and kinetics. This review aims to empower researchers to create more efficient and sustainable solutions for gaseous pollutant removal by providing a comprehensive overview of molecular simulation methods and their applications in adsorbent development. The insights gained from molecular simulation can accelerate the development of innovative adsorbents, ultimately contributing to a cleaner and healthier environment.}
}
@article{MANIERRE2025102923,
title = {Coexisting with ChatGPT: Evaluating a tool for AI-based paper revision},
journal = {Computers and Composition},
volume = {76},
pages = {102923},
year = {2025},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2025.102923},
url = {https://www.sciencedirect.com/science/article/pii/S8755461525000106},
author = {Matt Manierre and Lisa Propst and Alex Cohen and JoAnn Rogers},
keywords = {ChatGPT, Revision, Writing, Pedagogy, First-year},
abstract = {AI based tools such as ChatGPT have presented many challenges to educators since they entered the scene in 2022. We present our effort to coexist with ChatGPT in the classroom, developing an exercise for first year writing students to use ChatGPT while revising papers. The effectiveness of this activity was determined using pretest/posttest surveys (n = 64 and 53) and one- page reflective essays. Survey results indicated that students had largely positive appraisals of the different elements of the exercise, describing them as useful without reducing their appreciation of writing as an essential skill for the future. Yet, student writing self-efficacy also did not improve after working with ChatGPT. Qualitative responses were often positive but students frequently reported frustrations with ChatGPT rewriting work when told not to and providing only generic feedback. We offer our exercise as a means to engage students in critical thought about ChatGPT's uses, limitations, and implications for academic integrity. We suggest ways to iterate on this tool and to incorporate it in future work but emphasize that students must be taught to use AI tools with considerable skepticism.}
}
@article{GARCIA2025104267,
title = {Physics-informed digital twin design for supporting the selection of process settings in continuous manufacturing, with a focus in fiberboard production},
journal = {Computers in Industry},
volume = {168},
pages = {104267},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104267},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525000326},
author = {Francisco Ambrosio Garcia and Hendrik Devriendt and Hüseyin Metin and Merih Özer and Frank Naets},
keywords = {Digital twin, Machine learning, Smart manufacturing, Human-centric, Settings selection support},
abstract = {In process industry, plant operators often rely on their experience to choose suitable process settings that meet the productivity and quality goals. When these goals are not met, multiple changes to the settings might be necessary, which is time-consuming because each adjustment requires waiting for the new steady-state condition. A digital twin that quickly provides key performance indicators in steady-state as a function of these settings can speed up this task. The settings can be manually simulated before being adopted, or the digital twin can be integrated into an optimizer to automatically suggest optimal values to the operator, who ultimately makes the final decision. Despite advances in approaches to design such digital twins, most studies lack strategies to update the models when the plant behavior changes, and often overlook constraints and human-centric aspects of the plant operation. To address these gaps, we present a framework for training, tuning, and updating models for supporting the selection of process settings in continuous manufacturing. By directly mapping the steady-state conditions as a function of process settings, our approach enables informed decision-making and paves the way towards process optimization without requiring modifications to the plant control software, a crucial factor in established plants to ensure safety. We propose an interpretable model architecture, and a training process that incorporates both data and prior physical knowledge. Triggers detect deviations between the models’ predictions and the plant condition, in order to start model updates. The procedure for updating the models is tuned to perform consistently well in a variety of conditions, based on substantial simulations in historical data. To select the triggers, we balance technical and human aspects, by considering the trade-off between frequent model updates, increasing operator workload with frequent settings changes, versus how closely the models track the plant conditions. The framework is applied to five different stages of the fiberboard production process in a 1.4-year dataset, to predict key energy and quality-related variables as a function of process settings. The results show that the models, when connected to the data stream, are effectively updated when needed, show high sensitivity to the process settings and consistency with the available physical knowledge, making them well-suited to support the selection of process settings.}
}
@article{MCNAUGHTON2025100130,
title = {How frontline states tackle sanctions against Russia: Implementation and enforcement dynamics in Poland and the Baltics},
journal = {Journal of Economic Criminology},
volume = {8},
pages = {100130},
year = {2025},
issn = {2949-7914},
doi = {https://doi.org/10.1016/j.jeconc.2025.100130},
url = {https://www.sciencedirect.com/science/article/pii/S2949791425000065},
author = {Katarzyna J. McNaughton and Marcin Łukowski},
keywords = {EU Sanctions Enforcement on Russia, Comparative Study of Sanctions Implementation in Baltic States, Poland's Sanctions Enforcement Framework, Russian Sanctions Enforcement in Frontline Countries},
abstract = {Russia’s invasion of Ukraine in February 2022, reshaped the EU’s security landscape, prompting sanctions aimed at weakening Russia’s war capabilities. These sanctions also redefined the roles of public authorities and the private sector, introducing new challenges in a shifting geopolitical context. Public authorities, including financial intelligence units, customs, state security agencies, law-enforcement agencies, etc., must identify, prevent, and investigate sanctions evasion and circumvention. This requires robust legal frameworks, adequate resources, and expertise in sanctions evasion typologies. Similarly, businesses and financial institutions operate in legal ambiguity, often asking, “Who am I dealing with in this transaction?”, as they navigate complex compliance requirements. Both the public and private sectors need a strong framework for domestic and cross-border sharing of financial intelligence, trade data, and knowledge of sanctions evasion typologies, as well as insight into the corporate structures of sanctioned entities. However, the EU's decentralized approach of independently designed national enforcement models may hamper cooperation and cross-border financial intelligence sharing. This paper examines how Poland, Lithuania, Latvia, and Estonia that are post-Warsaw Pact EU countries bordering Russia, implement and enforce those sanctions. It explores who "does what" and whether national authorities are adapting their modi operandi to enforce sanctions effectively. The findings reveal distinct national approaches. Latvia’s FIU became Europe’s first sanctions authority, integrating intelligence and enforcement functions. Estonia’s FIU plays a significant role but shares responsibilities with other agencies. Lithuania’s FIU adopts a collaborative model, leveraging a public-private partnership with the Center of Excellence in Anti-Money Laundering. Poland has a fragmented enforcement structure and regulatory framework but is unique in implementing its own autonomous sanctions.}
}
@article{THELWALL2025103023,
title = {Estimating the quality of academic books from their descriptions with ChatGPT},
journal = {The Journal of Academic Librarianship},
volume = {51},
number = {2},
pages = {103023},
year = {2025},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2025.103023},
url = {https://www.sciencedirect.com/science/article/pii/S0099133325000199},
author = {Mike Thelwall and Andrew Cox},
keywords = {Large language models, Collection acquisition, Scientometrics, bibliometrics},
abstract = {Although indicators based on scholarly citations are widely used to support the evaluation of academic journals, alternatives are needed for scholarly book acquisitions. This article assesses the value of research quality scores from ChatGPT 4o-mini for 9830 social sciences, arts, and humanities books from 2019 indexed in Scopus, based on their titles and descriptions but not their full texts. Although most books scored the same (3* on a 1* to 4* scale), the citation rates correlate positively but weakly with ChatGPT 4o-mini research quality scores in both the social sciences and the arts and humanities. Part of the reason for the differences was the inclusion of textbooks, short books, and edited collections, all of which tended to be less cited and lower scoring. Some topics also tend to attract many/few citations and/or high/low ChatGPT scores. Descriptions explicitly mentioning theory and/or some methods also associated with higher scores and more citations. Overall, the results provide some evidence that both ChatGPT scores and citation counts are weak indicators of the research quality of books. Whilst not strong enough to support individual book quality judgements, they may help academic librarians seeking to evaluate new book collections, series, or publishers for potential acquisition.}
}
@article{ALOMAR2025107687,
title = {An empirical study on the impact of code duplication-aware refactoring practices on quality metrics},
journal = {Information and Software Technology},
volume = {182},
pages = {107687},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107687},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000266},
author = {Eman Abdullah AlOmar},
keywords = {Refactoring, Quality, Code duplicates, Metrics},
abstract = {Context:
Code refactoring is widely recognized as an essential software engineering practice that improves the understandability and maintainability of source code. Several studies attempted to detect refactoring activities through mining software repositories, allowing one to collect, analyze, and get actionable data-driven insights about refactoring practices within software projects.
Objective:
Our goal is to identify, among the various quality models presented in the literature, the ones that align with the developer’s vision of eliminating duplicates of code, when they explicitly mention that they refactor the code to improve them.
Method:
We extract a corpus of 332 refactoring commits applied and documented by developers during their daily changes from 128 open-source Java projects. In particular, we extract 32 structural metrics from which we identify code duplicate removal commits with their corresponding refactoring operations, as perceived by software engineers. Thereafter, we empirically analyze the impact of these refactoring operations on a set of common state-of-the-art design quality metrics.
Results:
The statistical analysis of the results obtained shows that (i) some state-of-the-art metrics are capable of capturing the developer’s intention of removing code duplication; and (ii) some metrics are being more emphasized than others. We confirm that various structural metrics can effectively represent code duplication, leading to different impacts on software quality. Some metrics contribute to improvements, while others may lead to degradation.
Conclusion:
Most of the mapped metrics associated with the main quality attributes successfully capture developers’ intentions for removing code duplicates, as is evident from the commit messages. However, certain metrics do not fully capture these intentions.}
}
@article{GARCIALAZARO2025112294,
title = {The digital skill premium: Evidence from job vacancy data},
journal = {Economics Letters},
volume = {250},
pages = {112294},
year = {2025},
issn = {0165-1765},
doi = {https://doi.org/10.1016/j.econlet.2025.112294},
url = {https://www.sciencedirect.com/science/article/pii/S0165176525001314},
author = {Aida Garcia-Lazaro and Jorge Mendez-Astudillo and Susan Lattanzio and Charles Larkin and Linda Newnes},
keywords = {Digital skill premium, AI, Cybersecurity, Posted wages, Xgboost},
abstract = {This paper examines the relationship between digital skills demand and posted wages in the UK using novel vacancy data. Digital skills — classified into basic, intermediate, and advanced using an XGBoost model — are linked to significant wage premiums. Within occupations, they are associated with 5.8% higher wages, with advanced and intermediate skills increasing wages by up to 8.9% when listed in job postings. Each additional digital skill increases wages by 1%, rising to 1.6% for advanced and intermediate skills. Artificial intelligence (AI) and cybersecurity skills yield particularly high returns, increasing wages by 8.6%–9.7% when listed and by 4.8%–5.4% per additional skill.}
}
@article{KENNEDY2025106116,
title = {Asia-Pacific Developments},
journal = {Computer Law & Security Review},
volume = {56},
pages = {106116},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106116},
url = {https://www.sciencedirect.com/science/article/pii/S0267364925000111},
author = {Gabriela Kennedy and Joanna Wong and Justin Lai and James North and Philip Catania and Michael do Rozario and Jack Matthews and Arun Babu and Gayathri Poti and Ishita Vats and Kiyoko Nakaoka and Lam Chung Nian and Emma Choe},
abstract = {This column provides a country by country analysis of the latest legal developments, cases and issues relevant to the IT, media and telecommunications' industries in key jurisdictions across the Asia Pacific region. The articles appearing in this column are intended to serve as ‘alerts’ and are not submitted as detailed analyses of cases or legal developments.}
}
@article{SALARI2025100652,
title = {Impacts of generative artificial intelligence on the future of labor market: A systematic review},
journal = {Computers in Human Behavior Reports},
volume = {18},
pages = {100652},
year = {2025},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2025.100652},
url = {https://www.sciencedirect.com/science/article/pii/S2451958825000673},
author = {Nader Salari and Mahan Beiromvand and Amin Hosseinian-Far and Javad Habibi and Fateme Babajani and Masoud Mohammadi},
keywords = {Job market, AI, ChatGPT, Labor market, GenAI},
abstract = {Background
Generative AI (GenAI) has the ability to autonomously collect and process data to generate contents, inform decisions, solve problems, and perform tasks that typically require human reasoning. This Systematic Review is conducted to examine the impacts of GenAI on the future of employment, focusing on concerns about rising unemployment, and the positive and negative perspectives outlined within exiting studies. The findings from this review can help identify research gaps, guide organizational planning, and improve AI governance frameworks and policies.
Methods
To identify relevant studies, the PubMed, Scopus, Web of Science, Embase, ScienceDirect and Google Scholar databases and repositories were systematically searched using the keywords: ‘Future of work’, ‘Job market’, ‘Generative AI’, ‘Generative AI’, and ‘ChatGPT’. Additionally, the reference lists of the identified related articles were reviewed for grey literature.
Results
Following the PRISMA guidelines, a total of 14 articles were selected for analysis. Selected studies have examined the positive and negative viewpoints on GenAI, together with pertinent challenges and opportunities. Accordingly, GenAI, when compliant with security and ethical issues, has the potential to increase efficiency whilst reducing costs and time.
Conclusion
Considering the rapid growth and adoption of AI technologies, examining the impacts of GenAI on the future of labor market is crucial. GenAI is likely to create new roles in some sectors yet reduce opportunities in others. A nuanced assessment of the impacts, and ongoing monitoring are vital for effective preparation and adaptation to the evolving work landscape in the presence of advanced AI technologies.}
}
@article{PEDRO2025100664,
title = {Screening and enhancing intellectual capital consistency: A scoping review of systematised literature reviews},
journal = {Journal of Innovation & Knowledge},
volume = {10},
number = {2},
pages = {100664},
year = {2025},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2025.100664},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X25000150},
author = {Eugénia Pedro and João Leitão and Helena Alves},
keywords = {Digital transformation, Digitalisation, Ecosystems, Entrepreneurship, Innovation, Intellectual capital, Knowledge, Sustainability},
abstract = {This scoping review aimed to screen and enhance intellectual capital consistency, providing a general overview of existing systematised literature reviews of intellectual capital. This scoping review addressed five research questions: 1. How did the typology of publications, both theoretical and empirical, evolve considering the different stages of intellectual capital? 2. What are the milestone studies marking the beginning of the five stages of evolution of the intellectual capital framework? 3. How has the intellectual capital construct evolved? 4. What are the main streams of research on intellectual capital and how are they characterised? 5. What are the most relevant topics, gaps, and future trends in the field of intellectual capital? To answer the research questions, a scoping review was conducted following a search of the Web of Science and SCOPUS databases. The final search identified 78 full-text articles, published between 2005 and 2023. The evidence revealed the emergence of new topics and identified 13 clusters. We found evidence of the beginning of the fifth stage around 2018 due to the change in the research paradigm observed in this study and the development of new themes, such as innovation, digitalisation, knowledge, sustainability, and entrepreneurship, contributing to the ecosystem development of cities, regions, and nations. This pioneering scoping review systematised literature reviews about intellectual capital, providing important implications for theory, as it presents paths to follow and relevant indications for the evolving fifth stage of intellectual capital.}
}
@article{FU2025101248,
title = {The future of pharmaceuticals: Artificial intelligence in drug discovery and development},
journal = {Journal of Pharmaceutical Analysis},
pages = {101248},
year = {2025},
issn = {2095-1779},
doi = {https://doi.org/10.1016/j.jpha.2025.101248},
url = {https://www.sciencedirect.com/science/article/pii/S2095177925000656},
author = {Chen Fu and Qiuchen Chen},
keywords = {AI, Drugs, Research and development, Machine learning},
abstract = {Artificial Intelligence (AI) is revolutionizing traditional drug discovery and development models by seamlessly integrating data, computational power, and algorithms. This synergy enhances the efficiency, accuracy, and success rates of drug research, shortens development timelines, and reduces costs. Coupled with machine learning (ML) and deep learning (DL), AI has demonstrated significant advancements across various domains, including drug characterization, target discovery and validation, small molecule drug design, and the acceleration of clinical trials. Through molecular generation techniques, AI facilitates the creation of novel drug molecules, predicting their properties and activities, while virtual screening optimizes drug candidates. Additionally, AI enhances clinical trial efficiency by predicting outcomes, designing trials, and enabling drug repositioning. However, AI's application in drug development faces challenges, including the need for robust data-sharing mechanisms and the establishment of more comprehensive intellectual property protections for algorithms. AI-driven pharmaceutical companies must also integrate biological sciences and algorithms effectively, ensuring the successful fusion of wet and dry laboratory experiments. Despite these challenges, the potential of AI in drug development remains undeniable. As AI technology evolves and these barriers are addressed, AI-driven therapeutics are poised for a broader and more impactful future in the pharmaceutical industry.}
}
@article{BERKOWITZ2025103026,
title = {“Domo arigato, Mr. Roboto”: A qualitative content analysis of AI music in WorldCat},
journal = {The Journal of Academic Librarianship},
volume = {51},
number = {2},
pages = {103026},
year = {2025},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2025.103026},
url = {https://www.sciencedirect.com/science/article/pii/S0099133325000229},
author = {Adam Eric Berkowitz},
keywords = {Artificial intelligence, Librarianship, Ethics, Cataloging, WorldCat, Music},
abstract = {Artificial intelligence (AI) has been used for experimentation in generating music for the last seventy years, but recent advances in generative AI (genAI) have led to novel, creative, and even surprising results. Issues arise when genAI and human efforts are simultaneously recognized in a creative work, constituting the uncanny valley and leading to discomfort among listeners. Additionally, the lack of transparency required of media producers regarding genAI use robs audiences of their right to choose whether to engage or avoid genAI content. This has sparked discussions among researchers, industry leaders, and lawmakers about regulating genAI use with priority given to enforcing transparency. Libraries can play a role in this by curating metadata when cataloging genAI materials, but current cataloging practices and policies inhibit the cataloger's ability to maximize accuracy and transparency when describing genAI items. This study features a content analysis that examines WorldCat item records belonging to genAI songs and music albums and finds inconsistent item record descriptions, often vaguely referring to or omitting genAI use. Supported by epidata theory, this study recommends adopting Resource Description and Access (RDA) standards to improve accuracy and transparency in cataloging genAI music.}
}
@article{GILMOUR2025124051,
title = {Registers of beneficial owners based on blockchain technology: Implications for the accounting profession},
journal = {Technological Forecasting and Social Change},
volume = {214},
pages = {124051},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2025.124051},
url = {https://www.sciencedirect.com/science/article/pii/S0040162525000824},
author = {Paul Gilmour and Durgesh Pandey and Doron Goldbarsht},
keywords = {Blockchain, Beneficial ownership, Accounting, Registers, Trust, AML/CTF},
abstract = {Central registers of beneficial owners are fraught with legal loopholes, and trust, privacy, and verification issues that devalue accountants' anti-money laundering compliance efforts. Yet, central registers have become important policy tools for governments in enhancing corporate transparency. We propose a blockchain-based solution that provides a more open, verifiable, and secure framework for registering beneficial ownership information, while ensuring greater transparency and trust. This paper examines the role of blockchain technology within registers of beneficial owners and highlights important implications for the accounting sector. We offer a fresh perspective into how blockchain technology supports company disclosure and contribute new insights into research on ‘digital trust’. This paper bridges the gap between the conceptual and real-world implementation of registers of beneficial owners and promotes a more nuanced understanding on how new digital infrastructures impact accountants' compliance responsibilities.}
}
@article{QU2025102530,
title = {Conjugated STING agonists},
journal = {Molecular Therapy Nucleic Acids},
pages = {102530},
year = {2025},
issn = {2162-2531},
doi = {https://doi.org/10.1016/j.omtn.2025.102530},
url = {https://www.sciencedirect.com/science/article/pii/S2162253125000848},
author = {Shuhao Qu and Hong Dai},
abstract = {ABSTRACT
Innate immune system is the first line of defense and is to prevent host from infection and attack the invading pathogens. Stimulator of interferon genes (STING) plays a vital role in the innate immune system. STING activation by STING agonists leads to phosphorylation of TANK-binding kinase 1 (TBK1) and interferon regulatory factor 3 (IRF3) with the release of type I interferons and proinflammatory cytokines, further promoting the adaptive immune response and activating T-cells by increased antigen presentation. Natural STING agonist cyclic dinucleotides (CDNs) face many defects such as high polarity by negative charges, low stability and circulative half-life, off-target systemic toxicity and low response efficacy in clinical trials. To overcome these challenges, massive efforts have been put on following aspects: chemical modifications of CDNs, development of non-CDNs STING agonists and delivery of these STING agonists either by conjugations or liposomes/nanoparticles. Considering there have been a great number of reports regarding nanosystem-aided delivery, here we address the development of STING agonists especially for non-CDNs and their delivery specifically by conjugation strategy, with a typical focus on the STING agonists in clinical trials and current challenges of their potential in cancer immunotherapy.}
}
@article{BUNDUCHI2025124095,
title = {A legitimacy-based explanation for user acceptance of controversial technologies: The case of Generative AI},
journal = {Technological Forecasting and Social Change},
volume = {215},
pages = {124095},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2025.124095},
url = {https://www.sciencedirect.com/science/article/pii/S004016252500126X},
author = {Raluca Bunduchi and Dan-Andrei Sitar-Tăut and Daniel Mican},
keywords = {Technology acceptance, Legitimacy, Technology uncertainty, Technology variation},
abstract = {Controversial technologies are technologies where social concerns play a disproportionate role in shaping the public attitudes to their adoption. An example of such controversial technologies is Generative Artificial Intelligence (GenAI), whose rapid diffusion is fuelled by expectations for significant performance improvements, while also facing concerns at individual (trust in technology), technology (accuracy and quality), and institutional (cultural, ethical and regulatory) level. Individual and technology factors are well accounted for by rational choice-based models which underpin most technology acceptance research. Such models are less suited to explore the role of institutional factors in shaping technology acceptance. Drawing from legitimacy and technology lifecycle research, we develop a legitimacy-based model of GenAI adoption which accounts for the institutional context in which technology use happens, and for technology characteristics, namely its maturity, in shaping users' acceptance. Surveying 483 information systems students who are GenAI users, we find that users' perceptions of technology uncertainty and variation positively affect their technology legitimacy evaluations and that their pragmatic and cognitive legitimacy evaluations, but not moral, affect their intention to use. We answer recent calls to examine alternative theoretical predictors of technology acceptance, and to consider the role of context in examining the acceptance of controversial technologies.}
}
@article{SHAIKH2025104005,
title = {Fields of the future: Digital transformation in smart agriculture with large language models and generative AI},
journal = {Computer Standards & Interfaces},
volume = {94},
pages = {104005},
year = {2025},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2025.104005},
url = {https://www.sciencedirect.com/science/article/pii/S0920548925000340},
author = {Tawseef Ayoub Shaikh and Tabasum Rasool and Waseem Ahmad Mir},
keywords = {Language models, Agricultural text classification, Very large pre-trained language model, Generative pre-trained enerative pre-trained transformer (GPT), ChatGPT, Generative AI, Natural language processing, Semantic matching},
abstract = {Language models (LLMs) have shown to be very useful in many fields like healthcare and finance, as natural language comprehension and generation have advanced. The capacity of LLM to participate in textual discussion has been the subject of much research, and the findings have proved encouraging across several domains. The inability of conventional image classification networks to comprehend the causes of crop diseases and etiology further impedes precise diagnosis. Agricultural diagnostic models on a grand scale will be based on generative pre-trained transformers (GPT) assisted with agrarian settings. By examining the efficacy of text corpora linked to agriculture for pretraining transformer-based language (TBL) models, this research delves into agricultural natural language processing (ANLP). To make the most of it, we looked at several important aspects, including prompt building, response parsing, and several ChatGPT versions. Despite the proven effectiveness and huge potential, there has been little exploration of LLM and Generative AI to agriculture artificial intelligence (AI). Therefore, this study aims to explore the possibility of LLM and Generative AI in smart agriculture. In particular, we present conceptual tools and technical background to facilitate understanding the problem space and uncover new research directions in this field. The paper presents an overview of the evolution of generative adversarial network (GAN) architectures followed by a first systematic review of various applications in smart agriculture and precision farming systems, involving a diversity of visual recognition tasks for smart farming and livestock, precision agriculture, agricultural language processing (ALP), agricultural robots (AR), plant phenotyping (PP), and postharvest quality assessment. We outline the possibilities, difficulties, constraints, and shortcomings. The study lays forth a road map of accessible areas in agriculture where LLM integration is likely to happen shortly. The research suggests exciting directions for further study in this area, which could lead to better agricultural NLP applications.}
}
@article{TAN2025100023,
title = {Recent advances in nanomedicines loaded with MicroRNAs for cancer demethylation therapy},
journal = {Precision Medicine and Engineering},
volume = {2},
number = {1},
pages = {100023},
year = {2025},
issn = {2950-4821},
doi = {https://doi.org/10.1016/j.preme.2025.100023},
url = {https://www.sciencedirect.com/science/article/pii/S2950482125000087},
author = {Qiuyu Tan and Jinxia Wang and Ye Tian and João Rodrigues and Zhaojun Li and Xiangyang Shi and Mingwu Shen},
keywords = {miRNA, DNA methylation, Nanomedicines, Cancer demethylation therapy},
abstract = {DNA methylation constitutes an important form of epigenetic modification that can regulate gene expression through the addition of methyl groups to specific positions on DNA without modifying the DNA sequence. High levels of DNA methylation have been implicated in the progression of cancer; thus, DNA demethylation therapy can be considered a potential therapeutic strategy. MicroRNAs (miRNAs), essential epigenetic regulators, play pivotal roles in cancer development. In DNA demethylation therapy, miRNAs can regulate DNA methylation-related proteins to reduce or reverse DNA methylation levels and restore normal gene expression. Nanomaterials possess distinctive physicochemical properties that facilitate the efficient delivery of miRNAs to target cells. Through the optimization of the surface properties and structure of nanomaterials, their loading capacity and delivery efficiency of miRNAs can be further enhanced. Recently, nanomedicines loaded with miRNAs have been continuously developed for cancer demethylation therapy. Herein, we concisely review the relationship between miRNAs and cancer DNA methylation, the mechanisms of cancer DNA methylation, and the advances in cancer DNA demethylation treatment for nanomedicines loaded with miRNAs, with the intent of offering novel insights into cancer demethylation therapy.}
}
@article{YAO2025107704,
title = {Open source oriented cross-platform survey},
journal = {Information and Software Technology},
volume = {182},
pages = {107704},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107704},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000436},
author = {Simeng Yao and Xunhui Zhang and Yang Zhang and Tao Wang},
keywords = {Open source, Cross-platform, Systematic literature review, GitHub, StackOverflow, Twitter},
abstract = {Context:
Open-source software development has become a widely adopted approach to software creation. However, developers’ activities extend beyond social coding platforms (e.g., GitHub), encompassing social Q&A platforms (e.g., StackOverflow) and social media platforms (e.g., Twitter). Therefore, cross-platform research is essential for a deeper understanding of the nature of software development activities.
Objective:
This paper focuses on open-source platforms and systematically summarizes relevant cross-platform research. It aims to assess the current state of cross-platform research and provide insights into the challenges and future developments in this field.
Method:
This paper reviews 69 cross-platform research papers related to open-source software from 2013 to 2024, with a focus on several key areas, including platform interconnections, research themes, experimental design methods, challenges and research opportunities.
Results:
Through the analysis of 69 papers, we found that cross-platform research primarily involves platforms such as social coding, social Q&A, and social media. Researchers typically rely on information traces, including user personal info, technical info, project/post/bug report metadata, interaction info, to facilitate connections between platforms. Cross-platform research in the open-source domain mainly focuses on problem classification and feature extraction. The predominant research methods include data-driven approaches, qualitative studies, modeling and machine learning, and tool development and implementation. Despite these advancements, common challenges remain, such as subjective evaluation bias in manual data classification, insufficient data source coverage, and inaccurate data recognition. Future research opportunities may focus on increasing the diversity of data sources, improving data recognition accuracy, optimizing data classification methods, and clarifying user skill requirements.
Conclusions:
Based on our findings, we propose six future directions for cross-platform research in the open-source domain and provide corresponding recommendations for developers, researchers, and service/tool providers.}
}
@article{XIE2025105161,
title = {Transforming intangible cultural heritage in destinations: A fashion communication perspective},
journal = {Tourism Management},
volume = {110},
pages = {105161},
year = {2025},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2025.105161},
url = {https://www.sciencedirect.com/science/article/pii/S0261517725000317},
author = {Chaowu Xie and Feifei Lai and Jiangchi Zhang and Songshan (Sam) Huang},
keywords = {Destination, Intangible cultural heritage (ICH), Fashion communication, Framework construction, Diffusion of innovation theory},
abstract = {Intangible cultural heritage (ICH) is a key attraction for the development of tourist destinations, but few studies have examined the popularization of ICH in destinations through the lens of fashion communication. This research pioneers the conceptualization of ICH fashion communication in tourist destinations. Using qualitative and quantitative methods, we identify and construct a theoretical framework of ICH fashion communication. Study 1 reveals that the fashion communication of ICH in tourist destinations follows a process framework of “fashion communication elements - fashion communication channels - fashion communication results,” involving six distinct constructs. Study 2 and Study 3 demonstrate that the fashion communication elements (fashion representation, fashion ontology, and fashion construction) significantly influence tourists’ fashion perception. Additionally, fashion communication channels (diffusion of exhibition spaces and participation of diverse groups) mediate the relationship between these elements and tourists’ fashion perception. This research enhances the theoretical understanding of ICH communication and marketing in tourism.}
}
@article{MENG2025104181,
title = {A survey of secure semantic communications},
journal = {Journal of Network and Computer Applications},
pages = {104181},
year = {2025},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2025.104181},
url = {https://www.sciencedirect.com/science/article/pii/S1084804525000785},
author = {Rui Meng and Song Gao and Dayu Fan and Haixiao Gao and Yining Wang and Xiaodong Xu and Bizhu Wang and Suyu Lv and Zhidi Zhang and Mengying Sun and Shujun Han and Chen Dong and Xiaofeng Tao and Ping Zhang},
keywords = {Semantic communication, Wireless security, Privacy, 6G},
abstract = {Semantic communication (SemCom) is regarded as a promising and revolutionary technology in 6G, aiming to transcend the constraints of “Shannon’s trap” by filtering out redundant information and extracting the core of effective data. Compared to traditional communication paradigms, SemCom offers several notable advantages, such as reducing the burden on data transmission, enhancing network management efficiency, and optimizing resource allocation. Numerous researchers have extensively explored SemCom from various perspectives, including network architecture, theoretical analysis, potential technologies, and future applications. However, as SemCom continues to evolve, a multitude of security and privacy concerns have arisen, posing threats to the confidentiality, integrity, and availability of SemCom systems. This paper presents a comprehensive survey of the technologies that can be utilized to secure SemCom. Firstly, we elaborate on the entire life cycle of SemCom, which includes the model training, model transfer, and semantic information transmission phases. Then, we identify the security and privacy issues that emerge during these three stages. Furthermore, we summarize the techniques available to mitigate these security and privacy threats, including data cleaning, robust learning, defensive strategies against backdoor attacks, adversarial training, differential privacy, cryptography, blockchain technology, model compression, and physical-layer security. Lastly, this paper outlines future research directions to guide researchers in related fields.}
}
@incollection{SHARMA2025,
title = {Quantum paradigm In 6G revolutionising network},
series = {Advances in Computers},
publisher = {Elsevier},
year = {2025},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2025.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0065245825000452},
author = {Rajesh Sharma and Pethuru Chelliah},
keywords = {Quantum computing, Quantum mechanics, Exponential speedup, Information processing, Computation, Complexity, Optimization, Data security, 6G, Ultra-fast data transfer, Ultra-low latency, Synergies, Transformative impact},
abstract = {Assessing quantum paradigm in 6G revolutionising 6G
Quantum Computing and 6G are two rapidly evolving fields that have the potential to revolutionize information processing and communication technologies. Quantum computing leverages the principles of quantum mechanics to perform computations exponentially faster than classical computers. It promises breakthroughs in solving complex problems, optimizing processes, and enhancing data security. On the other hand, 6G is the next generation of wireless communication networks that aims to provide ultra-fast data transfer rates, ultra-low latency, massive connectivity, and support for emerging technologies such as artificial intelligence and the Internet of Things. This abstract provides an overview of the critical concepts and advancements in quantum computing and 6G, highlighting their potential applications, challenges, and the transformative impact they can have on various industries and society. It also explores the possible synergies between these two fields, envisioning a future where quantum computing enables advanced algorithms and optimization to enhance the performance and capabilities of 6G networks, leading to a new era of computing and communication. The advent of 6G networks promises unprecedented connectivity, speed, and reliability. However, the complex computational challenges associated with 6G systems demand novel approaches to meet the demands of future applications. Quantum computing holds tremendous potential to revolutionize various aspects of 6G, such as network optimization, security, and resource allocation. This research proposal explores the integration of quantum computing with 6G networks and investigates its potential benefits.
Objectives
The primary objectives of this research proposal are as follows:Investigate the potential applications of quantum computing in 6G networks.Develop quantum algorithms and protocols for key 6G functionalities.Assess the performance enhancements offered by quantum computing in 6G networks.Evaluate the feasibility and practicality of implementing quantum computing in 6G systems.Address the security challenges and vulnerabilities introduced by quantum computing in 6G networks.}
}
@article{EXNER2025,
title = {Going Digital to Boost Safe and Sustainable Materials Innovation Markets. The Digital Safe-and-Sustainability-by-Design Innovation Approach of the PINK Project},
journal = {Computational and Structural Biotechnology Journal},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2025.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S2001037025000881},
author = {Thomas E. Exner and Joh Dokler and Steffi Friedrichs and Christian Seitz and Francesca L. Bleken and Jesper Friis and Thomas F. Hagelien and Francesco Mercuri and Anna L. Costa and Irini Furxhi and Haralambos Sarimveis and Antreas Afantitis and Antonino Marvuglia and Gustavo M. Larrea-Gallegos and Tommaso Serchi and Angela Serra and Dario Greco and Penny Nymark and Martin Himly and Karin Wiench and Nico Watzek and Eva-Kathrin Schillinger and Jérôme Gavillet and Iseult Lynch and Andreas Karwath and Alexe L. Haywood and Georgios V. Gkoutos and Roland Hischier},
keywords = {advanced materials, safe-and-sustainable-by-design, computational modelling and simulations, functionality modelling, safety assessment, life cycle assessment, life cycle costing},
abstract = {In this innovation report, we present the vision of the PINK project to foster Safe-and-Sustainable-by-Design (SSbD) advanced materials and chemicals (AdMas&Chems) development by integrating state-of-the-art computational modelling, simulation tools and data resources. PINK proposes a novel approach for the use of the SSbD framework, whose innovative approach is based on the application of a multi-objective optimisation procedure for the criteria of functionality, safety, sustainability and cost efficiency. At the core is the PINK open innovation platform, a distributed system that integrates all relevant modelling resources enriched with advanced data visualisation and an AI-driven decision support system. Data and modelling tools from the, in large parts, independently developed areas of functional design, safety assessment, life cycle assessment & costing are brought together based on a newly created Interoperability Framework. The PINK In Silico Hub, as the user Interface to the platform, finally guides the user through the complete AdMas&Chems development process from idea creation to market introduction. Guided by two Developmental Case Studies, the process of building of the PINK Platform is iterative, ensuring industry readiness to implement and apply it. Additionally, the Industrial Demonstrator programme will be introduced as part of the final project phase, which allows industry partners and especially small and medium enterprises (SMEs) to become part of the PINK consortium. Feedback from the Demonstrators as well as other stakeholder-engagement activities and collaborations will shape the platform’s final look and feel and, even more important, activities to assure long-term technical sustainability.}
}
@article{KAUR2025126756,
title = {Hybrid image splicing detection: Integrating CLAHE, improved CNN, and SVM for digital image forensics},
journal = {Expert Systems with Applications},
volume = {273},
pages = {126756},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126756},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425003781},
author = {Navneet Kaur},
keywords = {Image splicing forgery, CLAHE, Accuracy, Improved CNN},
abstract = {This article introduces a novel hybrid method for the detection of image splicing forgery (ISF) that integrates an improved convolutional neural network (CNN), support vector machine (SVM) classifier, and contrast-limited adaptive histogram equalization (CLAHE). The imperceptibility of counterfeit images has made detection a challenge, as the increasing accessibility of image editing applications has resulted in a surge in amateur image manipulation. The proposed methodology employs CLAHE to enhance the extraction of hidden features that forgery has obscured. The improved CNN employs sophisticated feature extraction techniques to achieve superior classification accuracy without the necessity of custom algorithms. Furthermore, SVM is incorporated due to its exceptional processing speed and efficiency. The objective of this hybrid framework is to address the constraints of current deep learning models in terms of computational efficiency and accuracy, thereby demonstrating substantial enhancements in performance metrics for image splicing forgery detection (ISFD). The findings suggest that the proposed system effectively differentiates between authentic and manipulated images, offering an effective solution to the challenges of image splicing forgery.}
}
@article{ABDULLAH2025102667,
title = {Comprehensive review of 3D/4D printing of soft materials, methods and applications},
journal = {Applied Materials Today},
volume = {43},
pages = {102667},
year = {2025},
issn = {2352-9407},
doi = {https://doi.org/10.1016/j.apmt.2025.102667},
url = {https://www.sciencedirect.com/science/article/pii/S2352940725000861},
author = {Muhammad Raies Abdullah and Zhen Peng and Vignesh babu Rajendren and Farooq Ahmad and Syed Sohail Ahmed Shah and Abdul Wasy Zia and Amjad Ali and Guanjun Qiao and Khurram Shehzad},
keywords = {3D/4D Printing, Smart Materials, Bioprinting, Soft materials, Hydrogels, Stimuli-Responsive Materials},
abstract = {This article extensively reviews the transforming landscape of 3D and 4D printing of soft materials, their printing methods, and their applications in wider engineering and technological sectors. Soft materials are known for their suppleness and plasticity and have a wider scope in rapidly growing additive manufacturing sector. The contents of this article are appropriately designed to provide detailed subject knowledge to young or new researchers and also critically analysing several enabling strategies on manufacturing process, material design and formulation, and multifunctional application for subject experts. This article reviews the high potential 3D/4D printing methods for materials such as extrusion-based printing, inkjet-based printing, stereolithography, selective laser sintering, direct ink writing, vat photopolymerization, and comparison of techniques and compares their scope and limitations by process and materials. Further, the article reviews the printability of emerging soft materials such as elastomers, hydrogels, biopolymers, conductive and flexible materials, and biomimetic materials. The application of 3D/4D printed soft materials in wider engineering sectors are discussed such as biomedical and healthcare applications, soft robotics, sensors, and actuators, wearable devices and smart textiles, food technology, and pharmaceutical products. The later sections describe design considerations, challenges, and limitations of 3D/4D printing of soft materials in terms of geometric complexity, support structures, printing resolution, accuracy, cost, and fabrication time, material compatibility, scalability, and safety issues. The article concludes by summarizing advancements in printing technologies, material innovations and developments and reflecting on future challenges such as integration of 3D/4D printing of soft materials with other technologies and compounds and their regulatory and ethical considerations.}
}
@article{LIU2025130068,
title = {An adversarial contrastive learning based cross-modality zero-watermarking scheme for DIBR 3D video copyright protection},
journal = {Neurocomputing},
volume = {637},
pages = {130068},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130068},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225007404},
author = {Xiyao Liu and Qingyu Dang and Huiyi Wang and Xiaoheng Deng and Xunli Fan and Cundian Yang and Zhihong Chen and Hui Fang},
keywords = {Zero-watermarking, DIBR 3D videos, Contrastive learning, Adversarial distortion simulator, Cross-modality feature fusion},
abstract = {Copyright protection of depth image-based rendering (DIBR) videos has raised significant concerns due to their increasing popularity. Zero-watermarking, emerging as a powerful tool to protect the copyright of DIBR 3D videos, mainly relies on traditional feature extraction methods, thus necessitating improvements in robustness against complex geometric attacks and its ability to strike a balance between robustness and distinguishability. This paper presents a novel zero-watermarking scheme based on cross-modality feature fusion within a contrastive learning framework. Our approach integrates complementary information from 2D frames and depth maps using a cross-modality attention feature fusion mechanism to obtain discriminative features. Moreover, our features achieve a better trade-off between robustness and distinguishability by leveraging a designed contrastive learning strategy with an adversarial distortion simulator. Experimental results demonstrate our remarkable performance by reducing the false negative rates to around 0.2% when the false positive rate is equal to 0.5%, which is superior to the state-of-the-art zero-watermarking methods.}
}
@article{VINKEN2025108574,
title = {Taking the 3Rs to a higher level: Replacement and reduction of animal testing in life sciences in space research},
journal = {Biotechnology Advances},
pages = {108574},
year = {2025},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2025.108574},
url = {https://www.sciencedirect.com/science/article/pii/S0734975025000606},
author = {Mathieu Vinken and Daniela Grimm and Sarah Baatout and Bjorn Baselet and Afshin Beheshti and Markus Braun and Anna Catharina Carstens and James A. Casaletto and Ben Cools and Sylvain V. Costes and Phoebe {De Meulemeester} and Bartu Doruk and Sara Eyal and Miguel J.S. Ferreira and Silvana Miranda and Christiane Hahn and Sinem Helvacıoğlu Akyüz and Stefan Herbert and Dmitriy Krepkiy and Yannick Lichterfeld and Christian Liemersdorf and Marcus Krüger and Shannon Marchal and Jette Ritz and Theresa Schmakeit and Hilde Stenuit and Kevin Tabury and Torsten Trittel and Markus Wehland and Yu Shrike Zhang and Karson S. Putt and Zhong-Yin Zhang and Danilo A. Tagle},
keywords = {Microgravity and radiation simulators, Primary cells, And stem cells, Spheroids and organoids, Microphysiological system, Bioprinting, Systems biology, Live-cell, high-content and real-time analysis, High-throughput screening, Artificial intelligence, Digital twins},
abstract = {Human settlements on the Moon, crewed missions to Mars and space tourism will become a reality in the next few decades. Human presence in space, especially for extended periods of time, will therefore steeply increase. However, despite more than 60 years of spaceflight, the mechanisms underlying the effects of the space environment on human physiology are still not fully understood. Animals, ranging in complexity from flies to monkeys, have played a pioneering role in understanding the (patho)physiological outcome of critical environmental factors in space, in particular altered gravity and cosmic radiation. The use of animals in biomedical research is increasingly being criticized because of ethical reasons and limited human relevance. Driven by the 3Rs concept, calling for replacement, reduction and refinement of animal experimentation, major efforts have been focused in the past decades on the development of alternative methods that fully bypass animal testing or so-called new approach methodologies. These new approach methodologies range from simple monolayer cultures of individual primary or stem cells all up to bioprinted 3D organoids and microfluidic chips that recapitulate the complex cellular architecture of organs. Other approaches applied in life sciences in space research contribute to the reduction of animal experimentation. These include methods to mimic space conditions on Earth, such as microgravity and radiation simulators, as well as tools to support the processing, analysis or application of testing results obtained in life sciences in space research, including systems biology, live-cell, high-content and real-time analysis, high-throughput analysis, artificial intelligence and digital twins. The present paper provides an in-depth overview of such methods to replace or reduce animal testing in life sciences in space research.}
}
@article{XIE2025111230,
title = {A scalable phishing website detection model based on dual-branch TCN and mask attention},
journal = {Computer Networks},
volume = {263},
pages = {111230},
year = {2025},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2025.111230},
url = {https://www.sciencedirect.com/science/article/pii/S1389128625001987},
author = {Lixia Xie and Hao Zhang and Hongyu Yang and Ze Hu and Xiang Cheng},
keywords = {Phishing detection, Deep learning, Mask attention, Phishing attacks, Malicious websites},
abstract = {Phishing website detection models face challenges such as missing features, limited feature extraction capabilities, and significant computational resource consumption when processing multidimensional features. Additionally, publicly available datasets often lack diversity and scalability, and are vulnerable to disguise attacks, resulting in poor model generalizability.This paper addresses these issues by proposing a multiclass scalable dataset, Crawling2024, collected using a WebDriver-based collector that simulates human operations to avoid attacker disguises. Through data analysis, we identify handcrafted features from access information and URLs. These features help reduce the computational load of deep learning models and expand feature dimensions. Crawling2024 retains data identifiers (IDs), enabling further extension through data scraping.We also introduce a scalable phishing website detection model (SPWDM) that utilizes a dual-branch temporal convolution network (TCN) to extract local correlations and long-term dependencies of domain names. The model incorporates a lightweight spatial-channel (SC) attention mechanism to enhance interactions between channels and space. Additionally, it uses a mask attention mechanism to manage extended features and adjust focus when features are missing. Our feature fusion method combines enhanced features extracted by the dual-branch TCN, with various features processed by the mask attention mechanism.The experimental results demonstrate that our proposed detection method achieves excellent performance, with an accuracy of 97.66% on the Crawling2024 dataset. This is 0.52% to 2.72% higher than other methods, and it maintains a leading position on other public datasets.}
}
@article{PUZELLA2025104902,
title = {A virtual reality environment to study work-related objectification},
journal = {Acta Psychologica},
volume = {255},
pages = {104902},
year = {2025},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2025.104902},
url = {https://www.sciencedirect.com/science/article/pii/S000169182500215X},
author = {Giulio Puzella and Antonio Sterlicchio and Cristina Baldissarri and Anna Manfredi and Tobias Greitemeyer and Alessandro Gabbiadini},
abstract = {The present work introduces the ACME VR paradigm, a novel virtual reality-based approach for investigating work-related objectification. Traditional laboratory methods often lack ecological validity. Therefore, VR has been used for creating a realistic paradigm maintaining high experimental control. Two scenarios were designed: an assembly line task, characterized by repetitiveness, external control, and fragmentation, and a woodworking task, adopted as a control scenario, emphasizing autonomy and holistic engagement. The effectiveness of the VR paradigm was assessed by examining a specific manifestation of objectification: self-objectification. A preliminary study assessed usability levels and interaction quality and explored the paradigm's ability to elicit self-objectification. Results demonstrated that the designed scenarios did not present interaction issues, and both user experience and overall usability were satisfactory. Moreover, the objectifying scenario induced significantly higher self-objectification and perceptions of objectifying activity than the non-objectifying scenario. These findings were further validated in the main study, where the objectifying scenario elicited higher self-objectification in terms of self-perception as instruments-like and reduced self-attribution of human mental states, confirming the impact of task characteristics on these outcomes. The paradigm's design ensures high ecological validity while maintaining rigorous experimental control. VR-specific measures, such as sense of presence and embodiment, were consistent across scenarios, validating the reliability of the simulations. This research highlights VR's potential to replicate complex workplace dynamics and manipulate key variables. It also provides researchers with an innovative, reliable, and validated tool for experimental studies to deepen the understanding of psychological mechanisms related to objectification in the workplace.}
}
@article{CARDOEN2025101181,
title = {Closing the multichannel gap through computational reconstruction of interaction in super-resolution microscopy},
journal = {Patterns},
pages = {101181},
year = {2025},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2025.101181},
url = {https://www.sciencedirect.com/science/article/pii/S2666389925000297},
author = {Ben Cardoen and Hanene {Ben Yedder} and Ivan Robert Nabi and Ghassan Hamarneh},
keywords = {super-resolution microscopy, multichannel, colocalization, interaction, subcellular organelle, protein, interaction analysis, functional interaction, multichannel gap, expansion microscopy, STED, single-molecule localization microscopy, SMLM, voxel, point cloud, organelle contact, mitochondria-ER contact, MERC, MAM, chromatin tracing},
abstract = {Summary
Cellular function is defined by pathways that, in turn, are determined by distance-mediated interactions between and within subcellular organelles, protein complexes, and macromolecular structures. Multichannel super-resolution microscopy (SRM) is uniquely placed to quantify distance-mediated interactions at the nanometer scale with its ability to label individual biological targets with independent markers that fluoresce in different spectra. We review novel computational methods that quantify interaction from multichannel SRM data in both point-cloud and voxel form. We discuss in detail SRM-specific factors that can compromise interaction analysis and decompose different classes of interactions based on distinct representative cell biology use cases, the underappreciated non-linear physics of their scale, and the development of specialized methods for those use cases. An abstract mathematical model is introduced to facilitate the comparison and evaluation of interaction reconstruction methods and to quantify the computational bottlenecks. We discuss the different strategies for validation of interaction analysis results with sparse or incomplete ground-truth data. Finally, evolving trends and future directions are presented, highlighting the “multichannel gap,” where interaction analysis is trailing behind the rapid increase in novel modes of multichannel SRM acquisitions.}
}
@article{CIERIHUTCHERSON2025102360,
title = {Mixed-methods systematic review of pharmacist-administered injectable contraception: Insights from patients, pharmacists, and other health care professionals},
journal = {Journal of the American Pharmacists Association},
pages = {102360},
year = {2025},
issn = {1544-3191},
doi = {https://doi.org/10.1016/j.japh.2025.102360},
url = {https://www.sciencedirect.com/science/article/pii/S1544319125000391},
author = {Nicole E. Cieri-Hutcherson and Timothy C. Hutcherson and Elizabeth M. Bradley and John Rizk and Nicholas D. Steele},
abstract = {Background
The role of the pharmacist in reproductive health and contraception management continues to expand. Examination of the perspectives of patients, pharmacists, and other health care professionals can highlight both the benefits and challenges associated with pharmacist administration of injectable contraception.
Objectives
The objective of this systematic review was to assess the feasibility, applicability, and satisfaction of patients, pharmacists, and other health care professionals regarding pharmacist-administered injectable contraception.
Methods
Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines, a search of Medline and Embase databases, from inception through June 3, 2024, was conducted using a predefined search strategy to capture relevant records. Initial records were screened based on the prespecified inclusion criteria focusing on patient, pharmacist, and other health care professional outcomes related to the pharmacist administration of injectable contraceptives. After deduplication and screening, 3 independent reviewers (E.M.B., J.R., N.D.S.) extracted data, with any disagreements resolved through discussion by a fourth reviewer (T.C.H.). Risk of bias (RoB) was assessed using the revised Cochrane RoB tool for randomized studies and the Appraisal Tool for Cross-Sectional Studies. A convergent, integrated, mixed-methods approach was utilized to analyze both qualitative and quantitative data.
Results
Five cross-sectional studies and 1 randomized controlled trial were included. Pharmacists were interested in administering injectable contraceptives and reported that implementation would positively impact patient access and convenience. Quantitative analysis demonstrated that pharmacists felt confident and capable in this role and expressed the need for further training and resources. Qualitative analysis highlighted patient satisfaction for the convenience and accessibility, specifically in rural areas. Barriers included insufficient training, lack of infrastructure, mixed acceptance among other health care professionals, reimbursement, and regulatory frameworks.
Conclusion
Pharmacist-administered injectable contraception appears to be beneficial, accessible, and convenient for patients while aligning with pharmacists' capabilities and professional roles. Barriers should be addressed when considering implementation. Future research should aim to broaden the evidence-based research across different regions and explore long-term outcomes.}
}
@article{FERREIRA2025100117,
title = {Piloting a Maturity Model for Responsible Artificial Intelligence: A Portuguese case study},
journal = {Journal of Responsible Technology},
pages = {100117},
year = {2025},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2025.100117},
url = {https://www.sciencedirect.com/science/article/pii/S2666659625000137},
author = {Rui Miguel Frazão Dias Ferreira and António GRILO and Maria MAIA},
keywords = {Responsible Artificial Intelligence, Artificial Intelligence Regulation, Maturity Model, Ethical, Legal and Social Issues, Responsible Research and Innovation},
abstract = {Recently, frameworks and guidelines aiming to assist trustworthiness in organizations and assess ethical issues related to the development and use of Artificial Intelligence (AI) have been translated into self-assessment checklists and other instruments. However, such tools can be very time consuming to apply. Aiming to develop a more practical tool, an Industry-Wide Maturity Model for Responsible AI was piloted in 3 companies and 2 research centres, in Portugal. Results show that organizations are aware of requirements (44%) to deploy a responsible AI approach and have a reactive response to its implementation, as they are willing to integrate other requirements (33%) into their business processes. The proposed Model was welcomed and showed openness from companies to consistently use it, since it helped to identify gaps and needs when it comes to foster a more trustworthy approach to the development and deployment of AI.}
}
@article{ALABI2025103480,
title = {Multitask learning in minimally invasive surgical vision: A review},
journal = {Medical Image Analysis},
volume = {101},
pages = {103480},
year = {2025},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2025.103480},
url = {https://www.sciencedirect.com/science/article/pii/S1361841525000283},
author = {Oluwatosin Alabi and Tom Vercauteren and Miaojing Shi},
keywords = {Computer aided intervention, Multitask learning, Minimally invasive surgeries, Scene understanding},
abstract = {Minimally invasive surgery (MIS) has revolutionized many procedures and led to reduced recovery time and risk of patient injury. However, MIS poses additional complexity and burden on surgical teams. Data-driven surgical vision algorithms are thought to be key building blocks in the development of future MIS systems with improved autonomy. Recent advancements in machine learning and computer vision have led to successful applications in analysing videos obtained from MIS with the promise of alleviating challenges in MIS videos. Surgical scene and action understanding encompasses multiple related tasks that, when solved individually, can be memory-intensive, inefficient, and fail to capture task relationships. Multitask learning (MTL), a learning paradigm that leverages information from multiple related tasks to improve performance and aid generalization, is well-suited for fine-grained and high-level understanding of MIS data. This review provides a narrative overview of the current state-of-the-art MTL systems that leverage videos obtained from MIS. Beyond listing published approaches, we discuss the benefits and limitations of these MTL systems. Moreover, this manuscript presents an analysis of the literature for various application fields of MTL in MIS, including those with large models, highlighting notable trends, new directions of research, and developments.}
}
@article{ANUKIRUTHIKA2025102588,
title = {AI-driven grain storage solutions: Exploring current technologies, applications, and future trends},
journal = {Journal of Stored Products Research},
volume = {111},
pages = {102588},
year = {2025},
issn = {0022-474X},
doi = {https://doi.org/10.1016/j.jspr.2025.102588},
url = {https://www.sciencedirect.com/science/article/pii/S0022474X25000475},
author = {T. Anukiruthika and D.S. Jayas},
keywords = {Grain storage, Artificial intelligence, Machine learning, Internet of things, Blockchain, Quality management},
abstract = {The integration of artificial intelligence (AI) and machine learning (ML) technologies is revolutionizing the food grain industry, particularly in the storage and quality management. This work provides a comprehensive review on the integration of AI and ML in the food grain industry, focusing on current technologies, applications, and future advancements. Various AI technologies including artificial neural networks (ANNs), fuzzy logic systems, and ML methods such as deep learning, supervised learning, and anomaly detection have been discussed. The practical applications of these technologies in addressing critical areas such as pest and insect damage detection, grain classification, crop disease detection, mycotoxin contamination, and supply chain management are highlighted. Applications of innovative technological approaches, including edge computing, digital twins, Internet of Things (IoT), and blockchain, have been discussed for their impact on enhancing grain storage quality management. The review also critically examines the challenges and limitations associated with AI and ML, such as data privacy, inaccuracies, and regulatory concerns. In addition, the emerging trends that are set to revolutionize grain quality management such as smart sensors, robotics, remote sensing, and augmented reality are discussed. By synthesizing current knowledge and future prospects, this review aims to provide a holistic understanding of AI's transformative potential in the grain industry.}
}
@article{SHAFEE2025110146,
title = {Privacy and security vulnerabilities in edge intelligence: An analysis and countermeasures},
journal = {Computers and Electrical Engineering},
volume = {123},
pages = {110146},
year = {2025},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2025.110146},
url = {https://www.sciencedirect.com/science/article/pii/S0045790625000898},
author = {Ahmed Shafee and S.R. Hasan and Tasneem A. Awaad},
keywords = {Artificial intelligence, Cloud computing, Deep learning, Edge computing, Edge intelligence, Machine learning, Privacy-preserving, Security},
abstract = {Recent advancements in deep learning have significantly accelerated the growth of artificial intelligence (AI) technologies, powering applications like the Metaverse, augmented reality (AR), virtual reality (VR), and tactile communications on emerging 6G networks. The proliferation of Internet of Things (IoT) devices and mobile computing has connected vast numbers of devices to the internet, generating enormous amounts of data at the network edge. To harness the potential of this big data, extending AI capabilities to the network edge has become increasingly vital. Edge AI, or edge intelligence (EI), enables computing tasks to be performed closer to data sources, reducing latency and enhancing efficiency. However, this shift has amplified privacy concerns due to increased data sharing, compounded by the growing prevalence of data breaches. Research also reveals that sharing AI models instead of raw data does not fully safeguard privacy, as certain attacks can still compromise sensitive training information. This paper reviews Edge Intelligence with a focus on privacy and security issues, identifying critical challenges and vulnerabilities in edge and cloud computing environments. It provides a comprehensive analysis of state-of-the-art solutions to address these concerns, offering valuable insights into enhancing privacy and security in distributed computing systems.}
}
@article{SRIVASTAVA2025103996,
title = {FP-growth-based signature extraction and unknown variants of DoS/DDoS attack detection on real-time data stream},
journal = {Journal of Information Security and Applications},
volume = {89},
pages = {103996},
year = {2025},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2025.103996},
url = {https://www.sciencedirect.com/science/article/pii/S2214212625000341},
author = {Arpita Srivastava and Ditipriya Sinha},
keywords = {DoS/DDoS Attack, Real-time data capturing, Signature extraction, FP-growth algorithm, High-volume attack detection, Jaccard similarity},
abstract = {Protecting sensitive information on Internet from unknown attacks is challenging due to no known signatures, limited historical data, a high number of false positives, and a lack of vendor patches. This paper has proposed a statistical method to detect unknown variants of denial-of-service (DoS)/ distributed denial-of-service (DDoS) (high-volume) attacks. The proposed method is primarily divided into two modules: DoS/DDoS attack signature extraction and unknown variants of DoS/DDoS attack detection. A setup in laboratory of NITP is created to capture real-time traffic of six different variants of DoS or DDoS attacks with benign network traffic behavior, referred to as RTNITP24. Unique DoS/DDoS attack signatures are extracted by applying a Frequent-Pattern Growth (FP-Growth) algorithm using 71 % of RTNITP24 data having DoS/DDoS attack and benign traffic, assuming these signatures are primarily present in DoS/DDoS attack traffic but rarely in benign traffic. These signatures are stored in a high-volume attack (HVA) knowledge base (KB). Unknown variants of the DoS/DDoS (high-volume) attack detection module use an HVA knowledge base and pcap files of 29 % RTNITP24 and CICIDS2017 new data packets, which is not considered in the attack signature extraction module. Jaccard similarity score is computed between new data packets and attack signatures and scrutinizes the two main conditions: if similarity score of any of the signatures is greater than or equal to rule threshold or if the average similarity score of all the signatures is greater than or equal to the overall threshold. Packet is detected as malicious if any of aforementioned conditions are true. Otherwise, the packet is benign. Proposed model achieves high accuracy (91.66 % and 94.87 %) and low false alarm rates (5.32 % and 4.98 %) on RTNITP24 and CICIDS2017 datasets, respectively. Additionally, proposed model is compared to apriori-based rule extraction technique and current state-of-the-art methods, revealing that it outperforms both apriori-based and existing methods.}
}
@article{DEARAUJO2025160872,
title = {Recent developments in the use of machine learning in catalysis: A broad perspective with applications in kinetics},
journal = {Chemical Engineering Journal},
volume = {508},
pages = {160872},
year = {2025},
issn = {1385-8947},
doi = {https://doi.org/10.1016/j.cej.2025.160872},
url = {https://www.sciencedirect.com/science/article/pii/S1385894725016936},
author = {Leandro Goulart {de Araujo} and Léa Vilcocq and Pascal Fongarland and Yves Schuurman},
keywords = {Catalytic reaction, Catalyst, Catalysis informatics, Machine learning},
abstract = {A thorough grasp of the underlying mechanisms of catalytic reactions is indispensable for furthering our understanding of chemical kinetics. However, traditional phenomenological models present certain difficulties, including the tendency to converge to local minima and a reliance on parameters that are difficult to measure, particularly in complex catalytic systems. These systems frequently comprise intricate feedstock compositions or catalyst structures that are challenging to anticipate through theory-driven approaches. This often results in the utilization of unrealistic models or the allocation of considerable computational resources. While traditional methods offer valuable insights, they are constrained by these challenges and the lack of robust uncertainty assessments. In view of these limitations, data-driven modeling, in particular through machine learning (ML), has emerged as a promising alternative in catalysis in the last five years. This review examines recent advancements in ML applications within the field of catalysis, encompassing a broad range of applications, including data generation, descriptor identification, and feature engineering. While the review takes a general perspective on ML in catalysis, particular attention is given to applications in chemical kinetics wherever relevant, recognizing the interconnection between reaction kinetics, catalyst design, reaction conditions, and reactor configurations. The discussion includes various ML models, including interpretable yet less flexible models and more complex black-box models, and considers their applications in catalysis. It also examines key factors in model selection, such as generalizability, computational efficiency, data quality, and interpretability. Finally, it outlines future directions for ML in catalysis, emphasizing how these technologies can further enhance the optimization, design, and improvement of catalytic systems.}
}
@article{NAQVI2025100835,
title = {Enhancing semantic search using ontologies: A hybrid information retrieval approach for industrial text},
journal = {Journal of Industrial Information Integration},
volume = {45},
pages = {100835},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100835},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25000597},
author = {Syed Meesam Raza Naqvi and Mohammad Ghufran and Christophe Varnier and Jean-Marc Nicod and Noureddine Zerhouni},
keywords = {Industry 4.0, Industrial information integration, Machine documentation, Multi-modal learning, Semantic search, Large Language Models (LLMs)},
abstract = {Despite the increased focus on data in Industry 4.0, textual data has received little attention in the production and engineering management literature. Data sources such as maintenance records and machine documentation usually are not used to help maintenance decision-making. Available studies mainly focus on categorizing maintenance records or extracting meta-data, such as time of failure, maintenance cost, etc. One of the main reasons behind this underutilization is the complexity and unstructured nature of the industrial text. In this study, we propose a novel hybrid information retrieval approach for industrial text using multi-modal learning. Maintenance operators can use the proposed system to query maintenance records and find similar solutions to a given problem. The proposed system utilizes heterogeneous (multi-modal) data, a combination of maintenance records, and machine ontology to enhance semantic search results. We used the state-of-the-art Large Language Models (LLMs); BERT (Bidirectional Encoder Representations from Transformers) for textual similarity. For similarity among ontology labels, we used a modified version of Wu-Palmer’s similarity. A hybrid weighted similarity is proposed, incorporating text and ontology similarities to enhance semantic search results. The proposed approach was validated using an open-source dataset of real maintenance records from excavators collected over ten years from different mining sites. A retrieval comparison using only text and multi-modal data is performed to estimate the proposed system’s effectiveness. Quantitative and qualitative analysis of results indicates a performance improvement of 8% using the proposed hybrid similarity approach compared to only text-based retrieval. To the best of our knowledge, this is the first study to combine LLMs and machine ontology for semantic search in maintenance records.}
}
@article{FRANCE2025115273,
title = {Digital brand equity: The concept, antecedents, measurement, and future development},
journal = {Journal of Business Research},
volume = {192},
pages = {115273},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2025.115273},
url = {https://www.sciencedirect.com/science/article/pii/S0148296325000967},
author = {Stephen L. France and Nebojsa S. Davcik and Brett J. Kazandjian},
keywords = {Brand equity, Digital, Measurement, Social media, AI},
abstract = {Measuring brand equity is of vital importance to marketing practitioners and scholars. Academics and practitioners have developed a range of tools and metrics for measuring brand equity, but in the fast-paced and transformational digital era, it may be that current metrics are not sufficient. The authors develop a conceptual understanding of the brand equity paradigm using practitioner and scholarly views. A practitioner-focused analysis is given on how companies can best understand and measure brand performance in a digital environment and take actionable insights, using the share of search, digital brand awareness, and digital brand sentiment constructs. The authors argue that digital brand equity metrics cannot be based only on social media and current digital metrics indicators but also must include a human side of the brand and the technology-consumer nuances. The study proposes a research agenda and highlights important research and policy questions in developing digital brand equity.}
}
@article{YOON2025101491,
title = {Comprehensive examination of the bright and dark sides of generative AI services: A mixed-methods approach},
journal = {Electronic Commerce Research and Applications},
volume = {70},
pages = {101491},
year = {2025},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2025.101491},
url = {https://www.sciencedirect.com/science/article/pii/S156742232500016X},
author = {Sang-Hyeak Yoon and Sung-Byung Yang and So-Hyun Lee},
keywords = {Generative AI, Valence framework, Mixed-methods approach, Joint sentiment topic modeling, Expert interview, ChatGPT},
abstract = {Recent advancements in artificial intelligence (AI), particularly in generative AI (GAI), have significantly influenced society, prompting extensive discussions about their societal impact. While previous research has acknowledged both the benefits and challenges of AI, the rapid development of GAI has often proceeded without sufficient focus on actionable strategies to address potential risks and unintended consequences. Understanding both the positive and negative aspects of GAI is essential to ensure that technological progress is balanced and responsibly managed to mitigate potential risks and societal harm. This study identifies the positive and negative aspects of GAI from both public and expert viewpoints by applying a valence framework. Using a mixed-methods approach that integrates joint sentiment topic (JST) modeling with the combined use of ChatGPT and expert interviews, we investigated the key positive and negative factors associated with GAI. By integrating the insights gained from these different perspectives, the study proposes strategies for the effective and responsible use of GAI. The study contributes to the existing body of knowledge on GAI by offering a comprehensive understanding of its implications and providing guidance for its ethical and appropriate applications.}
}
@article{DEMARTINO2025107678,
title = {Classification and challenges of non-functional requirements in ML-enabled systems: A systematic literature review},
journal = {Information and Software Technology},
volume = {181},
pages = {107678},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107678},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000175},
author = {Vincenzo {De Martino} and Fabio Palomba},
keywords = {Software engineering for artificial intelligence, Non-functional requirements, Systematic literature reviews},
abstract = {Context:
Machine learning (ML) is nowadays so pervasive and diffused that virtually no application can avoid its use. Nonetheless, its enormous potential is often tempered by the need to manage non-functional requirements (NFRs) and navigate pressing, contrasting trade-offs.
Objective:
In this respect, we notice a lack of systematic synthesis of challenges explicitly tied to achieving and managing NFRs in ML-enabled systems. Such a synthesis may not only provide a comprehensive summary of the state of the art but also drive further research on the analysis, management, and optimization of NFRS of ML-enabled systems.
Method:
In this paper, we propose a systematic literature review targeting two key aspects such as (1) the classification of the NFRs investigated so far, and (2) the challenges associated with achieving and managing NFRs in ML-enabled systems during model development Through the combination of well-established guidelines for conducting systematic literature reviews and additional search criteria, we survey a total amount of 130 research articles.
Results:
Our findings report that current research identified 31 different NFRs, which can be grouped into six main classes. We also compiled a catalog of 26 software engineering challenges, emphasizing the need for further research to systematically address, prioritize, and balance NFRs in ML-enabled systems.
Conclusion:
We conclude our work by distilling implications and a future outlook on the topic.}
}
@article{AIMANBINABUSOFIAN2025101478,
title = {Upcycling and recycling of spent battery waste for a sustainable future: Progress and perspectives},
journal = {Progress in Materials Science},
pages = {101478},
year = {2025},
issn = {0079-6425},
doi = {https://doi.org/10.1016/j.pmatsci.2025.101478},
url = {https://www.sciencedirect.com/science/article/pii/S0079642525000532},
author = {Abu Danish {Aiman Bin Abu Sofian} and S.R. Majid and Kisuk Kang and Jang-Kyo Kim and P.L. Show},
keywords = {Battery Recycling, Electrochemistry, Graphene, Waste Management, Circular Economy, Catalyst, Sustainable Materials},
abstract = {The urgency of addressing the environmental and resource challenges posed by spent lithium-ion batteries (LIBs) has led to significant advancements in recycling and upcycling methodologies. This work aims to provide a comprehensive understanding of the progress made for LIB recycling and upcycling, offering perspectives for achieving a circular economy in battery technology. The review examines the latest innovations in LIB material recovery, focusing on both conventional recycling techniques and emerging upcycling strategies. It explores the motivation and importance of recycling spent LIBs, showing the critical need for sustainable solutions. A comprehensive overview of LIB recycling methodologies is provided, including pretreatment, preprocessing, pyrometallurgical, hydrometallurgical, bioleaching, direct recovery processes, electrochemical processes, and deep eutectic solvents. Emphasis is placed on the advanced upcycling of the cathode, anode, and separator materials, exploring composition/crystallisation engineering and structural modifications, including doping and surface coating. Furthermore, upcycling spent LIB materials into high-value products like catalysts and graphene is explored. The environmental impact, legislative landscape, and socioeconomic implications of battery recycling are critically analysed, with life cycle assessments underscoring the ecological benefits of these processes. Global perspectives on battery recycling practices are also examined, considering the varied approaches across different regions. Additionally, integrating artificial intelligence and the internet of things in optimising battery recycling is explored, demonstrating their potential to enhance efficiency and sustainability. The review concludes by identifying current challenges and proposing recommendations for future research and policy development.}
}
@article{DACOSTAFERREIRA2025100647,
title = {Teachers’ awareness and emotional response to cyberbullying: Exploring emotional regulation strategies in the classroom},
journal = {Computers in Human Behavior Reports},
volume = {18},
pages = {100647},
year = {2025},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2025.100647},
url = {https://www.sciencedirect.com/science/article/pii/S2451958825000624},
author = {Paula {da Costa Ferreira} and Nádia Pereira and Carlos Martinho and Hugo Marques and Hélio Martins and Alexandra Marques Pinto and Alexandra Barros and Aristides Ferreira and Mafalda Gomes and Ana Margarida {Veiga Simão}},
keywords = {Student cyberbullying, Teacher awareness, Emotions, Emotion regulation, Serious game-based interventions, Teacher professional development},
abstract = {Cyberbullying is a major and pressing issue in schools, reducing students' well-being and placing emotional burden on teachers. Despite increased attention to students' experiences, there is limited research on teachers' knowledge, emotional responses, and emotion regulation when witnessing student cyberbullying. Thus, this study explores teachers' knowledge, emotional responses, and emotion regulation strategies to respond to cyberbullying, and the impact of an innovative game-based platform on their emotion regulation abilities. Employing an integrated emotions and emotion regulation appraisal model, the mixed-methods study employed a sequential explanatory design. 543 teachers and 533 students were surveyed to examine a substantial difference between teachers' self-reports and students' self-reports of cyberbullying, with students reporting more instances. 63 teachers provided qualitative interviews with information on how they responded emotionally, which were predominantly negative in valence—concern, surprise, and frustration—and led to the prevalent use of response modulation strategies. Quasi-experimental, longitudinal research conducted among 64 teachers analyzed whether a serious game-based intervention can be effective. Emotion regulation was still a concern, however, pre-post-tests indicated that the platform supported increased use of cognitive reappraisal and coping responses for responding to cyberbullying. These findings are supportive of the need for training interventions to specifically target strengthening teachers' socio-emotional abilities, potentially contributing to early intervention and healthy school climate development. Schools can use technology-supported interventions to prompt teachers of their socio-emotional abilities and hence manage cyberbullying effectively. Future studies should use larger samples and longer longitudinal designs to adequately address and assess teachers’ emotion regulation approaches across time.}
}
@article{JAKHAR2025104086,
title = {Effective near-duplicate image detection using perceptual hashing and deep learning},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104086},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104086},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000287},
author = {Yash Jakhar and Malaya Dutta Borah},
keywords = {Near-duplicate images, Neural network, Generative Adversarial Network, Perceptual hashing, Siamese network, Vision Transformer},
abstract = {Computer vision has always been concerned with near-duplicate image detection. Previous approaches for detecting near duplicates highlighted the necessity to adequately explore the aspect of image transformations for effectively handling complex images. We proposed a method of finding near duplicate images using the integration of three different techniques: perceptual hashing, Siamese network, and Vision Transformer. Perceptual hashing gives us a quick way to filter out similar-looking pictures, while the Siamese network architecture paired with the Vision transformer helps us identify more complex near duplicate instances. The integrated approach learns a metric space from data, which reflects both visual similarity and perceptual closeness among items in the dataset. The results demonstrate the effectiveness and robustness of our proposed method, achieving an AUROC of 0.99 and a precision of 0.987 on the California-ND dataset, and an AUROC of 0.92 with a precision of 0.884 on the INRIA Holidays dataset, significantly outperforming traditional methods by over 10% in both metrics. This represents a significant step forward in near-duplicate image detection research.}
}
@article{KPODO2025,
title = {Navigating challenges/opportunities in developing smart agricultural extension platforms: Multi-media data mining techniques},
journal = {Artificial Intelligence in Agriculture},
year = {2025},
issn = {2589-7217},
doi = {https://doi.org/10.1016/j.aiia.2025.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2589721725000418},
author = {Josué Kpodo and A. Pouyan Nejadhashemi},
keywords = {Agriculture extension, Artificial intelligence, Multi-media data mining, Decision-making, Large language models},
abstract = {Agricultural Extension (AE) research faces significant challenges in producing relevant and practical knowledge due to rapid advancements in artificial intelligence (AI). AE struggles to keep pace with these advancements, complicating the development of actionable information. One major challenge is the absence of intelligent platforms that enable efficient information retrieval and quick decision-making. Investigations have shown a shortage of AI-assisted solutions that effectively use AE materials across various media formats while preserving scientific accuracy and contextual relevance. Although mainstream AI systems can potentially reduce decision-making risks, their usage remains limited. This limitation arises primarily from the lack of standardized datasets and concerns regarding user data privacy. For AE datasets to be standardized, they must satisfy four key criteria: inclusion of critical domain-specific knowledge, expert curation, consistent structure, and acceptance by peers. Addressing data privacy issues involves adhering to open-access principles and enforcing strict data encryption and anonymization standards. To address these gaps, a conceptual framework is introduced. This framework extends beyond typical user-oriented platforms and comprises five core modules. It features a neurosymbolic pipeline integrating large language models with physically based agricultural modeling software, further enhanced by Reinforcement Learning from Human Feedback. Notable aspects of the framework include a dedicated human-in-the-loop process and a governance structure consisting of three primary bodies focused on data standardization, ethics and security, and accountability and transparency. Overall, this work represents a significant advancement in agricultural knowledge systems, potentially transforming how AE services deliver critical information to farmers and other stakeholders.}
}
@article{KURU2025,
title = {UMetaBE-DPPML: Urban Metaverse & Blockchain-Enabled Decentralised Privacy-Preserving Machine Learning Verification And Authentication With Metaverse Immersive Devices},
journal = {Internet of Things and Cyber-Physical Systems},
year = {2025},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2025.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667345225000094},
author = {Kaya Kuru and Kaan Kuru},
keywords = {Metaverse, cybercommunity, urban twins, cybersecurity, collaborative deep learning, federated learning, blockchain},
abstract = {It is anticipated that cybercrime activities will be widespread in the urban metaverse ecosystem due to its high economic value with new types of assets and its immersive nature with a variety of experiences. Ensuring reliable urban metaverse cyberspaces requires addressing two critical challenges, namely, cybersecurity and privacy protection. This study, by analysing potential cyberthreats in the urban metaverse cyberspaces, proposes a blockchain-based Decentralised Privacy-Preserving Machine Learning (DPPML) authentication and verification methodology, which uses the metaverse immersive devices and can be instrumented effectively against identity impersonation and theft of credentials, identity, or avatars. Blockchain technology and Federated Learning (FL) are merged in the developed DPPML approach not only to eliminate the requirement of a trusted third party for the verification of the authenticity of transactions and immersive actions, but also, to avoid Single Point of Failure (SPoF) and Generative Adversarial Networks (GAN) attacks by detecting malicious nodes. The developed methodology has been tested using Motion Capture Suits (MoCaps) in a co-simulation environment with the Proof-of-Work (PoW) consensus mechanism. The preliminary results suggest that the built techniques in the DPPML approach can prevent unreal transactions, impersonation, identity theft, and theft of credentials or avatars promptly before any transactions have been executed or immersive experiences have been shared with others. The proposed system will be tested with a larger number of nodes involving the Proof-of-Stake (PoS) consensus mechanism using several other metaverse immersive devices as a future job.}
}
@article{AREVALOANCONA2025100650,
title = {Robust zero-watermarking based on dual branch neural network for ownership authentication, auxiliary information delivery and tamper detection},
journal = {Egyptian Informatics Journal},
volume = {30},
pages = {100650},
year = {2025},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2025.100650},
url = {https://www.sciencedirect.com/science/article/pii/S111086652500043X},
author = {Rodrigo Eduardo Arevalo-Ancona and Manuel Cedillo-Hernandez},
keywords = {Zero-watermarking, Deep learning, Dual-branch neural network, Tamper detection, Ownership authentication},
abstract = {This paper presents a robust multitask zero-watermarking scheme for ownership authentication, auxiliary information embedding, and tamper detection using a dual-branch neural network. The proposed method generates three zero-watermarking codes stored in a three-layer structure, where each layer corresponds to a different type of watermark: a binary logo for ownership authentication, a QR code for auxiliary data, and a halftone version of the original image for tamper detection. The first and third zero-watermarking codes are generated by a logical linking between the binary logo and halftone version, respectively, with a set of neural network weights. The second zero-watermarking code is created by linking the QR code with features extracted from the dual-branch neural network. This approach ensures that the original image remains undistorted and protected at the same time. Experimental results demonstrate the robustness of the proposed method against geometric distortions, common signal processing attacks, and combined attacks, achieving bit error rates below 0.005 and normalized correlation values close to or equal to 1. Additionally, the method attained an average accuracy of 98.7 % or higher in tamper detection tasks across multiple datasets, demonstrating its versatility.}
}
@article{SHARMILA2025110107,
title = {Leveraging Memory Forensic Features for Explainable Obfuscated Malware Detection with Isolated Family Distinction Paradigm},
journal = {Computers and Electrical Engineering},
volume = {123},
pages = {110107},
year = {2025},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2025.110107},
url = {https://www.sciencedirect.com/science/article/pii/S0045790625000503},
author = {S.P. Sharmila and Shubham Gupta and Aruna Tiwari and Narendra S. Chaudhari},
keywords = {Ransomware, Spyware, Trojan, Multi-class classification, Explainable AI, Sparse Projections},
abstract = {In the IoT edge computing era, inevitable and ubiquitous presence of the internet is opening the door for numerous cyberattacks. Obfuscated malware adds layers of difficulty to detect complex modern cyber attacks by evading AI-enabled Next-Generation Anti-Virus (NGAV) scanners and breaching digital privacy. To tackle this problem, in this paper, we propose “Augmented Sparse Projection Oblique Random Forest (AugSPORF)”, an Explainable sparse projections based Oblique Random Forest (ORF) with Isolated Family Distinction (IFD) Paradigm to detect multiple obfuscated malware belonging to Spyware, Ransomware, and Trojan families effectively. Irrespective of obfuscation, malware variants possess common behavior and family traits aligned with their families and leave traces in the memory on execution. To begin with this motivation, we handle the huge dimension of memory forensic features with sparse random projections. Next, we perform feature importance aware training with ORF to learn inherent behavioral features of malware families by isolating the target family, and distinguishing with other families. Further, the model’s scalability is assessed by increasing the number of malware families. To offer an insightful conclusion on the predictions, an Interpretable Machine Learning (IML) layer is interleaved to generate a report of explanations, thereby enhancing the interpretability of the model. The proposed approach yields an average accuracy of 96.76%, 96.45%, and 97.33% in detecting sub-families of Spyware, Ransomware, and Trojan respectively. Improved accuracy is also demonstrated by benchmarking the performance of AugSPORF on UCI repository datasets.}
}
@article{DAGUANOGASTALDI2025101011,
title = {A comprehensive and standardized pipeline for automated profiling of higher cognition in mice},
journal = {Cell Reports Methods},
volume = {5},
number = {3},
pages = {101011},
year = {2025},
issn = {2667-2375},
doi = {https://doi.org/10.1016/j.crmeth.2025.101011},
url = {https://www.sciencedirect.com/science/article/pii/S2667237525000475},
author = {Vinicius {Daguano Gastaldi} and Martin Hindermann and Justus B.H. Wilke and Anja Ronnenberg and Sahab Arinrad and Sabine Kraus and Anne-Fleur Wildenburg and Antonios Ntolkeras and Micah J. Provost and Liu Ye and Yasmina Curto and Jonathan-Alexis Cortés-Silva and Umer Javed Butt and Klaus-Armin Nave and Kamilla Woznica Miskowiak and Hannelore Ehrenreich},
keywords = {IntelliCage, behavior, automated phenotyping, cognitive domains, spatial memory, episodic-like memory, working memory, reversal learning, cognitive flexibility},
abstract = {Summary
In rodent behavior research, observer-independent methods, such as the IntelliCage, enhance data collection in a social, and thus stress-reduced, environment. The IntelliCage system allows experimenters to create cognitive challenges for mice motivated by rewards. Given the extensive and diverse data from IntelliCage, there is a high demand for automated analysis. Here, we introduce IntelliR, a free and standardized pipeline for analyzing IntelliCage data, including a cognition index for performance comparison across challenges. IntelliR supports the automatic analysis of three challenges that cover spatial, episodic-like, and working memory with their reversal tests and can also be adapted for other designs. Results from three cohorts of adult female C57B6 mice showed improved task proficiency over time. To validate cognitive impairment detection, we used adult female NexCreERT2xRosa26-eGFP-DTA mice after neuron ablation in cortex and hippocampus, in which we observed reduced learning capabilities. IntelliR integrates easily into research, improving time management and reproducibility.}
}
@article{YANG2025126694,
title = {The Imitation Game revisited: A comprehensive survey on recent advances in AI-generated text detection},
journal = {Expert Systems with Applications},
volume = {272},
pages = {126694},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126694},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425003161},
author = {Zhiwei Yang and Zhengjie Feng and Rongxin Huo and Huiru Lin and Hanghan Zheng and Ruichi Nie and Hongrui Chen},
keywords = {AI-generated text detection, Methodological review, Hierarchical analysis, Potential future research},
abstract = {In recent years, AI-generated text detection (AIGTD) has attracted more and more attention, with numerous novel methodologies being proposed. However, most existing reviews on this topic tend to be fragmented and incoherent in content, lacking a coherent and comprehensive framework for understanding. This paper comprehensively analyzes and summarizes the latest advancements and prominent technologies in this fast-moving field. In order to do that, we introduce a novel comprehensive multi-level taxonomy for AIGTD approaches, where the existing research can be broadly categorized into three directions, tackling the key challenges of classifier training, intrinsic attributes, and information embedding, respectively. To help researchers and practitioners understand and address detection and attack scenarios, we also introduce a classification of black-box and white-box models based on interpretability and transparency, as well as the computational requirements required to use the baseline methods. Moreover, we carefully provide a comprehensive performance comparison and analysis across several datasets for these methods, collect commonly used benchmark datasets, and outline potential future research directions in this field. To facilitate sharing, we consistently maintain the relevant materials at: https://github.com/Nicozwy/AIGTD-Survey.}
}
@article{MARTINI2025e03489,
title = {Non-native plants in road verges attract pollinators despite associated declines in native flowers},
journal = {Global Ecology and Conservation},
volume = {58},
pages = {e03489},
year = {2025},
issn = {2351-9894},
doi = {https://doi.org/10.1016/j.gecco.2025.e03489},
url = {https://www.sciencedirect.com/science/article/pii/S2351989425000903},
author = {Massimo Martini and Emily Kaul and Reid Miller and Jason Gibbs and Kyle Bobiwash},
keywords = {Pollinator conservation, Marginal habitats, Roadsides, Invasive plants, Bee diversity},
abstract = {Marginal habitats are increasingly recognized for their potential value in pollinator conservation. Road verges, which cover extensive areas, provide abundant floral resources and contribute to habitat heterogeneity and connectivity in homogeneous landscapes. However, road verges are also hotspots for the establishment and dispersal of non-native plants, raising doubts on their suitability to support diverse pollinator populations. We sampled flowering plants and visiting insects in roadsides of southeastern Manitoba, Canada, and leveraged datasets of bee communities from surrounding areas and Wildlife Management Areas (WMAs) to compare pollinator communities across habitats. Plant communities in road verges were dominated by a subset of abundant non-native species and were disproportionately visited by generalist pollinators. Non-native plant abundance was negatively correlated with native plant richness and abundance in the verges but positively associated with bee richness and abundance. Landscape context and scale also influenced pollinators. We found strong differences in pollinator richness, abundance, and community composition at larger (ecozone) scales, with local landscape composition and configuration also contributing significantly, albeit to a lesser extent. Road verge bee communities were distinct and less even than those in surrounding areas and WMAs, and exhibited a markedly higher proportion of polylectic to oligolectic individuals. These findings suggest that road verges can support generalist pollinators but are less suitable for specialists, highlighting their potential to maintain pollination services in heavily disturbed or densely-forested landscapes while also revealing limitations in harboring representatively diverse and even communities.}
}
@article{LI2025105179,
title = {Generative artificial intelligence in tourism management: An integrative review and roadmap for future research},
journal = {Tourism Management},
volume = {110},
pages = {105179},
year = {2025},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2025.105179},
url = {https://www.sciencedirect.com/science/article/pii/S0261517725000494},
author = {Hengyun Li and Jingbo Xi and Cathy H.C. Hsu and Bruce X.B. Yu and Xiang (Kevin) Zheng},
keywords = {Generative artificial intelligence, Integrative review, Tourism, Hospitality, Business, Future research},
abstract = {Rapid technical advances have spurred the potential of generative artificial intelligence (GenAI) in various business settings. However, the tourism industry is in the early stages of understanding and applying GenAI, and comprehensive knowledge is needed. This paper presents a systematic review of the empirical literature, published between 2022 and 2024, related to GenAI in the business and tourism fields. Findings draw a detailed picture of the state of GenAI research. In total, 170 published articles are reviewed based on topics, theories, and methods. Three main topic clusters are identified: 1) antecedents of using GenAI; 2) impacts and applications of GenAI; and 3) technicalities of GenAI. Theoretically, most studies have borrowed foundations from other fields without substantial development. Methodologically, existing research—particularly in tourism—has tended to feature quantitative techniques. This research also compares business and tourism literature based on an antecedents, process and outcomes framework, outlining potential research gaps and opportunities. In addition to synthesizing the current research landscape, this paper presents a multidimensional framework (i.e., theories, contexts, characteristics, and methods – TCCM) that suggests multiple future research questions to inform GenAI studies in tourism.}
}
@article{LENG2025528,
title = {Review of manufacturing system design in the interplay of Industry 4.0 and Industry 5.0 (Part II): Design processes and enablers},
journal = {Journal of Manufacturing Systems},
volume = {79},
pages = {528-562},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525000366},
author = {Jiewu Leng and Jiwei Guo and Junxing Xie and Xueliang Zhou and Ang Liu and Xi Gu and Dimitris Mourtzis and Qinglin Qi and Qiang Liu and Weiming Shen and Lihui Wang},
keywords = {Manufacturing system design, Production system design, Smart manufacturing, Industry 5.0, Design methods},
abstract = {Following up on our previous review paper ‘Review of manufacturing system design in the interplay of Industry 4.0 and Industry 5.0 (Part I): Design thinking and modeling methods’ [1], based on the proposed Thinking-Modelling-Process-Enabler (TMPE) framework of Manufacturing System Design (MSD), this paper (Part II of the two-part review) further reviews the Process and Enabler dimensions of MSD in the interplay of Industry 4.0 and Industry 5.0. MSD methods are reviewed from the single-dimensional design process and cross-dimensional design process perspectives, respectively. MSD methods are reorganized and categorized from the key enabler's perspective. Finally, challenges are discussed along with directions for future research in the domain of MSD. This review is anticipated to offer novel insights for advancing MSD research and engineering in the interplay of Industry 4.0 and Industry 5.0.}
}
@article{QU2025107707,
title = {A review of backdoor attacks and defenses in code large language models: Implications for security measures},
journal = {Information and Software Technology},
volume = {182},
pages = {107707},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107707},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000461},
author = {Yubin Qu and Song Huang and Peng Nie},
keywords = {Deep neural networks, Triggers, Large language models for code, Backdoor attacks, Software engineering},
abstract = {Context:
Large Language Models (LLMS) have revolutionized software engineering by bridging human language understanding and complex problem solving. However, resource constraints often lead users to rely on open-source models or third-party platforms for training and prompt engineering, introducing significant security vulnerabilities.
Objective:
This study provides a comprehensive analysis of backdoor attacks targeting LLMS in software engineering, with a particular focus on fine-tuning methods. Our work addresses a critical gap in existing literature by proposing a novel three-category framework for backdoor attacks: full-parameter fine-tuning, parameter-efficient fine-tuning, and no-tuning attacks.
Methods:
We systematically reviewed existing studies and analyzed attack success rates across different methods. Full-parameter fine-tuning generally achieves high success rates but requires significant computational resources. Parameter-efficient fine-tuning offers comparable success rates with lower resource demands, while no-tuning attacks exhibit variable success rates depending on prompt design, posing unique challenges due to their minimal resource requirements.
Results:
Our findings underscore the evolving landscape of backdoor attacks, highlighting the shift towards more resource-efficient and stealthy methods. These trends emphasize the need for advanced detection mechanisms and robust defense strategies.
Conclusion:
By focusing on code-specific threats, this study provides unique insights into securing LLMS in software engineering. Our work lays the foundation for future research on developing sophisticated defense mechanisms and understanding stealthy backdoor attacks.}
}
@article{BENSAOUD2025100834,
title = {Advancing software security: DCodeBERT for automatic vulnerability detection and repair},
journal = {Journal of Industrial Information Integration},
volume = {45},
pages = {100834},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100834},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25000585},
author = {Ahmed Bensaoud and Jugal Kalita},
keywords = {Large Language Models, Adversarial attacks, Vulnerability detection, Data privacy, Natural language processing},
abstract = {The exponential growth of software complexity has led to a corresponding increase in software vulnerabilities, necessitating robust methods for automatic vulnerability detection and repair. This paper proposes DCodeBERT, a large language model (LLM) fine-tuned for vulnerability detection and repair in software code. Leveraging the pre-trained CodeBERT model, DCodeBERT is designed to understand both natural language and programming language context, enabling it to effectively identify vulnerabilities and suggest repairs. We conduct experiments to evaluate DCodeBERT’s performance, comparing it against several baseline models. The results demonstrate that DCodeBERT outperforms the baselines in both vulnerability detection and repair tasks across multiple programming languages, showcasing its effectiveness in enhancing software security.}
}
@article{GARCES2025129620,
title = {Leveraging language models for automated distribution of review notes in animated productions},
journal = {Neurocomputing},
volume = {626},
pages = {129620},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129620},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225002929},
author = {Diego Garcés and Matilde Santos and David Fernández-Llorca},
keywords = {Movie production, Review notes, Text Classification, Large Language Models (LLM), Natural Language Processing},
abstract = {During the production of an animated film, professionals at the animation studio prepare thousands of notes. These notes describe improvements and corrections identified by supervisors and directors during daily meetings where the film’s progress is reviewed. After each meeting, these notes are manually distributed to the appropriate departments that need to address them. Due to the manual nature of this process, many notes are not assigned correctly, and the identified issues are not addressed, reducing the final quality of the film. This article describes and compares several approaches to automatically distribute notes using multi-label text classification with different language models (LM). Implemented methods include logistic regression models, encoder-only models such as the BERT family, and decoder-only models such as Llama 2 including fine-tuning and QLoRA techniques. Training and inference were conducted on a local RTX-3090. The results of the different techniques have been compared, achieving a maximum average accuracy of 0.83 and an f1-score of 0.89 with the fine-tuned Multilingual BERT model. This demonstrates the validity of these models for multi-label text classification, as well as their usefulness in a hitherto unexplored area such as animation studios.}
}
@article{SEO2025105182,
title = {AI-infused video marketing: Exploring the influence of AI-generated tourism videos on tourist decision-making},
journal = {Tourism Management},
volume = {110},
pages = {105182},
year = {2025},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2025.105182},
url = {https://www.sciencedirect.com/science/article/pii/S0261517725000524},
author = {Ilsoo Todd Seo and Hongbo Liu and Hengyun Li and Jin-Soo Lee},
keywords = {AI-generated video, Generative AI, AI-generated content, Topic modeling, Thematic analysis},
abstract = {As Generative Artificial Intelligence (AI) becomes increasingly integrated into daily experiences, AI-generated content (AIGC) is gaining prominence in marketing. Despite the significant potential of AI-generated videos to transform the tourism industry, there has been limited exploration of their impact in both practical and academic contexts. This paper addresses this gap by applying topic modeling and thematic analysis to social media comments and survey responses. This study proposes a conceptual framework for AI-generated tourism destination videos, identifying 17 themes. The findings highlight key differences compared to human-generated tourism videos, particularly in terms of authenticity and trustworthiness. This paper also contributes to the literature on tourism videos by presenting a pioneering study of AIGC. Furthermore, this research offers practical insights for tourism marketers seeking to effectively integrate AIGC into their marketing strategies.}
}
@article{HLAVATY2025102922,
title = {All the attention, all the time: How first-year students experience writing in a horizontal digital ecosystem},
journal = {Computers and Composition},
volume = {75},
pages = {102922},
year = {2025},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2025.102922},
url = {https://www.sciencedirect.com/science/article/pii/S875546152500009X},
author = {Greg Hlavaty and Heather Lindenman and Travis Maynard},
keywords = {Digital writing ecosystem, attention economy, distraction, horizontal attention ecosystem, embodiment, interpersonal support, environment selecting- and structuring-practices, writing processes, pedagogy, technology},
abstract = {This article examines how first-year composition students navigate digital attention ecosystems while writing. It presents findings from a qualitative focus group study in which undergraduate students participated in writing and reflection activities. The findings indicate that students are immersed in a “horizontal attention ecosystem,” in which all online tasks, communications, and media feel equally worthy of their attention. Although students attempt to manage their physical-digital writing environments strategically, the intrusive nature of current technology hinders their ability to focus, especially on academic writing assignments. When completing academic assignments, students report relying on self-restrictive measures and approaching writing as a solitary act, contrasting with writing studies’ understanding of writing as a social act. This article suggests pedagogical approaches that privilege embodied writing strategies and encourage writing-oriented social interactions between students.}
}
@article{FAEZ2025112093,
title = {The pivotal role of open source knowledge transfer to achieve universal energy access},
journal = {iScience},
volume = {28},
number = {3},
pages = {112093},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.112093},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225003530},
author = {Sanli Faez and Vivien Barnier and Dimitrios Mentis},
keywords = {Energy policy, Open source software, Energy sustainability, Energy systems},
abstract = {Summary
Community-company collaboration based on open source technologies is emerging as an alternative to proprietary solutions for planning, implementation, and maintenance of localized access to clean renewable electricity. We highlight some recent breakthroughs in creation of local technology ecosystems that follow an open access approach to knowledge transfer. On the other hand, on top of engineering, manufacturing, and maintenance questions, community led projects need to address social acceptance, economic viability, and regulatory barriers. Despite these barriers, open source in energy access is expanding and new sustainable business models are practiced. By comparing the advances in open source hardware scholarship with that of free and open source software, we anticipate an increasing communal pressure for adopting open-source friendly innovation policies. Academia and in particular university knowledge transfer policies play an essential role in achieving universal energy access via open source technologies.}
}
@article{PEREZVELASCO2025350,
title = {In-hospital linagliptin for management simplification and hypoglycemia reduction in very old patients with type 2 diabetes},
journal = {Medicina Clínica (English Edition)},
volume = {164},
number = {7},
pages = {350-357},
year = {2025},
issn = {2387-0206},
doi = {https://doi.org/10.1016/j.medcle.2024.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S2387020625001421},
author = {Miguel A. Pérez-Velasco and Julio Osuna-Sánchez and Mercedes Millán-Gómez and Michele Ricci and Almudena López-Sampalo and María-Rosa Bernal-López and Ricardo Gómez-Huelgas and Luis M. Pérez-Belmonte},
keywords = {Age≥80, Type 2 diabetes, Linagliptin, Insulin, Hospitalization, Edad ≥80, Diabetes mellitus tipo 2, Linagliptina, Insulina, Hospitalización},
abstract = {Introduction and objectives
The role of in-hospital dipeptidyl peptidase-4 inhibitors in very old patients has not been widely described. This work analyzes the simplification of in-hospital antihyperglycemic management (less insulin use) and reductions in hypoglycemia events using linagliptin in patients aged≥80 years with type 2 diabetes.
Patients and methods
This real-world observational study included hospitalized patients≥80 years with type 2 diabetes treated with an antihyperglycemic protocol of either basal-bolus insulin or linagliptin between January 2016 and December 2023. A 1:1 propensity score matching analysis was performed.
Results
Post-matching, 944 patients were included in each group. The total and basal insulin doses and number of daily injections were significantly lower in the linagliptin group than the basal-bolus insulin group with no differences in glycemic efficacy. Regarding safety, patients on the basal-bolus insulin regimen had more hypoglycemic events. The use of basal-bolus insulin regimen (odds ratio: 4.22; 95% confidence interval: 2.14–6.28; p<0.001), a higher total insulin dose (odds ratio: 3.55; 95% confidence interval: 2.02–5.36; p<0.001) and the number of insulin injections (odds ratio: 2.86; 95% confidence interval: 1.50–4.12; p=0.002) were associated with a greater risk of hypoglycemia. Other hypoglycemia risk factors were older age, moderate–severe functional dependence, moderate–severe dementia, polypharmacy, and complex health status.
Conclusions
The linagliptin regimen simplified in-hospital antihyperglycemic management and reduced hypoglycemia events compared to basal-bolus insulin regimen in patients with type 2 diabetes aged≥80 years. Basal-bolus insulin use and clinical factors were associated with hypoglycemia. The linagliptin regimen could be considered as standard of care for older adult type 2 diabetes patients in the hospital setting.
Resumen
Antecedentes y objetivo
El papel de los inhibidores de la dipeptidil-peptidasa-4 intrahospitalario en pacientes de edad avanzada no ha sido ampliamente descrito. Este estudio analiza la simplificación del manejo antihiperglucémico intrahospitalario (uso de menos insulina) y la reducción de hipoglucemia usando linagliptina en pacientes de ≥80 años con diabetes mellitus tipo 2.
Materiales y métodos
Estudio observacional en pacientes hospitalizados con ≥80 años con diabetes tipo 2 tratados con el protocolo antihiperglucémico que incluye el regimen insulina en bolo-basal o linagliptina, entre enero 2016-diciembre 2023. Se realizó un análisis de puntuaciones de propensión 1:1.
Resultados
Tras la propensión, 944 pacientes fueron incluidos por grupo. Las dosis de insulina total y basal y el número de inyecciones fueron significativamente menores en el grupo linagliptina sin diferencias en la eficacia glucémica. Respecto a la seguridad, los pacientes con insulina bolo-basal tuvieron más hipoglucemias. El uso de insulina bolo-basal (Odds Ratio: 4.22; Intervalo de confianza 95%: 2.14-6.28; p<0.001), mayor dosis de insulina total (Odds Ratio: 3.55; Intervalo de confianza 95%: 2.02-5.36; p<0.001) y número de inyecciones (Odds Ratio: 2.86; Intervalo de confianza 95%: 1.50-4.12; p=0.002) fueron asociados con mayor riesgo de hipoglucemia. Otros factores fueron la edad avanzada, dependencia funcional moderada-severa, demencia moderada-severa, polifarmacia y estado de salud complejo.
Conclusiones
Linagliptina simplificó el manejo antihiperglucémico y redujo hipoglucemias respecto al regimen de insulina en bolo-basal en pacientes de ≥80 años con diabetes tipo 2. El uso del régimen bolo-basal y factores clínicos fueron asociados con la hipoglucemia. Linagliptina intrahospitalaria podría ser considerada como el tratamiento estándar para pacientes de edad avanzada con diabetes tipo 2.}
}
@article{YAVUZ2025106108,
title = {Adverse human rights impacts of dissemination of nonconsensual sexual deepfakes in the framework of European Convention on Human Rights: A victim-centered perspective},
journal = {Computer Law & Security Review},
volume = {56},
pages = {106108},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106108},
url = {https://www.sciencedirect.com/science/article/pii/S0267364925000032},
author = {Can Yavuz},
keywords = {Deepfake, Generative artificial intelligence, Image-based sexual abuse, Deepfake pornography, Technology-facilitated violence, Gender-based violence, Sexual violence, European Convention on Human Rights, Right to respect for private and family life, Freedom of expression, Protection of property},
abstract = {Generative artificial intelligence systems have advanced significantly over the past decade and can now generate synthetic but highly realistic audio, photo, and video, commonly referred to as deepfake. Image-based sexual abuse was the first widespread (mis)use of deepfake technology and continues to be the most common form of its misuse. However, further (empirical) research is needed to examine this phenomenon's adverse human rights implications. This paper analyses the potential adverse human rights impacts of the dissemination of nonconsensual sexual deepfakes in the framework of the European Convention on Human Rights and argues that the dissemination of such deepfakes can hinder the rights protected by the Convention. These include the right to respect for private and family life, as nonconsensual sexual deepfakes can undermine data protection, harm one's image and reputation, and compromise psychological integrity and personal autonomy. Additionally, such deepfakes can threaten freedom of expression by creating a silencing effect on public watchdogs, politicians, and private individuals. Finally, nonconsensual sexual deepfakes can impair the economic and moral rights of pornography performers by abusing their work and bodies to abuse others without authorization and compensation. These findings highlight that the Council of Europe member states must fulfil their obligations to provide effective protection against this technology-facilitated, gender-based, and sexual violence.}
}
@article{NIE2025124373,
title = {Large language models: Tools for new environmental decision-making},
journal = {Journal of Environmental Management},
volume = {375},
pages = {124373},
year = {2025},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2025.124373},
url = {https://www.sciencedirect.com/science/article/pii/S0301479725003494},
author = {Qiyang Nie and Tong Liu},
keywords = {Large language models, GPT, PFAS, Environmental decisions, Water resources management, Multi-objective optimization},
abstract = {This study represents the first exploration of Large Language Models (LLMs) in environmental decision-making, examining their potential benefits and limitations. To address environmental issues, we propose and compare two generalizable frameworks: an LLMs-assisted framework that leverages LLMs to augment human expertise and coding in traditional decision workflows, and an LLMs-driven framework that aims to automate optimization. Through a water engineering case study focusing on PFAS control, where Environmental Fluid Dynamics Code (EFDC) was used for water environment simulation, we illustrate how to instantiate these frameworks and assess their performance. The case study reveals generalizable insights about these frameworks. Results indicate that both frameworks can contribute to environmental decision-making optimization to varying degrees, though their applicability differs significantly when facing complex decision scenarios. The LLMs-assisted framework, which effectively regulates flow rates and achieves higher PFAS interception, demonstrates how AI can enhance human decision-making while preserving the essential role of domain expertise and professional judgment. In contrast, the LLMs-driven framework faces challenges in handling complex parameter optimization tasks due to constraints such as context window and maximum output length. The findings emphasize the advantages of integrating Artificial Intelligence (AI) with conventional environmental modeling and management practices. This work confirms a crucial principle: LLMs should enhance rather than replace human expertise, with the ultimate responsibility for environmental decisions remaining with humans. The originality of this research lies in its innovative methodological approach, which leverages process design and prompt engineering to integrate cutting-edge AI with conventional environmental models, establishing a foundation for responsible human-AI collaboration in environmental decision-making. While examining current strengths and limitations, this framework robustly generates optimized environmental decision strategies, marking a new exploration in the field.}
}
@article{JAMIL2025111505,
title = {From archives to AI: Residential property data across three decades in Brunei Darussalam},
journal = {Data in Brief},
volume = {60},
pages = {111505},
year = {2025},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2025.111505},
url = {https://www.sciencedirect.com/science/article/pii/S2352340925002379},
author = {Haziq Jamil and Amira Barizah Noorosmawie and Hafeezul Waezz Rabu and Lutfi {Abdul Razak}},
keywords = {Housing market, Property listings, Spatial data, Web scraping, Large language models, Brunei},
abstract = {This article introduces the first publicly available data set for analysing the Brunei housing market, covering more than 30,000 property listings from 1993 to early 2025. The data set, curated from property advertisements in newspapers and online platforms, includes key attributes such as price, location, property type, and physical characteristics, enriched with area-level spatial information. Comprehensive and historical, it complements the Brunei Darussalam Central Bank's Residential Property Price Index (RPPI), addressing the limitations of restricted access to raw RPPI data and its relatively short timeline since its inception in 2015. Data collection involved manual transcription from archival sources and automated web scraping using programmatic techniques, supported by innovative processing with Large Language Models (LLMs) to codify unstructured text. The data set enables spatial and temporal analysis, with potential applications in economics, urban planning, and real estate research. Although listing prices are only a proxy for market values and may deviate from actual sale prices due to negotiation dynamics and other factors, this data set still provides a valuable resource for quantitative analyses of housing market trends and for informing policy decisions.}
}
@article{HARUTYUNYAN2025106484,
title = {Outside board director experience and the growth of new ventures},
journal = {Journal of Business Venturing},
volume = {40},
number = {3},
pages = {106484},
year = {2025},
issn = {0883-9026},
doi = {https://doi.org/10.1016/j.jbusvent.2025.106484},
url = {https://www.sciencedirect.com/science/article/pii/S0883902625000126},
author = {Tatevik Harutyunyan and Bram Timmermans and Lars Frederiksen},
keywords = {Board of directors, Industry experience, Directorial experience, New ventures, New venture growth, Environmental characteristics},
abstract = {Most research on entrepreneurship focuses on entrepreneurs' human and social capital as the drivers of new venture performance. However, less is known about how much the endowments of other strategic human resources, namely board directors, influence new venture performance. To generate new insights on this topic, we theorize and empirically investigate to what extent, and under which conditions, the experience of outside board directors affects new venture growth. Our analysis of Norwegian registry data on 15,594 new ventures does not provide immediate evidence that the presence of outside board directors or their experiences drive new venture growth. However, post hoc analysis suggests that the timing of board entry, combined with industry and directorial experience, plays a significant role in shaping growth outcomes. Additionally, the impact of industrial and directorial experience varies depending on the industry environment.}
}
@article{CAI2025101187,
title = {Ethical-Lens: Curbing malicious usages of open-source text-to-image models},
journal = {Patterns},
volume = {6},
number = {3},
pages = {101187},
year = {2025},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2025.101187},
url = {https://www.sciencedirect.com/science/article/pii/S2666389925000352},
author = {Yuzhu Cai and Sheng Yin and Yuxi Wei and Chenxin Xu and Weibo Mao and Felix Juefei-Xu and Siheng Chen and Yanfeng Wang},
keywords = {value alignment, text-to-image models, large language models, AI safety},
abstract = {Summary
The burgeoning landscape of text-to-image models, exemplified by innovations such as Midjourney and DALL·E 3, has revolutionized content creation across diverse sectors. However, these advances bring forth critical ethical concerns, particularly with the misuse of open-source models to generate content that violates societal norms. Addressing this, we introduce Ethical-Lens, a framework designed to facilitate the value-aligned usage of text-to-image tools without necessitating internal model revision. Ethical-Lens ensures value alignment in text-to-image models across toxicity and bias dimensions by refining user commands and rectifying model outputs. Systematic evaluation metrics, combining GPT4-V, HEIM, and FairFace scores, assess alignment capability. Our experiments reveal that Ethical-Lens enhances alignment capabilities to levels comparable with or superior to commercial models such as DALL·E 3, while preserving the quality of generated images. This study indicates the potential of Ethical-Lens to promote the sustainable development of open-source text-to-image tools and their beneficial integration into society.}
}
@article{GOVINDARAJAN2025124031,
title = {Blockchain technologies adoption in healthcare: Overcoming barriers amid the hype cycle to enhance patient care},
journal = {Technological Forecasting and Social Change},
volume = {213},
pages = {124031},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2025.124031},
url = {https://www.sciencedirect.com/science/article/pii/S0040162525000629},
author = {Usharani Hareesh Govindarajan and Gagan Narang and Dhiraj Kumar Singh and Vinay Surendra Yadav},
keywords = {Adoption barriers, Blockchain technologies, Healthcare ecosphere, Patent analysis, SWARA, Top2Vec},
abstract = {Blockchain technologies are increasingly recognized as a transformative force across industries, offering potential solutions for information management, data security, and operational efficiency improvement. However, integration into the healthcare sector faces significant barriers, ranging from technical challenges to organizational resistance. In this study, a methodology is proposed that examines these critical barriers through a comprehensive analysis of 3265 academic papers and derives actionable solutions from 1566 patents published between 2016 and 2023. This approach bridges the gap between identifying challenges and implementing solutions. Using the “Stepwise Weight Assessment Ratio Analysis (SWARA)”, twelve critical adoption challenges are analyzed, while Top2Vec-based topic modeling identifies innovations that best address the ranked barriers. In addition to this, the proposed ‘healthcare ecosphere’ knowledge map serves as a comprehensive tool to analyze key stakeholders, their interactions, and the alignment of adoption barriers in solution spaces. The findings show that innovations in blockchain technologies are heavily concentrated in areas such as data security and application functionalities, whereas other critical domains—such as consensus mechanisms, governance, and regulatory frameworks—remain underexplored, pointing to opportunities for growth and development. The mapping of barriers to solutions provides practical guidance for healthcare providers, policymakers, and technologists seeking to implement the blockchain technologies effectively.}
}