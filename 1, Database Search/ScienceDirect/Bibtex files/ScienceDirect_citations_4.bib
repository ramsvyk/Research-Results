@article{SYRIANI2024101287,
title = {Screening articles for systematic reviews with ChatGPT},
journal = {Journal of Computer Languages},
volume = {80},
pages = {101287},
year = {2024},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2024.101287},
url = {https://www.sciencedirect.com/science/article/pii/S2590118424000303},
author = {Eugene Syriani and Istvan David and Gauransh Kumar},
keywords = {Generative AI, GPT, Empirical research, Large language model, Literature review, Mapping study, Screening},
abstract = {Systematic reviews (SRs) provide valuable evidence for guiding new research directions. However, the manual effort involved in selecting articles for inclusion in an SR is error-prone and time-consuming. While screening articles has traditionally been considered challenging to automate, the advent of large language models offers new possibilities. In this paper, we discuss the effect of using ChatGPT on the SR process. In particular, we investigate the effectiveness of different prompt strategies for automating the article screening process using five real SR datasets. Our results show that ChatGPT can reach up to 82% accuracy. The best performing prompts specify exclusion criteria and avoid negative shots. However, prompts should be adapted to different corpus characteristics.}
}
@article{CASTILLOORTIZ2024100997,
title = {Computer vision solution for uniform adherence in gastronomy schools: An artificial intelligence case study},
journal = {International Journal of Gastronomy and Food Science},
volume = {37},
pages = {100997},
year = {2024},
issn = {1878-450X},
doi = {https://doi.org/10.1016/j.ijgfs.2024.100997},
url = {https://www.sciencedirect.com/science/article/pii/S1878450X24001306},
author = {Ismael Castillo-Ortiz and Carmen Villar-Patiño and Elizabeth Guevara-Martínez},
keywords = {Culinary education, Uniform standards detection, Deep learning, Gastronomy schools, Code-free},
abstract = {This research study presents an innovative application of computer vision technology in culinary education to ensure consistent student uniform adherence, crucial to accomplishing hygiene, safety, and professionalism standards. The proposed approach utilizes the Cross-Industry Standard Process for Data Mining (CRISP-DM) methodology to generate a computer vision application prototype to identify specific culinary uniforms components, such as chef's jackets, aprons, hats, and pants. The development process using LandingLens, a code-free platform, involves several stages: business and data understanding, preparation, modeling, evaluation, and deployment. The final model was training with 77 images, and the application deployment was tested using 38 images. Results demonstrate the potential of artificial intelligence to enhance operational efficiency and uphold professional standards in culinary education. Integrating computer vision addresses the challenges associated with manual monitoring and opens opportunities for broader adoption of technology in culinary pedagogy and training.}
}
@article{RIEMER2024102824,
title = {Conceptualizing generative AI as style engines: Application archetypes and implications},
journal = {International Journal of Information Management},
volume = {79},
pages = {102824},
year = {2024},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2024.102824},
url = {https://www.sciencedirect.com/science/article/pii/S0268401224000720},
author = {Kai Riemer and Sandra Peter},
keywords = {Generative AI, Large language models, Style engines, AI assistants, AI agents},
abstract = {The rise of generative AI has brought with it a surprising paradox: systems that excel at tasks once thought to be uniquely human, like fluent conversation or persuasive writing, while simultaneously failing to meet traditional expectations of computing, in terms of reliability, accuracy, and veracity (e.g., given the various issues with so-called ‘hallucinations’). We argue that, when generative AI is seen through a traditional computing lens, its development focuses on optimizing for traditional computing traits that remain in principle unattainable. This risks backgrounding what is most novel and defining about it. As probabilistic technologies, generative AIs do not store, in any traditional sense, any data or content. Rather, essential features of training data become encoded in deep neural networks as patterns, that become practically available as styles. We discuss what happens when the distinction between objects and their appearance dissolves and all aspects of images or text become understood as styles, accessible for exploration and creative combination and generation. For example, defining visual qualities of entities like ‘chair’ or ‘cat’ become available as ‘chair-ness’ or ‘cat-ness’ for creative image generation. We argue that, when understood as style engines, unique generative AI capabilities become conceptualized as complementing traditional computing ones. This will aid both computing practitioners and information systems researchers in reconciling and integrating generative AI into the traditional IS landscape. Our conceptualization leads us to propose four archetypes of generative AI application and use, and to highlight future avenues for information systems research made visible by this conceptualization, as well as implications for practice and policymaking.}
}
@article{ZHOU2024104658,
title = {Face masks facilitate discrimination of genuine and fake smiles – But people believe the opposite},
journal = {Journal of Experimental Social Psychology},
volume = {115},
pages = {104658},
year = {2024},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2024.104658},
url = {https://www.sciencedirect.com/science/article/pii/S0022103124000714},
author = {Haotian Zhou and Meiying Wang and Yu Yang and Elizabeth A. Majka},
keywords = {Mask, Genuine smile, Fake smile, Facial expressions, Deception detection, Cross-cultural difference},
abstract = {It seems a foregone conclusion that face mask-wearing hinders the interpretation of facial expressions, increasing the risk of interpersonal miscommunication. This research identifies a notable counter-case to this apparent truism. In multiple experiments, perceivers were more accurate distinguishing between genuine and fake smiles when the mouth region was concealed under a mask versus exposed. Masks improved accuracy by shielding perceivers from the undue influence of non-diagnostic cues hidden behind masks. However, perceivers were unaware of the advantage bestowed by masks, holding, instead, the misbelief that masks severely obscure the distinction between genuine and fake smiles. Furthermore, these patterns proved to be culturally invariant rather than culturally contingent, holding true for both Westerners and Easterners.}
}
@article{MADRIDGARCIA2024108920,
title = {From Web to RheumaLpack: Creating a Linguistic Corpus for Exploitation and Knowledge Discovery in Rheumatology},
journal = {Computers in Biology and Medicine},
volume = {179},
pages = {108920},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108920},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524010059},
author = {Alfredo Madrid-García and Beatriz Merino-Barbancho and Dalifer Freites-Núñez and Luis Rodríguez-Rodríguez and Ernestina Menasalvas-Ruíz and Alejandro Rodríguez-González and Anselmo Peñas},
keywords = {Web corpus, Artificial intelligence, Rheumatology, Natural language processing},
abstract = {This study introduces RheumaLinguisticpack (RheumaLpack), the first specialised linguistic web corpus designed for the field of musculoskeletal disorders. By combining web mining (i.e., web scraping) and natural language processing (NLP) techniques, as well as clinical expertise, RheumaLpack systematically captures and curates structured and unstructured data across a spectrum of web sources including clinical trials registers (i.e., ClinicalTrials.gov), bibliographic databases (i.e., PubMed), medical agencies (i.e. European Medicines Agency), social media (i.e., Reddit), and accredited health websites (i.e., MedlinePlus, Harvard Health Publishing, and Cleveland Clinic). Given the complexity of rheumatic and musculoskeletal diseases (RMDs) and their significant impact on quality of life, this resource can be proposed as a useful tool to train algorithms that could mitigate the diseases' effects. Therefore, the corpus aims to improve the training of artificial intelligence (AI) algorithms and facilitate knowledge discovery in RMDs. The development of RheumaLpack involved a systematic six-step methodology covering data identification, characterisation, selection, collection, processing, and corpus description. The result is a non-annotated, monolingual, and dynamic corpus, featuring almost 3 million records spanning from 2000 to 2023. RheumaLpack represents a pioneering contribution to rheumatology research, providing a useful resource for the development of advanced AI and NLP applications. This corpus highlights the value of web data to address the challenges posed by musculoskeletal diseases, illustrating the corpus's potential to improve research and treatment paradigms in rheumatology. Finally, the methodology shown can be replicated to obtain data from other medical specialities. The code and details on how to build RheumaLpack are also provided to facilitate the dissemination of such resource.}
}
@article{MA2024102371,
title = {A bibliometric review on application of machine learning in additive manufacturing and practical justification},
journal = {Applied Materials Today},
volume = {40},
pages = {102371},
year = {2024},
issn = {2352-9407},
doi = {https://doi.org/10.1016/j.apmt.2024.102371},
url = {https://www.sciencedirect.com/science/article/pii/S2352940724003160},
author = {Quoc-Phu Ma and Hoang-Sy Nguyen and Jiri Hajnys and Jakub Mesicek and Marek Pagac and Jana Petru},
keywords = {Additive manufacturing, Machine learning, Bibliometric analysis},
abstract = {This paper delves into the cutting-edge applications of Machine Learning (ML) within modern Additive Manufacturing (AM), employing bibliometric analysis as its methodology. Formulated around three pivotal research questions, the study navigates through the current landscape of the research field. Utilizing data sourced from Web of Science, the paper conducts a comprehensive statistical and visual analysis to unveil underlying patterns within the existing literature. Each category of ML techniques is elucidated alongside its specific applications, providing researchers with a holistic overview of the research terrain and serving as a practical checklist for those seeking to address particular challenges. Culminating in a vision for the Smart Additive Manufacturing Factory (SAMF), the paper envisions seamless integration of reviewed ML techniques. Furthermore, it offers critical insights from a practical standpoint, thereby facilitating shaping future research directions in the field.}
}
@article{NENE2024e6,
title = {Evolution of Drug Development and Regulatory Affairs: The Demonstrated Power of Artificial Intelligence},
journal = {Clinical Therapeutics},
volume = {46},
number = {8},
pages = {e6-e14},
year = {2024},
issn = {0149-2918},
doi = {https://doi.org/10.1016/j.clinthera.2024.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0149291824001383},
author = {Linda Nene and Brian Thabile Flepisi and Sarel Jacobus Brand and Charlise Basson and Marissa Balmith},
keywords = {Artificial intelligence, Drug development, Regulatory affairs, Machine learning, Natural language processing, Deep learning},
abstract = {Purpose
Artificial intelligence (AI) refers to technology capable of mimicking human cognitive functions and has important applications across all sectors and industries, including drug development. This has considerable implications for the regulation of drug development processes, as it is expected to transform both the way drugs are brought to market and the systems through which this process is controlled. There is currently insufficient evidence in published literature of the real-world applications of AI. Therefore, this narrative review investigated, collated, and elucidated the applications of AI in drug development and its regulatory processes.
Methods
A narrative review was conducted to ascertain the role of AI in streamlining drug development and regulatory processes.
Findings
The findings of this review revealed that machine learning or deep learning, natural language processing, and robotic process automation were favored applications of AI. Each of them had considerable implications on the operations they were intended to support. Overall, the AI tools facilitated access and provided manageability of information for decision-making across the drug development lifecycle. However, the findings also indicate that additional work is required by regulatory authorities to set out appropriate guidance on applications of the technology, which has critical implications for safety, regulatory process workflow and product development costs.
Implications
AI has adequately proven its utility in drug development, prompting further investigations into the translational value of its utility based on cost and time saved for the delivery of essential drugs.}
}
@article{JIANG2024102614,
title = {Science and technology evaluation reform and universities’ innovation performance},
journal = {Technology in Society},
volume = {78},
pages = {102614},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102614},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24001623},
author = {Chun Jiang and Shihan Li and Qi Shen},
keywords = {Science and technology evaluation reform, University innovation, Basic research, Applied research, Difference in differences},
abstract = {Does science and technology evaluation (STE) policy reform improve universities' innovation performance? Research typically argues that more upstream R&D investment leads to more downstream innovation performance, but it is less clear what the trade-offs over time might be for targeted investments in universities influenced by STE reform. In 2013, China proposed STE reform measures focused on ‘innovation quality, efficiency, and contribution’. Using a difference-in-differences research design on a comprehensive longitudinal database of 62 universities spanning from 2009 to 2016, we show that this STE policy positively affects university basic research outputs and quality but weakens applied research outputs. This effect is pronounced in pilot universities with better resources. Empirical evidence suggests that the STE policy works mainly through mobilising the enthusiasm of human capital, improving R&D intensity in science and technology funds, and promoting industrialisation-oriented R&D projects. We consider the following possible perspectives on the mechanisms of change: knowledge asset development, economic competition, and a socio-political process. This analysis leads to theoretical developments about how basic versus applied science produces outputs in the Chinese context. The paper also shows that the STE policy promotes technology transfer in universities.}
}
@article{OSADCHAYA2024571,
title = {To ChatGPT, or not to ChatGPT: Navigating the paradoxes of generative AI in the advertising industry},
journal = {Business Horizons},
volume = {67},
number = {5},
pages = {571-581},
year = {2024},
note = {SPECIAL ISSUE: WRITTEN BY CHATGPT},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2024.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0007681324000624},
author = {Elena Osadchaya and Ben Marder and Jennifer A. Yule and Amy Yau and Laura Lavertu and Nikolaos Stylos and Sebastian Oliver and Rob Angell and Anouk de Regt and Liyu Gao and Kang Qi and Will Zhiyuan Zhang and Yiwei Zhang and Jiayuan Li and Sara AlRabiah},
keywords = {ChatGPT, Generative AI, Paradoxes, Advertising, Chatbots},
abstract = {Generative AI (GenAI) technology is evoking both excitement and fear about its potential impact across a host of industries—including advertising, where it is expected to have a significant disruptive effect. This article utilizes the paradox lens to explore the implications of text-to-text GenAI in the form of ChatGPT for the advertising industry. Drawing on 48 interviews with advertising professionals, we identify three operational paradoxes that are associated with conducting research, creativity, efficiency, and one psychological paradox related to work identity. To gain a competitive advantage, we urge practitioners to adopt a confrontation-based coping strategy to navigate these paradoxes. This can be mobilized via an ambidexterity or contingency paradox management approach. We outline specific tactics in this article.}
}
@article{OHSE2024101663,
title = {Zero-Shot Strike: Testing the generalisation capabilities of out-of-the-box LLM models for depression detection},
journal = {Computer Speech & Language},
volume = {88},
pages = {101663},
year = {2024},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101663},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000469},
author = {Julia Ohse and Bakir Hadžić and Parvez Mohammed and Nicolina Peperkorn and Michael Danner and Akihiro Yorita and Naoyuki Kubota and Matthias Rätsch and Youssef Shiban},
keywords = {Depression detection, GPT-4, GPT-3.5, LLM, NLP, Artificial intelligence},
abstract = {Depression is a significant global health challenge. Still, many people suffering from depression remain undiagnosed. Furthermore, the assessment of depression can be subject to human bias. Natural Language Processing (NLP) models offer a promising solution. We investigated the potential of four NLP models (BERT, Llama2-13B, GPT-3.5, and GPT-4) for depression detection in clinical interviews. Participants (N = 82) underwent clinical interviews and completed a self-report depression questionnaire. NLP models inferred depression scores from interview transcripts. Questionnaire cut-off values for depression were used as a classifier for depression. GPT-4 showed the highest accuracy for depression classification (F1 score 0.73), while zero-shot GPT-3.5 initially performed with low accuracy (0.34), improved to 0.82 after fine-tuning, and achieved 0.68 with clustered data. GPT-4 estimates of symptom severity PHQ-8 score correlated strongly (r = 0.71) with true symptom severity. These findings demonstrate the potential of AI models for depression detection. However, further research is necessary before widespread deployment can be considered.}
}
@article{SAAD2024110874,
title = {Bangla news article dataset},
journal = {Data in Brief},
volume = {57},
pages = {110874},
year = {2024},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2024.110874},
url = {https://www.sciencedirect.com/science/article/pii/S2352340924008382},
author = {Asif Mohammed Saad and Umme Niraj Mahi and Md. Shahidul Salim and Sk Imran Hossain},
keywords = {Data analysis, Classification, Natural language processing},
abstract = {In this research, we present an updated standard Bangla dataset based on gathered Bangla news articles. In total, more than 1.9 million articles from nine Bangla news websites were gathered; the selection process was led by a number of categories, including sports, economy, politics, local news, tech, tourism, entertainment, education, health, the arts, and many more. The dataset per newspaper contains varying attributes, such as title, content, time, tags, meta, category, etc. This dataset will enable data scientists to investigate and assess theories related to Bangla natural language processing. Furthermore, there is a greater chance that the dataset will be utilized for domain-specific large language models in the context of Bangladesh, and it may be used to develop deep learning and machine learning models that categorize articles according to subjects.}
}
@article{WOLD2024109167,
title = {Enhancing wind field resolution in complex terrain through a knowledge-driven machine learning approach},
journal = {Engineering Applications of Artificial Intelligence},
volume = {137},
pages = {109167},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109167},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624013253},
author = {Jacob Wulff Wold and Florian Stadtmann and Adil Rasheed and Mandar Tabib and Omer San and Jan-Tore Horn},
keywords = {Generative adversarial networks, Turbulence, Physics-based simulator, Super-resolution upscaling, Computational methods},
abstract = {Wind energy, an essential component in the fight against climate change, relies heavily on precise, detailed wind field simulations to optimize wind farms, particularly in complex terrain with intricate wind patterns. However, conventional high-resolution simulations come with a hefty computational cost, limiting their applicability in real-time decision-making. This research addresses this challenge by proposing a machine learning-driven, computationally efficient approach utilizing a modified Generative Adversarial Network. The contribution of this work is threefold. Firstly, we provide access to a unique high-resolution dataset of wind fields in complex terrain. Secondly, we introduce a knowledge-based modification to the loss function, ensuring that the algorithm captures crucial characteristics of the flow within complex terrains. Finally, we demonstrate the potential of our approach to enhance wind flow resolution in real-life wind farms. Through this, our method delivers comparable accuracy to high-resolution simulations while substantially reducing computational demands. This advancement greatly enhances the accessibility and efficiency of high-resolution wind field simulations, facilitating real-time optimization of wind farms. Moreover, we illustrate that by designing an appropriate loss function informed by domain knowledge, we can mitigate the need for adversarial training.}
}
@article{BELLOLEPE2024287,
title = {Speech pauses in speakers with and without aphasia: A usage-based approach},
journal = {Cortex},
volume = {178},
pages = {287-298},
year = {2024},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2024.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0010945224001850},
author = {Sebastian Bello-Lepe and Sabrina Mahmood and Rosemary Varley and Vitor Zimmerer},
keywords = {Pauses, Aphasia, Connected speech, Collocation strength, Usage-based approaches},
abstract = {Pauses in speech are indicators of cognitive effort during language production and have been examined to inform theories of lexical, grammatical and discourse processing in healthy speakers and individuals with aphasia (IWA). Studies of pauses have commonly focused on their location and duration in relation to grammatical properties such as word class or phrase complexity. However, recent studies of speech output in aphasia have revealed that utterances of IWA are characterised by stronger collocations, i.e., combinations of words that are often used together. We investigated the effects of collocation strength and lexical frequency on pause duration in comic strip narrations of IWA and non-brain-damaged (NBD) individuals with part of speech (PoS; content and function words) as covariate. Both groups showed a decrease in pause duration within more strongly collocated bigrams and before more frequent content words, with stronger effects in IWA. These results are consistent with frameworks which propose that strong collocations are more likely to be processed as holistic, perhaps even word-like, units. Usage-based approaches prove valuable in explaining patterns of preservation and impairment in aphasic language production.}
}
@article{ZYLA2024e00351,
title = {Scanning, modelling and dissemination of the interior appearance of wooden historic churches in the Maramures region of Romania},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {34},
pages = {e00351},
year = {2024},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2024.e00351},
url = {https://www.sciencedirect.com/science/article/pii/S2212054824000365},
author = {Kamil Żyła and Jacek Kęsik and Sylwester Korga and Marek Miłosz and Karolina Rybak},
keywords = {Material cultural heritage, 3D scanning, Historic wooden churches, Interior appearance, Digital dissemination},
abstract = {In the Maramureș region of Romania there is a large number of historic wooden churches dating back to the 17th and 18th centuries. These objects of tangible cultural heritage continue to be exposed to degradation factors. Meanwhile, their interiors hide important and historically valuable artefacts of cultural heritage. Their loss would noticeably impoverish the heritage of local communities. This article presents the problems of 3D scanning the interiors of wooden historic churches, the scanning methodology specifically developed for this occasion, the equipment used and its parameters, and the results of its application. Four examples of wooden churches from the Maramureș region were studied: modern laser scanning technologies with appropriate selection of parameters permits a satisfactorily precise scan of the interior of a wooden religious building, even in difficult lighting conditions. The methodology and equipment choice was proven to be successful and is recommended here for future projects in the region and for structures facing similar challenges.}
}
@article{WANG2024103496,
title = {The role of trademark rights expansion in the formation and abuse of market power},
journal = {International Review of Economics & Finance},
volume = {95},
pages = {103496},
year = {2024},
issn = {1059-0560},
doi = {https://doi.org/10.1016/j.iref.2024.103496},
url = {https://www.sciencedirect.com/science/article/pii/S105905602400488X},
author = {Jun Wang and Xingyu Yan},
keywords = {Expansion of trademark rights, Market power, Antitrust, Differential treatment, Tied sales},
abstract = {Trademarks play a critical role in the creation and abuse of market power, in the sense that when expansively protected, trademark rights yield legal advantages that can be leveraged to lock in consumers and raise entry barriers. But this role has long been underestimated or even overlooked. Based on empirical research and qualitative case analysis, we find that the legal advantages created by the expansion of trademark rights are a key factor in the formation of firms’ market power in product domains with a high degree of information asymmetry. In situations where firms have market power, the geographical and cross-category expansion of trademark rights can enable and rationalize the implementation of abusive practices such as price discrimination and tying. Limiting the anti-competitive effects of the expansion of trademark rights requires the combined efforts of ex post antitrust and ex ante regulation.}
}
@article{ZHANG2024110186,
title = {Real-time data visual monitoring of triboelectric nanogenerators enabled by Deep learning},
journal = {Nano Energy},
volume = {130},
pages = {110186},
year = {2024},
issn = {2211-2855},
doi = {https://doi.org/10.1016/j.nanoen.2024.110186},
url = {https://www.sciencedirect.com/science/article/pii/S2211285524009376},
author = {Huiya Zhang and Tao Liu and Xuelian Zou and Yunpeng Zhu and Mingchao Chi and Di Wu and Keyang Jiang and Sijia Zhu and Wenxia Zhai and Shuangfei Wang and Shuangxi Nie and Zhiwei Wang},
keywords = {Triboelectric nanogenerator, Deep learning, Self-powered sensing, Real-time monitoring},
abstract = {The rapid advancement of smart sensors and logic algorithms has propelled the widespread adoption of the Internet of Things (IoT) and expedited the advent of the intelligent era. The integration of triboelectric nanogenerator (TENG) sensors with Deep learning (DL) leverages unique advantages of TENG such as self-powered sensing, high sensitivity, and broad applicability, along with DL's robust data processing capabilities to effectively, efficiently, and visually monitor various relevant signals. This amalgamation exhibits significantly superior sensing performance and immense developmental potential, finding extensive utility in domains like smart homes, healthcare system, environmental monitoring, among others. Currently, the synergistic working principle of integrating these two technologies remains insufficiently elucidated. This review presents a comprehensive overview of cutting-edge DL techniques and related research aimed at enhancing real-time visual monitoring of TENG. Specifically, it focuses on DL algorithms such as Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and Long Short-Term Memory (LSTM) for processing intricate TENG-generated datasets. Furthermore, this review outlines the advantages and synergistic mechanisms resulting from the integration of DL with TENG sensors, providing a comprehensive summary of their latest applications in various fields requiring real-time data visual monitoring. Finally, it analyzes the prospects, challenges, and countermeasures associated with the integrated development of TENG and DL while offering a comprehensive theoretical foundation and practical guidance for future advancements in this field.}
}
@article{NOSAL2024109701,
title = {Blackcurrants shape gut microbiota profile and reduce risk of postmenopausal osteoporosis via the gut-bone axis: Evidence from a pilot randomized controlled trial},
journal = {The Journal of Nutritional Biochemistry},
volume = {133},
pages = {109701},
year = {2024},
issn = {0955-2863},
doi = {https://doi.org/10.1016/j.jnutbio.2024.109701},
url = {https://www.sciencedirect.com/science/article/pii/S0955286324001347},
author = {Briana M. Nosal and Staci N. Thornton and Manije {Darooghegi Mofrad} and Junichi R. Sakaki and Kyle J. Mahoney and Zachary Macdonald and Lauren Daddi and Thi Dong Binh Tran and George Weinstock and Yanjiao Zhou and Elaine Choung-Hee Lee and Ock K. Chun},
keywords = {Blackcurrant, Bone, Women, Menopause, Gut microbiota, Osteoporosis},
abstract = {This study aimed to investigate the effects of blackcurrant (BC) on gut microbiota abundance and composition, inflammatory and immune responses, and their relationship with bone mass changes. The effects of BC on bone mineral density (BMD), gut microbiota, and blood inflammatory and immune biomarkers were evaluated using DXA, stool and fasting blood collected from a pilot three-arm, randomized, double-blind, placebo-controlled clinical trial. Fifty-one peri- and early postmenopausal women aged 45–60 years were randomly assigned into one of three treatment groups for 6 months: control, low BC (392 mg/day) and high BC (784 mg/day); and 40 women completed the trial. BC supplementation for 6 months effectively mitigated the loss of whole-body BMD (P<.05). Six-month changes (%) in peripheral IL-1β (P=.056) and RANKL (P=.052) for the high BC group were marginally significantly lower than the control group. Six-month changes in whole-body BMD were inversely correlated with changes in RANKL (P<.01). In proteome analysis, four plasma proteins showed increased expression in the high BC group: IGFBP4, tetranectin, fetuin-B, and vitamin K-dependent protein S. BC dose-dependently increased the relative abundance of Ruminococcus 2 (P<.05), one of six bacteria correlated with BMD changes in the high BC group (P<.05), suggesting it might be the key bacteria that drove bone protective effects. Daily BC consumption for 6 months mitigated bone loss in this population potentially through modulating the gut microbiota composition and suppressing osteoclastogenic cytokines. Larger-scale clinical trials on the potential benefits of BC and connection of Ruminococcus 2 with BMD maintenance in postmenopausal women are warranted. Trial Registration: NCT04431960, https://classic.clinicaltrials.gov/ct2/show/NCT04431960.}
}
@article{JALALI2024109801,
title = {Large language models in electronic laboratory notebooks: Transforming materials science research workflows},
journal = {Materials Today Communications},
volume = {40},
pages = {109801},
year = {2024},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2024.109801},
url = {https://www.sciencedirect.com/science/article/pii/S2352492824017823},
author = {Mehrdad Jalali and Yi Luo and Lachlan Caulfield and Eric Sauter and Alexei Nefedov and Christof Wöll},
keywords = {Materials science research, Natural language processing (NLP), Electronic laboratory notebooks (ELNs), Large language models (LLMs), Knowledge extraction, Scientific data management},
abstract = {In recent years, there has been a surge in research efforts dedicated to harnessing the capabilities of Large Language Models (LLMs) in various domains, particularly in material science. This paper delves into the transformative role of LLMs within Electronic Laboratory Notebooks (ELNs) for scientific research. ELNs represent a pivotal technological advancement, providing a digital platform for researchers to record and manage their experiments, data, and findings. This study explores the potential of LLMs to revolutionize fundamental aspects of science, including experimental methodologies, data analysis, and knowledge extraction within the ELN framework. We present a demonstrative showcase of LLM applications in ELN environments and, furthermore, we conduct a series of empirical evaluations to critically assess the practical impact of LLMs in enhancing research processes within the dynamic field of materials science. Our findings illustrate how LLMs can significantly elevate the quality and efficiency of research outcomes in ELNs, thereby advancing knowledge and innovation in materials science research and beyond.}
}
@article{DONG2024106016,
title = {Meta-Regulation: An ideal alternative to the primary responsibility as the regulatory model of generative AI in China},
journal = {Computer Law & Security Review},
volume = {54},
pages = {106016},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106016},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000827},
author = {Huijuan Dong and Junkai Chen},
keywords = {Generative AI, Primary responsibility, Meta-Regulation, Collaborative governance},
abstract = {Generative AI with stronger responsiveness and emergent abilities has triggered a global boom and is facing challenges such as data compliance risks during the pretraining process and risks of generating fake information, which has raised concerns among global regulatory authorities. The European Union, United States, United Kingdom, and other countries and regions are gradually establishing risk-based, scenario-based, and outcome-based governance models for generative AI. China recently introduced new regulations for the management of generative AI, which adopt a governance model focusing on generative AI service providers. It suggests that China is continuing the principle of primary responsibility in Internet governance, which encompasses legal responsibility, contractual obligations, and ethical responsibility. However, the governance model based on primary responsibility emphasizes the accountability of generative AI model service providers, with relatively limited regulation on other important entities such as users and large-scale dissemination platforms, which may not be conducive to achieving China's regulatory goals for the AI industry. In comparison, the Meta-Regulation model could be an ideal alternative for China. As a classic theory explaining the public-private relationship, the ‘Meta-Regulation’ aligns with the generative AI governance requirements. Based on the Meta-Regulation theory, the governance of generative AI in China should move towards a direction of emphasizing safety, transparency, collaborative governance, and accountability. In line with this, it is necessary to include users and large-scale dissemination platforms within the regulatory scope and establish overarching governance objectives that ensure the responsible distribution of duties among stakeholders, with regulatory authorities assuming ultimate oversight responsibility and technical coordination. At the level of specific improvement measures, it is possible to integrate the three stages of model development, usage, and content dissemination of generative AI. During the model development stage, generative AI providers have specific transparency obligations. In the usage stage, a self-regulatory system centered around platform autonomy should be constructed. In the content dissemination stage, the proactive notification obligations of the dissemination platforms should be clearly defined. Additionally, the enforcement of technical interoperability requirements is necessary, thereby promoting the orderly development of generative AI applications.}
}
@article{CHEN2024128167,
title = {A comprehensive survey for generative data augmentation},
journal = {Neurocomputing},
volume = {600},
pages = {128167},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128167},
url = {https://www.sciencedirect.com/science/article/pii/S092523122400938X},
author = {Yunhao Chen and Zihui Yan and Yunjie Zhu},
keywords = {Generative data augmentation, Synthetic data, Data augmentation},
abstract = {Generative data augmentation (GDA) has emerged as a promising technique to alleviate data scarcity in machine learning applications. This thesis presents a comprehensive survey and unified framework of the GDA landscape. We first provide an overview of GDA, discussing its motivation, taxonomy, and key distinctions from synthetic data generation. We then systematically analyze the critical aspects of GDA—selection of generative models, techniques to utilize them, data selection methodologies, validation approaches, and diverse applications. Our proposed unified framework categorizes the extensive GDA literature, revealing gaps such as the lack of universal benchmarks. The thesis summarizes promising research directions, including , effective data selection, theoretical development for large-scale models’ application in GDA and establishing a benchmark for GDA. By laying a structured foundation, this thesis aims to nurture more cohesive development and accelerate progress in the vital arena of generative data augmentation.}
}
@article{MAKAROV2024108632,
title = {Good machine learning practices: Learnings from the modern pharmaceutical discovery enterprise},
journal = {Computers in Biology and Medicine},
volume = {177},
pages = {108632},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108632},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524007170},
author = {Vladimir Makarov and Christophe Chabbert and Elina Koletou and Fotis Psomopoulos and Natalja Kurbatova and Samuel Ramirez and Chas Nelson and Prashant Natarajan and Bikalpa Neupane},
keywords = {Artificial intelligence, Machine learning, Pharmaceutical, Drug discovery, Best practice, Life sciences},
abstract = {Machine Learning (ML) and Artificial Intelligence (AI) have become an integral part of the drug discovery and development value chain. Many teams in the pharmaceutical industry nevertheless report the challenges associated with the timely, cost effective and meaningful delivery of ML and AI powered solutions for their scientists. We sought to better understand what these challenges were and how to overcome them by performing an industry wide assessment of the practices in AI and Machine Learning. Here we report results of the systematic business analysis of the personas in the modern pharmaceutical discovery enterprise in relation to their work with the AI and ML technologies. We identify 23 common business problems that individuals in these roles face when they encounter AI and ML technologies at work, and describe best practices (Good Machine Learning Practices) that address these issues.}
}
@article{CANCELAOUTEDA2024101291,
title = {The EU's AI act: A framework for collaborative governance},
journal = {Internet of Things},
volume = {27},
pages = {101291},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101291},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524002324},
author = {Celso Cancela-Outeda},
keywords = {Popularization, Civil society, Stakeholders, Governance, “black box”, Collaborative logic},
abstract = {In February 2024, the Council and the European Parliament (EP) agreed on the Artificial Intelligence Regulation (usually known as AI Act, AIA) .22We use the AI Act version P9_TA(2024)0138 Artificial Intelligence Act European Parliament legislative resolution of 13 March 2024 on the proposal for a regulation of the European Parliament and of the Council on laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union Legislative Acts (COM(2021)0206–C9-0146/2021–2021/0106(COD)). https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf This regulation evaluates AI applications to ensure they are used ethically and responsibly, promoting the development of safe and lawful AI across the EU's single market. It establishes a comprehensive legal framework with a risk-based approach, aiming to achieve a balance between protecting the health, safety, and fundamental rights of European citizens and ensuring that the growing AI industry in Europe remains competitive and continues to innovate. The AIA also includes governance mechanisms oriented towards achieving effective implementation throughout the EU. For this purpose, a European Artificial Intelligence Office has already been established. In accordance with the provisions of the forthcoming AIA, it will establish a European Artificial Intelligence Board, an advisory forum, and a scientific panel. Furthermore, it will be set up at the national level the so-called national competent authorities. In this way, a single European governance system for AI is emerging, inspired by collaborative governance, which is essential for achieving fair and effective implementation of AI regulations across the EU. The main objective of this text is to critically examine the governance system established by the AIA. Using the contents of the current version of the AIA (April 2024), this analysis delves into the mechanisms and structures designed to implement AI across the EU. As a conclusion, it offers a critical perspective on the collaborative governance, highlighting its strengths and potential areas for improvement.}
}
@article{SEETHARAM2024100273,
title = {Headlines or Hashtags? The battle in social media for investor sentiment in the stock market},
journal = {International Journal of Information Management Data Insights},
volume = {4},
number = {2},
pages = {100273},
year = {2024},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100273},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000624},
author = {Yudhvir Seetharam and Kingstone Nyakurukwa},
keywords = {Behavioural finance, Transfer entropy, Online investor sentiment},
abstract = {This study tackles the complex task of measuring investor sentiment, a latent variable often measured through various proxies. The focus here is on textual sentiment extracted from online sources, specifically news media and social media sentiment. The central inquiry is whether these proxies are equivalent indicators of investor sentiment. Employing firm-level daily sentiment scores for DJIA stocks and leveraging Granger causality and transfer entropy, the research investigates the dynamics of information flow between these proxies. The findings show a prevailing pattern: information predominantly flows from social media to news for the majority of stocks while a reverse relationship is established for some stocks. The variations across stocks suggest that these proxies do not uniformly capture the same underlying phenomena. The study shows the significant role of social media in shaping news media sentiment and prompts considerations about regulating social media platforms in the context of their impact on financial markets.}
}
@article{CARCASSI2024105703,
title = {Additive manufacturing of natural materials},
journal = {Automation in Construction},
volume = {167},
pages = {105703},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105703},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524004394},
author = {Olga Beatrice Carcassi and Lola Ben-Alon},
keywords = {Natural materials, Earth materials, Biobased-materials, Living materials, Additive manufacturing, Mix design, Systematic review},
abstract = {As additive manufacturing (AM) technology continues to advance for computer-aided design and engineering applications, a parallel imperative emerges — a conscientious shift towards more responsible material practices, aligning with ethical, environmental, and social sustainability considerations. The present systematic review analyzes the state-of-the-art developments in relation to AM using natural, low-carbon, and readily available material practices. The results show that published work is situated at the intersection of material science, digital fabrication, and construction, with an array of geo-, bio-, and living mix designs, and different properties analyzed. Under certain conditions, a move towards more use of natural materials could be the solution to source more responsible materials while contributing to the quality of the built environment and the planet Earth itself. The long-term contribution is to provide leading guidance for future research aimed at developing novel and bespoke natural materials in digital fabrication and advanced manufacturing.}
}
@article{GUO2024489,
title = {Recent progress of sensing and machine learning technologies for process monitoring and defects detection in wire arc additive manufacturing},
journal = {Journal of Manufacturing Processes},
volume = {125},
pages = {489-511},
year = {2024},
issn = {1526-6125},
doi = {https://doi.org/10.1016/j.jmapro.2024.07.060},
url = {https://www.sciencedirect.com/science/article/pii/S152661252400714X},
author = {Yibo Guo and Yuming Zhang and Zengxi Pan and Wei Zhou},
keywords = {Wire arc additive manufacturing (WAAM), Defects, Detection, Machine learning, In-process sensing},
abstract = {Wire Arc Additive Manufacturing possesses advantages of high deposition rate and low cost compared with other metal additive manufacturing processes. However, potential defects may occur during the process, such as pores, cracks, lack of fusion, inclusions, delamination, and geometrical deviations. These defects are undesirable and have negative effects. To optimize the performance of the as-built components, and to reduce the potential defects, a feasible solution is to conduct in-process sensing and provide feedback to the control system. This article aims to give a comprehensive review of recent progress on sensing technologies, such as optical, acoustic, vision, thermal, and multiple signals-based sensing technologies, and the application of machine learning to enhance the ability to extract the needed feedback from the in-process monitoring raw data. Effective monitoring of different types of defects typically requires different sensing technologies, focus points, and attentions. Multi-sensor-based sensing systems may thus be needed to provide full-scale information. These necessities include the need for in-time data fusion and more complex data processing. This review analyzes recently explored sensing technologies for their principles and remaining challenges to provide directions for future invention, exploration, and investigation.}
}
@article{MA2024100278,
title = {Exploring ChatGPT literacy in language education: A global perspective and comprehensive approach},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100278},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100278},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X2400081X},
author = {Qing Ma and Peter Crosthwaite and Daner Sun and Di Zou},
keywords = {ChatGPT literacy, Framework, Language teachers, Model validation, Confirmatory factor analysis},
abstract = {With the widespread use of Generative AI in education, effectively utilizing and integrating it into teaching have become key focal points and challenges in education. Different subjects and target audiences require varied norms and strategies for implementing Generative AI, such as ChatGPT. These differences directly impact the educational integration of Generative AI in various educational contexts. To address these disparities and establish common ground, we propose the concept of ChatGPT literacy to bridge research gaps. In this study, we tailor the concept of ChatGPT literacy specifically for language teachers, aiming to delineate the essential competencies needed to proficiently and ethically use ChatGPT as a language learning and teaching tool. We propose a theoretical framework encompassing six fundamental constructs: benefits, limitations, prompts, evaluation (of ChatGPT responses), assessment (assisted by ChatGPT), and ethics, to comprehensively conceptualise and evaluate ChatGPT literacy. Drawing on both quantitative and qualitative survey data from 492 language teachers across 41 countries, we validate the proposed ChatGPT literacy framework by examining teachers' practices and challenges associated with ChatGPT usage. Our analysis of Likert-scale data, utilizing item and confirmatory techniques, confirms the effectiveness of the six-construct framework in defining ChatGPT literacy. In addition, we collected qualitative data through open questions and conducted thematic analysis, demonstrating that ChatGPT has been integrated throughout the instructional cycle, from material preparation to formative and summative assessment phases. These quantitative and qualitative findings have significant implications for a range of stakeholders, including language educators, learners, AI technology developers, and policymakers, providing valuable insights to inform decisions regarding ChatGPT integration in language education. Ultimately, our study equips relevant stakeholders with the necessary competencies to responsibly exploiting ChatGPT's potential in language and other subject areas.}
}
@article{MERSHA2024128111,
title = {Explainable artificial intelligence: A survey of needs, techniques, applications, and future direction},
journal = {Neurocomputing},
volume = {599},
pages = {128111},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128111},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224008828},
author = {Melkamu Mersha and Khang Lam and Joseph Wood and Ali K. AlShami and Jugal Kalita},
keywords = {XAI, Explainable artificial intelligence, Interpretable deep learning, Machine learning, Neural networks, Evaluation methods, Computer vision, Natural language processing, NLP, Transformers, Time series, Healthcare, Autonomous cars},
abstract = {Artificial intelligence models encounter significant challenges due to their black-box nature, particularly in safety-critical domains such as healthcare, finance, and autonomous vehicles. Explainable Artificial Intelligence (XAI) addresses these challenges by providing explanations for how these models make decisions and predictions, ensuring transparency, accountability, and fairness. Existing studies have examined the fundamental concepts of XAI, its general principles, and the scope of XAI techniques. However, there remains a gap in the literature as there are no comprehensive reviews that delve into the detailed mathematical representations, design methodologies of XAI models, and other associated aspects. This paper provides a comprehensive literature review encompassing common terminologies and definitions, the need for XAI, beneficiaries of XAI, a taxonomy of XAI methods, and the application of XAI methods in different application areas. The survey is aimed at XAI researchers, XAI practitioners, AI model developers, and XAI beneficiaries who are interested in enhancing the trustworthiness, transparency, accountability, and fairness of their AI models.}
}
@article{BROESICKE2024103827,
title = {Water consumption in absorption chillers is not negligible: Water-for-cooling consumption of chiller systems for commercial buildings in the United States},
journal = {Sustainable Energy Technologies and Assessments},
volume = {67},
pages = {103827},
year = {2024},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2024.103827},
url = {https://www.sciencedirect.com/science/article/pii/S2213138824002236},
author = {Osvaldo A. Broesicke and Valerie M. Thomas and Emily Grubert and John C. Crittenden},
keywords = {Water consumption, Chiller, Cooling, Distributed energy generation},
abstract = {We compare peak electricity demand and water-for-cooling consumption of two electric chillers –air-cooled and water-cooled– to that of a natural gas-fired heat-driven chiller – an absorption chiller – in the United States. We develop a mass-and-energy-balance model in which each chiller supplies the cooling demands of 16 commercial building types in 15 climate zones of the contiguous US. We quantify the water-for-cooling of each chiller within two categories: (1) ‘cooling and power’ (C&P) – the sum of water consumed directly by each chiller and water consumed at the point of power generation; and (2) ‘total’ – the sum of C&P water consumption and water consumption upstream from the power generation. The air-cooled, water-cooled and absorption chillers consume an average of 2.43, 3.73 ± 0.25, and 3.78 ± 0.35 m3 of C&P water per MWh of cooling, respectively. They consume an average of 9.26, 8.32 ± 0.25, and 3.89 ± 0.34 m3 of total water per MWh of cooling, respectively. That is, life cycle water consumption for natural gas-based absorption chilling is not negligible, though it is lower than for the electricity-based chillers under current grid conditions. Lower power grid life cycle water consumption, e.g., under decarbonization, could change this relationship.}
}
@article{LOGHMANIKHOUZANI2024104052,
title = {Can citizen science in water-related nature-based solutions deliver transformative participation in agri-food systems? A review},
journal = {Agricultural Systems},
volume = {220},
pages = {104052},
year = {2024},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2024.104052},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X24002026},
author = {Taha Loghmani-Khouzani and Victoria Dany and Nadine Seifert and Kaveh Madani and Edeltraud Guenther},
keywords = {Agri-food, Participation, Sustainability, Resilience, Transformation, Behavior},
abstract = {CONTEXT
Highly water-dependent agri-food systems are impacted by external shocks, revealing their vulnerabilities and stressing the need to transform them towards increased sustainability and resilience. Various disciplines and scholars highlight the role of Nature-based Solutions (NbS) in addressing societal challenges while creating sustainable and resilient contexts.
OBJECTIVE
In steering transformative processes, participation is vital as a governance variable. However, motivating stakeholders' engagement with NbS uptake in decision-making requires evidence proving its potential to effectively address their direct and indirect environmental, societal, and economic concerns. This review systematically analyzed the potential of Citizen Science (CS) to overcome the barriers to NbS adoption and to drive stakeholders' attitudes towards sustainability.
METHODS
Focused on water as an essential for the agri-food system, 46 articles were systematically analyzed to examine water-related NbS, locate relevant drivers and barriers of NbS and ecosystem services, including associated advantages and disadvantages.
RESULTS AND CONCLUSIONS
Current research focuses heavily on NbS that benefit people, often overlooking the broader environmental benefits. While a trend towards using NbS for extreme weather events is evident, other critical areas like irrigation, groundwater management, food security, and water sanitation (WASH) need more attention. These elements are vital for sustainable and resilient agri-food systems. The literature identifies three central challenges to implementing NbS: knowledge gaps, participation, and funding. Novel participatory research methods like CS could prove pivotal in addressing NbS adoption barriers. CS in NbS can enhance engagement through improved and informed stakeholder participation while ensuring cost-effective and transparent processes of monitoring and evaluating potential success. Although NbS are gaining traction, scopes and scales of implementation must be more inclusive of various stakeholders and ecological services for the broader environment.
SIGNIFICANCE
CS in NbS can promote sustainable attitudes within the individuals of the society, and by design, NbS provides a sustainable context. Upon proper alignment, CS-NbS can increase the harmony between human and natural systems, shedding light on the Resource Nexus cycle and ultimately causing a visible change in behavior within the engaged stakeholder network. This approach values and amplifies notions of inclusiveness and the incorporation of local knowledge. Living labs and mixed-method research in CS-NbS can initiate inter and transdisciplinarity, collaborative learning, knowledge sharing, and enhanced participation in decision-making while unlocking the transformative capacities of NbS and strengthening the science-policy-society interface.}
}
@article{KENNEDY2024106026,
title = {Asia–Pacific developments},
journal = {Computer Law & Security Review},
volume = {54},
pages = {106026},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106026},
url = {https://www.sciencedirect.com/science/article/pii/S026736492400092X},
author = {Gabriela Kennedy},
abstract = {This column provides a country by country analysis of the latest legal developments, cases and issues relevant to the IT, media and telecommunications' industries in key jurisdictions across the Asia Pacific region. The articles appearing in this column are intended to serve as ‘alerts’ and are not submitted as detailed analyses of cases or legal developments.}
}
@article{WANG2024123586,
title = {Extensive growth of inventions: Evidence from U.S. patenting},
journal = {Technological Forecasting and Social Change},
volume = {207},
pages = {123586},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123586},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524003822},
author = {Jieshu Wang and José Lobo},
keywords = {Invention, Patenting, Growth, Productivity},
abstract = {Despite the seemingly fast development and wide diffusion of technologies in recent decades, concerns have been raised as to whether invention is slowing down. A question has also arisen as to whether the vast accumulation of technical knowledge, instead of speeding up the productivity of subsequent knowledge creation, has, on the contrary, become a “burden of knowledge” that makes it harder to find new ideas. We engage with these concerns by examining nearly 7 million utility patents granted by the U.S. Patent Office and characterizing the growth process of patenting from 1976 to 2018. Although the rate of patenting has steadily increased, patenting productivity as measured as patents per distinct inventor has continuously declined in utility patents in general and for technological frontier fields of biotechnology, climate change mitigation and adaptation, and artificial intelligence. The rapid growth rate of new patents can be credited to an increase in the number of individuals engaged in inventive activity rather than improved productivity. In the U.S., the proportion of the population engaging in patenting has grown significantly. Nevertheless, the growth of the inventive labor force and new patents relies more heavily on experienced inventors than new inventors. As the size of patenting teams keeps growing, the typical inventor participates in a growing number of patents while representing a declining proportion of the inventive labor responsible for patented inventions. We find evidence that as the stock of accumulated patented inventions grows, patenting productivity declines, suggesting that past invention makes it harder for inventors to find new knowledge. In the language of economics, invention (as tracked by patenting) has experienced extensive growth driven by the increase of the inventive labor force with declining productivity and a growing division of labor.}
}
@article{WANG2024127790,
title = {How machine learning boosts the understanding of organic pollutant adsorption on carbonaceous materials: A comprehensive review with statistical insights},
journal = {Separation and Purification Technology},
volume = {350},
pages = {127790},
year = {2024},
issn = {1383-5866},
doi = {https://doi.org/10.1016/j.seppur.2024.127790},
url = {https://www.sciencedirect.com/science/article/pii/S1383586624015296},
author = {Zichu Wang and Qi Wang and Fan Yang and Chunmiao Wang and Min Yang and Jianwei Yu},
keywords = {Adsorption, Intelligence algorithms, Organic contaminants, Carbon-based materials},
abstract = {The application of machine learning (ML) is promising to solve the difficulty of predicting the adsorption of various organic pollutants on carbonaceous materials. This study highlights how ML advances the adsorption research, emphasizes the robust model construction specialized for presenting various application scenarios of adsorption models. We introduce, for the first time, a systematic data preparation workflow tailored for optimizing adsorption studies. Emphasis is given to addressing key challenges in data preparation, including managing adsorption datasets, preventing data leakage, and choosing descriptors wisely. Various algorithms used in 39 previous related studies were included in statistical analysis, and the applications of emerging algorithms in adsorption were prospected. For the data-driven model, the application of importance analysis is beneficial for comprehending adsorption mechanisms, transforming the black-box models into a glass-box ones. It facilitates the identification of primary features governing the adsorption of distinct emerging contaminants and the optimized design of efficient carbonaceous adsorbents. In addition, this review provides prospects for the advanced ML applications in adsorption research, such as its integration with reinforcement learning policies. We also explore the potential of ML in addressing the complexities associated with multi-component adsorption. In sum, this review offers unprecedented illumination into the opportunities and challenges posed by ML in the realm of aqueous adsorption processes.}
}
@article{FAN2024114251,
title = {Blockchain as a trust machine: From disillusionment to enlightenment in the era of generative AI},
journal = {Decision Support Systems},
volume = {182},
pages = {114251},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114251},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624000848},
author = {Shaokun Fan and Noyan Ilk and Akhil Kumar and Ruiyun Xu and J. Leon Zhao},
keywords = {Blockchain, Decentralized finance, Research directions, Trust, Accountability},
abstract = {Since the Economist magazine heralded blockchain as “the trust machine” in 2015, the blockchain paradigm has experienced crests and falls, including a recent phase of disillusionment due to its failure to meet the high expectations, e.g., to revolutionize record keeping, data management, and workflow, envisioned during its early history. However, despite the waning interest in this technology in some quarters, its deployment has become ever more essential in areas such as decentralized finance (DeFi), Non-fungible Tokens (NFTs), and other application domains beyond cryptocurrencies. In particular, recent advancements in Artificial Intelligence (AI) surrounding Large Language Models (LLM) offer new opportunities for blockchain adoption where trust and reliability become critical. As the blockchain technology transitions from a stage of disillusionment to one of enlightenment, anticipation is building for its mainstream adoption, with focused endeavors towards removing adoption barriers across diverse business contexts, exemplified by studies included in this special issue on Blockchain Technology and Applications. In this paper, we first survey the current state of the blockchain technology and then highlight its potential for enhancing trust and accountability in emerging phenomena such as AI generated content (AIGC). We conclude by introducing the papers included in the special issue.}
}
@article{GENDRON2024102759,
title = {On the juggernaut of artificial intelligence in organizations, research and society},
journal = {Critical Perspectives on Accounting},
volume = {100},
pages = {102759},
year = {2024},
issn = {1045-2354},
doi = {https://doi.org/10.1016/j.cpa.2024.102759},
url = {https://www.sciencedirect.com/science/article/pii/S1045235424000583},
author = {Yves Gendron and Jane Andrew and Christine Cooper and Helen Tregidga},
keywords = {Artificial intelligence, Colonization, Dangers, Juggernaut, Research, Trust in science},
abstract = {Capitalizing on what we currently know about artificial intelligence (AI), the editorial of this special issue, entitled “Artificial Intelligence in the Spotlight”, adds our voice to a call to order in the face of the unbridled enthusiasm we often encounter regarding the benefits of AI. In short, we maintain that there is a crucial need for skepticism about the all-out colonization project vigorously pursued by AI and its sustaining infrastructure. We draw on our own analysis and that of the contributors to this special issue to consider what we see as a bold agenda for colonizing our communities, our ways of doing, and our minds – so that we become fundamentally dependent on technologies whose reliability is dubious and whose algorithms are secretly maintained behind the safety of corporate walls. Our thesis is that the cacophony of aberrations, disorder, and worries that emerge in the wake of AI can be meaningfully viewed as a juggernaut, an inexorable force that is ready to unsettle all things in its tedious path. The juggernaut metaphor constitutes our way of putting “artificial intelligence in the spotlight”. We call for researchers from all disciplines to engage in the study of the AI juggernaut and speak out as much as they can, in public and in academic spheres, about its dangers.}
}
@article{VANPOUCKE2024102871,
title = {ChatGPT, the perfect virtual teaching assistant? Ideological bias in learner-chatbot interactions},
journal = {Computers and Composition},
volume = {73},
pages = {102871},
year = {2024},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2024.102871},
url = {https://www.sciencedirect.com/science/article/pii/S8755461524000471},
author = {Margo {Van Poucke}},
keywords = {Appraisal theory, Systemic functional linguistics, ChatGPT, Human-chatbot interaction, Interpersonal metaphors, Ethical considerations, Bias, AI learning tools, Education},
abstract = {This paper examines ChatGPT's use of evaluative language and engagement strategies while addressing information-seeking queries. It assesses the chatbot's role as a virtual teaching assistant (VTA) across various educational settings. By employing Appraisal theory, the analysis contrasts responses generated by ChatGPT and those added by humans, focusing on the interactants’ attitude, deployment of interpersonal metaphors and evaluations of entities, revealing their views on Australian cultural practice. Two datasets were analysed: the first sample (15,909 words) was retrieved from the subreddit r/AskAnAustralian and the second (10,696 words) was obtained by prompting ChatGPT with the same questions. The findings show that, while human experts mainly opt for subjective explicit formulations to express personal viewpoints, the chatbot's preference goes out to incongruent ‘it is’-constructions to share pre-programmed perspectives, which may reflect ideological bias. Even though ChatGPT displays promising socio-communicative capabilities (SCs), its lack of contextual awareness, required to function cross-culturally as a VTA, may lead to considerable ethical issues. The study's novel contribution lies in the in-depth investigation of how the chatbot's SCs and lexicogrammatical selections may impact its role as a VTA, highlighting the need to develop students’ critical digital literacy skills while using AI learning tools.}
}
@article{LIN2024101038,
title = {Exposing image splicing traces in scientific publications via uncertainty-guided refinement},
journal = {Patterns},
volume = {5},
number = {9},
pages = {101038},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101038},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924001806},
author = {Xun Lin and Wenzhong Tang and Haoran Wang and Yizhong Liu and Yakun Ju and Shuai Wang and Zitong Yu},
keywords = {scientific integrity, image splicing detection, convolution neural network, and uncertainty},
abstract = {Summary
Recently, a surge in image manipulations in scientific publications has led to numerous retractions, highlighting the importance of image integrity. Although forensic detectors for image duplication and synthesis have been researched, the detection of image splicing in scientific publications remains largely unexplored. Splicing detection is more challenging than duplication detection due to the lack of reference images and more difficult than synthesis detection because of the presence of smaller tampered-with areas. Moreover, disruptive factors in scientific images, such as artifacts, abnormal patterns, and noise, present misleading features like splicing traces, rendering this task difficult. In addition, the scarcity of high-quality datasets of spliced scientific images has limited advancements. Therefore, we propose the uncertainty-guided refinement network (URN) to mitigate these disruptive factors. We also construct a dataset for image splicing detection (SciSp) with 1,290 spliced images by collecting and manually splicing. Comprehensive experiments demonstrate the URN’s superior splicing detection performance.}
}
@article{SALAHSHOORI2024125513,
title = {Advancements in molecular simulation for understanding pharmaceutical pollutant Adsorption: A State-of-the-Art review},
journal = {Journal of Molecular Liquids},
volume = {410},
pages = {125513},
year = {2024},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2024.125513},
url = {https://www.sciencedirect.com/science/article/pii/S0167732224015708},
author = {Iman Salahshoori and Shahla Mahdavi and Zahra Moradi and Maryam Otadi and Fatemeh {Zare Kazemabadi} and Marcos A.L. Nobre and Hossein {Ali Khonakdar} and Alireza Baghban and Qilin Wang and Amir H. Mohammadi},
keywords = {In Silico study, Molecular dynamics, Monte Carlo simulations, Pharmaceutical pollutants, Quantum mechanics},
abstract = {The contamination of natural water resources by pharmaceutical pollutants has become a significant environmental concern. Traditional experimental approaches for understanding the adsorption behavior of these contaminants on different surfaces are often time-consuming and resource-intensive. In response, this review article explores the powerful combination of in silico techniques, including molecular dynamics (MD), Monte Carlo simulations (MC), and quantum mechanics (QM), as a comprehensive toolset to obtain broad perspectives into the adsorption of pharmaceutical pollutants. By bridging multiple scales, from molecular-level interactions to macroscopic environmental impact, these computational methods offer a holistic understanding of the processes involved. We provide an overview of pharmaceutical pollutants and their ecological effects, emphasizing the need for efficient and sustainable adsorption solutions. Subsequently, we delve into the theoretical foundations of MD, MC, and QM, highlighting their respective strengths in simulating pharmaceutical pollutant adsorption. Moreover, the synergistic potential of combining these methodologies is also discussed for a more comprehensive characterization of adsorption processes. Recent case studies illustrate the successful application of in silico techniques in predicting adsorption behaviors on various surfaces and environmental conditions. Finally, the environmental implications of pharmaceutical pollutant adsorption are discussed, along with how in silico modelling can guide sustainable solutions for mitigating their impact.}
}
@article{ROSIC2024451,
title = {Legal implications of artificial intelligence in health care},
journal = {Clinics in Dermatology},
volume = {42},
number = {5},
pages = {451-459},
year = {2024},
note = {Artificial Intelligence II},
issn = {0738-081X},
doi = {https://doi.org/10.1016/j.clindermatol.2024.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S0738081X24000981},
author = {Ana Rosic},
abstract = {The last few years have seen a boom in the popularity of artificial intelligence (AI) around the world, and the health care sector has not been immune from what has been perceived by some as a revolutionary technology. Although AI has been around for many years, including in the field of health care, the recent introduction of consumer-facing generative AI tools has put a spotlight on the technology that has drawn attention from governments, corporations, consumers and more. Health care systems, physician groups, health insurance companies, and others in the space have shown an eagerness to explore AI's potential to improve various aspects of health care, but new legal risks and challenges are unfolding every day. This contribution looks at the latest health care-related measures in the United States and international legal and regulatory landscapes, as well as data privacy implications and discrimination concerns coming out of AI-enabled solutions. It also discusses concerns that health care systems and physicians alike are monitoring, including the potential for medical errors resulting from AI, liability considerations, and malpractice insurance trends.}
}
@article{SALAHSHOORI2024125592,
title = {Navigating the molecular landscape of environmental science and heavy metal removal: A simulation-based approach},
journal = {Journal of Molecular Liquids},
volume = {410},
pages = {125592},
year = {2024},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2024.125592},
url = {https://www.sciencedirect.com/science/article/pii/S0167732224016519},
author = {Iman Salahshoori and Marcos A.L. Nobre and Amirhosein Yazdanbakhsh and Rahime {Eshaghi Malekshah} and Morteza Asghari and Hossein {Ali Khonakdar} and Amir H. Mohammadi},
keywords = {Computational methods, Environmental pollutants, Heavy metals removal, Molecular simulations, wastewater treatments},
abstract = {Heavy metals pose a significant threat to ecosystems and human health because of their toxic properties and their ability to bioaccumulate in living organisms. Traditional removal methods often fall short in terms of cost, energy efficiency, and minimizing secondary pollutant generation, especially in complex environmental settings. In contrast, molecular simulation methods offer a promising solution by providing in-depth insights into atomic and molecular interactions between heavy metals and potential adsorbents. This review highlights the potential of molecular simulation methods for removing types of pollutants in environmental science, specifically heavy metals. These methods offer a powerful tool for predicting and designing materials and processes for environmental remediation. We focus on removing specific heavy metals like lead, Cadmium, and mercury, utilizing cutting-edge simulation techniques such as Molecular Dynamics (MD), Monte Carlo (MC) simulations, Quantum Chemical Calculations (QCC), and Artificial Intelligence (AI). By leveraging these methods, we aim to develop highly efficient and selective materials and processes for environmental remediation. By unravelling the underlying mechanisms, these techniques pave the way for developing more efficient and selective removal technologies. This comprehensive review addresses a critical gap in the scientific literature, providing valuable insights for researchers in environmental protection and human health. Molecular modelling methods hold significant promise for revolutionizing the prediction and removal of heavy metals, ultimately contributing to sustainable solutions for a cleaner and healthier future.}
}
@article{MOORHOUSE2024103399,
title = {Developing language teachers’ professional generative AI competence: An intervention study in an initial language teacher education course},
journal = {System},
volume = {125},
pages = {103399},
year = {2024},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2024.103399},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X24001817},
author = {Benjamin Luke Moorhouse and Yuwei Wan and Chenze Wu and Lucas Kohnke and Tsz Ying Ho and Theresa Kwong},
abstract = {Generative Artificial Intelligence (GenAI) tools have been argued to have transformative potential in education; yet existing literature suggests that language teachers generally lack the abilities to leverage these tools effectively and critically. Conducted in an initial language teacher education programme at a Hong Kong university, this mixed-method intervention study aims to explore the effects of explicit training for using GenAI tools for language teaching in rising pre-service language teachers’ professional GenAI competence (P-GenAI-C). 54 M.Ed students took part in an 11-week course intervention aiming to enhance the five aspects in the P-GenAI-C framework. Analysis of pre- and post-intervention questionnaires, which encompassed a mix of open and closed items to gather participants’ knowledge and perceptions of utilising GenAI tools, as well as the follow-up interviews, revealed that the intervention was effective in stretching all aspects of pre-service teachers’ P-GenAI-C. While there was greater evidence of improvement in participants’ pedagogical competence and critical awareness of GenAI tools deployment, there was less evidence of development in other aspects, such as teachers’ capacity to guide their students to use GenAI tools effectively and responsibly. This discrepancy might be attributed to the lack of such content in the course intervention. Implications for incorporating elements of P-GenAI-C into teacher preparation courses and programmes are discussed.}
}
@article{RAVI2024100279,
title = {RICo: Reddit ideological communities},
journal = {Online Social Networks and Media},
volume = {42},
pages = {100279},
year = {2024},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2024.100279},
url = {https://www.sciencedirect.com/science/article/pii/S2468696424000041},
author = {Kamalakkannan Ravi and Adan Ernesto Vela},
keywords = {Social networking (online), Learning (artificial intelligence), Predictive models, Transformers, Support vector machines, Text analysis, Context modeling, Natural language processing},
abstract = {The main objective of our research is to gain a comprehensive understanding of the relationship between language usage within different communities and delineating the ideological narratives. We focus specifically on utilizing Natural Language Processing techniques to identify underlying narratives in the coded or suggestive language employed by non-normative communities associated with targeted violence. Earlier studies addressed the detection of ideological affiliation through surveys, user studies, and a limited number based on the content of text articles, which still require label curation. Previous work addressed label curation by using ideological subreddits (r/Liberal and r/Conservative for Liberal and Conservative classes) to label the articles shared on those subreddits according to their prescribed ideologies, albeit with a limited dataset. Building upon previous work, we use subreddit ideologies to categorize shared articles. In addition to the conservative and liberal classes, we introduce a new category called “Restricted” which encompasses text articles shared in subreddits that are restricted, privatized, or banned, such as r/TheDonald. The “Restricted” class encompasses posts tied to violence, regardless of conservative or liberal affiliations. Additionally, we augment our dataset with text articles from self-identified subreddits like r/progressive and r/askaconservative for the liberal and conservative classes, respectively. This results in an expanded dataset of 377,144 text articles, consisting of 72,488 liberal, 79,573 conservative, and 225,083 restricted class articles. Our goal is to analyze language variances in different ideological communities, investigate keyword relevance in labeling article orientations, especially in unseen cases (922,522 text articles), and delve into radicalized communities, conducting thorough analysis and interpretation of the results.}
}
@article{RANDO2024105006,
title = {Methyl Red-loaded halloysite nanotubes-based silica coatings for durable dyeing of polyester fabrics},
journal = {Surfaces and Interfaces},
volume = {53},
pages = {105006},
year = {2024},
issn = {2468-0230},
doi = {https://doi.org/10.1016/j.surfin.2024.105006},
url = {https://www.sciencedirect.com/science/article/pii/S2468023024011623},
author = {Giulia Rando and Silvia Sfameni and Mariam Hadhri and Alessio Mezzi and Marco Brucale and Giovanna {De Luca} and Elpida Piperopoulos and Candida Milone and Dario Drommi and Giuseppe Rosace and Valentina Trovato and Maria Rosaria Plutino},
keywords = {Halloysite, Methyl red, Polyester fabrics, Sol-gel, (3-glycidyloxypropyl)trimethoxysilane, Dyeing coating},
abstract = {Since unmodified polyester fibres have no reactive groups like those in cellulose and protein fibres, they do not show an affinity for water-soluble acid, basic and direct dyestuffs. Only disperse dyestuff, a non-ionic dyestuff class with low molar mass molecules, proved to be useful for dyeing this man-made fibre following a solid–solid interaction; disperse dyestuffs do not form primary chemical bonds with polymer chains rather the dye colour is retained by H-bonds and Van der Waals forces. Herein, a new strategy for dyeing polyester fabrics with a direct dyestuff in a two-step strategy was designed and realized, using an organic–inorganic composite coating based on methyl red-loaded sol-gel modified halloysite nanotubes. In the first step two distinct reaction methods were compared to functionalize halloysite nanotubes with (3-Glycidyloxypropyl)trimethoxysilane, as a covalent crosslinker between the halloysite nanotubes and fibres, (i) in water and (ii) in ethanol, using BF3O(C2H5) and chloridric acid (HCl) as catalysts, respectively. In the second step, methyl red loaded GPTMS modified halloysite sols were applied onto polyester fabrics by impregnation. The amount of methyl red dyestuff was evaluated to be superior for the complex realized in ethanol than in water, thus promoting homogeneous nanocomposite coatings on treated polyester samples. Methyl red loaded sol-gel modified halloysite complex, as well as treated and untreated samples, were investigated to characterize their properties and morphology. NMR investigation confirmed the structure of the new complex, validating the successful dyestuff coordination reaction at GPTMS. The influence of treatments on the morphology of fibres surfaces was demonstrated by Scanning Electron Microscopy (SEM), Energy Dispersive X-ray spectroscopy (EDX), and Atomic Force Microscopy (AFM) analyses, highlighting the influence of GPTMS-based composites on the microstructure of functionalized polyester fibres. To further confirm if the suggested approach offers a stable dyestuff loading on PE, diffuse reflectance spectroscopic studies, X-ray Photoelectron Spectroscopy (XPS) and colour fastness to rubbing and washing tests were carried out on the coated polyester. All findings make sol-gel based modification of halloysite a reliable and promising method for eco-friendly dyeing processes of polyester fabrics.}
}
@article{ALKAEED2024103989,
title = {Privacy preservation in Artificial Intelligence and Extended Reality (AI-XR) metaverses: A survey},
journal = {Journal of Network and Computer Applications},
volume = {231},
pages = {103989},
year = {2024},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2024.103989},
url = {https://www.sciencedirect.com/science/article/pii/S1084804524001668},
author = {Mahdi Alkaeed and Adnan Qayyum and Junaid Qadir},
keywords = {Machine learning, Metaverse, Artificial Intelligence, Virtual Reality, Extended Reality, Mixed reality, Homomorphic encryption, Federated learning},
abstract = {The metaverse is a nascent concept that envisions a virtual universe, a collaborative space where individuals can interact, create, and participate in a wide range of activities. Privacy in the metaverse is a critical concern as the concept evolves and immersive virtual experiences become more prevalent. The metaverse privacy problem refers to the challenges and concerns surrounding the privacy of personal information and data within Virtual Reality (VR) environments as the concept of a shared VR space becomes more accessible. Metaverse will harness advancements from various technologies such as Artificial Intelligence (AI), Extended Reality (XR) and Mixed Reality (MR) to provide personalized and immersive services to its users. Moreover, to enable more personalized experiences, the metaverse relies on the collection of fine-grained user data that leads to various privacy issues. Therefore, before the potential of the metaverse can be fully realized, privacy concerns related to personal information and data within VR environments must be addressed. This includes safeguarding users’ control over their data, ensuring the security of their personal information, and protecting in-world actions and interactions from unauthorized sharing. In this paper, we explore various privacy challenges that future metaverses are expected to face, given their reliance on AI for tracking users, creating XR and MR experiences, and facilitating interactions. Moreover, we thoroughly analyze technical solutions such as differential privacy, Homomorphic Encryption, and Federated Learning and discuss related sociotechnical issues regarding privacy.}
}
@article{LEE2024105138,
title = {Localization of diffusion model-based inpainting through the inter-intra similarity of frequency features},
journal = {Image and Vision Computing},
volume = {148},
pages = {105138},
year = {2024},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2024.105138},
url = {https://www.sciencedirect.com/science/article/pii/S0262885624002427},
author = {Seung-Lee Lee and Minjae Kang and Jong-Uk Hou},
keywords = {Image forensics, Image inpainting, Generative model, Diffusion model, Localization},
abstract = {Recently, the enhanced abilities of diffusion models have led to more realistic inpainting results, which raises the potential for criminal activity through image forgery. In this study, we explore the detection of inpainted images generated by a diffusion model. We propose a method for inpainting localization using an inter-intra similarity (IIS) module based on image frequency features. The proposed IIS module learns the inter-patch relationship through the learnable frequency filter and subsequently covers the intra-patch relationship through the self-similarity operation. We provide the Diffusion Model Inpainting Dataset (DMID), a benchmark dataset comprising inpainted images using four different diffusion models and three types of masks. Additionally, a test dataset that includes three sampling steps is provided. We validated the effectiveness of our proposed approach by conducting comparative tests with existing forgery detectors using our dataset and testing the robustness of JPEG compression. Additionally, we tested our proposed method on datasets with different sampling step sizes. Our work provides a starting point for research on the detection of inpainting-based forgery using diffusion models. Additionally, by openly releasing the dataset, we offer an opportunity to advance future in-depth research related to forensics.}
}
@article{OPREA20243827,
title = {Detecting Malicious Uniform Resource Locators Using an Applied Intelligence Framework},
journal = {Computers, Materials and Continua},
volume = {79},
number = {3},
pages = {3827-3853},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.051598},
url = {https://www.sciencedirect.com/science/article/pii/S154622182400078X},
author = {Simona-Vasilica Oprea and Adela Bâra},
keywords = {Detecting malicious URL, classifiers, text to feature, deep learning, ranking algorithms, feature building time},
abstract = {The potential of text analytics is revealed by Machine Learning (ML) and Natural Language Processing (NLP) techniques. In this paper, we propose an NLP framework that is applied to multiple datasets to detect malicious Uniform Resource Locators (URLs). Three categories of features, both ML and Deep Learning (DL) algorithms and a ranking schema are included in the proposed framework. We apply frequency and prediction-based embeddings, such as hash vectorizer, Term Frequency-Inverse Dense Frequency (TF-IDF) and predictors, word to vector-word2vec (continuous bag of words, skip-gram) from Google, to extract features from text. Further, we apply more state-of-the-art methods to create vectorized features, such as GloVe. Additionally, feature engineering that is specific to URL structure is deployed to detect scams and other threats. For framework assessment, four ranking indicators are weighted: computational time and performance as accuracy, F1 score and type error II. For the computational time, we propose a new metric-Feature Building Time (FBT) as the cutting-edge feature builders (like doc2vec or GloVe) require more time. By applying the proposed assessment step, the skip-gram algorithm of word2vec surpasses other feature builders in performance. Additionally, eXtreme Gradient Boost (XGB) outperforms other classifiers. With this setup, we attain an accuracy of 99.5% and an F1 score of 0.99.}
}
@article{ALENIZI20242463,
title = {A Review of Image Steganography Based on Multiple Hashing Algorithm},
journal = {Computers, Materials and Continua},
volume = {80},
number = {2},
pages = {2463-2494},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.051826},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824005848},
author = {Abdullah Alenizi and Mohammad Sajid Mohammadi and Ahmad A. Al-Hajji and Arshiya Sajid Ansari},
keywords = {Image steganography, multiple hashing algorithms, Hash-LSB approach, RSA algorithm, discrete cosine transform (DCT) algorithm, blowfish algorithm},
abstract = {Steganography is a technique for hiding secret messages while sending and receiving communications through a cover item. From ancient times to the present, the security of secret or vital information has always been a significant problem. The development of secure communication methods that keep recipient-only data transmissions secret has always been an area of interest. Therefore, several approaches, including steganography, have been developed by researchers over time to enable safe data transit. In this review, we have discussed image steganography based on Discrete Cosine Transform (DCT) algorithm, etc. We have also discussed image steganography based on multiple hashing algorithms like the Rivest–Shamir–Adleman (RSA) method, the Blowfish technique, and the hash-least significant bit (LSB) approach. In this review, a novel method of hiding information in images has been developed with minimal variance in image bits, making our method secure and effective. A cryptography mechanism was also used in this strategy. Before encoding the data and embedding it into a carry image, this review verifies that it has been encrypted. Usually, embedded text in photos conveys crucial signals about the content. This review employs hash table encryption on the message before hiding it within the picture to provide a more secure method of data transport. If the message is ever intercepted by a third party, there are several ways to stop this operation. A second level of security process implementation involves encrypting and decrypting steganography images using different hashing algorithms.}
}
@article{ABBAS2024124260,
title = {Unmasking deepfakes: A systematic review of deepfake detection and generation techniques using artificial intelligence},
journal = {Expert Systems with Applications},
volume = {252},
pages = {124260},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124260},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424011266},
author = {Fakhar Abbas and Araz Taeihagh},
keywords = {Deep learning, Deepfakes, Detection and generation, Artificial Intelligence (AI), Policy recommendations, Literature review},
abstract = {Due to the fast spread of data through digital media, individuals and societies must assess the reliability of information. Deepfakes are not a novel idea but they are now a widespread phenomenon. The impact of deepfakes and disinformation can range from infuriating individuals to affecting and misleading entire societies and even nations. There are several ways to detect and generate deepfakes online. By conducting a systematic literature analysis, in this study we explore automatic key detection and generation methods, frameworks, algorithms, and tools for identifying deepfakes (audio, images, and videos), and how these approaches can be employed within different situations to counter the spread of deepfakes and the generation of disinformation. Moreover, we explore state-of-the-art frameworks related to deepfakes to understand how emerging machine learning and deep learning approaches affect online disinformation. We also highlight practical challenges and trends in implementing policies to counter deepfakes. Finally, we provide policy recommendations based on analyzing how emerging artificial intelligence (AI) techniques can be employed to detect and generate deepfakes online. This study benefits the community and readers by providing a better understanding of recent developments in deepfake detection and generation frameworks. The study also sheds a light on the potential of AI in relation to deepfakes.}
}
@article{PHALAAGAE2024e02287,
title = {An energy efficient authentication scheme for cluster-based wireless IoT sensor networks},
journal = {Scientific African},
volume = {25},
pages = {e02287},
year = {2024},
issn = {2468-2276},
doi = {https://doi.org/10.1016/j.sciaf.2024.e02287},
url = {https://www.sciencedirect.com/science/article/pii/S2468227624002321},
author = {Pendukeni Phalaagae and Adamu Murtala Zungeru and Boyce Sigweni and Selvaraj Rajalakshmi and Herbet Batte and Odongo S. Eyobu},
keywords = {Bi-phase authentication scheme, Digital watermarking, Energy adaptive clustering hierarchy, Internet of Things, Randomized Bi-phase authentication scheme, Wireless Internet of Things sensor network},
abstract = {Wireless Internet of Things (IoT) sensor networks (WITSN) play a pivotal role in modern society, facilitating a myriad of applications ranging from smart homes to industrial automation. Therefore, safeguarding these networks against security threats is paramount to ensure the integrity, confidentiality, and availability of data transmitted within them. However, WITSNs face escalating security threats due to their diverse structures and platforms. Existing literature has identified these vulnerabilities but lacks comprehensive solutions to address them effectively. To bridge this gap, this paper proposes a novel security approach termed Randomized Bi-Phase Authentication Scheme (RBAS), which integrates digital watermarking techniques to fortify both external and internal network security. RBAS not only tackles data availability, confidentiality, and authenticity challenges prevalent in WITSNs but also strives to maintain a delicate equilibrium between robust security measures and energy efficiency. Key contributions of this work include the meticulous examination and validation of the integration of cyclic redundancy check (CRC) codes in IoT sensor network authentication, demonstrating their efficacy in error detection through simulations. Furthermore, RBAS employs advanced hashing, cryptography, and dynamic verification codes to ensure strong error detection and data authenticity, leveraging robust CRC codes and randomization to thwart potential attacks. The scheme's complexity acts as a deterrent against manipulation, while its cluster awareness enhances adaptability, and cryptographic principles bolster overall security. Extensive performance evaluation using the Network Simulator-2 (NS2) reveals significant benefits of RBAS, including an 8 % reduction in power consumption and a 7 % increase in network longevity. Notably, RBAS surpasses existing solutions with a 14 % reduction in dropping ratio and an 8 % decrease in latency. The success of RBAS stems from its innovative utilization of lightweight watermarking techniques and cluster-based routing, enabling proactive identification of data tampering and thereby enhancing the network's overall security posture. This pioneering work not only advances the state of the art in WITSN security but also holds profound implications for the practical deployment of secure and efficient IoT sensor networks in real-world scenarios.}
}
@article{ZHANG2024105632,
title = {Knowledge management for off-site construction},
journal = {Automation in Construction},
volume = {166},
pages = {105632},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105632},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524003686},
author = {Zhen Zhang and Yang Zou and Brian H.W. Guo and Johannes Dimyadi and Roy Davies and Lixin Jiang},
keywords = {Off-site construction (OSC), Knowledge management (KM), Artificial intelligence (AI), Systematic literature review},
abstract = {Off-site construction (OSC) is expected to boost productivity, shorten construction time, and reduce labour and material wastage. Despite these benefits, most OSC projects have not fully achieved these advantages, where a primary obstacle lies in the limited management of OSC knowledge. However, there is still no holistic understanding of the integration of KM in the OSC context. Therefore, this paper explores the latest development in KM for OSC through a systematic literature review using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and template analysis. The review is based on 66 screened and assessed journal articles from all years to 2024 with a particular focus on KM and OSC. Through the quantitative and qualitative analysis, this study groups four main research themes including KM for OSC design, KM for OSC project management, knowledge-based OSC decision-making, and the management of OSC knowledge. The results are discussed to gain a systematic understanding of key OSC knowledge domains, investigate the integration of KM for OSC, and explore future research needs including emerging artificial intelligence (AI) technologies.}
}
@article{PANTANOWITZ2024102095,
title = {Synthetic Data and Its Utility in Pathology and Laboratory Medicine},
journal = {Laboratory Investigation},
volume = {104},
number = {8},
pages = {102095},
year = {2024},
issn = {0023-6837},
doi = {https://doi.org/10.1016/j.labinv.2024.102095},
url = {https://www.sciencedirect.com/science/article/pii/S0023683724017732},
author = {Joshua Pantanowitz and Christopher D. Manko and Liron Pantanowitz and Hooman H. Rashidi},
keywords = {artificial intelligence, data simulation, generative AI, laboratory medicine, machine learning models, pathology artificial intelligence, pathology education, synthetic data},
abstract = {In our rapidly expanding landscape of artificial intelligence, synthetic data have become a topic of great promise and also some concern. This review aimed to provide pathologists and laboratory professionals with a primer on the role of synthetic data and how it may soon shape the landscape within our field. Using synthetic data presents many advantages but also introduces a milieu of new obstacles and limitations. This review aimed to provide pathologists and laboratory professionals with a primer on the general concept of synthetic data and its potential to transform our field. By leveraging synthetic data, we can help accelerate the development of various machine learning models and enhance our medical education and research/quality study needs. This review explored the methods for generating synthetic data, including rule-based, machine learning model-based and hybrid approaches, as they apply to applications within pathology and laboratory medicine. We also discussed the limitations and challenges associated with such synthetic data, including data quality, malicious use, and ethical bias/concerns and challenges. By understanding the potential benefits (ie, medical education, training artificial intelligence programs, and proficiency testing, etc) and limitations of this new data realm, we can not only harness its power to improve patient outcomes, advance research, and enhance the practice of pathology but also become readily aware of their intrinsic limitations.}
}
@article{BARRIENTOSESPILLCO2024111849,
title = {Integration of object detection and semantic segmentation based on convolutional neural networks for navigation and monitoring of cyanobacterial blooms in lentic water scenes},
journal = {Applied Soft Computing},
volume = {163},
pages = {111849},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111849},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624006239},
author = {Fredy Barrientos-Espillco and María J. Gómez-Silva and Eva Besada-Portas and Gonzalo Pajares},
keywords = {Dual-task, Object detection, Semantic segmentation, Convolutional Neural Networks, Autonomous surface vehicles, Lentic waters, Cyanobacterial blooms},
abstract = {Lentic waters, such as lakes, lagoons, reservoirs, and wetlands are characterized by their absence of current. In recent decades, they have been threatened by pollution and scarcity due to various environmental factors. Therefore, they require frequent monitoring to ensure their health and purity, especially to control the proliferation of harmful cyanobacteria (pollutants). Machine Vision Systems (MVS) on board Autonomous Surface Vehicles (ASVs) is a good option for automatic image processing in this context. ASVs must navigate safely, and obstacle detection is essential. In addition, the segmentation of pollutants in water is crucial. We propose an architecture based on convolutional neural networks that integrates both object detection and semantic segmentation. The goal is to simultaneously extract all available global information to detect objects and amorphous textures (cyanobacterial patches and water bodies), considering their variations in size, pose, and appearance. The architecture includes two branches: object detection and semantic segmentation, sharing the same backbone and neck. We evaluate the model on our dataset and the results show that it can holistically understand lentic water scenes with high accuracy, and the integration of the attention mechanism improves its overall performance.}
}
@article{RAJABI2024100090,
title = {Unleashing ChatGPT's impact in higher education: Student and faculty perspectives},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {2},
pages = {100090},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100090},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000501},
author = {Parsa Rajabi and Parnian Taghipour and Diana Cukierman and Tenzin Doleck},
keywords = {ChatGPT, Conversational AI, Artificial intelligence in education, Post-secondary, Higher education, Assessment},
abstract = {As Chat Generative Pre-trained Transformer (ChatGPT) gains traction, its impact on post-secondary education is increasingly being debated. This qualitative study explores the perception of students and faculty members at a research university in Canada regarding ChatGPT's use in a post-secondary setting, focusing on how it could be incorporated and what ways instructors can respond to this technology. We present the summary of a discussion that took place in a 2-hour focus group session with 40 participants from the computer science and engineering departments, and highlight issues surrounding plagiarism, assessment methods, and the appropriate use of ChatGPT. Findings suggest that students are likely to use ChatGPT, but there is a need for specific guidelines, more classroom assessments, and mandatory reporting of ChatGPT use. The study contributes to the emergent research on ChatGPT in higher education and emphasizes the importance of proactively addressing challenges and opportunities associated with ChatGPT adoption and use. The novelty of the study involves capturing the perspectives of students and faculty members. This paper aims to provide a more refined understanding of the complex interplay between AI chatbots and higher education that will help educators navigate the rapidly evolving landscape of AI-driven education.}
}
@article{LU2024102244,
title = {Price of going green: The employment effects of the environmental protection tax in China},
journal = {China Economic Review},
volume = {87},
pages = {102244},
year = {2024},
issn = {1043-951X},
doi = {https://doi.org/10.1016/j.chieco.2024.102244},
url = {https://www.sciencedirect.com/science/article/pii/S1043951X24001330},
author = {Shuling Lu and Qijing Yang},
keywords = {Environmental protection tax, Employment effects, Manufacturing company, China},
abstract = {Compared with command-and-control regulations, it is less known about the labor market consequences of environmental taxes. This study examines the employment impact of the 2018 Environmental Protection Tax (EPT). Applying a triple-difference framework, we empirically establish the employment-suppressing consequence of EPT, which is primarily attributable to output reductions and green technological advances. Moreover, our analysis highlights a size-dependent strategy adopted by companies to navigate the escalating environmental costs: while small companies opt for production downsizing, larger counterparts tend to invest more in technical abatement initiatives. Heterogeneity analysis reveals that the unemployment effect is more pronounced in companies facing higher financial constraints and greater public environmental attention, with low-skilled workers bearing the brunt, albeit without significant wage inequality. Further, we find that government green subsidies can mute this job-reduction effect. Our study illuminates an unintended incidence of environmental policy costs on labor in China and underscores comprehensive policy evaluation.}
}
@article{TEO2024100091,
title = {Cybersecurity in the generative artificial intelligence era},
journal = {Asia-Pacific Journal of Ophthalmology},
volume = {13},
number = {4},
pages = {100091},
year = {2024},
issn = {2162-0989},
doi = {https://doi.org/10.1016/j.apjo.2024.100091},
url = {https://www.sciencedirect.com/science/article/pii/S2162098924000926},
author = {Zhen Ling Teo and Chrystie Wan Ning Quek and Joy Le Yi Wong and Daniel Shu Wei Ting},
keywords = {Generative Artificial Intelligence, ChatGPT, Cybersecurity, Privacy risks, Large language model},
abstract = {Generative Artificial Intelligence (GenAI) are algorithms capable of generating original content. The ability of GenAI to learn and generate novel outputs alike human cognition has taken the world by storm and ushered in a new era. In this review, we explore the role of GenAI in healthcare, including clinical, operational, and research applications, and delve into the cybersecurity risks of this technology. We discuss risks such as data privacy risks, data poisoning attacks, the propagation of bias, and hallucinations. In this review, we recommend risk mitigation strategies to enhance cybersecurity in GenAI technologies and further explore the use of GenAI as a tool in itself to enhance cybersecurity across the various AI algorithms. GenAI is emerging as a pivotal catalyst across various industries including the healthcare domain. Comprehending the intricacies of this technology and its potential risks will be imperative for us to fully capitalise on the benefits that GenAI can bring.}
}
@article{YANG2024120893,
title = {Defending against similarity shift attack for EaaS via adaptive multi-target watermarking},
journal = {Information Sciences},
volume = {678},
pages = {120893},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120893},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524008077},
author = {Zuopeng Yang and Pengyu Chen and Tao Li and Kangjun Liu and Yuan Huang and Xin Lin},
keywords = {EaaS, Similarity shift, Embedding watermarking, Adaptive multi-target watermarking, Copyright protection},
abstract = {Large language models have revolutionized natural language processing, leading to the emergence of Embedding as a Service (EaaS). While EaaS facilitates access to advanced embedding models, it also presents challenges in copyright protection. Current research primarily relies on single-target watermarking frameworks, where a predefined vector is integrated as a watermark into text embeddings. However, these approaches are vulnerable to watermark information leakage. To investigate this issue, we introduce the Embedding Similarity Shift Attack (ESSA), an innovative attack algorithm designed to detect trigger instances in single-target watermarking systems by analyzing similarity shifts among constructed reference sentence pairs. Additionally, to defend against such an attack, we propose Adaptive Multi-Target Watermarking (AMT-WM). AMT-WM stands as the pioneering multi-target watermarking method aimed at safeguarding the copyright of EaaS. Specifically, AMT-WM constructs multiple watermarks through the utilization of orthogonal vectors to mitigate selection bias towards a particular vector. Furthermore, it incorporates a randomly selected sentence embedding as the base embedding to enhance the confidentiality of backdoored embeddings. For multi-target watermarking, we implement adaptive watermark injection and validation based on similarity. Comprehensive experiments conducted on various datasets validate the effectiveness of ESSA in trigger detection performance and the efficacy of AMT-WM in copyright protection. Our code will be available soon.}
}
@article{KRISHNAPRIYA20242675,
title = {A Comprehensive Survey on Advanced Persistent Threat (APT) Detection Techniques},
journal = {Computers, Materials and Continua},
volume = {80},
number = {2},
pages = {2675-2719},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.052447},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824005952},
author = {Singamaneni Krishnapriya and Sukhvinder Singh},
keywords = {Advanced persistent threats, APT, cyber security, intrusion detection, cyber attacks},
abstract = {The increase in number of people using the Internet leads to increased cyberattack opportunities. Advanced Persistent Threats, or APTs, are among the most dangerous targeted cyberattacks. APT attacks utilize various advanced tools and techniques for attacking targets with specific goals. Even countries with advanced technologies, like the US, Russia, the UK, and India, are susceptible to this targeted attack. APT is a sophisticated attack that involves multiple stages and specific strategies. Besides, TTP (Tools, Techniques, and Procedures) involved in the APT attack are commonly new and developed by an attacker to evade the security system. However, APTs are generally implemented in multiple stages. If one of the stages is detected, we may apply a defense mechanism for subsequent stages, leading to the entire APT attack failure. The detection at the early stage of APT and the prediction of the next step in the APT kill chain are ongoing challenges. This survey paper will provide knowledge about APT attacks and their essential steps. This follows the case study of known APT attacks, which will give clear information about the APT attack process—in later sections, highlighting the various detection methods defined by different researchers along with the limitations of the work. Data used in this article comes from the various annual reports published by security experts and blogs and information released by the enterprise networks targeted by the attack.}
}
@article{2024209,
title = {Guide for authors},
journal = {Intelligent Medicine},
volume = {4},
number = {3},
pages = {209-214},
year = {2024},
issn = {2667-1026},
doi = {https://doi.org/10.1016/S2667-1026(24)00049-4},
url = {https://www.sciencedirect.com/science/article/pii/S2667102624000494}
}
@article{JIN2024113086,
title = {Big data, machine learning, and digital twin assisted additive manufacturing: A review},
journal = {Materials & Design},
volume = {244},
pages = {113086},
year = {2024},
issn = {0264-1275},
doi = {https://doi.org/10.1016/j.matdes.2024.113086},
url = {https://www.sciencedirect.com/science/article/pii/S026412752400460X},
author = {Liuchao Jin and Xiaoya Zhai and Kang Wang and Kang Zhang and Dazhong Wu and Aamer Nazir and Jingchao Jiang and Wei-Hsin Liao},
keywords = {Additive manufacturing, Big data, Machine learning, Digital twin, Data-driven},
abstract = {Additive manufacturing (AM) has undergone significant development over the past decades, resulting in vast amounts of data that carry valuable information. Numerous research studies have been conducted to extract insights from AM data and utilize it for optimizing various aspects such as the manufacturing process, supply chain, and real-time monitoring. Data integration into proposed digital twin frameworks and the application of machine learning techniques is expected to play pivotal roles in advancing AM in the future. In this paper, we provide an overview of machine learning and digital twin-assisted AM. On one hand, we discuss the research domain and highlight the machine-learning methods utilized in this field, including material analysis, design optimization, process parameter optimization, defect detection and monitoring, and sustainability. On the other hand, we examine the status of digital twin-assisted AM from the current research status to the technical approach and offer insights into future developments and perspectives in this area. This review paper aims to examine present research and development in the convergence of big data, machine learning, and digital twin-assisted AM. Although there are numerous review papers on machine learning for additive manufacturing and others on digital twins for AM, no existing paper has considered how these concepts are intrinsically connected and interrelated. Our paper is the first to integrate the three concepts big data, machine learning, and digital twins and propose a cohesive framework for how they can work together to improve the efficiency, accuracy, and sustainability of AM processes. By exploring latest advancements and applications within these domains, our objective is to emphasize the potential advantages and future possibilities associated with integration of these technologies in AM.}
}
@article{SIOTTO202459,
title = {Digital methods and techniques for reconstructing and visualizing ancient 3D polychromy – An overview},
journal = {Journal of Cultural Heritage},
volume = {68},
pages = {59-85},
year = {2024},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2024.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1296207424001006},
author = {Eliana Siotto and Paolo Cignoni},
keywords = {Colour, Literature survey, 2D/3D computer graphics, VR/AR/mix reality, Artificial Intelligence},
abstract = {The digital technologies employed in archaeology since the 1990s have progressively and experimentally been utilized over the last two decades to document and re-present the ancient polychromy of Greek and Roman marble artworks. Given that this remains a developing field of investigation and application, this study offers, for the first time, a systematic review of the endeavours undertaken thus far in implementing information technology for the documentation, analysis, reconstruction, visualization, and presentation of ancient polychromy. This overview is supported by a literature review and existing implementations, organized into methods and techniques employed for 3D colour preservation, analysis, and reconstruction, as well as those used for the visualization and dissemination of findings. The goal is to identify gaps and provide intriguing insights for future research concerning the use of digital technologies as an essential tool in the stages of documenting and disseminating ancient polychromy in architecture and archaeological artefacts. This, in turn, aims to encourage data sharing, contribute to the dissemination of science-based knowledge and resolve substantial barriers associated with the long-term retention of digital data.}
}
@article{XU2024102578,
title = {Detecting Artificial Intelligence-Generated images via deep trace representations and interactive feature fusion},
journal = {Information Fusion},
volume = {112},
pages = {102578},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102578},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524003567},
author = {Qiang Xu and Xinghao Jiang and Tanfeng Sun and Hao Wang and Laijin Meng and Hong Yan},
keywords = {Artificial Intelligence-Generated (AIG), Natural photographs, Global feature, Multi-scale feature, Feature fusion},
abstract = {The detection of Artificial Intelligence-Generated (AIG) images plays an important role in verifying the authenticity and originality of digital images. However, recent advancements in state-of-the-art image generation methods have significantly challenged the ability to differentiate AIG images from natural photographs (NP). To address this issue, a novel approach based on deep trace representations and dual-branch interactive feature fusion is presented. Firstly, a global feature extraction module that leverages attention-based MobileViT (AT-MobileViT) is designed to learn the deep representations of the global trace information. Besides, we apply multiple enhanced residual blocks to extract discriminative multi-scale features. After that, a low-level feature extraction module incorporating a channel-spatial attention (CSA) block is also carefully employed to enhance the learning of trace representations. To facilitate the capture of complementary information between features, a dual-branch interactive feature fusion module is introduced by reshaping feature vectors into interactive matrices. By conducting experiments on both seen and unseen images, results demonstrate the better performance and robustness of the proposed method.}
}
@article{2024A9,
title = {Guide for Authors},
journal = {Journal of the American Society of Echocardiography},
volume = {37},
number = {7},
pages = {A9-A18},
year = {2024},
note = {35th ASE Annual Scientific Sessions},
issn = {0894-7317},
doi = {https://doi.org/10.1016/S0894-7317(24)00239-6},
url = {https://www.sciencedirect.com/science/article/pii/S0894731724002396}
}
@article{HERMANN2024114720,
title = {Artificial intelligence and consumer behavior: From predictive to generative AI},
journal = {Journal of Business Research},
volume = {180},
pages = {114720},
year = {2024},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2024.114720},
url = {https://www.sciencedirect.com/science/article/pii/S0148296324002248},
author = {Erik Hermann and Stefano Puntoni},
keywords = {Artificial intelligence, Consumer behavior, Algorithms, Predictive AI, Generative AI},
abstract = {Since the introduction of ChatGPT, the leading example of Generative Artificial Intelligence (GenAI), the research community and the general public have been captivated by GenAI’s remarkable advances in performance, and its ability to both imitate and, in some respects, surpass human capabilities. This paper offers a comprehensive analysis of the impact of AI on consumer behavior, focusing on the two pivotal phases of AI development over the past 15 years. We start by reviewing the extensively researched, yet still growing, field of algorithmic predictions and decision-making, alongside the varied positive and negative consumer reactions it elicits. Subsequently, we delve into the just emerging field of GenAI. Here, we differentiate between Convergent Thinking GenAI, which is more domain-specific and geared towards pre-defined task completion, and Divergent Thinking GenAI, which is more domain-general and oriented towards new task fulfillment. For each of these realms, we identify key areas for future investigation.}
}
@article{MANTELERO2024106020,
title = {The Fundamental Rights Impact Assessment (FRIA) in the AI Act: Roots, legal obligations and key elements for a model template},
journal = {Computer Law & Security Review},
volume = {54},
pages = {106020},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106020},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000864},
author = {Alessandro Mantelero},
keywords = {AI Act, Fundamental rights impact assessment, FRIA, Fundamental Rights, AI},
abstract = {What is the context which gave rise to the obligation to carry out a Fundamental Rights Impact Assessment (FRIA) in the AI Act? How has assessment of the impact on fundamental rights been framed by the EU legislator in the AI Act? What methodological criteria should be followed in developing the FRIA? These are the three main research questions that this article aims to address, through both legal analysis of the relevant provisions of the AI Act and discussion of various possible models for assessment of the impact of AI on fundamental rights. The overall objective of this article is to fill existing gaps in the theoretical and methodological elaboration of the FRIA, as outlined in the AI Act. In order to facilitate the future work of EU and national bodies and AI operators in placing this key tool for human-centric and trustworthy AI at the heart of the EU approach to AI design and development, this article outlines the main building blocks of a model template for the FRIA. While this proposal is consistent with the rationale and scope of the AI Act, it is also applicable beyond the cases listed in Article 27 and can serve as a blueprint for other national and international regulatory initiatives to ensure that AI is fully consistent with human rights.}
}
@article{LIM2024114760,
title = {How to combine and clean bibliometric data and use bibliometric tools synergistically: Guidelines using metaverse research},
journal = {Journal of Business Research},
volume = {182},
pages = {114760},
year = {2024},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2024.114760},
url = {https://www.sciencedirect.com/science/article/pii/S0148296324002649},
author = {Weng Marc Lim and Satish Kumar and Naveen Donthu},
keywords = {Bibliometrics, Bibliometric analysis, Scientometrics, Scientometric analysis, Performance analysis, Science mapping, Bibliographic coupling, Co-occurrence analysis, Co-citation analysis, Trend analysis, Bibliometrix, Biblioshiny, R, RStudio, VOSviewer, Scopus, Web of Science, Metaverse},
abstract = {Bibliometrics (or scientometrics) is a powerful technique to assess the trajectory of scientific research. Building on the Journal of Business Research’s seminal guides for bibliometric analysis—i.e., “How to conduct a bibliometric analysis: An overview and guidelines” and “Guidelines for advancing theory and practice through bibliometric research”—and using metaverse research as a case, this article presents in-depth procedural guidelines for (i) combing and cleaning bibliometric data from multiple databases (Scopus and Web of Science) and (ii) conducting bibliometric analysis using multiple tools (bibliometrix and VOSviewer). Besides serving as a guide to harness the potential of bibliometrics for insightful assessments of scientific research, this article provides noteworthy insights into various features of the metaverse. This includes an examination of decentralized systems and the integration of digital assets, alongside innovations, the influence of industrial revolutions, and ethical and sustainable development. The dynamics of digital identity, ownership, and business models are explored in tandem with engagement strategies and multi-disciplinary perspectives of the metaverse. This comprehensive analysis also addresses metaverse challenges, market behaviors, and marketing strategies. Collectively, these insights offer a robust foundation for scholars, practitioners, and policymakers to shape the future of the metaverse with clarity, purpose, and impact.}
}
@article{ZAMAN2024113386,
title = {Therapeutic peptides targeting intracellular molecules},
journal = {European Polymer Journal},
volume = {219},
pages = {113386},
year = {2024},
issn = {0014-3057},
doi = {https://doi.org/10.1016/j.eurpolymj.2024.113386},
url = {https://www.sciencedirect.com/science/article/pii/S0014305724006475},
author = {Rahela Zaman and Ezharul Hoque Chowdhury},
keywords = {Peptide therapeutics, Intracellular targeting, Protein–protein interactions (PPI)},
abstract = {The increasing popularity of peptides and proteins targeted therapeutics is primarily attributed to their inherent advantages, such as precise targeting and the reduction of side effects, compared to traditional nonspecific small molecular drugs. In recent times, another achievement in peptide-based therapeutics is identified in their potential to target intracellular protein–protein interactions (PPI) deemed previously “undruggable”. As a matter of fact, precision targeting of intracellular PPI is a newfound hope for a large number of conditions which had no treatment. The review explores potential intracellular targeting strategies by peptide-based molecules, with their potential mode of activity in biological system, the transport mechanisms involved, targeting strategies and common barriers that need to be overcome. It also discusses the challenges of lab to clinic translation focusing on intracellular delivery of peptides.}
}
@article{MULLER2024102366,
title = {A tour d'horizon of de Casteljau's work},
journal = {Computer Aided Geometric Design},
volume = {113},
pages = {102366},
year = {2024},
issn = {0167-8396},
doi = {https://doi.org/10.1016/j.cagd.2024.102366},
url = {https://www.sciencedirect.com/science/article/pii/S0167839624001006},
author = {Andreas Müller},
keywords = {de Casteljau algorithm, Blossoming and polar forms, Quasi-interpolation, Metric geometry, Geometric optics, Quaternions, Regular polygons, Generalised Euclidean algorithm, 14-point strophoid},
abstract = {Whilst Paul de Casteljau is now famous for his fundamental algorithm of curve and surface approximation, little is known about his other findings. This article offers an insight into his results in geometry, algebra and number theory. Related to geometry, his classical algorithm is reviewed as an index reduction of a polar form. This idea is used to show de Casteljau's algebraic way of smoothing, which long went unnoticed. We will also see an analytic polar form and its use in finding the intersection of two curves. The article summarises unpublished material on metric geometry. It includes theoretical advances, e.g., the 14-point strophoid or a way to link Apollonian circles with confocal conics, and also practical applications such as a recurrence for conjugate mirrors in geometric optics. A view on regular polygons leads to an approximation of their diagonals by golden matrices, a generalisation of the golden ratio. Relevant algebraic findings include matrix quaternions (and anti-quaternions) and their link with Lorentz' equations. De Casteljau generalised the Euclidean algorithm and developed an automated method for approximating the roots of a class of polynomial equations. His contributions to number theory not only include aspects on the sum of four squares as in quaternions, but also a view on a particular sum of three cubes. After a review of a complete quadrilateral in a heptagon and its angles, the paper concludes with a summary of de Casteljau's key achievements. The article contains a comprehensive bibliography of de Casteljau's works, including previously unpublished material.}
}
@article{LIU2024111883,
title = {DG Embeddings: The unsupervised definition embeddings learned from dictionary and glossary to gloss context words of Cloze task},
journal = {Knowledge-Based Systems},
volume = {296},
pages = {111883},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111883},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124005173},
author = {Xiaodong Liu and Rafal Rzepka and Kenji Araki},
keywords = {Unsupervised definition embeddings, Semantic features of glosses, Context words, Auto-encoding models, Natural language processing},
abstract = {For both humans and machines to acquire vocabulary, it is effective to learn words from context while using dictionaries as an auxiliary tool. It has been shown in previous linguistic studies that for humans, glossing either target words to be learned or words comprising context is an effective approach. For machines, however, previous NLP studies are mainly focused on the former. In this paper, we investigate the potentiality of context words-glossed setting. During pre-training BERT, to infuse context words with semantic features of glosses, we propose DG embeddings — the unsupervised definition embeddings learned from dictionaries and glossaries. To employ unsupervised learning is inspired by a real-world scenario of dictionary use called headword search. This can also prevent a technical duplicate from happening, as learning words from context is already based on auto-encoding models with self-supervised learning. BERT-base is used for evaluation, and we refer to BERT-base with DG embeddings as DG-BERT. According to our experimental results, compared to the vanilla BERT, DG-BERT shows the following strengths: faster pre-training convergence, noticeable improvements on various downstream tasks, a better grasp of figurative semantics, more accurate self-attention for collocation of phrases, and higher sensitivity to context words for target-word predictions in psycholinguistic diagnostics.}
}
@article{KIRA2024106024,
title = {When non-consensual intimate deepfakes go viral: The insufficiency of the UK Online Safety Act},
journal = {Computer Law & Security Review},
volume = {54},
pages = {106024},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106024},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000906},
author = {Beatriz Kira},
keywords = {Deepfakes, Synthetic media, Image-based sexual abuse, Content moderation, AI regulation, Online Safety Act, Social media platforms, United Kingdom},
abstract = {Advancements in artificial intelligence (AI) have drastically simplified the creation of synthetic media. While concerns often focus on potential misinformation harms, ‘non-consensual intimate deepfakes’ (NCID) – a form of image-based sexual abuse – pose a current, severe, and growing threat, disproportionately impacting women and girls. This article examines the measures implemented with the recently adopted Online Safety Act 2023 (OSA) and argues that the new criminal offences and the ‘systems and processes’ approach the law adopts are insufficient to counter NCID in the UK. This is because the OSA relies on platform policies that often lack consistency regarding synthetic media and on platforms’ content removal mechanisms which offer limited redress to victim-survivors after the harm has already occurred. The article argues that stronger prevention mechanisms are necessary and proposes that the law should mandate all AI-powered deepfake creation tools to ban the generation of intimate synthetic content and require the implementation of comprehensive and enforceable content moderation systems.}
}
@article{ALAMSYAH2024200394,
title = {Empowering Indonesian internet users: An approach to counter online toxicity and enhance digital well-being},
journal = {Intelligent Systems with Applications},
volume = {22},
pages = {200394},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200394},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324000693},
author = {Andry Alamsyah and Yoga Sagama},
keywords = {Online toxicity, Content moderation, Indonesian language, IndoBERTweet, Indonesian RoBERTa},
abstract = {The proliferation of online toxicity, characterized by offensive and disrespectful language, has been a pervasive issue in Indonesia’s digital environment, impacting users’ mental health and well-being. Simultaneously, the potential of Natural Language Processing (NLP) in detecting and managing toxic comments provides a promising avenue for mitigating online toxicity. This study presents a 3-stages methodology consisting of type, target audience, and topics to detect and categorize online toxicity in the Indonesian language using fine-tuned IndoBERTweet and Indonesian RoBERTa models. The results indicate that the IndoBERTweet model, with optimally adjusted hyperparameters, consistently outperforms the Indonesian RoBERTa model in all stages of our proposed methodology. These outcomes are substantiated by higher precision, recall, and F1 score metrics exhibited by the IndoBERTweet model. This model also exhibits remarkable performance in real-world applicability, accurately classifying new Indonesian language content from Twitter (now X). This research establishes a stepping stone for future work, including exploring other language models, applying the methodology to other languages, training the models on larger and more diverse datasets, and applying it to other social media platforms or forums. Our proposal contributes to create safer online spaces, and the results provide insights for the development of automated moderation tools, playing a significant role in combating online harassment and ensuring online community well-being.}
}
@article{VERONEZE2024109083,
title = {Feature selection for packer classification based on association rule mining},
journal = {Engineering Applications of Artificial Intelligence},
volume = {137},
pages = {109083},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109083},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624012417},
author = {Rosana Veroneze and Charles-Henry {Bertrand Van Ouytsel} and Khanh Huu The Dam and Axel Legay},
keywords = {Feature selection, Malware, Packer detection, Pattern mining, Association rules, Associative classification},
abstract = {Malware often uses packing, an obfuscation strategy, to bypass antivirus. Identifying and understanding packers is therefore essential for analyzing suspicious binary files. Proposed machine learning methods for packer classification use a wide range of features, but many of them are redundant or irrelevant. It leads to waste of computational resources, so minimizing such features without reducing the effectiveness of packer identification is essential to analyze the ever-growing number of malware. This paper presents a novel embedded feature selection method for packer classification named Feature Selection based on Associative Classification (FSbAC). FSbAC exploits established concepts of association rule mining, particularly associative classification. As a result, FSbAC can define important sets of features per each packer individually, not just for the entire dataset. This makes it possible to directly learn the characteristics of each packer, improving the knowledge of analysts against packing. It also allows the costs associated with feature extraction to be taken into account during the selection process, ensuring the efficiency of the selected feature set. Seven different classification algorithms were used to assess the performance of FSbAC against eight FS methods. The evaluation encompassed different scenarios of packer classification, employing both a synthetic dataset and real-world datasets. The performance of the FS methods was evaluated with and without the inclusion of byte features vulnerable to bypass by hackers. Our results indicate that FSbAC is efficient for packer classification and leads to a substantial decrease in the number of features and in the computational resources required, without compromising predictive performance.}
}
@article{CHOUNG2024152757,
title = {Rise of machine learning potentials in heterogeneous catalysis: Developments, applications, and prospects},
journal = {Chemical Engineering Journal},
volume = {494},
pages = {152757},
year = {2024},
issn = {1385-8947},
doi = {https://doi.org/10.1016/j.cej.2024.152757},
url = {https://www.sciencedirect.com/science/article/pii/S138589472404244X},
author = {Seokhyun Choung and Wongyu Park and Jinuk Moon and Jeong Woo Han},
keywords = {Catalysis, Energy materials, Quantum calculations, Density functional theory, Molecular dynamics, Machine learning potential, Machine learning, Artificial intelligence},
abstract = {The urgency of tackling climate change is driving a global shift towards renewable sources of energy, with a growing contribution from alternative energy sources such as solar, wind and hydroelectric power. With the global push for the sustainable energy, the demand for effective catalysts for sustainable chemical production and energy storage has been rapidly increasing. Computational simulations have contributed to the rational design of catalysts by allowing profound analysis of catalyst properties. Machine learning potential (MLP) has emerged as a potential tool to bridge the gap between quantum mechanical accuracy and computational efficiency, overcoming the computational cost limitations of quantum chemistry-based simulations. This review discusses the development and application of MLP in multiscale simulations of heterogeneous catalysis. It covers the basic concepts of computational catalysis, the construction of MLP focusing on efficient datasets, atomic structure representations, and the process of training and evaluating of ML models. Furthermore, the potential applications of MLP are discussed in addressing computational challenges within the field, as MLP has potential to overcome limitations in simulation time and length scale. Lastly, the prospects for MLP are presented, taking advantage of the rapid advancements in artificial intelligence architectures. It is expected that the integration of MLP will accelerate progress within the catalyst research community and will bridge the gap between theoretical and experimental approaches in catalytic research.}
}
@article{VASSILIADIS2024380,
title = {Reloading Process Systems Engineering within Chemical Engineering},
journal = {Chemical Engineering Research and Design},
volume = {209},
pages = {380-398},
year = {2024},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2024.07.066},
url = {https://www.sciencedirect.com/science/article/pii/S0263876224004568},
author = {Vassilios S. Vassiliadis and Vasileios Mappas and Thomas A. Espaas and Bogdan Dorneanu and Adeniyi Isafiade and Klaus Möller and Harvey Arellano-Garcia},
keywords = {Chemical Engineering, Process Systems Engineering, Process model construction and deployment, Digital Twinning, Machine Learning},
abstract = {Established as a sub-discipline of Chemical Engineering in the 1960s by the late Professor R.W.H. Sargent at Imperial College London, Process Systems Engineering (PSE) has played a significant role in advancing the field, positioning it as a leading engineering discipline in the contemporary technological landscape. Rooted in Applied Mathematics and Computing, PSE aligns with the key components driving advancements in our modern, information-centric era. Sargent’s visionary foresight anticipated the evolution of early computational tools into fundamental elements for future technological and scientific breakthroughs, all while maintaining a central focus on Chemical Engineering. This paper aims to present concise and concrete ideas for propelling PSE into a new era of progress. The objective is twofold: to preserve PSE’s extensive and diverse knowledge base and to reposition it more prominently within modern Chemical Engineering, while also establishing robust connections with other data-driven engineering and applied science domains that play important roles in industrial and technological advancements. Rather than merely reacting to contemporary challenges, this article seeks to proactively create opportunities to lead the future of Chemical Engineering across its vital contributions in education, research, technology transfer, and business creation, fully leveraging its inherent multidisciplinarity and versatile character.}
}
@article{PATSAKIS2024124912,
title = {Assessing LLMs in malicious code deobfuscation of real-world malware campaigns},
journal = {Expert Systems with Applications},
volume = {256},
pages = {124912},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124912},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424017792},
author = {Constantinos Patsakis and Fran Casino and Nikolaos Lykousas},
keywords = {Malware analysis, Code deobfuscation, Large language models, Cybersecurity},
abstract = {The integration of large language models (LLMs) into various cybersecurity pipelines has become increasingly prevalent, enabling the automation of numerous manual tasks and often surpassing human performance. Recognising this potential, cybersecurity researchers and practitioners are actively investigating the application of LLMs to process vast volumes of heterogeneous data for anomaly detection, potential bypass identification, attack mitigation, and fraud prevention. Moreover, LLMs’ advanced capabilities in generating functional code, interpreting code context, and code summarisation present significant opportunities for reverse engineering and malware deobfuscation. In this work, we comprehensively examine the deobfuscation capabilities of state-of-the-art LLMs. Specifically, we conducted a detailed evaluation of four prominent LLMs using real-world malicious scripts from the notorious Emotet malware campaign. Our findings reveal that while current LLMs are not yet perfectly accurate, they demonstrate substantial potential in efficiently deobfuscating payloads. This study highlights the importance of fine-tuning LLMs for specialised tasks, suggesting that such optimisation could pave the way for future AI-powered threat intelligence pipelines to combat obfuscated malware. Our contributions include a thorough analysis of LLM performance in malware deobfuscation, identifying strengths and limitations, and discussing the potential for integrating LLMs into cybersecurity frameworks for enhanced threat detection and mitigation. Our experiments illustrate that LLMs can automatically and accurately extract the necessary indicators of compromise from a real-world campaign with an accuracy of 69.56% and 88.78% for the URLs and the corresponding domains of the droppers, respectively.}
}
@article{MIZUMOTO2024100116,
title = {Testing the viability of ChatGPT as a companion in L2 writing accuracy assessment},
journal = {Research Methods in Applied Linguistics},
volume = {3},
number = {2},
pages = {100116},
year = {2024},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2024.100116},
url = {https://www.sciencedirect.com/science/article/pii/S2772766124000223},
author = {Atsushi Mizumoto and Natsuko Shintani and Miyuki Sasaki and Mark Feng Teng},
keywords = {Linguistic accuracy, Learner corpora, ChatGPT, Grammarly},
abstract = {This study explores the effectiveness of ChatGPT as a tool for evaluating linguistic accuracy in second language (L2) writing, situated within the complexity, accuracy, and fluency (CAF) framework. By using the Cambridge Learner Corpus First Certificate in English (CLC FCE) dataset, an error-tagged learner corpus, it compares ChatGPT's performance to human evaluators and Grammarly in assessing errors or accuracy rates across 232 writing samples. The findings indicate a strong correlation between ChatGPT's assessments and human accuracy ratings, demonstrating its precision in automated assessments. In comparison to Grammarly, ChatGPT shows a closer alignment with human judgments and students’ writing scores. Thus, ChatGPT can be a potential tool for enhancing efficiency in L2 research and L2 writing pedagogy.}
}
@article{YAO2024105023,
title = {Winning the second race of technology standardization: Strategic maneuvers in SEP follow-on innovations},
journal = {Research Policy},
volume = {53},
number = {6},
pages = {105023},
year = {2024},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2024.105023},
url = {https://www.sciencedirect.com/science/article/pii/S0048733324000726},
author = {Li Yao and Jun Li and Kaihua Chen and Rongjian Yu},
keywords = {Follow-on innovation, Patent citation, Standardization strategy, Standard essential patent, China},
abstract = {Innovation is cumulative in nature, and follow-on innovation is a way to extract value from existing technology. Thus, to appropriate the value of standard essential patents (SEPs), firms need to win the first race in pushing proprietary technologies to become SEPs and the second race in exploiting SEP-related opportunities in follow-on innovations. This study investigates how, when, and to what effect firms strategically maneuver in follow-on innovations to maximize the value appropriation from SEPs. Drawing from a resource-based logic, we argue that SEP firms’ winning strategic maneuver involves such moves: (a) engaging in early follow-on innovation targeting SEPs, (b) prioritizing the exploitation of SEPs over non-SEPs, and (c) exploiting cross-follow-on innovations to take advantage of the information and network resources they obtain from their participation in the standardization process. We further argue that SEP firms’ strategic maneuvering results in high-quality follow-on innovations, which together with SEPs form stronger patent portfolios. Analyzing a unique dataset of patents from leading Chinese ICT firms matched with forward citations, we find empirical evidence that supports our arguments. Our research has important theoretical and managerial implications for firms’ strategic innovation management with a focus on technology standardization.}
}
@article{JIANG2024100287,
title = {Evaluating technological and instructional factors influencing the acceptance of AIGC-assisted design courses},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100287},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100287},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000900},
author = {Qianling Jiang and Yuzhuo Zhang and Wei Wei and Chao Gu},
keywords = {Artificial intelligence assisted design, Course acceptance, Factor analysis, Entropy method},
abstract = {Purpose
This study aims to explore the key factors influencing design students' acceptance of AIGC-assisted design courses, providing specific strategies for course design to help students better learn this new technology and enhance their competitiveness in the design industry. The research focuses on evaluating technological and course-level factors, providing actionable insights for course developers.
Design/methodology/approach
The research establishes and validates evaluation dimensions and indicators affecting acceptance using structured questionnaires to collect data and employs factor analysis and weight analysis to determine the importance of each factor.
Findings
The results of the study reveal that the main dimensions influencing student acceptance include technology application and innovation, teaching content and methods, and extracurricular learning support and resources. Regarding indicators, data privacy, timeliness of extracurricular learning support, and availability of extracurricular learning resources are identified as the most critical factors.
Originality
The uniqueness of this study lies in providing specific course design strategies for AIGC-assisted design courses based on the weight analysis results for different dimensions and indicators. These strategies aim to help students better adapt to these courses and enhance their acceptance. Furthermore, the conclusions and recommendations of this study offer valuable insights for educational institutions and instructors, promoting further optimization and development of AIGC-assisted design courses.}
}
@article{WANG2024128010,
title = {A review of deep learning based malware detection techniques},
journal = {Neurocomputing},
volume = {598},
pages = {128010},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128010},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224007811},
author = {Huijuan Wang and Boyan Cui and Quanbo Yuan and Ruonan Shi and Mengying Huang},
keywords = {Malware detection, Deep learning, Malware datasets},
abstract = {With the popularization of computer technology, the number of malware has increased dramatically in recent years. Some malware can threaten the network security of users by downloading and installing, and even spreading widely on the Internet, causing consequences such as private data leakage in the operating system, extortion, and network paralysis. In order to deal with these threats, researchers analyze malicious samples through various analysis techniques, which are usually divided into static and dynamic analysis based on the principle of whether the code needs to be executed or not. This paper analyzes in detail several classical methods of feature extraction in malware detection techniques. With the technological development of artificial intelligence, deep learning is gradually being introduced into malware detection, which does not require the identification of professional security personnel and greatly improves the generalization ability of detection. In the paper, text-based detection methods, image visualization-based detection, and graph structure-based detection techniques are reviewed according to different feature extraction methods. In addition, the paper compares 26 datasets that have been commonly used in recent years applied in the research field and explains the main contents and specifications of the datasets. Finally, a summary and outlook of the malware research field is given.}
}
@article{ALEXEEV2024666,
title = {Quantum-centric supercomputing for materials science: A perspective on challenges and future directions},
journal = {Future Generation Computer Systems},
volume = {160},
pages = {666-710},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.04.060},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24002012},
author = {Yuri Alexeev and Maximilian Amsler and Marco Antonio Barroca and Sanzio Bassini and Torey Battelle and Daan Camps and David Casanova and Young Jay Choi and Frederic T. Chong and Charles Chung and Christopher Codella and Antonio D. Córcoles and James Cruise and Alberto {Di Meglio} and Ivan Duran and Thomas Eckl and Sophia Economou and Stephan Eidenbenz and Bruce Elmegreen and Clyde Fare and Ismael Faro and Cristina Sanz Fernández and Rodrigo Neumann Barros Ferreira and Keisuke Fuji and Bryce Fuller and Laura Gagliardi and Giulia Galli and Jennifer R. Glick and Isacco Gobbi and Pranav Gokhale and Salvador {de la Puente Gonzalez} and Johannes Greiner and Bill Gropp and Michele Grossi and Emanuel Gull and Burns Healy and Matthew R. Hermes and Benchen Huang and Travis S. Humble and Nobuyasu Ito and Artur F. Izmaylov and Ali Javadi-Abhari and Douglas Jennewein and Shantenu Jha and Liang Jiang and Barbara Jones and Wibe Albert {de Jong} and Petar Jurcevic and William Kirby and Stefan Kister and Masahiro Kitagawa and Joel Klassen and Katherine Klymko and Kwangwon Koh and Masaaki Kondo and Dog̃a Murat Kürkçüog̃lu and Krzysztof Kurowski and Teodoro Laino and Ryan Landfield and Matt Leininger and Vicente Leyton-Ortega and Ang Li and Meifeng Lin and Junyu Liu and Nicolas Lorente and Andre Luckow and Simon Martiel and Francisco Martin-Fernandez and Margaret Martonosi and Claire Marvinney and Arcesio Castaneda Medina and Dirk Merten and Antonio Mezzacapo and Kristel Michielsen and Abhishek Mitra and Tushar Mittal and Kyungsun Moon and Joel Moore and Sarah Mostame and Mario Motta and Young-Hye Na and Yunseong Nam and Prineha Narang and Yu-ya Ohnishi and Daniele Ottaviani and Matthew Otten and Scott Pakin and Vincent R. Pascuzzi and Edwin Pednault and Tomasz Piontek and Jed Pitera and Patrick Rall and Gokul Subramanian Ravi and Niall Robertson and Matteo A.C. Rossi and Piotr Rydlichowski and Hoon Ryu and Georgy Samsonidze and Mitsuhisa Sato and Nishant Saurabh and Vidushi Sharma and Kunal Sharma and Soyoung Shin and George Slessman and Mathias Steiner and Iskandar Sitdikov and In-Saeng Suh and Eric D. Switzer and Wei Tang and Joel Thompson and Synge Todo and Minh C. Tran and Dimitar Trenev and Christian Trott and Huan-Hsin Tseng and Norm M. Tubman and Esin Tureci and David García Valiñas and Sofia Vallecorsa and Christopher Wever and Konrad Wojciechowski and Xiaodi Wu and Shinjae Yoo and Nobuyuki Yoshioka and Victor Wen-zhe Yu and Seiji Yunoki and Sergiy Zhuk and Dmitry Zubarev},
keywords = {Quantum-centric supercomputing, Quantum computing, Materials science, High-performance computing},
abstract = {Computational models are an essential tool for the design, characterization, and discovery of novel materials. Computationally hard tasks in materials science stretch the limits of existing high-performance supercomputing centers, consuming much of their resources for simulation, analysis, and data processing. Quantum computing, on the other hand, is an emerging technology with the potential to accelerate many of the computational tasks needed for materials science. In order to do that, the quantum technology must interact with conventional high-performance computing in several ways: approximate results validation, identification of hard problems, and synergies in quantum-centric supercomputing. In this paper, we provide a perspective on how quantum-centric supercomputing can help address critical computational problems in materials science, the challenges to face in order to solve representative use cases, and new suggested directions.}
}
@article{KUEHN2024100079,
title = {A new business model in the fine arts realm based on NFT certificates and pearl codes},
journal = {Digital Business},
volume = {4},
number = {2},
pages = {100079},
year = {2024},
issn = {2666-9544},
doi = {https://doi.org/10.1016/j.digbus.2024.100079},
url = {https://www.sciencedirect.com/science/article/pii/S2666954424000073},
author = {Eva Maria Kuehn},
keywords = {NFT, Fine arts, Business model, Coordination, Digital twin, Real and virtual worlds, Formal model},
abstract = {The potential of Non-Fungible Tokens (NFTs) is highly anticipated, particularly in the realm of visual arts. However, current applications within fine arts often involve trivial processes, such as creating digital versions of artworks or replicas of masterpieces as NFT images, and selling only these NFTs. To unlock the full potential of NFTs, more innovative models are needed. This paper introduces a novel model that establishes a permanent link between an NFT and a physical craft object. This linkage is utilized to orchestrate the trade workflow, ensuring a sustained connection between real and digital artifacts. A distinguishing feature is that they can be sold together or seperately and later reunited with a buyer. The NFT serves as a multifunctional certificate, tracing, and communication token. Through a comprehensive analysis, this paper explores diverse scenarios that may arise in the relationship between the physical object and its digital twin. It presents a systematic and formal description of the proposed model and its various cases, marking a pioneering effort in the field. Noteworthy advantages include the ability to detect plagiarism and fraud. By strategically incorporating stakeholder roles, the model preserves the anonymity of art collectors while extracting valuable information about the ownership of physical artworks. The primary objective is to enhance security in the art trade and foster new business opportunities for stakeholders. As a proof-of-concept, the model was implemented in a real-world scenario on a leading NFT marketplace platform.}
}
@article{GANGWAL2024108734,
title = {Current strategies to address data scarcity in artificial intelligence-based drug discovery: A comprehensive review},
journal = {Computers in Biology and Medicine},
volume = {179},
pages = {108734},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108734},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524008199},
author = {Amit Gangwal and Azim Ansari and Iqrar Ahmad and Abul Kalam Azad and Wan Mohd Azizi {Wan Sulaiman}},
keywords = {Artificial intelligence federated learning, Drug discovery, Machine learning, Data privacy, Deep learning, Active learning, Transfer learning, One-shot learning, Data augmentation, Multi-task learning, Data synthesis},
abstract = {Artificial intelligence (AI) has played a vital role in computer-aided drug design (CADD). This development has been further accelerated with the increasing use of machine learning (ML), mainly deep learning (DL), and computing hardware and software advancements. As a result, initial doubts about the application of AI in drug discovery have been dispelled, leading to significant benefits in medicinal chemistry. At the same time, it is crucial to recognize that AI is still in its infancy and faces a few limitations that need to be addressed to harness its full potential in drug discovery. Some notable limitations are insufficient, unlabeled, and non-uniform data, the resemblance of some AI-generated molecules with existing molecules, unavailability of inadequate benchmarks, intellectual property rights (IPRs) related hurdles in data sharing, poor understanding of biology, focus on proxy data and ligands, lack of holistic methods to represent input (molecular structures) to prevent pre-processing of input molecules (feature engineering), etc. The major component in AI infrastructure is input data, as most of the successes of AI-driven efforts to improve drug discovery depend on the quality and quantity of data, used to train and test AI algorithms, besides a few other factors. Additionally, data-gulping DL approaches, without sufficient data, may collapse to live up to their promise. Current literature suggests a few methods, to certain extent, effectively handle low data for better output from the AI models in the context of drug discovery. These are transferring learning (TL), active learning (AL), single or one-shot learning (OSL), multi-task learning (MTL), data augmentation (DA), data synthesis (DS), etc. One different method, which enables sharing of proprietary data on a common platform (without compromising data privacy) to train ML model, is federated learning (FL). In this review, we compare and discuss these methods, their recent applications, and limitations while modeling small molecule data to get the improved output of AI methods in drug discovery. Article also sums up some other novel methods to handle inadequate data.}
}
@article{ZHANG2024109104,
title = {Advanced informatic technologies for intelligent construction: A review},
journal = {Engineering Applications of Artificial Intelligence},
volume = {137},
pages = {109104},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109104},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624012624},
author = {Limao Zhang and Yongsheng Li and Yue Pan and Lieyun Ding},
keywords = {Intelligent construction, Digital transformation, Construction automation and informatics, Critical review},
abstract = {Since there is a growing interest in driving the digital transformation of the construction industry, the goal of this paper is to provide a full picture and understanding of the emerging research front on intelligent construction. Through the quantitative and qualitative analysis, relevant pieces of literature are thoroughly reviewed to extract useful knowledge to raise awareness of intelligent construction, resulting in the summary of research status and the determination of future needs. In light of the research findings, the rapid development of intelligent construction follows the strategies of standardization, digitization, intellectualization, collaboration, greenization, and customization, which mainly support “4S” technologies. The popular research topics covering intelligent engineering services for the project's full lifecycle (i.e., design, construction, operation, and maintenance) and even the city level have been deeply explored, which are proven to dramatically improve project efficiency, safety, and automation. Moreover, challenges exist in three potential paths for future research, including the digital twin-enabled engineering cloud platform, man-machine-environment interaction, great sustainability under carbon peaking and carbon neutrality goals. As for the practical value, a synthesized point of up-to-date reference is created to guide managers to promote intelligent construction in real-world engineering.}
}
@article{SACORANSKY2024728,
title = {ChatGPT and assistive AI in structured radiology reporting: A systematic review},
journal = {Current Problems in Diagnostic Radiology},
volume = {53},
number = {6},
pages = {728-737},
year = {2024},
issn = {0363-0188},
doi = {https://doi.org/10.1067/j.cpradiol.2024.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0363018824001130},
author = {Ethan Sacoransky and Benjamin Y.M. Kwan and Donald Soboleski},
keywords = {ChatGPT, Artificial intelligence, Radiology, Radiologist, Large language models},
abstract = {Introduction
The rise of transformer-based large language models (LLMs), such as ChatGPT, has captured global attention with recent advancements in artificial intelligence (AI). ChatGPT demonstrates growing potential in structured radiology reporting—a field where AI has traditionally focused on image analysis.
Methods
A comprehensive search of MEDLINE and Embase was conducted from inception through May 2024, and primary studies discussing ChatGPT's role in structured radiology reporting were selected based on their content.
Results
Of the 268 articles screened, eight were ultimately included in this review. These articles explored various applications of ChatGPT, such as generating structured reports from unstructured reports, extracting data from free text, generating impressions from radiology findings and creating structured reports from imaging data. All studies demonstrated optimism regarding ChatGPT's potential to aid radiologists, though common critiques included data privacy concerns, reliability, medical errors, and lack of medical-specific training.
Conclusion
ChatGPT and assistive AI have significant potential to transform radiology reporting, enhancing accuracy and standardization while optimizing healthcare resources. Future developments may involve integrating dynamic few-shot prompting, ChatGPT, and Retrieval Augmented Generation (RAG) into diagnostic workflows. Continued research, development, and ethical oversight are crucial to fully realize AI's potential in radiology.}
}
@article{RAEES2024103301,
title = {From explainable to interactive AI: A literature review on current trends in human-AI interaction},
journal = {International Journal of Human-Computer Studies},
volume = {189},
pages = {103301},
year = {2024},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2024.103301},
url = {https://www.sciencedirect.com/science/article/pii/S1071581924000855},
author = {Muhammad Raees and Inge Meijerink and Ioanna Lykourentzou and Vassilis-Javed Khan and Konstantinos Papangelis},
keywords = {Human-centered AI, Interactivity, Collaboration, Explainability},
abstract = {AI systems are increasingly being adopted across various domains and application areas. With this surge, there is a growing research focus and societal concern for actively involving humans in developing, operating, and adopting these systems. Despite this concern, most existing literature on AI and Human–Computer Interaction (HCI) primarily focuses on explaining how AI systems operate and, at times, allowing users to contest AI decisions. Existing studies often overlook more impactful forms of user interaction with AI systems, such as giving users agency beyond contestability and enabling them to adapt and even co-design the AI’s internal mechanics. In this survey, we aim to bridge this gap by reviewing the state-of-the-art in Human-Centered AI literature, the domain where AI and HCI studies converge, extending past Explainable and Contestable AI, delving into the Interactive AI and beyond. Our analysis contributes to shaping the trajectory of future Interactive AI design and advocates for a more user-centric approach that provides users with greater agency, fostering not only their understanding of AI’s workings but also their active engagement in its development and evolution.}
}
@article{LEE2024114920,
title = {Exploring determinants of non-fungible token creators’ engagement behaviors on metaverse-based NFT platforms: A multi-analytical SEM-IPMA method},
journal = {Journal of Business Research},
volume = {185},
pages = {114920},
year = {2024},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2024.114920},
url = {https://www.sciencedirect.com/science/article/pii/S0148296324004247},
author = {Crystal T. Lee and Yung-Cheng Shen},
keywords = {NFT, Uses and gratifications (U&G), NFT creators, Web 3.0, Blockchain},
abstract = {The advancing technologies of blockchain and Web 3.0 are transforming the decentralized nature of the Internet. Token-based services, particularly nonfungible tokens (NFTs), are innovative methods of performing financial transactions that have contributed to the growth of crypto commerce. Previous studies on NFT have focused on the role of investors, with a limited understanding of NFT creators. This study aims to provide a comprehensive examination of how NFT technology is utilized in cryptographic art by NFT creators. Based on the uses and gratifications (U&G) theory, the research utilizes a mixed-method approach combining in-depth interviews and online surveys of 1331 NFT creators from eight prominent NFT platforms. The results demonstrated that content and reward gratification were associated with NFT creation identification, whereas social gratification facilitated community identification. Both NFT and community identification facilitate consumption and creation behaviors. We also discussed theoretical and practical implications for stakeholders.}
}
@article{2024A17,
title = {Instructions for authors},
journal = {Gastrointestinal Endoscopy},
volume = {100},
number = {1},
pages = {A17-A23},
year = {2024},
issn = {0016-5107},
doi = {https://doi.org/10.1016/j.gie.2024.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0016510724032541}
}
@article{FRATTINI2024112120,
title = {Requirements quality research artifacts: Recovery, analysis, and management guideline},
journal = {Journal of Systems and Software},
volume = {216},
pages = {112120},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112120},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224001651},
author = {Julian Frattini and Lloyd Montgomery and Davide Fucci and Michael Unterkalmsteiner and Daniel Mendez and Jannik Fischbach},
keywords = {Requirements engineering, Artifact, Availability, Bayesian data analysis, Guideline},
abstract = {Requirements quality research, which is dedicated to assessing and improving the quality of requirements specifications, is dependent on research artifacts like data sets (containing information about quality defects) and implementations (automatically detecting and removing these defects). However, recent research exposed that the majority of these research artifacts have become unavailable or have never been disclosed, which inhibits progress in the research domain. In this work, we aim to improve the availability of research artifacts in requirements quality research. To this end, we (1) extend an artifact recovery initiative, (2) empirically evaluate the reasons for artifact unavailability using Bayesian data analysis, and (3) compile a concise guideline for open science artifact disclosure. Our results include 10 recovered data sets and 7 recovered implementations, empirical support for artifact availability improving over time and the positive effect of public hosting services, and a pragmatic artifact management guideline open for community comments. With this work, we hope to encourage and support adherence to open science principles and improve the availability of research artifacts for the requirements research quality community.}
}
@article{GUPTA2024103997,
title = {Exploring the generative AI adoption in service industry: A mixed-method analysis},
journal = {Journal of Retailing and Consumer Services},
volume = {81},
pages = {103997},
year = {2024},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2024.103997},
url = {https://www.sciencedirect.com/science/article/pii/S0969698924002935},
author = {Rohit Gupta and Bhawana Rathore},
keywords = {Generative AI, Service industry, Text mining, Topic modelling, FDM, Fuzzy AHP, Fuzzy DEMATEL},
abstract = {In the last few years, many service organisations have been exploring the use of Generative Artificial Intelligence (GAI) tools for their businesses and upgrading their existing processes. These tools have the potential and capability to transform the business world in various aspects. However, serval service organisations are facing many challenges while adopting the GAI tools in their organisations. In a similar context, this study explores the adoption of GAI barriers through two studies by a mixed-method approach. The first study is based on YouTube datasets of selected videos where GAI adoption challenges, problems, and barriers were discussed. Further, these YouTube datasets were analysed through text mining and empirical modelling techniques. In the second study, an extensive literature review was done and critical barriers to GAI adoption were identified based on the extensive literature review. Further, these barriers were analysed through three theoretical lenses and a hybrid fuzzy multicriteria decision-making approach. In addition, the results from the first study were further matched and verified with our second study. This establishes the relevance of adopting a mixed-method approach. Our major findings are: (i) trust, anticipation, and surprise emerged as the strongest emotions of the viewers who posted their comments on the YouTube videos; (ii) Five major barriers are revealed through topic analysis of YouTube transcripts and these are ethical, technological, regulations & policies, cost, and human resources; (iii) Six major barriers are identified through second study are privacy & security, return on investment, running cost, misuse, over-reliance, and Lack of digital infrastructure.}
}
@article{XU2024e32364,
title = {Generative artificial intelligence in healthcare from the perspective of digital media: Applications, opportunities and challenges},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e32364},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32364},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024083956},
author = {Rui Xu and Zhong Wang},
keywords = {ChatGPT, Healthcare, Digital media, Applications, Opportunities, Challenges, Digital health, Generative artificial intelligence, Large language models, Artificial intelligence generated content},
abstract = {Introduction
The emergence and application of generative artificial intelligence/large language models (hereafter GenAI LLMs) have the potential for significant impact on the healthcare industry. However, there is currently a lack of systematic research on GenAI LLMs in healthcare based on reliable data. This article aims to conduct an exploratory study of the application of GenAI LLMs (i.e., ChatGPT) in healthcare from the perspective of digital media (i.e., online news), including the application scenarios, potential opportunities, and challenges.
Methods
This research used thematic qualitative text analysis in five steps: firstly, developing main topical categories based on relevant articles; secondly, encoding the search keywords using these categories; thirdly, conducting searches for news articles via Google ; fourthly, encoding the sub-categories using the elaborate category system; and finally, conducting category-based analysis and presenting the results. Natural language processing techniques, including the TermRaider and AntConc tool, were applied in the aforementioned steps to assist in text qualitative analysis. Additionally, this study built a framework, using for analyzing the above three topics, from the perspective of five different stakeholders, including healthcare demanders and providers.
Results
This study summarizes 26 applications (e.g., provide medical advice, provide diagnosis and triage recommendations, provide mental health support, etc.), 21 opportunities (e.g., make healthcare more accessible, reduce healthcare costs, improve patients care, etc.), and 17 challenges (e.g., generate inaccurate/misleading/wrong answers, raise privacy concerns, lack of transparency, etc.), and analyzes the reasons for the formation of these key items and the links between the three research topics.
Conclusions
The application of GenAI LLMs in healthcare is primarily focused on transforming the way healthcare demanders access medical services (i.e., making it more intelligent, refined, and humane) and optimizing the processes through which healthcare providers offer medical services (i.e., simplifying, ensuring timeliness, and reducing errors). As the application becomes more widespread and deepens, GenAI LLMs is expected to have a revolutionary impact on traditional healthcare service models, but it also inevitably raises ethical and security concerns. Furthermore, GenAI LLMs applied in healthcare is still in the initial stage, which can be accelerated from a specific healthcare field (e.g., mental health) or a specific mechanism (e.g., GenAI LLMs’ economic benefits allocation mechanism applied to healthcare) with empirical or clinical research.}
}
@article{ASHOURI2024123618,
title = {Measuring digitalization at scale using web scraped data},
journal = {Technological Forecasting and Social Change},
volume = {207},
pages = {123618},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123618},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524004165},
author = {Sajad Ashouri and Arash Hajikhani and Arho Suominen and Lukas Pukelis and Scott W. Cunningham},
keywords = {Digitalization, Innovation, Web scraping, Big data, Text mining},
abstract = {Measuring digitalization has been a central topic in academic discourse. While evaluating firms' efforts in increasing digitalization is crucial, quantifying it at scale, presents considerable challenges. This paper uses website information as a source of data to operationalize a measure of digitalization. Drawing on a sample of 60,942 firms, our approach proposes two distinct measures of digitalization: one at the product level and the other at the general organizational level. We substantiate these measures using a blend of qualitative and quantitative methods. The study validates the content of websites as a relevant source of innovation indicator data and verifies the indicators using multiple experiments. The developed digitalization indicators offer future research an empirical measure of digitalization that can be run at scale, across industries and regions through time.}
}
@article{SJOSTROM2024108803,
title = {Holocene storminess dynamics in northwestern Ireland: Shifts in storm duration and frequency between the mid- and late Holocene},
journal = {Quaternary Science Reviews},
volume = {337},
pages = {108803},
year = {2024},
issn = {0277-3791},
doi = {https://doi.org/10.1016/j.quascirev.2024.108803},
url = {https://www.sciencedirect.com/science/article/pii/S0277379124003044},
author = {Jenny K. Sjöström and Richard Gyllencreutz and Antonio {Martínez Cortizas} and Andreas Nylund and Sanna R. Piilo and Frederik Schenk and Michelle McKeown and Eleonor E. Ryberg and Malin E. Kylander},
abstract = {Substantial uncertainties exist regarding how future climate change will affect storminess (storm frequency and intensity) in Ireland and the United Kingdom (UK). Knowledge about spatiotemporal variations of past storminess gives us a better understanding of its mechanisms on centennial to millennial time scales, as well as the impact of external forcing on future storminess in climate models. Here, we present the oldest storm record to date from Ireland, covering the last 8000 years, reconstructed from the Roycarter Bog, a coastal blanket bog in north-western Ireland. The sequence was analysed for grain-size, chemical, mineral and organic molecular composition. The chronology was built on 11 AMS radiocarbon dates. The deposit characteristics, location and low inorganic content suggest aeolian transport of particles to the bog throughout the studied period. Cluster analysis of the grain-size frequency curves, along with the coarse to fine sand ratio, allowed the identification of eleven storm periods (cal yr BP): 6150–5500 (1); 4970–4130 (2); 4000 (3); 3490–3290 (4); 3230 (5); 2850–2590 (6); 2170–1920 (7); 1440 (8); 1225–890 (9); 620–470 (10); and 290–230 (11). During the mid-Holocene, the relative sea level was lower and the local beach sources located further away, giving a longer transport distance compared to the late Holocene. In the latter part of the mid-Holocene (6150–4130 cal yr BP), during the Holocene thermal maximum, increased storminess and wind strengths were inferred for north-western Ireland, manifested as two longer storm periods. During the late Holocene the storm frequency increased, and a greater number (9) of shorter storm periods were recorded. Comparison between our results and regional peat palaeostorm records from Scotland, north of our study site, showed an antiphase relationship between storminess in Ireland and Scotland during the latter part of the mid-Holocene, but mostly in-phase storminess over the last 3000 years. Taken together, enhanced wind strength and storminess were recorded during the warmer mid-Holocene, while an increased frequency of storm events occurred in the cooler late Holocene. Mid-Holocene storm periods occurred during locally wet periods, while most of the storm periods during late Holocene occurred during drier phases. Alternatively, the elevated mineral input during late Holocene promoted microbial activity and peat decomposition. The apparent variability in cyclicity and frequency between the mid- and late Holocene indicates that the processes governing storminess in the region shifted. This calls for further studies ahead, including climate modelling, to disentangle the complex processes governing storminess on millennial to centennial time scale.}
}
@article{ZHU2024100667,
title = {A survey of blockchain, artificial intelligence, and edge computing for Web 3.0},
journal = {Computer Science Review},
volume = {54},
pages = {100667},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100667},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000510},
author = {Jianjun Zhu and Fan Li and Jinyuan Chen},
keywords = {Web 3.0, Decentralization, Ownership, Blockchain, Artificial intelligence, Edge computing},
abstract = {Web 3.0, as the third generation of the World Wide Web, aims to solve contemporary problems of trust, centralization, and data ownership. Driven by the latest advances in cutting-edge technologies, Web 3.0 is moving towards a more open, decentralized, intelligent, and interconnected network. Currently, increasingly widespread data breaches have raised awareness of online privacy and security of personal data. Additionally, since Web 3.0 is a complex integration, the technical details are not as clear as the characteristics it presents. In this survey, we conduct an in-depth exploration of Web 3.0 from the perspectives of blockchain, artificial intelligence, and edge computing. The methodology includes a comprehensive literature review, using specific keywords to identify relevant studies and applying strict inclusion and exclusion criteria to ensure a focus on high-quality literature. The main contributions include identifying the key challenges of Web 3.0, examining the fundamental role of each underlying technology, and surveying state-of-the-art practical applications within this ecosystem. Moreover, we introduce an innovative decentralized storage solution that facilitates secure communication and data processing without relying on centralized servers. We also introduce a novel decentralized computing solution that enhances the capabilities of Web 3.0 by enabling edge devices to perform data analysis locally, reducing dependence on traditional centralized servers. Finally, we highlight key challenges and potential research directions. Through the combination and mutual complementation of multiple technologies, Web 3.0 is expected to give users more control and ownership of data and digital assets.}
}
@article{CARROLL2024102899,
title = {Integrating large language models and generative artificial intelligence tools into information literacy instruction},
journal = {The Journal of Academic Librarianship},
volume = {50},
number = {4},
pages = {102899},
year = {2024},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2024.102899},
url = {https://www.sciencedirect.com/science/article/pii/S0099133324000600},
author = {Alexander J. Carroll and Joshua Borycz},
keywords = {Generative artificial intelligence, Large language models, Information literacy, STEM education, Information retrieval, Critical thinking},
abstract = {Generative artificial intelligence (AI) and large language models (LLMs) have induced a mixture of excitement and panic among educators. However, there is a lack of consensus over how much experience science and engineering students have with using these tools for research-related tasks. Likewise, it is not yet known how educators and information professionals can leverage these tools to teach students strategies for information retrieval and knowledge synthesis. This study assesses the extent of students' use of AI tools in research-related tasks and if information literacy instruction could impact their perception of these tools. Responses to Likert-scale questions indicate that many students did not have extensive experience using LLMs for research-related purposes prior to the information literacy sessions. However, after participating in a didactic lecture and discussion with an engineering librarian that explored how to use these tools effectively and responsibly, many students reported viewing these tools as potentially useful for future assignments. Student responses to open-response questions suggest that librarian-led information literacy training can assist students in developing more sophisticated understandings of the limitations and use cases for artificial intelligence in inquiry-based coursework.}
}
@article{RODRIGUEZ2024105228,
title = {Sound of freshness: Crafting multisensory experience in perfumery},
journal = {Food Quality and Preference},
volume = {119},
pages = {105228},
year = {2024},
issn = {0950-3293},
doi = {https://doi.org/10.1016/j.foodqual.2024.105228},
url = {https://www.sciencedirect.com/science/article/pii/S0950329324001307},
author = {Brayan Rodríguez and Monique {Alves Frazon Cantu} and Luis H. Reyes and Vanessa {Jaqueline De Almeida Ribas Pereira} and Larissa {Carmona Zonta Santos} and Felipe Reinoso-Carvalho},
keywords = {Freshness, Multisensory, Perfume, Sound, Brand, Implicit},
abstract = {The current landscape of the perfume industry faces the challenge of enhancing product appeal and captivating consumers through innovation. Integrating multisensory elements into perfume experiences has the potential to drive this innovation forward. This research presents a novel methodology for characterizing sounds that evoke the perception of freshness in fragrances. Three experiments were conducted to explore the transferability of identified sound characteristics into the experience of certain fragrances. Experiment 1 assessed selected sounds to identify auditory parameters that effectively triggered primary freshness attributes. Based on these results, originally composed brand-aligned soundtracks were tested in Experiment 2 using the Implicit Association Test (IAT) to investigate associations with freshness and brand values. Experiment 3 examined whether these soundtracks effectively modulated the implicit olfactory experience of two commercial fragrances. The findings showed that the soundtracks successfully elicited specific enhanced effects in how people implicitly perceived the freshness of a fragrance. These outcomes revealed how soundtracks can enhance the perceptions of freshness attributes (e.g., cold, blue, light) in the fragrance experience, consistencies and variations between self-report and implicit measures, and practical applications in multisensory strategies for cosmetic companies.}
}
@article{ALKFAIRY2024100459,
title = {Factors impacting users’ willingness to adopt and utilize the metaverse in education: A systematic review},
journal = {Computers in Human Behavior Reports},
volume = {15},
pages = {100459},
year = {2024},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2024.100459},
url = {https://www.sciencedirect.com/science/article/pii/S2451958824000927},
author = {Mousa Al-kfairy and Soha Ahmed and Ashraf Khalil},
keywords = {Metaverse education, Intention to use, Acceptance, Systematic review, Information systems theories},
abstract = {Purpose
This study explores the factors influencing the adoption and acceptance of Metaverse technologies in educational settings. Despite the growing interest in immersive educational environments provided by the Metaverse, there is a lack of comprehensive understanding regarding the elements that affect user engagement and acceptance. This paper aims to bridge this gap through a systematic review of empirical studies that apply Information Systems theories such as TAM, UTAUT, TPB, and their extensions.
Methods
A total of 35 empirical studies were analyzed using a methodical review approach. The research methodologies employed in these studies include surveys, structural equation modeling, and interviews, providing a broad spectrum of data on how different factors influence educational outcomes in the Metaverse.
Results
The findings reveal that user adoption of the Metaverse in educational contexts is influenced by multiple factors at individual, technological, and environmental levels. Key factors identified include effort expectancy, behavioral intention, self-efficacy, enjoyment, and immersion. These factors are subject to moderating effects, suggesting that the dynamics of Metaverse adoption are highly context-dependent.
Conclusion
The insights gained from this review provide valuable guidelines for educators, policymakers, and technology developers aiming to effectively integrate Metaverse technologies into educational frameworks. The study also outlines limitations and suggests directions for future research, highlighting the need for further investigations into the longitudinal impacts and cultural adaptability of Metaverse applications in education.}
}
@article{RUDMARK2024101850,
title = {Open data platforms: Design principles for embracing outlaw innovators},
journal = {The Journal of Strategic Information Systems},
volume = {33},
number = {3},
pages = {101850},
year = {2024},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2024.101850},
url = {https://www.sciencedirect.com/science/article/pii/S0963868724000325},
author = {Daniel Rudmark and Rikard Lindgren and Ulrike Schultze},
keywords = {Open data platforms, Digital transformation, Public–private tension, Action design research},
abstract = {Open data platforms freely provide citizens with access to public data, thus enabling improved governance transparency, enhanced public services, and increased civic engagement. However, unlocking the potential of this digital transformation strategy requires that public institutions manage the tension between public and private interests. Furthermore, even when public institutions break down traditional barriers for citizens’ access to data, the potential users often lack the knowledge to leverage it in meaningful ways. Open data platforms therefore tend to fall short of expectations. Leveraging a 10-year action design research study (ADR) in the Swedish Transport Administration (STA), this paper develops design principles for creating value-generating open data platforms in the public domain. The ADR project was initiated to assist STA in its efforts to deal with outlaw innovators who scraped train data from different websites to develop travel apps. Through three iterative design cycles that eventually led to the formation of a new open data platform, the outlaw innovators increasingly became valued partners in the digital transformation process. Theorizing this development process, this paper offers three design principles that provide guidance to public institutions aspiring to digitally transform by making public data accessible. We also reflect upon how these institutions might mitigate the risks associated with partnering with outlaw innovators in the pursuit of an open data strategy.}
}
@article{ZHENG2024105669,
title = {Decentralized artificial intelligence in construction using blockchain},
journal = {Automation in Construction},
volume = {166},
pages = {105669},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105669},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524004059},
author = {Chengliang Zheng and Xingyu Tao and Liang Dong and Umer Zukaib and Jingyuan Tang and Haohua Zhou and Jack C.P. Cheng and Xiaohui Cui and Zhidong Shen},
keywords = {Artificial intelligence (AI), Blockchain, Smart contract, Decentralized machine learning, Excavator pose recognition},
abstract = {Alleviating cybersecurity risks associated with centralized AI training and implementation is a burgeoning challenge in the construction industry. This paper addresses two primary questions: (1) What is the knowledge of AI security vulnerability in construction, and (2) How can AI be decentralized using blockchain? To this end, this paper proposes a blockchain-AI integrated framework (BAII), enabling AI to be trained, verified, and applied on a decentralized blockchain. The framework has been successfully validated in an excavator pose recognition scenario, demonstrating acceptable latency and high performance with 95 % accuracy, 94 % precision, and 96 % recall. This research is pivotal for construction managers and IT security professionals, enhancing the reliability and safety of AI applications in construction. The decentralized AI (DAI) approach can also inspire further research into motivating constructors to contribute to AI modeling and training through incentive mechanisms in the blockchain.}
}
@article{BERNADO2024101010,
title = {Revisiting EntreComp through a systematic literature review of entrepreneurial competences. Implications for entrepreneurship education and future research},
journal = {The International Journal of Management Education},
volume = {22},
number = {3},
pages = {101010},
year = {2024},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101010},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724000818},
author = {Ester Bernadó and Florian Bratzke},
keywords = {EntreComp, Entrepreneurial competences, Entrepreneurship education, Systematic literature review, Content analysis},
abstract = {This study conducts a comprehensive review of entrepreneurial competences, utilising a systematic literature analysis encompassing 140 documents up to 2022. The primary objective is to critically examine the EntreComp framework, validating its structure and content through a meticulous comparison with competences identified in the literature. The findings confirm the robustness of EntreComp, emphasising the process-oriented nature of entrepreneurship, from opportunity identification to value creation. While EntreComp aligns with the majority of competences identified in the literature, certain nuances related to psychological and contextual factors, as well as practical considerations in current entrepreneurship practices, warrant attention. Furthermore, the study sheds light on the antecedents and developmental aspects of each competence, establishing a foundation for competence development informed by literature evidence. The paper also discusses the implications of the study's findings for entrepreneurship education, providing insights for the development of effective educational programs and suggesting avenues for future research in the field. Overall, this study contributes to advancing the understanding of entrepreneurship competences and their consideration in educational settings.}
}
@article{BERRICHE2024e32976,
title = {Unveiling ChatGPT text using writing style},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e32976},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32976},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024090078},
author = {Lamia Berriche and Souad Larabi-Marie-Sainte},
keywords = {ChatGPT, Stylometry, Plagiarism, Ensemble learning, Writing style},
abstract = {Extensive use of AI-generated texts culminated recently after the advent of large language models. Although the use of AI text generators, such as ChatGPT, is beneficial, it also threatens the academic level as students may resort to it. In this work, we propose a technique leveraging the intrinsic stylometric features of documents to detect ChatGPT-based plagiarism. The stylometric features were normalized and fed to classical classifiers, such as k-Nearest Neighbors, Decision Tree, and Naïve Bayes, as well as ensemble classifiers, such as XGBoost and Stacking. A thorough examination of the classifier was conducted by using Cross-Fold validation, hyperparameters tuning, and multiple training iterations. The results show the efficacy of both classical and ensemble learning classifiers in distinguishing between human and ChatGPT writing styles with a noteworthy performance of XGBoost where 100 % was achieved for accuracy, recall, and precision metrics. Moreover, the proposed XGBoost classifier outperformed the state-of-the-art result on the same dataset and same classifier highlighting the superiority of the proposed feature style extraction method over TF-IDF techniques. The ensemble learning classifiers were also applied to the generated dataset with mixed texts, where paragraphs are written by ChatGPT and humans. The results show that 98 % of the documents were classified correctly as either mixed or human. The last contribution consists in the authorship attribution of the paragraphs of a single document where the accuracy reached 92.3 %.}
}
@article{OKA2024101606,
title = {Evaluating relational reasoning ability with the Semantic Similarities Test: The impact of fluid intelligence and vocabulary indices},
journal = {Thinking Skills and Creativity},
volume = {53},
pages = {101606},
year = {2024},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101606},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124001445},
author = {Ryunosuke Oka and Akira Utsumi and Takashi Kusumi},
keywords = {Fluid intelligence, Semantic knowledge, Big Five personality, Similarity reasoning skills, Semantic Similarities Test},
abstract = {This study investigated the roles of fluid intelligence, semantic knowledge, and personality using the semantic similarity test (SST), which assesses participants’ relational reasoning abilities. Based on the original SST (Stamenković et al., Journal of Memory and Language, 105: 108–118, 2019), in Study 1, we developed the Japanese version of the SST with 20-word pairs (e.g., “time-river”) and rubrics (e.g., “it flows” corresponds to 2 pts) to classify participants’ brief explanations into three levels (2 pts, perfectly captures the relationship of two words; 1 pt, partially captures the relationship of two words; 0 pt, bad response) for each. In Studies 2 and 4, we confirmed that the SST scores showed a weak to moderate positive correlation with both vocabulary indices (the Japanese Vocabulary Size Estimation Test and vocabulary test derived from the WAIS-IV Vocabulary subscale) and the fluid intelligence index (Raven Progressive Matrices Short); however, it did not show a positive correlation with openness. In Study 3, we confirmed that the SST had reasonable test–retest reliability. These results showed that the Japanese version of the SST is related to both semantic knowledge and fluid intelligence, but has a limited relationship with the Big Five personality traits, especially openness.}
}
@article{PAN20242849,
title = {Comparing Fine-Tuning, Zero and Few-Shot Strategies with Large Language Models in Hate Speech Detection in English},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {140},
number = {3},
pages = {2849-2868},
year = {2024},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2024.049631},
url = {https://www.sciencedirect.com/science/article/pii/S1526149224000493},
author = {Ronghao Pan and José {Antonio García-Díaz} and Rafael Valencia-García},
keywords = {Hate speech detection, zero-shot, few-shot, fine-tuning, natural language processing},
abstract = {Large Language Models (LLMs) are increasingly demonstrating their ability to understand natural language and solve complex tasks, especially through text generation. One of the relevant capabilities is contextual learning, which involves the ability to receive instructions in natural language or task demonstrations to generate expected outputs for test instances without the need for additional training or gradient updates. In recent years, the popularity of social networking has provided a medium through which some users can engage in offensive and harmful online behavior. In this study, we investigate the ability of different LLMs, ranging from zero-shot and few-shot learning to fine-tuning. Our experiments show that LLMs can identify sexist and hateful online texts using zero-shot and few-shot approaches through information retrieval. Furthermore, it is found that the encoder-decoder model called Zephyr achieves the best results with the fine-tuning approach, scoring 86.811% on the Explainable Detection of Online Sexism (EDOS) test-set and 57.453% on the Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter (HatEval) test-set. Finally, it is confirmed that the evaluated models perform well in hate text detection, as they beat the best result in the HatEval task leaderboard. The error analysis shows that contextual learning had difficulty distinguishing between types of hate speech and figurative language. However, the fine-tuned approach tends to produce many false positives.}
}