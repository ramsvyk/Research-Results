Filippo Chiarello, Vito Giordano, Irene Spada, Simone Barandoni, Gualtiero Fantoni,
Future applications of generative large language models: A data-driven case study on ChatGPT,
Technovation,
Volume 133,
2024,
103002,
ISSN 0166-4972,
https://doi.org/10.1016/j.technovation.2024.103002.
(https://www.sciencedirect.com/science/article/pii/S016649722400052X)
Abstract: This study delves into the evolving role of generative Large Language Models (LLMs). We develop a data-driven approach to collect and analyse tasks that users are asking to generative LLMs. Thanks to the focus on tasks this paper contributes to give a quantitative and granular understanding of the potential influence of LLMs in different business areas. Utilizing a dataset comprising over 3.8 million tweets, we identify and cluster 31,747 unique tasks, with a specific case study on ChatGPT. To reach this goal, the proposed method combines two Natural Language Processing (NLP) Techniques, Named Entity Recognition (NER) and BERTopic. The combination makes it possible to collect granular tasks of LLMs (NER) and clusters them in business areas (BERTopic). Our findings reveal a wide spectrum of applications, from programming assistance to creative content generation, highlighting LLM's versatility. The analysis highlighted six emerging areas of application for ChatGPT: human resources, programming, social media, office automation, search engines, education. The study also examines the implications of these findings for innovation management, proposing a research agenda to explore the intersection of the identified areas, with four stages of the innovation process: idea generation, screening/idea selection, development, and diffusion/sales/marketing.
Keywords: Generative artificial intelligence; Generative large language models; ChatGPT; Social media analysis; Technology adoption; Emerging technologies

Ido Finder, Eitam Sheetrit, Nir Nissim,
Time-interval temporal patterns can beat and explain the malware,
Knowledge-Based Systems,
Volume 241,
2022,
108266,
ISSN 0950-7051,
https://doi.org/10.1016/j.knosys.2022.108266.
(https://www.sciencedirect.com/science/article/pii/S0950705122000843)
Abstract: Malware-based cyber-attacks are mainly aimed at obtaining sensitive data, intellectual property theft, denying critical services and data, and financial gain. Malware has continuously evolved, becoming more sophisticated and evasive, and thus it remains a major cyber-security threat. To keep pace with malware’s evolution, there is a critical need to develop new, advanced malware detection methods. Widely-used solutions, such as antivirus software and other static host-based intrusion detection systems, have limitations, particularly in detecting new, unknown, and evasive malware. Many of the limitations of static analysis can be overcome when dynamic malware analysis is leveraged by machine learning (ML) algorithms by executing the malware in an isolated environment (e.g., sandbox), which enables the acquisition of rich behavioral and time-oriented information associated with malware behavior. Prior studies have proposed various detection methods based on dynamically extracted API calls for malware detection, but other than simple order-based approaches, the use of more advanced time-based methods has not been explored. In this paper, we propose a more comprehensive detection framework which, by analyzing the raw multivariate time-series data associated with malware execution, can accurately capture malware behavior and provide clear explainability regarding malware behavior and detection model decisions. We are the first to mine and automatically discover meaningful and explainable time-interval temporal API call patterns associated with malware behavior and leverage them, using a variety of ML algorithms, for malware detection and categorization. To evaluate our proposed solution, we established a comprehensive dynamic-analysis environment using Cuckoo Sandbox and analyzed more than 17,000 portable executables executed in Windows 10, the most widely-used operating system today. We conducted extensive experiments on malware detection and categorization and compared the performance of our solution to state-of-the-art methods, including non-time-oriented (classic ML algorithms) and order-based methods (LSTM networks). The results show that our proposed solution outperforms the other methods, obtaining 99.6% detection accuracy for unknown malware and 97.65% categorization accuracy. In a more complex scenario of detecting an unknown malware type with unseen modus operandi, our method obtained almost 90% detection accuracy, outperforming the state-of-the-art methods. To demonstrate our ability to provide human explainability, we present some temporal patterns of different malware families that we discovered which shed light on malware behavior that can be used by cyber-security experts to better understand malware, better defend against future attacks, and even attribute malware campaigns to the cyber-attackers launching them.
Keywords: Malware; Detection; Time series; Time interval; Machine learning

Trang Anh MAC,
Bias and discrimination in ML-based systems of administrative decision-making and support,
Computer Law & Security Review,
Volume 55,
2024,
106070,
ISSN 2212-473X,
https://doi.org/10.1016/j.clsr.2024.106070.
(https://www.sciencedirect.com/science/article/pii/S0267364924001365)
Abstract: In 2020, the alleged wilful and gross negligence of four social workers, who did not notice and failed to report the risks to an eight-year-old boy's life from the violent abuses by his mother and her boyfriend back in 2013, ultimately leading to his death, had been heavily criticised.11*Trang Anh MAC, LLM. Digital Law, University of Paris XII Est-Créteil, reporter at AstraIA Gear. This paper is the English version of her master thesis, under supervision of Dr. Laurie MARGUET and Prof. Florent MADELAINE A. Reyes-Velarde, Charges dismissed against social workers linked to Gabriel Fernandez's killing, Los Angeles Times, 16 Jul 2020, available online at https://www.latimes.com/california/story/2020-07-15/charges-against-the-social-workers-linked-to-gabriel-fernandez-killing-will-be-dropped The documentary, Trials of Gabriel Fernandez in 2020,22https://www.imdb.com/title/tt11822998/ has discussed the Allegheny Family Screening Tool (AFST33Allegheny County, Allegheny Family Screening Tool, available online at https://www.alleghenycounty.us/Services/Human-Services-DHS/DHS-News-and-Events/Accomplishments-and-Innovations/Allegheny-Family-Screening-Tool), implemented by Allegheny County, US since 2016 to foresee involvement with the social services system. Rhema Vaithianathan44Bio of Prof. Rhema Vaithianathan. Available online at https://academics.aut.ac.nz/rhema.vaithianathan, the Centre for Social Data Analytics co-director, and the Children's Data Network55Our team, Children’s Data Network. Available online at https://www.datanetwork.org/people/ members, with Emily Putnam-Hornstein66Bio of PhD. Emily Putnam-Hornstein. Available online at https://www.datanetwork.org/people/#emily-putnam-hornstein, established the exemplary and screening tool, integrating and analysing enormous amounts of data details of the person allegedly associating to injustice to children, housed in DHS Data Warehouse77Allegheny County, DHS Data Warehouse. Available online at https://www.alleghenycounty.us/Services/Human-Services-DHS/DHS-News-and-Events/Accomplishments-and-Innovations/DHS-Data-Warehouse. They considered that may be the solution for the failure of the overwhelmed manual administrative systems. However, like other applications of AI in our modern world, in the public sector, Algorithmic Decisions Making and Support systems, it is also denounced because of the data and algorithmic bias.88N. LaGrone, Can AI Reduce Harm to Children?: Gabriel Fernandez and the Case for Machine Learning, 9 April 2020, available online at https://www.azavea.com/blog/2020/04/09/can-ai-reduce-harm-to-children/ This topic has been weighed up for the last few years but not has been put to an end yet. Therefore, this humble research is a glance through the problems - the bias and discrimination of AI based Administrative Decision Making and Support systems. At first, I determined the bias and discrimination, their blur boundary between two definitions from the legal perspective, then went into the details of the causes of bias in each stage of AI system development, mainly as the results of bias data sources and human decisions in the past, society and political contexts, and the developers’ ethics. In the same chapter, I presented the non-discrimination legal framework, including their application and convergence with the administration laws in regard to the automated decision making and support systems, as well as the involvement of ethics and regulations on personal data protection. In the next chapter, I tried to outline new proposals for potential solutions from both legal and technical perspectives. In respect to the former, my focus was fairness definitions and other current options for the developers, for example, the toolkits, benchmark datasets, debiased data, etc. For the latter, I reported the strategies and new proposals governing the datasets and AI systems development, implementation in the near future.
Keywords: Bias; Discrimination; AI; Machine learning; Decision-making; Support

Kai Riemer, Sandra Peter,
Conceptualizing generative AI as style engines: Application archetypes and implications,
International Journal of Information Management,
Volume 79,
2024,
102824,
ISSN 0268-4012,
https://doi.org/10.1016/j.ijinfomgt.2024.102824.
(https://www.sciencedirect.com/science/article/pii/S0268401224000720)
Abstract: The rise of generative AI has brought with it a surprising paradox: systems that excel at tasks once thought to be uniquely human, like fluent conversation or persuasive writing, while simultaneously failing to meet traditional expectations of computing, in terms of reliability, accuracy, and veracity (e.g., given the various issues with so-called ‘hallucinations’). We argue that, when generative AI is seen through a traditional computing lens, its development focuses on optimizing for traditional computing traits that remain in principle unattainable. This risks backgrounding what is most novel and defining about it. As probabilistic technologies, generative AIs do not store, in any traditional sense, any data or content. Rather, essential features of training data become encoded in deep neural networks as patterns, that become practically available as styles. We discuss what happens when the distinction between objects and their appearance dissolves and all aspects of images or text become understood as styles, accessible for exploration and creative combination and generation. For example, defining visual qualities of entities like ‘chair’ or ‘cat’ become available as ‘chair-ness’ or ‘cat-ness’ for creative image generation. We argue that, when understood as style engines, unique generative AI capabilities become conceptualized as complementing traditional computing ones. This will aid both computing practitioners and information systems researchers in reconciling and integrating generative AI into the traditional IS landscape. Our conceptualization leads us to propose four archetypes of generative AI application and use, and to highlight future avenues for information systems research made visible by this conceptualization, as well as implications for practice and policymaking.
Keywords: Generative AI; Large language models; Style engines; AI assistants; AI agents

Ashraful Tauhid, Lei Xu, Mostafizur Rahman, Emmett Tomai,
A survey on security analysis of machine learning-oriented hardware and software intellectual property,
High-Confidence Computing,
Volume 3, Issue 2,
2023,
100114,
ISSN 2667-2952,
https://doi.org/10.1016/j.hcc.2023.100114.
(https://www.sciencedirect.com/science/article/pii/S2667295223000120)
Abstract: Intellectual Property (IP) includes ideas, innovations, methodologies, works of authorship (viz., literary and artistic works), emblems, brands, images, etc. This property is intangible since it is pertinent to the human intellect. Therefore, IP entities are indisputably vulnerable to infringements and modifications without the owner’s consent. IP protection regulations have been deployed and are still in practice, including patents, copyrights, contracts, trademarks, trade secrets, etc., to address these challenges. Unfortunately, these protections are insufficient to keep IP entities from being changed or stolen without permission. As for this, some IPs require hardware IP protection mechanisms, and others require software IP protection techniques. To secure these IPs, researchers have explored the domain of Intellectual Property Protection (IPP) using different approaches. In this paper, we discuss the existing IP rights and concurrent breakthroughs in the field of IPP research; provide discussions on hardware IP and software IP attacks and defense techniques; summarize different applications of IP protection; and lastly, identify the challenges and future research prospects in hardware and software IP security.
Keywords: Intellectual property; IP protection; Patent; Copyright; Trademark; Infringement; Machine learning; Integrated circuit

Leo Kiss, Leo C. James, Brenda A. Schulman,
UbiREAD deciphers proteasomal degradation code of homotypic and branched K48 and K63 ubiquitin chains,
Molecular Cell,
2025,
,
ISSN 1097-2765,
https://doi.org/10.1016/j.molcel.2025.02.021.
(https://www.sciencedirect.com/science/article/pii/S1097276525001522)
Abstract: Summary
Ubiquitin chains define the fates of their modified proteins, often mediating proteasomal degradation in eukaryotes. Yet heterogeneity of intracellular ubiquitination has precluded systematically comparing the degradation capacities of different ubiquitin chains. We developed ubiquitinated reporter evaluation after intracellular delivery (UbiREAD), a technology that monitors cellular degradation and deubiquitination at high temporal resolution after bespoke ubiquitinated proteins are delivered into human cells. Comparing the degradation of a model substrate modified with various K48, K63, or K48/K63-branched ubiquitin chains revealed fundamental differences in their intracellular degradation capacities. K48 chains with three or more ubiquitins triggered degradation within minutes. K63-ubiquitinated substrate was rapidly deubiquitinated rather than degraded. Surprisingly, in K48/K63-branched chains, substrate-anchored chain identity determined the degradation and deubiquitination behavior, establishing that branched chains are not the sum of their parts. UbiREAD reveals a degradation code for ubiquitin chains varying by linkage, length, and topology and a functional hierarchy within branched ubiquitin chains.
Keywords: ubiquitin; ubiquitin code; protein degradation; K48; K63; branched ubiquitin chains; proteasome; deubiquitination; electroporation; proteostasis

Andrei Hagiu, Julian Wright,
Artificial intelligence and competition policy,
International Journal of Industrial Organization,
2025,
103134,
ISSN 0167-7187,
https://doi.org/10.1016/j.ijindorg.2025.103134.
(https://www.sciencedirect.com/science/article/pii/S0167718725000013)
Abstract: This paper examines competition policy implications of the rapidly expanding Artificial Intelligence (AI) sector. We analyze the vertical AI technology stack and data feedback loops to address three key questions: the potential for market concentration in core AI services, AI's likely impact on existing market structures, and emerging competition policy challenges. We identify key risks to competition in the AI sector, ways in which AI may disrupt some existing platforms, how AI could lead to new types of gatekeepers, and some novel competition policy concerns raised by AI.
Keywords: Antitrust; Data; Feedback loops; Generative AI; Network effects

Abdullah Önden, Karahan Kara, İsmail Önden, Galip Cihan Yalçın, Vladimir Simic, Dragan Pamucar,
Exploring the adoption of the metaverse and chat generative pre-trained transformer: A single-valued neutrosophic Dombi Bonferroni-based method for the selection of software development strategies,
Engineering Applications of Artificial Intelligence,
Volume 133, Part D,
2024,
108378,
ISSN 0952-1976,
https://doi.org/10.1016/j.engappai.2024.108378.
(https://www.sciencedirect.com/science/article/pii/S0952197624005360)
Abstract: The contemporary era has witnessed remarkable developments that seek to transform and reshape traditional software development methodologies. Notably, artificial intelligence (AI) supported software development as well as software development in virtual reality environments have gained considerable prominence. This article introduces software development strategies to examine how software developers and companies respond to this transformation. Also, an advanced decision model is developed using the alternative ranking order method accounting for two-step normalization (AROMAN) method and further analyzed with the single-valued neutrosophic set-based AROMAN technique. The single-valued neutrosophic weighted Dombi Bonferroni operator is employed in the analysis process. This research offers two case studies investigating the preferences of developers and managers in software development strategies. The first case study examines the preferences of developers, while the second focuses on the preferences of managers. In both case studies, three fundamental software development methods are presented. These include the “traditional developers approach”, “AI-supported developers approach”, and “mixed reality and AI-supported developers approach”. These methods are ranked based on expert opinions concerning 10 criteria that influence the software development process. In both case studies, “output quality” is identified as the most influential criterion. From the perspective of software development methods, in both case studies, the “mixed reality and AI-supported developers approach” is identified as the most effective. Recommendations are provided for developers and managers. The findings also have significant implications for guiding developers and managers in making informed decisions and optimizing software development practices to align with the evolving AI and virtual reality landscape.
Keywords: Virtual reality; Metaverse; Natural language processing; Single-valued neutrosophic sets; Alternative ranking order method accounting for two-step normalization

Yogesh K. Dwivedi, Nir Kshetri, Laurie Hughes, Emma Louise Slade, Anand Jeyaraj, Arpan Kumar Kar, Abdullah M. Baabdullah, Alex Koohang, Vishnupriya Raghavan, Manju Ahuja, Hanaa Albanna, Mousa Ahmad Albashrawi, Adil S. Al-Busaidi, Janarthanan Balakrishnan, Yves Barlette, Sriparna Basu, Indranil Bose, Laurence Brooks, Dimitrios Buhalis, Lemuria Carter, Soumyadeb Chowdhury, Tom Crick, Scott W. Cunningham, Gareth H. Davies, Robert M. Davison, Rahul Dé, Denis Dennehy, Yanqing Duan, Rameshwar Dubey, Rohita Dwivedi, John S. Edwards, Carlos Flavián, Robin Gauld, Varun Grover, Mei-Chih Hu, Marijn Janssen, Paul Jones, Iris Junglas, Sangeeta Khorana, Sascha Kraus, Kai R. Larsen, Paul Latreille, Sven Laumer, F. Tegwen Malik, Abbas Mardani, Marcello Mariani, Sunil Mithas, Emmanuel Mogaji, Jeretta Horn Nord, Siobhan O’Connor, Fevzi Okumus, Margherita Pagani, Neeraj Pandey, Savvas Papagiannidis, Ilias O. Pappas, Nishith Pathak, Jan Pries-Heje, Ramakrishnan Raman, Nripendra P. Rana, Sven-Volker Rehm, Samuel Ribeiro-Navarrete, Alexander Richter, Frantz Rowe, Suprateek Sarker, Bernd Carsten Stahl, Manoj Kumar Tiwari, Wil van der Aalst, Viswanath Venkatesh, Giampaolo Viglia, Michael Wade, Paul Walton, Jochen Wirtz, Ryan Wright,
Opinion Paper: “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy,
International Journal of Information Management,
Volume 71,
2023,
102642,
ISSN 0268-4012,
https://doi.org/10.1016/j.ijinfomgt.2023.102642.
(https://www.sciencedirect.com/science/article/pii/S0268401223000233)
Abstract: Transformative artificially intelligent tools, such as ChatGPT, designed to generate sophisticated text indistinguishable from that produced by a human, are applicable across a wide range of contexts. The technology presents opportunities as well as, often ethical and legal, challenges, and has the potential for both positive and negative impacts for organisations, society, and individuals. Offering multi-disciplinary insight into some of these, this article brings together 43 contributions from experts in fields such as computer science, marketing, information systems, education, policy, hospitality and tourism, management, publishing, and nursing. The contributors acknowledge ChatGPT’s capabilities to enhance productivity and suggest that it is likely to offer significant gains in the banking, hospitality and tourism, and information technology industries, and enhance business activities, such as management and marketing. Nevertheless, they also consider its limitations, disruptions to practices, threats to privacy and security, and consequences of biases, misuse, and misinformation. However, opinion is split on whether ChatGPT’s use should be restricted or legislated. Drawing on these contributions, the article identifies questions requiring further research across three thematic areas: knowledge, transparency, and ethics; digital transformation of organisations and societies; and teaching, learning, and scholarly research. The avenues for further research include: identifying skills, resources, and capabilities needed to handle generative AI; examining biases of generative AI attributable to training datasets and processes; exploring business and societal contexts best suited for generative AI implementation; determining optimal combinations of human and generative AI for various tasks; identifying ways to assess accuracy of text produced by generative AI; and uncovering the ethical and legal issues in using generative AI across different contexts.
Keywords: Conversational agent; Generative artificial intelligence; Generative AI; ChatGPT; Large language models

Mohammad Mahdi Jahani Yekta,
The general intelligence of GPT–4, its knowledge diffusive and societal influences, and its governance,
Meta-Radiology,
Volume 2, Issue 2,
2024,
100078,
ISSN 2950-1628,
https://doi.org/10.1016/j.metrad.2024.100078.
(https://www.sciencedirect.com/science/article/pii/S2950162824000316)
Abstract: Recent breakthroughs in artificial intelligence (AI) research include advancements in natural language processing (NLP) achieved by large language models (LLMs), and; in particular, generative pre–trained transformer (GPT) architectures. The latest GPT developed by OpenAI, GPT–4, has shown remarkable intelligence across various domains and tasks. It exhibits capabilities in abstraction, comprehension, vision, computer coding, mathematics, and more, suggesting it to be a significant step towards artificial general intelligence (AGI), a level of AI that possesses capabilities similar to human intelligence. This paper explores this AGI, its knowledge diffusive and societal influences, and its governance. In addition to coverage of the major associated topics studied in the literature, and making up for their loopholes, we scrutinize how GPT-4 can facilitate the diffusion of knowledge across different areas of science by promoting their interpretability and explainability (IE) to inexperts. Where applicable, the topics are also accompanied by their specific potential implications on medical imaging.
Keywords: GPT–4; Artificial general intelligence; Knowledge diffusion; Interpretability and explainability; Societal influences; Governance

Kadhim Hayawi, Sakib Shahriar, Hany Alashwal, Mohamed Adel Serhani,
Generative AI and large language models: A new frontier in reverse vaccinology,
Informatics in Medicine Unlocked,
Volume 48,
2024,
101533,
ISSN 2352-9148,
https://doi.org/10.1016/j.imu.2024.101533.
(https://www.sciencedirect.com/science/article/pii/S2352914824000893)
Abstract: Reverse vaccinology is an emerging concept in the field of vaccine development as it facilitates the identification of potential vaccine candidates. Biomedical research has been revolutionized with the recent innovations in Generative Artificial Intelligence (AI) and Large Language Models (LLMs). The intersection of these two technologies is explored in this study. In this study, the impact of Generative AI and LLMs in the field of vaccinology is explored. Through a comprehensive analysis of existing research, prospective use cases, and an experimental case study, this research highlights that LLMs and Generative AI have the potential to enhance the efficiency and accuracy of vaccine candidate identification. This work also discusses the ethical and privacy challenges, such as data consent and potential biases, raised by such applications that require careful consideration. This study paves the way for experts, researchers, and policymakers to further investigate the role and impact of Generative AI and LLM in vaccinology and medicine.
Keywords: Reverse vaccinology; Large language models (LLMs); AI; Generative AI; Vaccine candidate identification; AI ethics; Vaccines

Elena Osadchaya, Ben Marder, Jennifer A. Yule, Amy Yau, Laura Lavertu, Nikolaos Stylos, Sebastian Oliver, Rob Angell, Anouk de Regt, Liyu Gao, Kang Qi, Will Zhiyuan Zhang, Yiwei Zhang, Jiayuan Li, Sara AlRabiah,
To ChatGPT, or not to ChatGPT: Navigating the paradoxes of generative AI in the advertising industry,
Business Horizons,
Volume 67, Issue 5,
2024,
Pages 571-581,
ISSN 0007-6813,
https://doi.org/10.1016/j.bushor.2024.05.002.
(https://www.sciencedirect.com/science/article/pii/S0007681324000624)
Abstract: Generative AI (GenAI) technology is evoking both excitement and fear about its potential impact across a host of industries—including advertising, where it is expected to have a significant disruptive effect. This article utilizes the paradox lens to explore the implications of text-to-text GenAI in the form of ChatGPT for the advertising industry. Drawing on 48 interviews with advertising professionals, we identify three operational paradoxes that are associated with conducting research, creativity, efficiency, and one psychological paradox related to work identity. To gain a competitive advantage, we urge practitioners to adopt a confrontation-based coping strategy to navigate these paradoxes. This can be mobilized via an ambidexterity or contingency paradox management approach. We outline specific tactics in this article.
Keywords: ChatGPT; Generative AI; Paradoxes; Advertising; Chatbots

Julia Ohse, Bakir Hadžić, Parvez Mohammed, Nicolina Peperkorn, Michael Danner, Akihiro Yorita, Naoyuki Kubota, Matthias Rätsch, Youssef Shiban,
Zero-Shot Strike: Testing the generalisation capabilities of out-of-the-box LLM models for depression detection,
Computer Speech & Language,
Volume 88,
2024,
101663,
ISSN 0885-2308,
https://doi.org/10.1016/j.csl.2024.101663.
(https://www.sciencedirect.com/science/article/pii/S0885230824000469)
Abstract: Depression is a significant global health challenge. Still, many people suffering from depression remain undiagnosed. Furthermore, the assessment of depression can be subject to human bias. Natural Language Processing (NLP) models offer a promising solution. We investigated the potential of four NLP models (BERT, Llama2-13B, GPT-3.5, and GPT-4) for depression detection in clinical interviews. Participants (N = 82) underwent clinical interviews and completed a self-report depression questionnaire. NLP models inferred depression scores from interview transcripts. Questionnaire cut-off values for depression were used as a classifier for depression. GPT-4 showed the highest accuracy for depression classification (F1 score 0.73), while zero-shot GPT-3.5 initially performed with low accuracy (0.34), improved to 0.82 after fine-tuning, and achieved 0.68 with clustered data. GPT-4 estimates of symptom severity PHQ-8 score correlated strongly (r = 0.71) with true symptom severity. These findings demonstrate the potential of AI models for depression detection. However, further research is necessary before widespread deployment can be considered.
Keywords: Depression detection; GPT-4; GPT-3.5; LLM; NLP; Artificial intelligence

Jian Feng, Zhenfeng Liu,
Identifying potential technology opportunities for coal bed methane exploitation via patent analysis,
Sustainable Futures,
Volume 9,
2025,
100488,
ISSN 2666-1888,
https://doi.org/10.1016/j.sftr.2025.100488.
(https://www.sciencedirect.com/science/article/pii/S2666188825000589)
Abstract: To find the unexplored technology opportunities and indicate the potential breakthroughs for the technology development of coal bed methane (CBM) exploitation, this study proposes an automated identification approach by combining the patent classification codes and generative topographic mapping (GTM) based on patent analysis. The experimental findings reveal that the proposed model outperforms nine traditional link prediction benchmarks in identifying 15 vacant technologies. This automated identification approach not only saves R&D time to discover vacant technologies for CBM exploitation but also generates actionable policy insights for governments and enterprises to optimize strategic resource allocation and foster cross-sector innovation.
Keywords: Coal bed methane; Patent analysis; Vacant technology; Technology opportunity analysis

Yue Liu, Zhengwei Yang, Zhenyao Yu, Zitu Liu, Dahui Liu, Hailong Lin, Mingqing Li, Shuchang Ma, Maxim Avdeev, Siqi Shi,
Generative artificial intelligence and its applications in materials science: Current situation and future perspectives,
Journal of Materiomics,
Volume 9, Issue 4,
2023,
Pages 798-816,
ISSN 2352-8478,
https://doi.org/10.1016/j.jmat.2023.05.001.
(https://www.sciencedirect.com/science/article/pii/S2352847823000771)
Abstract: Generative Artificial Intelligence (GAI) is attracting the increasing attention of materials community for its excellent capability of generating required contents. With the introduction of Prompt paradigm and reinforcement learning from human feedback (RLHF), GAI shifts from the task-specific to general pattern gradually, enabling to tackle multiple complicated tasks involved in resolving the structure-activity relationships. Here, we review the development status of GAI comprehensively and analyze pros and cons of various generative models in the view of methodology. The applications of task-specific generative models involving materials inverse design and data augmentation are also dissected. Taking ChatGPT as an example, we explore the potential applications of general GAI in generating multiple materials content, solving differential equation as well as querying materials FAQs. Furthermore, we summarize six challenges encountered for the use of GAI in materials science and provide the corresponding solutions. This work paves the way for providing effective and explainable materials data generation and analysis approaches to accelerate the materials research and development.
Keywords: Machine learning; Artificial intelligence; Generative artificial intelligence; Materials science; Novel materials discovery; Deep learning

Fabio Martinelli, Francesco Mercaldo, Luca Petrillo, Antonella Santone,
A Method for AI-generated sentence detection through Large Language Models,
Procedia Computer Science,
Volume 246,
2024,
Pages 4853-4862,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2024.09.351.
(https://www.sciencedirect.com/science/article/pii/S1877050924023767)
Abstract: In recent years, we have seen an impressive expansion of a family of Artificial intelligence models, known as generative AI, that are capable of producing fresh, unique material, including text, images, audio, and even code. Large datasets of previously published information are used to train these models, enabling them to mimic the patterns and structures of the data and produce original output that is stylistically and qualitatively comparable to the training set. While these models have many promising applications, they also carry significant risks and potential dangers that must be carefully considered, such as misinformation, intellectual property violations, or biased information. For these reasons, in this work, we have proposed a method to detect whether a sentence is human-generated or AI-generated. To achieve this goal, we used a labeled dataset to train four different models from the BERT family, achieving an Accuracy of 96%.
Keywords: sentence detection; sentence classification; llm

Manojit Bhattacharya, Soumen Pal, Srijan Chatterjee, Abdulrahman Alshammari, Thamer H. Albekairi, Supriya Jagga, Elijah Ige Ohimain, Hatem Zayed, Siddappa N. Byrareddy, Sang-Soo Lee, Zhi-Hong Wen, Govindasamy Agoramoorthy, Prosun Bhattacharya, Chiranjib Chakraborty,
ChatGPT’s scorecard after the performance in a series of tests conducted at the multi-country level: A pattern of responses of generative artificial intelligence or large language models,
Current Research in Biotechnology,
Volume 7,
2024,
100194,
ISSN 2590-2628,
https://doi.org/10.1016/j.crbiot.2024.100194.
(https://www.sciencedirect.com/science/article/pii/S2590262824000200)
Abstract: Recently, researchers have shown concern about the ChatGPT-derived answers. Here, we conducted a series of tests using ChatGPT by individual researcher at multi-country level to understand the pattern of its answer accuracy, reproducibility, answer length, plagiarism, and in-depth using two questionnaires (the first set with 15 MCQs and the second 15 KBQ). Among 15 MCQ-generated answers, 13 ± 70 were correct (Median : 82.5; Coefficient variance : 4.85), 3 ± 0.77 were incorrect (Median: 3, Coefficient variance: 25.81), and 1 to 10 were reproducible, and 11 to 15 were not. Among 15 KBQ, the length of each question (in words) is about 294.5 ± 97.60 (mean range varies from 138.7 to 438.09), and the mean similarity index (in words) is about 29.53 ± 11.40 (Coefficient variance: 38.62) for each question. The statistical models were also developed using analyzed parameters of answers. The study shows a pattern of ChatGPT-derive answers with correctness and incorrectness and urges for an error-free, next-generation LLM to avoid users’ misguidance.
Keywords: ChatGPT; Accuracy; Reproducibility; Plagiarism; Answer length

Wenhan Lyu, Shuang Zhang, Tingting Chung, Yifan Sun, Yixuan Zhang,
Understanding the practices, perceptions, and (dis)trust of generative AI among instructors: A mixed-methods study in the U.S. higher education,
Computers and Education: Artificial Intelligence,
Volume 8,
2025,
100383,
ISSN 2666-920X,
https://doi.org/10.1016/j.caeai.2025.100383.
(https://www.sciencedirect.com/science/article/pii/S2666920X25000232)
Abstract: Generative AI (GenAI) has brought opportunities and challenges for higher education as it integrates into teaching and learning environments. As instructors navigate this new landscape, understanding their engagement with and attitudes toward GenAI is crucial. We surveyed 178 instructors from a single U.S. university to examine their current practices, perceptions, trust, and distrust of GenAI in higher education in March 2024. While most surveyed instructors reported moderate to high familiarity with GenAI-related concepts, their actual use of GenAI tools for direct instructional tasks remained limited. Our quantitative results show that trust and distrust in GenAI are related yet distinct; high trust does not necessarily imply low distrust, and vice versa. We also found significant differences in surveyed instructors' familiarity with GenAI across different trust and distrust groups. Our qualitative results show nuanced manifestations of trust and distrust among surveyed instructors and various approaches to support calibrated trust in GenAI. We discuss practical implications focused on (dis)trust calibration among instructors.
Keywords: Generative AI; Trust; Distrust; Survey study; Teaching and learning; Higher education

Nils M. Denter, Mei Yun Lai,
Measuring generative appropriability: Experiments with US semiconductor patents,
World Patent Information,
Volume 70,
2022,
102130,
ISSN 0172-2190,
https://doi.org/10.1016/j.wpi.2022.102130.
(https://www.sciencedirect.com/science/article/pii/S0172219022000369)
Abstract: This study presents a novel approach to quantify Generative Appropriability (GA) – a firm's capability to exploit current inventions to generate follow-on inventions (cumulative GA) and preclude rivals from them (preclusive GA). Unlike prior research, our approach relies not only on patent citations but also on texts. In the first step, we preprocess the texts of the focal patents and their citations. In the second step, we measure knowledge flow by similarity measurements. In the third step, we calculate cumulative and preclusive GA by means of constructed formulae. To test our approach, we select particularly active firms in patenting US semiconductor inventions. First, results from knowledge flow confirm our initial concerns on measuring knowledge flow by means of patent citation count. Second, results from GA show different patterns among the semiconductor firms and firms which are well-positioned in both perspectives and firms which lack cumulative GA but show good values in preclusive GA.
Keywords: Generative appropriability; Semiconductor patents; Knowledge flow; Textual data; USPTO patents

Huijuan Dong, Junkai Chen,
Meta-Regulation: An ideal alternative to the primary responsibility as the regulatory model of generative AI in China,
Computer Law & Security Review,
Volume 54,
2024,
106016,
ISSN 2212-473X,
https://doi.org/10.1016/j.clsr.2024.106016.
(https://www.sciencedirect.com/science/article/pii/S0267364924000827)
Abstract: Generative AI with stronger responsiveness and emergent abilities has triggered a global boom and is facing challenges such as data compliance risks during the pretraining process and risks of generating fake information, which has raised concerns among global regulatory authorities. The European Union, United States, United Kingdom, and other countries and regions are gradually establishing risk-based, scenario-based, and outcome-based governance models for generative AI. China recently introduced new regulations for the management of generative AI, which adopt a governance model focusing on generative AI service providers. It suggests that China is continuing the principle of primary responsibility in Internet governance, which encompasses legal responsibility, contractual obligations, and ethical responsibility. However, the governance model based on primary responsibility emphasizes the accountability of generative AI model service providers, with relatively limited regulation on other important entities such as users and large-scale dissemination platforms, which may not be conducive to achieving China's regulatory goals for the AI industry. In comparison, the Meta-Regulation model could be an ideal alternative for China. As a classic theory explaining the public-private relationship, the ‘Meta-Regulation’ aligns with the generative AI governance requirements. Based on the Meta-Regulation theory, the governance of generative AI in China should move towards a direction of emphasizing safety, transparency, collaborative governance, and accountability. In line with this, it is necessary to include users and large-scale dissemination platforms within the regulatory scope and establish overarching governance objectives that ensure the responsible distribution of duties among stakeholders, with regulatory authorities assuming ultimate oversight responsibility and technical coordination. At the level of specific improvement measures, it is possible to integrate the three stages of model development, usage, and content dissemination of generative AI. During the model development stage, generative AI providers have specific transparency obligations. In the usage stage, a self-regulatory system centered around platform autonomy should be constructed. In the content dissemination stage, the proactive notification obligations of the dissemination platforms should be clearly defined. Additionally, the enforcement of technical interoperability requirements is necessary, thereby promoting the orderly development of generative AI applications.
Keywords: Generative AI; Primary responsibility; Meta-Regulation; Collaborative governance

Mehrdad Jalali, Yi Luo, Lachlan Caulfield, Eric Sauter, Alexei Nefedov, Christof Wöll,
Large language models in electronic laboratory notebooks: Transforming materials science research workflows,
Materials Today Communications,
Volume 40,
2024,
109801,
ISSN 2352-4928,
https://doi.org/10.1016/j.mtcomm.2024.109801.
(https://www.sciencedirect.com/science/article/pii/S2352492824017823)
Abstract: In recent years, there has been a surge in research efforts dedicated to harnessing the capabilities of Large Language Models (LLMs) in various domains, particularly in material science. This paper delves into the transformative role of LLMs within Electronic Laboratory Notebooks (ELNs) for scientific research. ELNs represent a pivotal technological advancement, providing a digital platform for researchers to record and manage their experiments, data, and findings. This study explores the potential of LLMs to revolutionize fundamental aspects of science, including experimental methodologies, data analysis, and knowledge extraction within the ELN framework. We present a demonstrative showcase of LLM applications in ELN environments and, furthermore, we conduct a series of empirical evaluations to critically assess the practical impact of LLMs in enhancing research processes within the dynamic field of materials science. Our findings illustrate how LLMs can significantly elevate the quality and efficiency of research outcomes in ELNs, thereby advancing knowledge and innovation in materials science research and beyond.
Keywords: Materials science research; Natural language processing (NLP); Electronic laboratory notebooks (ELNs); Large language models (LLMs); Knowledge extraction; Scientific data management

Yunhao Chen, Zihui Yan, Yunjie Zhu,
A comprehensive survey for generative data augmentation,
Neurocomputing,
Volume 600,
2024,
128167,
ISSN 0925-2312,
https://doi.org/10.1016/j.neucom.2024.128167.
(https://www.sciencedirect.com/science/article/pii/S092523122400938X)
Abstract: Generative data augmentation (GDA) has emerged as a promising technique to alleviate data scarcity in machine learning applications. This thesis presents a comprehensive survey and unified framework of the GDA landscape. We first provide an overview of GDA, discussing its motivation, taxonomy, and key distinctions from synthetic data generation. We then systematically analyze the critical aspects of GDA—selection of generative models, techniques to utilize them, data selection methodologies, validation approaches, and diverse applications. Our proposed unified framework categorizes the extensive GDA literature, revealing gaps such as the lack of universal benchmarks. The thesis summarizes promising research directions, including , effective data selection, theoretical development for large-scale models’ application in GDA and establishing a benchmark for GDA. By laying a structured foundation, this thesis aims to nurture more cohesive development and accelerate progress in the vital arena of generative data augmentation.
Keywords: Generative data augmentation; Synthetic data; Data augmentation

Jiyoun Suk, Yini Zhang, Jiawei Liu, Yukyung Yang,
Communicative AI in the scientific public sphere: An analysis of Twitter discourse on generative AI tools,
Telematics and Informatics,
Volume 98,
2025,
102261,
ISSN 0736-5853,
https://doi.org/10.1016/j.tele.2025.102261.
(https://www.sciencedirect.com/science/article/pii/S0736585325000231)
Abstract: Drawing on the concept of the scientific public sphere, this study examines the public sense-making of communicative AI (e.g., generative AI) on social media. Advancing a framework encompassing cognitive (technology vs. use) and affective (positive vs. negative) dimensions of the public discourse on communicative AI, we analyzed global Twitter (now X) conversations about generative AI tools. Findings showed that the text generator (ChatGPT) discussions centered more on the technology-centered themes, whereas the image generator discussions emphasized their uses. ChatGPT received mixed sentiments in technology-related discussions, while there was more positive sentiment about the its uses. Theoretical and practical implications are discussed.
Keywords: Generative artificial intelligence; Scientific public sphere; Social media; Diffusion of innovation

Ridwan Taiwo, Idris Temitope Bello, Sulemana Fatoama Abdulai, Abdul-Mugis Yussif, Babatunde Abiodun Salami, Abdullahi Saka, Mohamed El Amine Ben Seghier, Tarek Zayed,
Generative artificial intelligence in construction: A Delphi approach, framework, and case study,
Alexandria Engineering Journal,
Volume 116,
2025,
Pages 672-698,
ISSN 1110-0168,
https://doi.org/10.1016/j.aej.2024.12.079.
(https://www.sciencedirect.com/science/article/pii/S1110016824016776)
Abstract: The construction industry plays a crucial role in the global economy, contributing approximately $10 trillion and employing over 220 million workers worldwide, but encounters numerous productivity challenges with only 1 % annual growth compared to 2.8 % for the global economy. These challenges span various processes, including design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (GenAI), capable of producing new and realistic data or content such as text, images, videos, or code from given inputs or existing knowledge, presents innovative solutions to these challenges. While there is an increasing interest in the applications of GenAI in construction, a detailed analysis of its practical uses, advantages, and areas ripe for development is still evolving. This study contributes to this emerging area by offering an insightful analysis of the current state of generative AI in construction. It has three objectives: (1) to identify and categorize the existing and emerging generative AI opportunities and challenges in the construction industry via a Delphi study; (2) to propose a framework enabling construction firms to build customized GenAI solutions; and (3) to illustrate this framework through a case study that employs GenAI model for querying contract documents. Through systematic review and expert consultation, the study identified 76 potential GenAI applications across construction phases and 18 key challenges distributed across domain-specific, technological, adoption, and ethical categories. The case study's findings show that retrieval augmented generation (RAG) improves the baseline large language model (LLM), GPT-4, by 5.2, 9.4, and 4.8 % in terms of quality, relevance, and reproducibility. The study recommends a structured approach to GenAI implementation, emphasizing the need for domain-specific customization, robust validation protocols, and careful consideration of ethical implications. This study equips academics and construction professionals with a comprehensive analysis and practical framework, facilitating the integration of GenAI techniques to enhance productivity, quality, safety, and sustainability across the construction industry.
Keywords: Generative artificial intelligence; Generative pre-trained transformer; Large language model; Multimodal AI; Retrieval augmented generation; Construction industry; GenAI; RAG; LLM; GPT; ChatGPT

Yan Chen, Pouyan Esmaeilzadeh,
Generative AI in Medical Practice: In-Depth Exploration of Privacy and Security Challenges,
Journal of Medical Internet Research,
Volume 26,
2024,
,
ISSN 1438-8871,
https://doi.org/10.2196/53008.
(https://www.sciencedirect.com/science/article/pii/S1438887124001055)
Abstract: As advances in artificial intelligence (AI) continue to transform and revolutionize the field of medicine, understanding the potential uses of generative AI in health care becomes increasingly important. Generative AI, including models such as generative adversarial networks and large language models, shows promise in transforming medical diagnostics, research, treatment planning, and patient care. However, these data-intensive systems pose new threats to protected health information. This Viewpoint paper aims to explore various categories of generative AI in health care, including medical diagnostics, drug discovery, virtual health assistants, medical research, and clinical decision support, while identifying security and privacy threats within each phase of the life cycle of such systems (ie, data collection, model development, and implementation phases). The objectives of this study were to analyze the current state of generative AI in health care, identify opportunities and privacy and security challenges posed by integrating these technologies into existing health care infrastructure, and propose strategies for mitigating security and privacy risks. This study highlights the importance of addressing the security and privacy threats associated with generative AI in health care to ensure the safe and effective use of these systems. The findings of this study can inform the development of future generative AI systems in health care and help health care organizations better understand the potential benefits and risks associated with these systems. By examining the use cases and benefits of generative AI across diverse domains within health care, this paper contributes to theoretical discussions surrounding AI ethics, security vulnerabilities, and data privacy regulations. In addition, this study provides practical insights for stakeholders looking to adopt generative AI solutions within their organizations.
Keywords: artificial intelligence; AI; generative artificial intelligence; generative AI; medical practices; potential benefits; security and privacy threats

Katherine C. Kellogg, Hila Lifshitz, Steven Randazzo, Ethan Mollick, Fabrizio Dell'Acqua, Edward McFowland, François Candelon, Karim R. Lakhani,
Novice risk work: How juniors coaching seniors on emerging technologies such as generative AI can lead to learning failures,
Information and Organization,
Volume 35, Issue 1,
2025,
100559,
ISSN 1471-7727,
https://doi.org/10.1016/j.infoandorg.2025.100559.
(https://www.sciencedirect.com/science/article/pii/S1471772725000053)
Abstract: Historically, junior professionals have mentored senior professionals around new technologies, because juniors are typically more willing than seniors to perform lower-level tasks to learn new skills, better able than seniors to engage in real-time experimentation close to the work itself, and more willing than seniors to learn innovative methods that conflict with traditional identities and norms. However, we know little about what happens when emerging technologies have a high level of uncertainty in their use, because they have wide-ranging capabilities and are exponentially changing. With the rise of Artificial Intelligence, specifically learning algorithms and LLMs, such contexts may be increasingly common. In our study conducted with the Boston Consulting Group, a global management consulting firm, we interviewed 78 junior consultants in July–August 2023 who had recently participated in a field experiment that gave them access for the first time to generative AI (GPT-4) for a strategic business problem solving task. Drawing from junior professionals' in situ reflections soon after the experiment, we found that junior professionals may fail to manage risks around uncertain emerging technologies because juniors are likely to recommend three kinds of novice risk work tactics that: 1) are grounded in a lack of deep understanding of technologies that have uncertain and wide-ranging capabilities and are changing exponentially, 2) focus on change to human routines rather than system design, and 3) focus on interventions at the project-level rather than system deployer- or ecosystem-level. The implications of novice risk work are that, when junior professionals are expected to be a source of expertise in the use of uncertain, emerging technologies, this can lead to learning failures. This study contributes to our understanding of occupational learning around emerging technologies, risk work in organizations, and human-computer interaction.
Keywords: Artificial intelligence; Experts; Generative AI; Risk work; Novices; Professional organizations; Professionals; HCI; Communities of practice; Work and occupations; Technology and organizations

Aleksei Krasnov, Shadrack J. Barnabas, Timo Boehme, Stephen K. Boyer, Lutz Weber,
Comparing software tools for optical chemical structure recognition,
Digital Discovery,
Volume 3, Issue 4,
2024,
Pages 681-693,
ISSN 2635-098X,
https://doi.org/10.1039/d3dd00228d.
(https://www.sciencedirect.com/science/article/pii/S2635098X24000585)
Abstract: The extraction of chemical information from images, also known as Optical Chemical Structure Recognition (OCSR) has recently gained new attention. This new interest is ignited by various machine learning methods introduced over the last years and the new possibilities to train image models for specific tasks such as OCSR. In the present paper, we have compared 8 open access OCSR methods (DECIMER, ReactionDataExtractor, MolScribe, RxnScribe, SwinOCSR, OCMR, MolVec, and OSRA) using an independent test set of images from patents and patent applications as this is an application area of general interest – precision and recall are highly desired by those who are analysing the intellectual property of chemistry patents. As a result, the used methods have shown different strengths when predicting structures from different images containing different modalities and chemistry categories. These existing methodologies for image extraction overall remain unsatisfactory, indicating a need for further advancements in the field. Further, we have created a machine learning image classifier, classifying images into one out of four image categories and applying the best performing OCSR method for each category. This classifier, the image comparator tools, and datasets have been made available to the public as open access tools.

Guang Hua, Andrew Beng Jin Teoh,
Deep fidelity in DNN watermarking: A study of backdoor watermarking for classification models,
Pattern Recognition,
Volume 144,
2023,
109844,
ISSN 0031-3203,
https://doi.org/10.1016/j.patcog.2023.109844.
(https://www.sciencedirect.com/science/article/pii/S0031320323005423)
Abstract: Backdoor watermarking is a promising paradigm to protect the copyright of deep neural network (DNN) models. In the existing works on this subject, researchers have intensively focused on watermarking robustness, while the concept of fidelity, which is concerned with the preservation of the model’s original functionality, has received less attention. In this paper, focusing on deep image classification models, we show that the existing shared notion of the sole measurement of learning accuracy is inadequate to characterize backdoor fidelity. Meanwhile, we show that the analogous concept of embedding distortion in multimedia watermarking, interpreted as the total weight loss (TWL) in DNN backdoor watermarking, is also problematic for fidelity measurement. To address this challenge, we propose the concept of deep fidelity, which states that the backdoor watermarked DNN model should preserve both the feature representation and decision boundary of the unwatermarked host model. To achieve deep fidelity, we propose two loss functions termed penultimate feature loss (PFL) and softmax probability-distribution loss (SPL) to preserve feature representation, while the decision boundary is preserved by the proposed fix last layer (FixLL) treatment, inspired by the recent discovery that deep learning with a fixed classifier causes no loss of learning accuracy. With the above designs, both embedding from scratch and fine-tuning strategies are implemented to evaluate the deep fidelity of backdoor embedding, whose advantages over the existing methods are verified via experiments using ResNet18 for MNIST and CIFAR-10 classifications, and wide residual network (i.e., WRN28_10) for CIFAR-100 task. PyTorch codes are available at https://github.com/ghua-ac/dnn_watermark.
Keywords: Deep fidelity; Backdoor watermarking; Backdoor fidelity; Deep learning security; Neural network watermarking; Intellectual property protection; Ownership verification

Megan McIntyre,
Equitable writing classrooms and programs in the shadow of AI,
Computers and Composition,
Volume 75,
2025,
102908,
ISSN 8755-4615,
https://doi.org/10.1016/j.compcom.2024.102908.
(https://www.sciencedirect.com/science/article/pii/S8755461524000847)
Abstract: Each year, in TA orientation, in the practicum course, and in professional development sessions, I ask TAs and instructors to consider what is, for me, the key question at the heart of our work as writing teachers: what do we owe our students? And a related and equally important question: what do we owe ourselves? In 2024, just over two years into the public existence of OpenAI's ChatGPT, the contexts for these questions are perhaps more complicated than ever, but I think the answers are mostly the same: we owe our students equitable classrooms, space to try and to fail, compassion and care, and authentic engagement. We owe them the rights our discipline affirmed almost fifty years ago when CCCC adopted Students’ Right to Their Own Language as the official position of the largest organization of writing teachers in the world. This article reviews an approach to the current Generative AI moment that is rooted in these commitments and reflects an approach I call “informed refusal,” which allows us to acknowledge the existence of generative AI without requiring students to use generative AI products. We can continue to teach critical literacies and attend to the things that make first-year writing classrooms unique, especially our attention to individualized feedback on students’ writing and our attention to helping students build self-efficacy via sustainable writing processes and reflective habits of mind. At the same time, I argue against the adoption of detectors and other writing surveillance technologies because of the ways that such tools reinforce overly simplistic notions of plagiarism (Moore-Howard) and can harm our relationships with students.
Keywords: Writing programs; Equitable writing pedagogy; Writing assessment; Plagiarism; Generative AI

Leif Sundberg, Jonny Holmström,
Innovating by prompting: How to facilitate innovation in the age of generative AI,
Business Horizons,
Volume 67, Issue 5,
2024,
Pages 561-570,
ISSN 0007-6813,
https://doi.org/10.1016/j.bushor.2024.04.014.
(https://www.sciencedirect.com/science/article/pii/S0007681324000594)
Abstract: This article focuses on how recent advances in artificial intelligence (AI), particularly chatbots based on large language models (LLMs), such as ChatGPT, can be used for innovation purposes. The article begins with a brief overview of the development and characteristics of generative AI (GenAI). Elaborating on the implications of GenAI, we provide examples to demonstrate four mechanisms of LLMs: translation, summarization, classification, and amplification. These mechanisms inform a framework that highlights how LLMs enable the creation of innovative solutions for organizations through capacities in two dimensions: context awareness and content awareness. The strength of LLMs lies in the combination of capacities in both these dimensions, which enables them to comprehend and amplify content. Four managerial suggestions are presented, ranging from starting out with small-scale projects and data exploration, to scaling through integration efforts and educating prompt engineers. By presenting the framework, recommendations, and examples of use cases in various contexts, the article contributes to the emerging literature on GenAI and innovation.
Keywords: Prompt engineering; Iterative prompting; ChatGPT; Generative AI; AI and innovation; Large language models

Yong Nam Gwon, Jae Heon Kim, Hyun Soo Chung, Eun Jee Jung, Joey Chun, Serin Lee, Sung Ryul Shim,
The Use of Generative AI for Scientific Literature Searches for Systematic Reviews: ChatGPT and Microsoft Bing AI Performance Evaluation,
JMIR Medical Informatics,
Volume 12,
2024,
,
ISSN 2291-9694,
https://doi.org/10.2196/51187.
(https://www.sciencedirect.com/science/article/pii/S2291969424000528)
Abstract: Background
A large language model is a type of artificial intelligence (AI) model that opens up great possibilities for health care practice, research, and education, although scholars have emphasized the need to proactively address the issue of unvalidated and inaccurate information regarding its use. One of the best-known large language models is ChatGPT (OpenAI). It is believed to be of great help to medical research, as it facilitates more efficient data set analysis, code generation, and literature review, allowing researchers to focus on experimental design as well as drug discovery and development.
Objective
This study aims to explore the potential of ChatGPT as a real-time literature search tool for systematic reviews and clinical decision support systems, to enhance their efficiency and accuracy in health care settings.
Methods
The search results of a published systematic review by human experts on the treatment of Peyronie disease were selected as a benchmark, and the literature search formula of the study was applied to ChatGPT and Microsoft Bing AI as a comparison to human researchers. Peyronie disease typically presents with discomfort, curvature, or deformity of the penis in association with palpable plaques and erectile dysfunction. To evaluate the quality of individual studies derived from AI answers, we created a structured rating system based on bibliographic information related to the publications. We classified its answers into 4 grades if the title existed: A, B, C, and F. No grade was given for a fake title or no answer.
Results
From ChatGPT, 7 (0.5%) out of 1287 identified studies were directly relevant, whereas Bing AI resulted in 19 (40%) relevant studies out of 48, compared to the human benchmark of 24 studies. In the qualitative evaluation, ChatGPT had 7 grade A, 18 grade B, 167 grade C, and 211 grade F studies, and Bing AI had 19 grade A and 28 grade C studies.
Conclusions
This is the first study to compare AI and conventional human systematic review methods as a real-time literature collection tool for evidence-based medicine. The results suggest that the use of ChatGPT as a tool for real-time evidence generation is not yet accurate and feasible. Therefore, researchers should be cautious about using such AI. The limitations of this study using the generative pre-trained transformer model are that the search for research topics was not diverse and that it did not prevent the hallucination of generative AI. However, this study will serve as a standard for future studies by providing an index to verify the reliability and consistency of generative AI from a user’s point of view. If the reliability and consistency of AI literature search services are verified, then the use of these technologies will help medical research greatly.
Keywords: artificial intelligence; search engine; systematic review; evidence-based medicine; ChatGPT; language model; education; tool; clinical decision support system; decision support; support; treatment

Lucinda McKnight, Cara Shipp,
“Just a tool”? Troubling language and power in generative AI writing,
English Teaching: Practice & Critique,
Volume 23, Issue 1,
2024,
Pages 23-35,
ISSN 1175-8708,
https://doi.org/10.1108/ETPC-08-2023-0092.
(https://www.sciencedirect.com/science/article/pii/S117587082400013X)
Abstract: Purpose
The purpose of this paper is to share findings from empirically driven conceptual research into the implications for English teachers of understanding generative AI as a “tool” for writing.
Design/methodology/approach
The paper reports early findings from an Australian National Survey of English teachers and interrogates the notion of the AI writer as “tool” through intersectional feminist discursive-material analysis of the metaphorical entailments of the term.
Findings
Through this work, the authors have developed the concept of “coloniser tool-thinking” and juxtaposed it with First Nations and feminist understandings of “tools” and “objects” to demonstrate risks to the pursuit of social and planetary justice through understanding generative AI as a tool for English teachers and students.
Originality/value
Bringing together white and First Nations English researchers in dialogue, the paper contributes a unique perspective to challenge widespread and common-sense use of “tool” for generative AI services.
Keywords: Pedagogy; Metaphor; Discourse analysis; Curriculum; Social justice; Feminism; Artificial intelligence; Professional learning; Writing; Generative AI writing tools

Carl Preiksaitis, Christian Rose,
Opportunities, Challenges, and Future Directions of Generative Artificial Intelligence in Medical Education: Scoping Review,
JMIR Medical Education,
Volume 9,
2023,
,
ISSN 2369-3762,
https://doi.org/10.2196/48785.
(https://www.sciencedirect.com/science/article/pii/S2369376223000697)
Abstract: Background
Generative artificial intelligence (AI) technologies are increasingly being utilized across various fields, with considerable interest and concern regarding their potential application in medical education. These technologies, such as Chat GPT and Bard, can generate new content and have a wide range of possible applications.
Objective
This study aimed to synthesize the potential opportunities and limitations of generative AI in medical education. It sought to identify prevalent themes within recent literature regarding potential applications and challenges of generative AI in medical education and use these to guide future areas for exploration.
Methods
We conducted a scoping review, following the framework by Arksey and O'Malley, of English language articles published from 2022 onward that discussed generative AI in the context of medical education. A literature search was performed using PubMed, Web of Science, and Google Scholar databases. We screened articles for inclusion, extracted data from relevant studies, and completed a quantitative and qualitative synthesis of the data.
Results
Thematic analysis revealed diverse potential applications for generative AI in medical education, including self-directed learning, simulation scenarios, and writing assistance. However, the literature also highlighted significant challenges, such as issues with academic integrity, data accuracy, and potential detriments to learning. Based on these themes and the current state of the literature, we propose the following 3 key areas for investigation: developing learners’ skills to evaluate AI critically, rethinking assessment methodology, and studying human-AI interactions.
Conclusions
The integration of generative AI in medical education presents exciting opportunities, alongside considerable challenges. There is a need to develop new skills and competencies related to AI as well as thoughtful, nuanced approaches to examine the growing use of generative AI in medical education.
Keywords: medical education; artificial intelligence; ChatGPT; Bard; AI; educator; scoping; review; learner; generative

Zhikai Zhou, Dewen Liu, Zhongjie Chen, Martin Pancho,
Government adoption of generative artificial intelligence and ambidextrous innovation,
International Review of Economics & Finance,
Volume 98,
2025,
103953,
ISSN 1059-0560,
https://doi.org/10.1016/j.iref.2025.103953.
(https://www.sciencedirect.com/science/article/pii/S1059056025001169)
Abstract: Every information technological revolution has brought about new possibilities for governmental organizational innovation, and the rapid development of Generative artificial intelligence (Gen-AI) is poised to profoundly impact government governance models and public service supply methods. Understanding the factors influencing government adoption of Gen-AI, and analyzing the impact of such adoption on governmental organizational innovation behavior, have emerged as urgent and cutting-edge topics. Based on the Technology-Organization-Environment (TOE) framework and the ambidextrous organization theory, this study systematically analyzes the three-layered driving factors that influence government organizations' adoption of Gen-AI, and examines the impact of Gen-AI on exploratory and exploitative innovation within government organizations. Furthermore, it delves into the influence mechanisms of technology adoption on different innovation behaviors from the meso-institutional and micro-implementation perspectives. At the theoretical level, this study constructs a conceptual framework for understanding the adoption of Gen-AI technology, extends the application scope of the TOE theory and enhances its explanatory power, while also providing new insights into the complexity of technology-enabled organizational innovation. At the practical level, it offers a more strategic perspective and profound implications for government organizations to maintain innovative vitality and achieve sustainable development amidst the wave of intelligent transformation.
Keywords: Generative artificial intelligence; TOE framework; Technology adoption; Organizational ambidextrous innovation

Xin Zhao, Andrew Cox, Xuanning Chen,
The use of generative AI by students with disabilities in higher education,
The Internet and Higher Education,
Volume 66,
2025,
101014,
ISSN 1096-7516,
https://doi.org/10.1016/j.iheduc.2025.101014.
(https://www.sciencedirect.com/science/article/pii/S1096751625000235)
Abstract: The use of generative AI is controversial in education largely because of its potential impact on academic integrity. Yet some scholars have suggested it could be particularly beneficial for students with disabilities. To date there has been no empirical research to discover how these students use generative AI in academic writing. Informed by a prior interview study and AI-literacy model, we surveyed students regarding their use of generative AI, and gained 124 valid responses from students with disabilities. We identified primary conditions affecting writing such as ADHD, dyslexia, dyspraxia, and autism. The main generative AI used were chatbots, particularly ChatGPT, and rewriting applications. They were used in a wide range of academic writing tasks. Key concerns students with disabilities had included the inaccuracy of AI answers, risks to academic integrity, and subscription cost barriers. Students expressed a strong desire to participate in AI policymaking and for universities to provide generative AI training. The paper concludes with recommendations to address educational disparities and foster inclusivity.
Keywords: ChatGPT; Artificial intelligence (AI); Generative AI; Students with disabilities; Academic writing; AI literacy

Usharani Hareesh Govindarajan, Amy J.C. Trappey, Charles V. Trappey,
Intelligent collaborative patent mining using excessive topic generation,
Advanced Engineering Informatics,
Volume 42,
2019,
100955,
ISSN 1474-0346,
https://doi.org/10.1016/j.aei.2019.100955.
(https://www.sciencedirect.com/science/article/pii/S1474034619300126)
Abstract: An inevitable consequence of the technology-driven economy has led to the increased importance of intellectual property protection through patents. Recent global pro-patenting shifts have further resulted in high technology overlaps. Technology components are now spread across a huge corpus of patent documents making its interpretation a knowledge-intensive engineering activity. Intelligent collaborative patent mining facilitates the integration of inputs from patented technology components held by diverse stakeholders. Topic generative models are powerful natural language tools used to decompose data corpus topics and associated word bag distributions. This research develops and validates a superior text mining methodology, called Excessive Topic Generation (ETG), as a preprocessing framework for topic analysis and visualization. The presented ETG methodology adapts the topic generation characteristics from Latent Dirichlet Allocation (LDA) with added capability to generate word distance relationships among key terms. The novel ETG approach is used as the core process for intelligent collaborative patent mining. A case study of 741 global Industrial Immersive Technology (IIT) patents covering inventive and novel concepts of Virtual Reality (VR), Augmented Reality (AR), and Brain Machine Interface (BMI) are systematically processed and analyzed using the proposed methodology. Based on the discovered topics of the IIT patents, patent classification (IPC/CPC) predictions are analyzed to validate the superior ETG results.
Keywords: Technology mining; Excessive topic generation; Industrial immersive patenting; Patent data visualization

Weiping Ding, Mohamed Abdel-Basset, Ahmed M. Ali, Nour Moustafa,
Large language models for cyber resilience: A comprehensive review, challenges, and future perspectives,
Applied Soft Computing,
Volume 170,
2025,
112663,
ISSN 1568-4946,
https://doi.org/10.1016/j.asoc.2024.112663.
(https://www.sciencedirect.com/science/article/pii/S1568494624014376)
Abstract: Interconnect cyber system is used by various users and organizations worldwide to perform different activities. These activities are combined with digital information and systems around the organizations to obtain higher accuracy and performance. However, these combinations of activities have faced cyber threats and attacks by single or multiple attackers. So, protecting and saving users' and organizations' sensitive data is a big challenge. So, the cyber resilience concept refers to the ability to prepare, absorb, recover, and adapt against cyberattacks and threats. It is used to mitigate cyberattacks and risks by the ability of the system to recover from threats. Artificial intelligence models enhance cyber resilience using machine learning and deep learning models. One of the most common components of artificial intelligence is large language models (LLM). It is used to understand language from text data and extract features to predict future words or missing in text datasets. LLM can enhance cyber resilience by providing various benefits for users and organizations. We divide the cyber resilience strategies into five parts. We review the LLM in each part, including security posture, data privacy and protection, security awareness, network security, and security automation. The fundamentals of LLMs are introduced as pre-trained models, transformers, encoders, and decoders. Then, we review the challenges of LLM in cyber resilience and cyber defense methods to overcome these challenges. We applied the LLM into three case studies including two for email spam text classifications and one for cyber threat detection. We obtained higher accuracy including 96.67 %, 90.70 %, and 89.94 % from three case studies respectively. Then we compared our LLM with other traditional machine learning models. The results show the LLM has higher accuracy, precision, recall, and f1 score compared with other models. Finally, the future directions of LLM in cyber resilience are provided.
Keywords: Large Language Model; Cyber Resilience; Cyber Security; Data Privacy and Protection; Network and Endpoint Security

Wojtek Buczynski, Felix Steffek, Mateja Jamnik, Fabio Cuzzolin, Barbara Sahakian,
Future themes in regulating artificial intelligence in investment management,
Computer Law & Security Review,
Volume 56,
2025,
106111,
ISSN 2212-473X,
https://doi.org/10.1016/j.clsr.2025.106111.
(https://www.sciencedirect.com/science/article/pii/S0267364925000068)
Abstract: We are witnessing the emergence of the “first generation” of AI and AI-adjacent soft and hard laws such as the EU AI Act or South Korea's Basic Act on AI. In parallel, existing industry regulations, such as GDPR, MIFID II or SM&CR, are being “retrofitted” and reinterpreted from the perspective of AI. In this paper we identify and analyze ten novel, “second generation” themes which are likely to become regulatory considerations in the near future: non-personal data, managerial accountability, robo-advisory, generative AI, privacy enhancing techniques (PETs), profiling, emergent behaviours, smart contracts, ESG and algorithm management. The themes have been identified on the basis of ongoing developments in AI, existing regulations and industry discussions. Prior to making any new regulatory recommendations we explore whether novel issues can be solved by existing regulations. The contribution of this paper is a comprehensive picture of emerging regulatory considerations for AI in investment management, as well as broader financial services, and the ways they might be addressed by regulations – future or existing ones.
Keywords: AI; Artificial intellogence; Investments; investment management; Finance; Financial services; Regulations; law; Laws; Regulation

Yingxuan Fu, Hing Kai Chan, Zhao Cai,
Generative artificial intelligence in operations,
Reference Module in Social Sciences,
Elsevier,
2024,
,
ISBN 9780443157851,
https://doi.org/10.1016/B978-0-443-28993-4.00057-3.
(https://www.sciencedirect.com/science/article/pii/B9780443289934000573)
Abstract: The rise of generative artificial intelligence (AI) may present a significant opportunity for a profound revolution in operations and supply chain management. However, such technological advancement is accompanied by a scholarly discourse that navigates the balance between its promising abilities and challenges. This chapter provides an overview of generative AI in operations and supply chain management. It begins by expositing its fundamental technical concepts and role alongside existing AI technologies. Subsequently, it delves into potential applications and challenges in implementing generative AI in operations. A future research agenda and takeaways for practitioners and Operations Management (OM) researchers are proposed at the end.
Keywords: Generative AI; ChatGPT; Generative adversarial network; Large language model; Transformer; Variational autoencoder

Huajie Chen, Chi Liu, Tianqing Zhu, Wanlei Zhou,
When deep learning meets watermarking: A survey of application, attacks and defenses,
Computer Standards & Interfaces,
Volume 89,
2024,
103830,
ISSN 0920-5489,
https://doi.org/10.1016/j.csi.2023.103830.
(https://www.sciencedirect.com/science/article/pii/S0920548923001113)
Abstract: Deep learning has been used to address various problems in a range of domains within both academia and industry. However, the issue of intellectual property with deep learning models has aroused broad attention. Watermarking, a proactive defense approach widely adopted to safeguard the copyright of digital content, is now sparking novel mechanisms for protecting the intellectual property of deep learning models. Further, significantly improved digital watermarking techniques have been developed to protect multimedia content, primarily images, with high efficiency and effectiveness. Yet, our current understandings of these two technical forefronts, i.e., deep learning model watermarking and image watermarking via deep learning, are unilaterally separated and application-oriented. To this end, we have undertaken a survey on emerging watermarking mechanisms in the two areas from a novel security perspective. That is, we have surveyed attacks and defenses in deep learning model watermarking and deep-learning-based image watermarking. Within the survey, we propose an objective taxonomy to unify the two domains, revealing their commonly shared properties with reference to design principles, functionalities, etc. Upon the taxonomy, a comprehensive analysis of attacks and defenses associated with the shared properties in both domains is presented. We have summarized the collected methods from a technical aspect and their advantages vs. disadvantages. A discussion of the joint characteristics and possible improvements of the methods are attached. Lastly, we have also proposed several potential research directions to inspire more ideas in these areas.
Keywords: Model watermarking; Deep steganography; Deep learning; Security and privacy

Shaokun Fan, Noyan Ilk, Akhil Kumar, Ruiyun Xu, J. Leon Zhao,
Blockchain as a trust machine: From disillusionment to enlightenment in the era of generative AI,
Decision Support Systems,
Volume 182,
2024,
114251,
ISSN 0167-9236,
https://doi.org/10.1016/j.dss.2024.114251.
(https://www.sciencedirect.com/science/article/pii/S0167923624000848)
Abstract: Since the Economist magazine heralded blockchain as “the trust machine” in 2015, the blockchain paradigm has experienced crests and falls, including a recent phase of disillusionment due to its failure to meet the high expectations, e.g., to revolutionize record keeping, data management, and workflow, envisioned during its early history. However, despite the waning interest in this technology in some quarters, its deployment has become ever more essential in areas such as decentralized finance (DeFi), Non-fungible Tokens (NFTs), and other application domains beyond cryptocurrencies. In particular, recent advancements in Artificial Intelligence (AI) surrounding Large Language Models (LLM) offer new opportunities for blockchain adoption where trust and reliability become critical. As the blockchain technology transitions from a stage of disillusionment to one of enlightenment, anticipation is building for its mainstream adoption, with focused endeavors towards removing adoption barriers across diverse business contexts, exemplified by studies included in this special issue on Blockchain Technology and Applications. In this paper, we first survey the current state of the blockchain technology and then highlight its potential for enhancing trust and accountability in emerging phenomena such as AI generated content (AIGC). We conclude by introducing the papers included in the special issue.
Keywords: Blockchain; Decentralized finance; Research directions; Trust; Accountability

Antonio Cordella, Francesco Gualdi,
Regulating generative AI: The limits of technology-neutral regulatory frameworks. Insights from Italy's intervention on ChatGPT,
Government Information Quarterly,
Volume 41, Issue 4,
2024,
101982,
ISSN 0740-624X,
https://doi.org/10.1016/j.giq.2024.101982.
(https://www.sciencedirect.com/science/article/pii/S0740624X24000741)
Abstract: Existing literature has predominantly concentrated on the legal, ethical, governance, political, and socioeconomic aspects of AI regulation, often relegating the technological dimension to the periphery, reflecting the design, use, and development of AI regulatory frameworks that are technology-neutral. The emergence and widespread use of generative AI models present new challenges for public regulators aiming at implementing effective regulatory interventions. Generative AI operates on distinctive technological properties that require a comprehensive understanding prior to the deployment of pertinent regulation. This paper focuses on the recent case of the suspension of ChatGPT in Italy to explore the impact the specific technological fabric of generative AI has on the effectiveness of technology-neutral regulation. By drawing on the findings of an exploratory case study, this paper contributes to the understanding of the tensions between the specific technological features of generative AI and the effectiveness of a technology-neutral regulatory framework. The paper offers relevant implications to practice arguing that until this tension is effectively addressed, public regulatory interventions are likely to underachieve their intended objectives.
Keywords: Artificial intelligence; Generative AI; Regulation; Technology-neutral regulatory framework; ChatGPT

Hoon Ko, Marek R. Ogiela,
Security Strategy of Digital Medical Contents Based on Blockchain in Generative AI Model,
Computers, Materials and Continua,
Volume 82, Issue 1,
2025,
Pages 259-278,
ISSN 1546-2218,
https://doi.org/10.32604/cmc.2024.057257.
(https://www.sciencedirect.com/science/article/pii/S1546221825000426)
Abstract: This study presents an innovative approach to enhancing the security of visual medical data in the generative AI environment through the integration of blockchain technology. By combining the strengths of blockchain and generative AI, the research team aimed to address the timely challenge of safeguarding visual medical content. The participating researchers conducted a comprehensive analysis, examining the vulnerabilities of medical AI services, personal information protection issues, and overall security weaknesses. This multifaceted exploration led to an in-depth evaluation of the model’s performance and security. Notably, the correlation between accuracy, detection rate, and error rate was scrutinized. This analysis revealed insights into the model’s strengths and limitations, while the consideration of standard deviation shed light on the model’s stability and performance variability. The study proposed practical improvements, emphasizing the reduction of false negatives to enhance detection rate and leveraging blockchain technology to ensure visual data integrity in medical applications. Applying blockchain to generative AI-created medical content addresses key personal information protection issues. By utilizing the distributed ledger system of blockchain, the research team aimed to protect the privacy and integrity of medical data especially medical images. This approach not only enhances security but also enables transparent and tamper-proof record-keeping. Additionally, the use of generative AI models ensures the creation of novel medical content without compromising personal information, further safeguarding patient privacy. In conclusion, this study showcases the potential of blockchain-based solutions in the medical field, particularly in securing sensitive medical data and protecting patient privacy. The proposed approach, combining blockchain and generative AI, offers a promising direction toward more robust and secure medical content management. Further research and advancements in this area will undoubtedly contribute to the development of robust and privacy-preserving healthcare systems, and visual diagnostic systems.
Keywords: Digital medical content; medical diagnostic visualization; security analysis; generative AI; blockchain; vulnerability; pattern recognition

Jialei Jiang,
When generative artificial intelligence meets multimodal composition: Rethinking the composition process through an AI-assisted design project,
Computers and Composition,
Volume 74,
2024,
102883,
ISSN 8755-4615,
https://doi.org/10.1016/j.compcom.2024.102883.
(https://www.sciencedirect.com/science/article/pii/S8755461524000598)
Abstract: This study explores the integration of generative artificial intelligence (GenAI) design technologies, including Adobe Firefly and DALL·E, into the teaching and learning of multimodal composition. Through focus group discussions and case studies, this paper demonstrates the potential of GenAI in reshaping the various stages of the composition process, including invention, designing, and revising. The findings reveal that GenAI technologies have the potential to enhance students’ multimodal composition practices and offer alternative solutions to the wicked problems encountered during the design process. Specifically, GenAI facilitates invention by offering design inspirations and enriches designing by expanding, removing, and editing the student-produced design contents. The students in this study also shared their critical stance on the revision process by modifying and iterating their designs after their uses of GenAI. Through showcasing both the opportunities and challenges of GenAI technologies, this paper contributes to the ongoing scholarly conversations on multimodal composition and pedagogy. Moreover, the paper offers implications for the future research and teaching of GenAI-assisted multimodal composition projects, with the aim of encouraging thoughtful integration of GenAI technologies to foster critical AI literacy among college composition students.
Keywords: Generative artificial intelligence; Multimodal composition process; Adobe Firefly; DALL·E; Wicked problems; Design; Writing studies

Nader Salari, Mahan Beiromvand, Amin Hosseinian-Far, Javad Habibi, Fateme Babajani, Masoud Mohammadi,
Impacts of generative artificial intelligence on the future of labor market: A systematic review,
Computers in Human Behavior Reports,
Volume 18,
2025,
100652,
ISSN 2451-9588,
https://doi.org/10.1016/j.chbr.2025.100652.
(https://www.sciencedirect.com/science/article/pii/S2451958825000673)
Abstract: Background
Generative AI (GenAI) has the ability to autonomously collect and process data to generate contents, inform decisions, solve problems, and perform tasks that typically require human reasoning. This Systematic Review is conducted to examine the impacts of GenAI on the future of employment, focusing on concerns about rising unemployment, and the positive and negative perspectives outlined within exiting studies. The findings from this review can help identify research gaps, guide organizational planning, and improve AI governance frameworks and policies.
Methods
To identify relevant studies, the PubMed, Scopus, Web of Science, Embase, ScienceDirect and Google Scholar databases and repositories were systematically searched using the keywords: ‘Future of work’, ‘Job market’, ‘Generative AI’, ‘Generative AI’, and ‘ChatGPT’. Additionally, the reference lists of the identified related articles were reviewed for grey literature.
Results
Following the PRISMA guidelines, a total of 14 articles were selected for analysis. Selected studies have examined the positive and negative viewpoints on GenAI, together with pertinent challenges and opportunities. Accordingly, GenAI, when compliant with security and ethical issues, has the potential to increase efficiency whilst reducing costs and time.
Conclusion
Considering the rapid growth and adoption of AI technologies, examining the impacts of GenAI on the future of labor market is crucial. GenAI is likely to create new roles in some sectors yet reduce opportunities in others. A nuanced assessment of the impacts, and ongoing monitoring are vital for effective preparation and adaptation to the evolving work landscape in the presence of advanced AI technologies.
Keywords: Job market; AI; ChatGPT; Labor market; GenAI

Wei Wang, Zhenping Xie,
A contracted container-based code component collaboration model with reusable but invisible right management,
Information Processing & Management,
Volume 62, Issue 3,
2025,
104057,
ISSN 0306-4573,
https://doi.org/10.1016/j.ipm.2024.104057.
(https://www.sciencedirect.com/science/article/pii/S0306457324004163)
Abstract: Existing methodologies for the code collaboration are facing challenges such as leakage of privacy data and insecure centralized management. To alleviate these challenges, we propose a contracted container-based code component collaboration model. Firstly, a specific blockchain is developed as a computational network that supports the storage and verification of sensitive data, which integrates an off-chain file system and encryption algorithms to realize the traceability and version control of privacy data. Secondly, the containerization and smart contract technology are introduced to enhance access control and the secure management and collaboration of code components. Thirdly, a protocol is designed to ensure the high reliability of cross-node information communication. Finally, the results of the prototype experiment demonstrate that the model effectively resists common attacks, meets critical security criteria, and maintains stable performance in the collaboration process. Moreover, compared to the state-of-the-art research, our model implements more valuable functionality characteristics, design goals and security attributes in the code component collaboration practice.
Keywords: Code collaboration; Right management; Blockchain; Smart contract; Containerization; Privacy protection

Jyoti Kumari, Shalini Pandey, Krishna Kant Jangde, Palanirajan Vijayaraj Kumar, Dinesh Kumar Mishra,
Evolution, integration, and challenges of 3D printing in pharmaceutical applications: A comprehensive review,
Bioprinting,
Volume 44,
2024,
e00367,
ISSN 2405-8866,
https://doi.org/10.1016/j.bprint.2024.e00367.
(https://www.sciencedirect.com/science/article/pii/S2405886624000393)
Abstract: Three-dimensional (3D) printing involves fabricating objects from digital designs by sequentially layering materials along the X, Y, and Z axes. Although this technology has existed since the 1960s, its adoption in the pharmaceutical industry remains limited. This review examines the evolution of 3D printing and its emerging significance in pharmaceuticals. The technique offers numerous advantages, such as product customization, cost-effectiveness, and efficient material usage. Several methods—such as inkjet printing, extrusion printing, and beam-based printing—are employed, utilizing materials ranging from lactose and hydroxypropyl methylcellulose to bioinks like chitosan and hyaluronic acid. Among these techniques, fused deposition modelling (FDM) is particularly noteworthy for its versatility in both biodegradable and non-biodegradable applications. Advances in 3D printing have paved the way for innovative pharmaceutical uses, including the production of complex oral dosage forms, drug delivery systems, and medical devices such as prosthetics. More recent breakthroughs have extended into bioprinting, organ-on-a-chip technologies, and robotics. However, several challenges hinder broader adoption, including limited compatibility with thermosensitive materials, difficulties in scaling production, and maintaining quality control. Additionally, the lack of standardized regulatory and ethical frameworks for clinical approval complicates progress. This review explores the key 3D printing techniques, materials, and trends relevant to pharmaceuticals, while addressing resource constraints, intellectual property issues, and regulatory hurdles. It concludes by identifying future directions for research and development, emphasizing the need to optimize these technologies for widespread pharmaceutical applications.
Keywords: 3D printing; Fused deposition modelling; Biodegradable filaments; Personalized medicine; Bioprinting

Joseph Lau,
(Let's) playing by the rules: A choice of law rule for communication of copyright material from video games to the public, through Let's Plays,
Computer Law & Security Review,
Volume 49,
2023,
105828,
ISSN 2212-473X,
https://doi.org/10.1016/j.clsr.2023.105828.
(https://www.sciencedirect.com/science/article/pii/S0267364923000389)
Abstract: Several proposals have been made regarding a choice of law rule for ‘ubiquitous infringements’ (the unauthorised dissemination of copyright material online) but none have been implemented by national courts, which continue to struggle with the issue of what law determines whether ubiquitous infringements have occurred. This article explores fresh solutions to that issue, focusing on the scenario where copyright material from video games is communicated to the public, through its inclusion in Let's Plays (playthroughs of video games streamed from platforms like YouTube), or where such use of that material, under the terms of a license, is contemplated. In this scenario, the issue of infringement should be governed by the law of the place of the video game developer's incorporation, as a proxy for laws qualifying as the lex loci protectionis (law of the country where protection is sought (Fawcett & Torremans (2011)), abbreviated as the LLP). Where any party can prove specific differences between the law of the place of the developer's incorporation and a law qualifying as the LLP (called State A's law for ease of reference), in aspects essential for deciding whether infringement has occurred, the forum court must issue separate rulings as to whether (i) the claimant's copyrights under State A's laws have been infringed; and (ii) the claimant's copyrights under laws besides those of State A have been infringed. Courts should also adopt, as a mandatory rule of their domestic law, a rule precluding de facto infringements of copyrights in video games and/or their constituent elements from giving rise to liability for infringement.
Keywords: Copyright; Internet; Let's Plays; Video games; Private international law

Chen Fu, Qiuchen Chen,
The future of pharmaceuticals: Artificial intelligence in drug discovery and development,
Journal of Pharmaceutical Analysis,
2025,
101248,
ISSN 2095-1779,
https://doi.org/10.1016/j.jpha.2025.101248.
(https://www.sciencedirect.com/science/article/pii/S2095177925000656)
Abstract: Artificial Intelligence (AI) is revolutionizing traditional drug discovery and development models by seamlessly integrating data, computational power, and algorithms. This synergy enhances the efficiency, accuracy, and success rates of drug research, shortens development timelines, and reduces costs. Coupled with machine learning (ML) and deep learning (DL), AI has demonstrated significant advancements across various domains, including drug characterization, target discovery and validation, small molecule drug design, and the acceleration of clinical trials. Through molecular generation techniques, AI facilitates the creation of novel drug molecules, predicting their properties and activities, while virtual screening optimizes drug candidates. Additionally, AI enhances clinical trial efficiency by predicting outcomes, designing trials, and enabling drug repositioning. However, AI's application in drug development faces challenges, including the need for robust data-sharing mechanisms and the establishment of more comprehensive intellectual property protections for algorithms. AI-driven pharmaceutical companies must also integrate biological sciences and algorithms effectively, ensuring the successful fusion of wet and dry laboratory experiments. Despite these challenges, the potential of AI in drug development remains undeniable. As AI technology evolves and these barriers are addressed, AI-driven therapeutics are poised for a broader and more impactful future in the pharmaceutical industry.
Keywords: AI; Drugs; Research and development; Machine learning

Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, Yue Zhang,
A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly,
High-Confidence Computing,
Volume 4, Issue 2,
2024,
100211,
ISSN 2667-2952,
https://doi.org/10.1016/j.hcc.2024.100211.
(https://www.sciencedirect.com/science/article/pii/S266729522400014X)
Abstract: Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs’ potential to both bolster and jeopardize cybersecurity.
Keywords: Large Language Model (LLM); LLM security; LLM privacy; ChatGPT; LLM attacks; LLM vulnerabilities

Abebe Diro, Shahriar Kaisar, Akanksha Saini, Samar Fatima, Pham Cong Hiep, Fikadu Erba,
Workplace security and privacy implications in the GenAI age: A survey,
Journal of Information Security and Applications,
Volume 89,
2025,
103960,
ISSN 2214-2126,
https://doi.org/10.1016/j.jisa.2024.103960.
(https://www.sciencedirect.com/science/article/pii/S221421262400262X)
Abstract: Generative Artificial Intelligence (GenAI) is transforming the workplace, but its adoption introduces significant risks to data security and privacy. Recent incidents underscore the urgency of addressing these issues. This comprehensive survey investigates the implications of GenAI integration in workplaces, focusing on its impact on organizational operations and security. We analyze vulnerabilities within GenAI systems, threats they face, and repercussions of AI-driven workplace monitoring. By examining diverse attack vectors like model attacks and automated cyberattacks, we expose their potential to undermine data integrity and privacy. Unlike previous works, this survey specifically focuses on the security and privacy implications of GenAI within workplace settings, addressing issues like employee monitoring, deepfakes, and regulatory compliance. We delve into emerging threats during model training and usage phases, proposing countermeasures such as differential privacy for training data and robust authentication for access control. Additionally, we provide a comprehensive analysis of evolving regulatory frameworks governing AI tools globally. Based on our comprehensive analysis, we propose targeted recommendations for future research and policy-making to promote responsible and secure adoption of GenAI in the workplace, such as incentivizing the development of explainable AI (XAI) and establishing clear guidelines for ethical data usage. This survey equips stakeholders with a comprehensive understanding of GenAI’s complex workplace landscape, empowering them to harness its benefits responsibly while mitigating risks.
Keywords: Cybersecurity; Generative AI; ChatGPT; Bard; Privacy; Security; Ethics

Zhen Ling Teo, Chrystie Wan Ning Quek, Joy Le Yi Wong, Daniel Shu Wei Ting,
Cybersecurity in the generative artificial intelligence era,
Asia-Pacific Journal of Ophthalmology,
Volume 13, Issue 4,
2024,
100091,
ISSN 2162-0989,
https://doi.org/10.1016/j.apjo.2024.100091.
(https://www.sciencedirect.com/science/article/pii/S2162098924000926)
Abstract: Generative Artificial Intelligence (GenAI) are algorithms capable of generating original content. The ability of GenAI to learn and generate novel outputs alike human cognition has taken the world by storm and ushered in a new era. In this review, we explore the role of GenAI in healthcare, including clinical, operational, and research applications, and delve into the cybersecurity risks of this technology. We discuss risks such as data privacy risks, data poisoning attacks, the propagation of bias, and hallucinations. In this review, we recommend risk mitigation strategies to enhance cybersecurity in GenAI technologies and further explore the use of GenAI as a tool in itself to enhance cybersecurity across the various AI algorithms. GenAI is emerging as a pivotal catalyst across various industries including the healthcare domain. Comprehending the intricacies of this technology and its potential risks will be imperative for us to fully capitalise on the benefits that GenAI can bring.
Keywords: Generative Artificial Intelligence; ChatGPT; Cybersecurity; Privacy risks; Large language model

Benjamin Luke Moorhouse, Lucas Kohnke,
The effects of generative AI on initial language teacher education: The perceptions of teacher educators,
System,
Volume 122,
2024,
103290,
ISSN 0346-251X,
https://doi.org/10.1016/j.system.2024.103290.
(https://www.sciencedirect.com/science/article/pii/S0346251X24000721)
Abstract: Since the public release of ChatGPT in November 2022, generative AI tools—capable of creating human-like content such as audio, code, images, text, simulations, 3D objects, and videos—have gained significant attention. While the impact of these tools on language teaching and learning has been widely speculated, the perspective of language teacher educators concerning their influence on initial language teacher education (ILTE) remains unexplored. This study investigates how teacher educators, who play a crucial role in adapting ILTE to technological advancements, perceive the effects of generative AI tools on ILTE. Data were collected through in-depth interviews with thirteen English language teacher educators from all four Hong Kong government-funded universities offering ILTE. Findings reveal that participants believe generative AI tools will substantially affect the ILTE curriculum, instruction, and assessment. However, most participants believed they lacked the confidence and competence to address the implications of generative AI tools effectively. This study highlights the need for further research and training to support teacher educators in adapting ILTE to the emerging influence of generative AI.
Keywords: Generative AI; Initial language teacher education; ChatGPT; Teacher educators

Timothy R. Hannigan, Ian P. McCarthy, André Spicer,
Beware of botshit: How to manage the epistemic risks of generative chatbots,
Business Horizons,
Volume 67, Issue 5,
2024,
Pages 471-486,
ISSN 0007-6813,
https://doi.org/10.1016/j.bushor.2024.03.001.
(https://www.sciencedirect.com/science/article/pii/S0007681324000272)
Abstract: Advances in large language model (LLM) technology enable chatbots to generate and analyze content for our work. Generative chatbots do this work by predicting responses rather than knowing the meaning of their responses. In other words, chatbots can produce coherent-sounding but inaccurate or fabricated content, referred to as hallucinations. When humans uncritically use this untruthful content, it becomes what we call botshit. This article focuses on how to use chatbots for content generation work while mitigating the epistemic (i.e., the process of producing knowledge) risks associated with botshit. Drawing on risk management research, we introduce a typology framework that orients how chatbots can be used based on two dimensions: response veracity verifiability and response veracity importance. The framework identifies four modes of chatbot work (authenticated, autonomous, automated, and augmented) with a botshit-related risk (ignorance, miscalibration, routinization, and black boxing). We describe and illustrate each mode and offer advice to help chatbot users guard against the botshit risks that come with each mode.
Keywords: Chatbots; Bullshit; Botshit; Artificial intelligence; Natural language processing

Stanislav Ivanov, Mohammad Soliman, Aarni Tuomi, Nasser Alhamar Alkathiri, Alamir N. Al-Alawi,
Drivers of generative AI adoption in higher education through the lens of the Theory of Planned Behaviour,
Technology in Society,
Volume 77,
2024,
102521,
ISSN 0160-791X,
https://doi.org/10.1016/j.techsoc.2024.102521.
(https://www.sciencedirect.com/science/article/pii/S0160791X24000691)
Abstract: Drawing on the Theory of Planned Behaviour (TPB), this study investigates the relationship between the perceived benefits, strengths, weaknesses, and risks of generative AI (GenAI) tools and the fundamental factors of the TPB model (i.e., attitude, subjective norms, and perceived behavioural control). The study also investigates the structural association between the TPB variables and intention to use GenAI tools, and how the latter might affect the actual usage of GenAI tools in higher education. The paper adopts a quantitative approach, relying on an anonymous self-administered online questionnaire to gather primary data from 130 lecturers and 168 students in higher education institutions (HEIs) in several countries, and PLS-SEM for data analysis. The results indicate that although lecturers' and students' perceptions of the risks and weaknesses of GenAI tools differ, the perceived strengths and advantages of GenAI technologies have a significant and positive impact on their attitudes, subjective norms, and perceived behavioural control. The TPB core variables positively and significantly impact lecturers' and students’ intentions to use GenAI tools, which in turn significantly and positively impact their adoption of such tools. This paper advances theory by outlining the factors shaping the adoption of GenAI technologies in HEIs. It provides stakeholders with a variety of managerial and policy implications for how to formulate suitable rules and regulations to utilise the advantages of these tools while mitigating the impacts of their disadvantages. Limitations and future research opportunities are also outlined.
Keywords: Generative AI; Theory of planned behaviour; Higher education

Raluca Bunduchi, Dan-Andrei Sitar-Tăut, Daniel Mican,
A legitimacy-based explanation for user acceptance of controversial technologies: The case of Generative AI,
Technological Forecasting and Social Change,
Volume 215,
2025,
124095,
ISSN 0040-1625,
https://doi.org/10.1016/j.techfore.2025.124095.
(https://www.sciencedirect.com/science/article/pii/S004016252500126X)
Abstract: Controversial technologies are technologies where social concerns play a disproportionate role in shaping the public attitudes to their adoption. An example of such controversial technologies is Generative Artificial Intelligence (GenAI), whose rapid diffusion is fuelled by expectations for significant performance improvements, while also facing concerns at individual (trust in technology), technology (accuracy and quality), and institutional (cultural, ethical and regulatory) level. Individual and technology factors are well accounted for by rational choice-based models which underpin most technology acceptance research. Such models are less suited to explore the role of institutional factors in shaping technology acceptance. Drawing from legitimacy and technology lifecycle research, we develop a legitimacy-based model of GenAI adoption which accounts for the institutional context in which technology use happens, and for technology characteristics, namely its maturity, in shaping users' acceptance. Surveying 483 information systems students who are GenAI users, we find that users' perceptions of technology uncertainty and variation positively affect their technology legitimacy evaluations and that their pragmatic and cognitive legitimacy evaluations, but not moral, affect their intention to use. We answer recent calls to examine alternative theoretical predictors of technology acceptance, and to consider the role of context in examining the acceptance of controversial technologies.
Keywords: Technology acceptance; Legitimacy; Technology uncertainty; Technology variation

Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato, Luciano Floridi,
Generative AI in EU law: Liability, privacy, intellectual property, and cybersecurity,
Computer Law & Security Review,
Volume 55,
2024,
106066,
ISSN 2212-473X,
https://doi.org/10.1016/j.clsr.2024.106066.
(https://www.sciencedirect.com/science/article/pii/S0267364924001328)
Abstract: The complexity and emergent autonomy of Generative AI systems introduce challenges in predictability and legal compliance. This paper analyses some of the legal and regulatory implications of such challenges in the European Union context, focusing on four areas: liability, privacy, intellectual property, and cybersecurity. It examines the adequacy of the existing and proposed EU legislation, including the Artificial Intelligence Act (AIA), in addressing the challenges posed by Generative AI in general and LLMs in particular. The paper identifies potential gaps and shortcomings in the EU legislative framework and proposes recommendations to ensure the safe and compliant deployment of generative models.
Keywords: Generative AI; EU law; Liability; Privacy; Intellectual property; Cybersecurity

Perry Collins,
Copyright,
Editor(s): David Baker, Lucy Ellis,
Encyclopedia of Libraries, Librarianship, and Information Science (First Edition),
Academic Press,
2025,
Pages 81-91,
ISBN 9780323956901,
https://doi.org/10.1016/B978-0-323-95689-5.00053-5.
(https://www.sciencedirect.com/science/article/pii/B9780323956895000535)
Abstract: Understanding the basics of copyright law is a crucial step toward understanding a broader information landscape that depends on the creation, dissemination, and reuse of cultural material. Despite its complexity, copyright is ubiquitous, and people around the world gain and share copyright every day as they write emails, record videos, and create new works of art and scholarship. With a focus on areas most applicable to library and information science professionals, this entry briefly considers a range of rights-related topics, including general information as well as more specialized starting points for inquiry. By examining areas such as copyright exceptions, digital libraries, open licensing, and contracts, the entry also reflects how copyright has permeated scholarly communication and impacts decision making around knowledge sharing on a global scale. Throughout, discussion considers how copyright law has been shaped, contested, and interpreted over time.
Keywords: Artificial intelligence; Berne Convention; Digital libraries; Fair dealing; Fair use; History of copyright; Infringement; Intellectual property; Plagiarism; Public domain; Traditional Cultural Expressions; Translation

Anna Guillaumet,
The power of generative AI for CRIS systems: a new paradigm for scientific information management,
Procedia Computer Science,
Volume 249,
2024,
Pages 131-149,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2024.11.057.
(https://www.sciencedirect.com/science/article/pii/S187705092403268X)
Abstract: The paper analyses the implications of the emergence of artificial intelligence (AI), especially generative AI, on current research information systems (CRIS). It reviews the recent European regulations for high-risk AI systems, the Spanish AI strategy, and the IntelComp project as use cases. The study found that the maturity of CRIS systems, coupled with the increasing complexity due to data aggregation, sets the stage for innovative AI applications. The paper proposes key domains where AI can impact and be applied in CRIS, including data management, research assessment, and advanced analytics. It also provides examples of how generative AI can be leveraged to enhance scientific information management within CRIS. The findings highlight the need to ensure the responsible and ethical development of AI technologies in the research domain.
Keywords: CRIS; AI; GenerativeAI; euroCRIS; FECYT; DRIS; OpenAccess; Research; CERIF; FAIR; AI-Act; ENIA; Sandbox; Law; Regulations; Standards; ethics

Fiammetta Marulli, Pierluigi Paganini, Fabio Lancellotti,
The Three Sides of the Moon LLMs in Cybersecurity: Guardians, Enablers and Targets,
Procedia Computer Science,
Volume 246,
2024,
Pages 5340-5348,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2024.09.653.
(https://www.sciencedirect.com/science/article/pii/S187705092402708X)
Abstract: Large Language Models (LLMs) are rapidly evolving, demonstrating impressive capabilities in multimedia objects generation, ranging from text and image generation from scratch to programming code and efficient conversational agents. From the perspective of cyber-security challenges, LLMs and cyber-security are in a controversial relationship: it can be observed that LLMs, as a type of AI, play a mainfold role: that of security guardians, that of security breachs ”unaware” enablers and that of victims of cyber attacks. In fact, LLMs are able to enhance security of several tasks and applications but they are also attractive for malicious users to be exploited as means to perform novel attacks and, finally they represent challenging assets for targeting attacks. In this work, we discuss this mainfold key reading by providing a brief landscape of both the current defence applications of LLMs against cyber attacks and the currently known LLMs security vulnerabilities along with potential cyber-attacks targeting and involving LLMs. The final aim of study is intended to provide a guideline to further explore specific cyber-security scenarios involving LLMs.
Keywords: Large Language Models; Cyber-security; Generative Models Vulnerabilities; Prompt Injection

Anne Håkansson, Gloria Phillips-Wren,
Generative AI and Large Language Models - Benefits, Drawbacks, Future and Recommendations,
Procedia Computer Science,
Volume 246,
2024,
Pages 5458-5468,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2024.09.689.
(https://www.sciencedirect.com/science/article/pii/S1877050924027492)
Abstract: Natural language processing, with parsing and generation, has a long tradition. Parsing has been easier to perform than a generation but with generative artificial intelligence (a.k.a Gen AI) and large language models (abbr. LLMs), this has changed. Generative artificial intelligence is a type of artificial intelligence that uses a large data set to create something in the genre of that data set. It can generate different outputs ranging from texts, audio, objects, pictures, and paintings to videos, but also synthetic data. LLMs use deep learning and deep neural networks to train on large text corpora for recognizing and generating texts. These models are based on massive data sets, collected from databases and the web. They use transformer models to detect how elements in sequences relate to each other. This provides context support. Two well-known large language models are the Generative Pre-trained Transformer, GPT, used in ChatGPT and Bidirectional Encoder Representations from Transformers, BERT. Although LLMs have advantages, they have problems. This paper presents generative artificial intelligence and LLMs with benefits and drawbacks. Results from applying these models have shown that they can work well for accuracy in specificity, user personalization and human-computer communication but they may not provide acceptable, reliable and truthful results. For example, ethics, hallucinations and incorrect information, or misjudgments, are some major problems. The paper ends with future directions, research questions on LLMs, and recommendations.
Keywords: Natural Language Processing; Generative AI; Large Language Models

Twinkle Tyagi, Amit Kumar Singh,
Deep learning models security: A systematic review,
Computers and Electrical Engineering,
Volume 120, Part B,
2024,
109792,
ISSN 0045-7906,
https://doi.org/10.1016/j.compeleceng.2024.109792.
(https://www.sciencedirect.com/science/article/pii/S0045790624007195)
Abstract: Deep learning models and the digital records they generate have remarkably increased their adoption of many practical applications. While the success of deep learning in multimedia applications, especially images, helps tackle some of the most challenging problems, one of its copyright violations, ownership conflict, poses a grave concern for many potential applications. Many works on intellectual property protection for such models have proposed to verify ownership. Therefore, it is necessary to conduct a comprehensive study on the security of deep learning models to evaluate their strong background, state-of-the-art solutions, possible attacks, current limitations and notable improvements. This survey attempts to systematically discuss and summarise the recent advanced security solutions for deep learning models through watermarking, encryption and fingerprinting. Our study explores the recent applications, possible attacks, current limitations and notable suggestions regarding deep learning. It also comprehensively evaluates the recent research gaps and opportunities in detail to empower researchers and practitioners to provide additional secure solutions for deep learning models. This extensive survey is the first to consider model security through several notable techniques.
Keywords: Deep learning; Security; Privacy; Watermarking; Encryption; Fingerprint

Xiyu Zhou, Peng Liang, Beiqi Zhang, Zengyang Li, Aakash Ahmad, Mojtaba Shahin, Muhammad Waseem,
Exploring the problems, their causes and solutions of AI pair programming: A study on GitHub and Stack Overflow,
Journal of Systems and Software,
Volume 219,
2025,
112204,
ISSN 0164-1212,
https://doi.org/10.1016/j.jss.2024.112204.
(https://www.sciencedirect.com/science/article/pii/S0164121224002486)
Abstract: With the recent advancement of Artificial Intelligence (AI) and Large Language Models (LLMs), AI-based code generation tools become a practical solution for software development. GitHub Copilot, the AI pair programmer, utilizes machine learning models trained on a large corpus of code snippets to generate code suggestions using natural language processing. Despite its popularity in software development, there is limited empirical evidence on the actual experiences of practitioners who work with Copilot. To this end, we conducted an empirical study to understand the problems that practitioners face when using Copilot, as well as their underlying causes and potential solutions. We collected data from 473 GitHub issues, 706 GitHub discussions, and 142 Stack Overflow posts. Our results reveal that (1) Operation Issue and Compatibility Issue are the most common problems faced by Copilot users, (2) Copilot Internal Error, Network Connection Error, and Editor/IDE Compatibility Issue are identified as the most frequent causes, and (3) Bug Fixed by Copilot, Modify Configuration/Setting, and Use Suitable Version are the predominant solutions. Based on the results, we discuss the potential areas of Copilot for enhancement, and provide the implications for the Copilot users, the Copilot team, and researchers.
Keywords: GitHub copilot; GitHub issues; GitHub discussions; StackOverflow post; Problem; Cause; Solution

Erik Hermann, Stefano Puntoni,
Artificial intelligence and consumer behavior: From predictive to generative AI,
Journal of Business Research,
Volume 180,
2024,
114720,
ISSN 0148-2963,
https://doi.org/10.1016/j.jbusres.2024.114720.
(https://www.sciencedirect.com/science/article/pii/S0148296324002248)
Abstract: Since the introduction of ChatGPT, the leading example of Generative Artificial Intelligence (GenAI), the research community and the general public have been captivated by GenAI’s remarkable advances in performance, and its ability to both imitate and, in some respects, surpass human capabilities. This paper offers a comprehensive analysis of the impact of AI on consumer behavior, focusing on the two pivotal phases of AI development over the past 15 years. We start by reviewing the extensively researched, yet still growing, field of algorithmic predictions and decision-making, alongside the varied positive and negative consumer reactions it elicits. Subsequently, we delve into the just emerging field of GenAI. Here, we differentiate between Convergent Thinking GenAI, which is more domain-specific and geared towards pre-defined task completion, and Divergent Thinking GenAI, which is more domain-general and oriented towards new task fulfillment. For each of these realms, we identify key areas for future investigation.
Keywords: Artificial intelligence; Consumer behavior; Algorithms; Predictive AI; Generative AI

Marcello Mariani, Yogesh K. Dwivedi,
Generative artificial intelligence in innovation management: A preview of future research developments,
Journal of Business Research,
Volume 175,
2024,
114542,
ISSN 0148-2963,
https://doi.org/10.1016/j.jbusres.2024.114542.
(https://www.sciencedirect.com/science/article/pii/S0148296324000468)
Abstract: This study outlines the future research opportunities related to Generative Artificial Intelligence (GenAI) in innovation management. To this end, it combines a review of the academic literature with the results of a Delphi study involving leading innovation management scholars. Ten major research themes emerged that can guide future research developments at the intersection of GenAI and innovation management: 1) Gen AI and innovation types; 2) GenAI, dominant designs and technology evolution; 3) Scientific and artistic creativity and GenAI-enabled innovations; 4) GenAI-enabled innovations and intellectual property; 5) GenAI and new product development; 6) Multimodal/unimodal GenAI and innovation outcomes; 7) GenAI, agency and ecosystems; 8) Policymakers, lawmakers and anti-trust authorities in the regulation of GenAI-enabled innovation; 9) Misuse and unethical use of GenAI leading to biased innovation; and 10) Organizational design and boundaries for GenAI-enabled innovation. The paper concludes by discussing how these themes can inform theoretical development in innovation management studies.
Keywords: Generative artificial intelligence; Delphi study; Management; Innovation

Yue Li, Hongxia Wang, Mauro Barni,
A survey of Deep Neural Network watermarking techniques,
Neurocomputing,
Volume 461,
2021,
Pages 171-193,
ISSN 0925-2312,
https://doi.org/10.1016/j.neucom.2021.07.051.
(https://www.sciencedirect.com/science/article/pii/S092523122101095X)
Abstract: Protecting the Intellectual Property Rights (IPR) associated to Deep Neural Networks (DNNs) is a pressing need pushed by the high costs required to train such networks and by the importance that DNNs are gaining in our society. Following its use for Multimedia (MM) IPR protection, digital watermarking has recently been considered as a mean to protect the IPR of DNNs. While DNN watermarking inherits some basic concepts and methods from MM watermarking, there are significant differences between the two application areas, thus calling for the adaptation of media watermarking techniques to the DNN scenario and the development of completely new methods. In this paper, we overview the most recent advances in DNN watermarking, by paying attention to cast them into the bulk of watermarking theory developed during the last two decades, while at the same time highlighting the new challenges and opportunities characterising DNN watermarking. Rather than trying to present a comprehensive description of all the methods proposed so far, we introduce a new taxonomy of DNN watermarking and present a few exemplary methods belonging to each class. We hope that this paper will inspire new research in this exciting area and will help researchers to focus on the most innovative and challenging problems in the field.
Keywords: Intellectual property protection; Deep Neural Networks; Watermarking; White box vs black box watermarking; Watermarking and DNN backdoors

Susanna Aromaa, Päivi Heikkilä, Marko Jurvansuu, Selen Pehlivan, Teijo Väärä, Marko Jurmu,
Company perspectives of generative artificial intelligence in industrial work,
Procedia Computer Science,
Volume 253,
2025,
Pages 217-226,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2025.01.085.
(https://www.sciencedirect.com/science/article/pii/S1877050925000936)
Abstract: The use of artificial intelligence (AI) technologies in the manufacturing industry is rapidly increasing. During this transformation, it can be difficult to understand how AI will change the way work is done. This study explores how generative AI could change manufacturing work. Data collection was conducted using interviews and a questionnaire with seven representatives from three industrial companies. They identified several application areas for GenAI in the industrial work context, such as design, planning, training, problem solving, coding and data management. They also expressed positive attitudes but raised concerns about trust, safety, acceptability and interoperability. Changes in work were identified as being more related to cognitive aspects such as changing the way of thinking and altering the interaction with people and machines. Therefore, human-AI design efforts should focus especially on cognitive ergonomics. Findings from this study can be used in the manufacturing industry when adopting AI, as well as in identifying research topics in the human-AI research community.
Keywords: Generative artificial intelligence; industry work; manufacturing; human factors; ergonomics; company perspective

David Álvarez-Fidalgo, Francisco Ortin,
CLAVE: A deep learning model for source code authorship verification with contrastive learning and transformer encoders,
Information Processing & Management,
Volume 62, Issue 3,
2025,
104005,
ISSN 0306-4573,
https://doi.org/10.1016/j.ipm.2024.104005.
(https://www.sciencedirect.com/science/article/pii/S0306457324003649)
Abstract: Source code authorship verification involves determining whether two code fragments are written by the same programmer. It has many uses, including malware authorship analysis, copyright dispute resolution and plagiarism detection. Source code authorship verification is challenging because it must generalize to code written by programmers not included in its training data. In this paper, we present CLAVE (Contrastive Learning for Authorship Verification with Encoder representations), a novel deep learning model for source code authorship verification that leverages contrastive learning and a Transformer Encoder-based architecture. We initially pre-train CLAVE on a dataset of 270,602 Python source code files extracted from GitHub. Subsequently, we fine-tune CLAVE for authorship verification using contrastive learning on Python submissions from 61,956 distinct programmers in Google Code Jam and Kick Start competitions. This approach allows the model to learn stylometric representations of source code, enabling comparison via vector distance for authorship verification. CLAVE achieves an AUC of 0.9782, reduces the error of the state-of-the-art source code authorship verification systems by at least 23.4% and improves the AUC of cutting-edge source code LLMs by 21.9% to 40%. We also evaluate the main components of CLAVE on its AUC performance improvement: pre-training (1.8%), loss function (0.2%–2.8%), input length (0.1%–0.7%), model size (0.2%), and tokenizer (0.1%–0.7%).
Keywords: Source code authorship verification; Contrastive learning; Transformer encoder; Deep learning; Stylometric representations; Large language models; Python

Hareem Kibriya, Wazir Zada Khan, Ayesha Siddiqa, Muhammad Khurram Khan,
Privacy issues in Large Language Models: A survey,
Computers and Electrical Engineering,
Volume 120, Part A,
2024,
109698,
ISSN 0045-7906,
https://doi.org/10.1016/j.compeleceng.2024.109698.
(https://www.sciencedirect.com/science/article/pii/S0045790624006256)
Abstract: In the fast-paced world of modern technology, the development of Large Language Models (LLMs) has increased drastically. However, this growth has also increased privacy concerns associated with these models. This paper investigates privacy concerns in the existing LLMs and their far-reaching implications. The paper categorizes privacy concerns of LLMs into two main groups: those occurring during training and those during inference, both of which can contribute to re-identification risks. Through an in-depth literature analysis, we have highlighted different requirements for safeguarding user privacy when interacting with LLMs. Moreover, this paper discusses the challenges that can arise in implementing privacy-preserving mechanisms in LLMs. It examines the complex interactions between ethical issues, legal requirements, and technology developments, highlighting the need for stakeholder collaboration to traverse this challenging environment successfully. This paper contributes to the ongoing discussion on the responsible development and deployment of LLMs. It aims to open the door for ethically acceptable Artificial Intelligence innovation processes by promoting a better awareness of privacy issues.

Rana A Barghout, Zhiqing Xu, Siddharth Betala, Radhakrishnan Mahadevan,
Advances in generative modeling methods and datasets to design novel enzymes for renewable chemicals and fuels,
Current Opinion in Biotechnology,
Volume 84,
2023,
103007,
ISSN 0958-1669,
https://doi.org/10.1016/j.copbio.2023.103007.
(https://www.sciencedirect.com/science/article/pii/S0958166923001179)
Abstract: Biotechnology has revolutionized the development of sustainable energy sources by harnessing biomass as a feedstock for energy production. However, challenges such as recalcitrant feedstocks and inefficient metabolic pathways hinder the large-scale integration of renewable energy systems. Enzyme engineering has emerged as a powerful tool to address these challenges by enhancing enzyme activity, specificity, and stability. Generative machine learning (ML) models have shown great promise in accelerating protein design, allowing for the generation of novel protein sequences with desired properties by navigating vast spaces. This review paper aims to summarize the state of the art in generative models for protein design and how they can be applied to bioenergy applications, including the underlying architectures and training strategies. Additionally, it highlights the importance of high-quality datasets for training and evaluating generative models, organizes available datasets for generative protein design, and discusses the potential of applying generative models to strain design for bioenergy production.

Insu Cho, Yonghan Ju,
Text mining method to identify artificial intelligence technologies for the semiconductor industry in Korea,
World Patent Information,
Volume 74,
2023,
102212,
ISSN 0172-2190,
https://doi.org/10.1016/j.wpi.2023.102212.
(https://www.sciencedirect.com/science/article/pii/S017221902300042X)
Abstract: Semiconductors are among the most important core technologies contributing to the Fourth Industrial Revolution. The United States, Taiwan, and China have been investing heavily in semiconductor research and development. To achieve international competitiveness in the semiconductor industry, Korea needs to establish a research and development (R&D) roadmap for small- and medium-sized enterprises (SMEs). Our study identified trends in the semiconductor industry by analyzing the characteristics of core technologies based on patents that disclose technologies instead of holding exclusive ownership. Specifically, we analyzed registered patents concerned with artificial intelligence and machine learning pertaining to the semiconductor industry, which are attracting considerable attention. Using the Korea Intellectual Property Rights Information Service database, we identified 3569 patent specifications related to AI technology and the semiconductor industry. The text mining and network analysis results indicated that the application of deep neural networks is the most important and affects various aspects of R&D. Particularly, AI technology is actively studied for monitoring manufacturing and etch processes. Additionally, technology convergence among virtual reality, visualization, smart factories, and etching technology was identified. The analysis results identify promising technologies related to semiconductors and provide insights that would enable SMEs in the Korean semiconductor industry to establish a technology roadmap.
Keywords: Semiconductor; Patent; International patent classification; Network analysis; Text mining

Sang-Hyeak Yoon, Sung-Byung Yang, So-Hyun Lee,
Comprehensive examination of the bright and dark sides of generative AI services: A mixed-methods approach,
Electronic Commerce Research and Applications,
Volume 70,
2025,
101491,
ISSN 1567-4223,
https://doi.org/10.1016/j.elerap.2025.101491.
(https://www.sciencedirect.com/science/article/pii/S156742232500016X)
Abstract: Recent advancements in artificial intelligence (AI), particularly in generative AI (GAI), have significantly influenced society, prompting extensive discussions about their societal impact. While previous research has acknowledged both the benefits and challenges of AI, the rapid development of GAI has often proceeded without sufficient focus on actionable strategies to address potential risks and unintended consequences. Understanding both the positive and negative aspects of GAI is essential to ensure that technological progress is balanced and responsibly managed to mitigate potential risks and societal harm. This study identifies the positive and negative aspects of GAI from both public and expert viewpoints by applying a valence framework. Using a mixed-methods approach that integrates joint sentiment topic (JST) modeling with the combined use of ChatGPT and expert interviews, we investigated the key positive and negative factors associated with GAI. By integrating the insights gained from these different perspectives, the study proposes strategies for the effective and responsible use of GAI. The study contributes to the existing body of knowledge on GAI by offering a comprehensive understanding of its implications and providing guidance for its ethical and appropriate applications.
Keywords: Generative AI; Valence framework; Mixed-methods approach; Joint sentiment topic modeling; Expert interview; ChatGPT

Constantinos Patsakis, Fran Casino, Nikolaos Lykousas,
Assessing LLMs in malicious code deobfuscation of real-world malware campaigns,
Expert Systems with Applications,
Volume 256,
2024,
124912,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2024.124912.
(https://www.sciencedirect.com/science/article/pii/S0957417424017792)
Abstract: The integration of large language models (LLMs) into various cybersecurity pipelines has become increasingly prevalent, enabling the automation of numerous manual tasks and often surpassing human performance. Recognising this potential, cybersecurity researchers and practitioners are actively investigating the application of LLMs to process vast volumes of heterogeneous data for anomaly detection, potential bypass identification, attack mitigation, and fraud prevention. Moreover, LLMs’ advanced capabilities in generating functional code, interpreting code context, and code summarisation present significant opportunities for reverse engineering and malware deobfuscation. In this work, we comprehensively examine the deobfuscation capabilities of state-of-the-art LLMs. Specifically, we conducted a detailed evaluation of four prominent LLMs using real-world malicious scripts from the notorious Emotet malware campaign. Our findings reveal that while current LLMs are not yet perfectly accurate, they demonstrate substantial potential in efficiently deobfuscating payloads. This study highlights the importance of fine-tuning LLMs for specialised tasks, suggesting that such optimisation could pave the way for future AI-powered threat intelligence pipelines to combat obfuscated malware. Our contributions include a thorough analysis of LLM performance in malware deobfuscation, identifying strengths and limitations, and discussing the potential for integrating LLMs into cybersecurity frameworks for enhanced threat detection and mitigation. Our experiments illustrate that LLMs can automatically and accurately extract the necessary indicators of compromise from a real-world campaign with an accuracy of 69.56% and 88.78% for the URLs and the corresponding domains of the droppers, respectively.
Keywords: Malware analysis; Code deobfuscation; Large language models; Cybersecurity

Chaochao Chen, Xiaohua Feng, Yuyuan Li, Lingjuan Lyu, Jun Zhou, Xiaolin Zheng, Jianwei Yin,
Integration of large language models and federated learning,
Patterns,
Volume 5, Issue 12,
2024,
101098,
ISSN 2666-3899,
https://doi.org/10.1016/j.patter.2024.101098.
(https://www.sciencedirect.com/science/article/pii/S2666389924002708)
Abstract: Summary
As the parameter size of large language models (LLMs) continues to expand, there is an urgent need to address the scarcity of high-quality data. In response, existing research has attempted to make a breakthrough by incorporating federated learning (FL) into LLMs. Conversely, considering the outstanding performance of LLMs in task generalization, researchers have also tried applying LLMs within FL to tackle challenges in relevant domains. The complementarity between LLMs and FL has already ignited widespread research interest. In this review, we aim to deeply explore the integration of LLMs and FL. We propose a research framework dividing the fusion of LLMs and FL into three parts: the combination of LLM sub-technologies with FL, the integration of FL sub-technologies with LLMs, and the overall merger of LLMs and FL. We first provide a comprehensive review of the current state of research in the domain of LLMs combined with FL, including their typical applications, integration advantages, challenges faced, and future directions for resolution. Subsequently, we discuss the practical applications of the combination of LLMs and FL in critical scenarios such as healthcare, finance, and education and provide new perspectives and insights into future research directions for LLMs and FL.
Keywords: large language models; federated learning; data scarcity; privacy and security

Jens Peter Andersen, Lise Degn, Rachel Fishberg, Ebbe K. Graversen, Serge P.J.M. Horbach, Evanthia Kalpazidou Schmidt, Jesper W. Schneider, Mads P. Sørensen,
Generative Artificial Intelligence (GenAI) in the research process – A survey of researchers’ practices and perceptions,
Technology in Society,
Volume 81,
2025,
102813,
ISSN 0160-791X,
https://doi.org/10.1016/j.techsoc.2025.102813.
(https://www.sciencedirect.com/science/article/pii/S0160791X2500003X)
Abstract: This study explores the use of generative AI (GenAI) and research integrity assessments of use cases by researchers, including PhD students, at Danish universities. Conducted through a survey sent to all Danish researchers from January to February 2024, the study received 2534 responses and evaluated 32 GenAI use cases across five research phases: idea generation, research design, data collection, data analysis, and writing/reporting. Respondents reported on their own and colleagues' GenAI usage. They also assessed whether the practices in the use cases were considered good research practice. Through an explorative factor analysis, we identified three clusters of perception: "GenAI as a work horse", "GenAI as a language assistant only", and "GenAI as a research accelerator". The findings further show varied opinions on GenAI's research integrity implications. Language editing and data analysis were generally viewed positively, whereas experiment design and peer review tasks faced more criticism. Controversial areas included image creation/modification and synthetic data, with comments highlighting the need for critical and reflexive use of GenAI. Usage differed by main research area, with technical and quantitative sciences reporting slightly higher usage and more positive assessments. Junior researchers used GenAI more than senior colleagues, while no significant gender differences were observed. The study underscores the need for adaptable, discipline-specific guidelines for GenAI use in research, developed collaboratively with experts to align with diverse research practices and minimize ethical and practical misalignment.
Keywords: Generative Artificial Intelligence (GenAI); Research process; Research practice; use cases; Research integrity

Shahab Saquib Sohail, Faiza Farhat, Yassine Himeur, Mohammad Nadeem, Dag Øivind Madsen, Yashbir Singh, Shadi Atalla, Wathiq Mansoor,
Decoding ChatGPT: A taxonomy of existing research, current challenges, and possible future directions,
Journal of King Saud University - Computer and Information Sciences,
Volume 35, Issue 8,
2023,
101675,
ISSN 1319-1578,
https://doi.org/10.1016/j.jksuci.2023.101675.
(https://www.sciencedirect.com/science/article/pii/S131915782300229X)
Abstract: Chat Generative Pre-trained Transformer (ChatGPT) has gained significant interest and attention since its launch in November 2022. It has shown impressive performance in various domains, including passing exams and creative writing. However, challenges and concerns related to biases and trust persist. In this work, we present a comprehensive review of over 100 Scopus-indexed publications on ChatGPT, aiming to provide a taxonomy of ChatGPT research and explore its applications. We critically analyze the existing literature, identifying common approaches employed in the studies. Additionally, we investigate diverse application areas where ChatGPT has found utility, such as healthcare, marketing and financial services, software engineering, academic and scientific writing, research and education, environmental science, and natural language processing. Through examining these applications, we gain valuable insights into the potential of ChatGPT in addressing real-world challenges. We also discuss crucial issues related to ChatGPT, including biases and trustworthiness, emphasizing the need for further research and development in these areas. Furthermore, we identify potential future directions for ChatGPT research, proposing solutions to current challenges and speculating on expected advancements. By fully leveraging the capabilities of ChatGPT, we can unlock its potential across various domains, leading to advancements in conversational AI and transformative impacts in society.
Keywords: ChatGPT; Large language models (LLMs); Generative Pre-trained Transformer (GPT); AI Generated Content (AIGC); Systematic review; Trustworthy AI

Chenlong Zhang, Senlin Luo, Jiawei Li, Limin Pan, Chuan Lu,
Self-enhancing defense for protecting against model stealing attacks on deep learning systems,
Expert Systems with Applications,
Volume 269,
2025,
126438,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2025.126438.
(https://www.sciencedirect.com/science/article/pii/S0957417425000600)
Abstract: Defending against model stealing (MS) is crucial for safeguarding intellectual property and the security of deep learning applications. Current countermeasures, however, have notable shortcomings. First, defense strategies reliant on distribution classification often fail to accurately identify attack samples with semantic and visual similarities, thereby reducing their effectiveness. Second, the method of leveraging query samples from unknown origins to bolster defense capability in application scenarios remains an unresolved yet critical issue. This paper presents SED (Self-Enhancing Model Stealing Defense Method), an innovative defense method against model stealing. SED incorporates a deep hashing model and introduces a novel Penalty-Weighted Hamming (PWH) distance for sample segmentation, which effectively overcomes the drawbacks of traditional distribution-based classification. Subsequently, SED employs dynamic temperature scaling and label flipping to realize defense. Moreover, SED maintains an archive of historical query samples and utilizes a greedy algorithm to construct a database of malicious samples, thereby improving defense tactics for future queries similar to those catalogued. Experimental results confirm that SED substantially diminishes the accuracy of the attackers’ substitute models and effectively utilizes historical data for self-enhancement.
Keywords: Model stealing defense; Self-enhancing method; Model stealing attack; Security and privacy; Deep hash

Mengshu Liu, Ju’e Guo, Dan Bi,
Comparison of administrative and regulatory green technologies development between China and the U.S. based on patent analysis,
Data Science and Management,
Volume 6, Issue 1,
2023,
Pages 34-45,
ISSN 2666-7649,
https://doi.org/10.1016/j.dsm.2023.01.001.
(https://www.sciencedirect.com/science/article/pii/S2666764923000012)
Abstract: With the increasing importance of computer intelligence in the new round of the industrial revolution, administrative, regulatory, or design (ARD) green technology contributes to improving national technological competitiveness and promoting the transformation of green technology, which is becoming an important field under sustainable development goals. The U.S. and China ranked top two in terms of paper influence and patent applications in the field of ARD green technology. However, few comparative studies have been conducted in these two countries. This study presents the evolution and landscapes of ARD green technology between China and the U.S., focusing on comparing development priorities and technical layouts in each five-year plan period. According to the “International Patent Classification (IPC) Green Inventory” launched by the World Intellectual Property Organization (WIPO), we retrieved 69,412 patents published between 2001 and 2020 from the PatSnap database. Descriptive, content, and thematic network analyses were conducted using latent dirichlet allocation (LDA) and community detection algorithms. The results show that both China and the U.S. strategically focus on ARD green technology development. The technical topics in this field can be divided into three themes: data processing systems, traffic control systems, and building designs. The emphasis on technology research and development (R&D) differs between China and the U.S. There is also evidence that the U.S. has advantages in terms of technological innovation and capabilities. However, China has an advantage in terms of data volume, and the gap between China and the U.S. is gradually narrowing. We also highlight the contributions and limitations of this study.
Keywords: Administrative, regulatory or design (ARD); Green technologies; Patent analysis; Text mining; Latent dirichlet allocation (LDA); Topic evolution

Amit Gangwal, Azim Ansari, Iqrar Ahmad, Abul Kalam Azad, Wan Mohd Azizi Wan Sulaiman,
Current strategies to address data scarcity in artificial intelligence-based drug discovery: A comprehensive review,
Computers in Biology and Medicine,
Volume 179,
2024,
108734,
ISSN 0010-4825,
https://doi.org/10.1016/j.compbiomed.2024.108734.
(https://www.sciencedirect.com/science/article/pii/S0010482524008199)
Abstract: Artificial intelligence (AI) has played a vital role in computer-aided drug design (CADD). This development has been further accelerated with the increasing use of machine learning (ML), mainly deep learning (DL), and computing hardware and software advancements. As a result, initial doubts about the application of AI in drug discovery have been dispelled, leading to significant benefits in medicinal chemistry. At the same time, it is crucial to recognize that AI is still in its infancy and faces a few limitations that need to be addressed to harness its full potential in drug discovery. Some notable limitations are insufficient, unlabeled, and non-uniform data, the resemblance of some AI-generated molecules with existing molecules, unavailability of inadequate benchmarks, intellectual property rights (IPRs) related hurdles in data sharing, poor understanding of biology, focus on proxy data and ligands, lack of holistic methods to represent input (molecular structures) to prevent pre-processing of input molecules (feature engineering), etc. The major component in AI infrastructure is input data, as most of the successes of AI-driven efforts to improve drug discovery depend on the quality and quantity of data, used to train and test AI algorithms, besides a few other factors. Additionally, data-gulping DL approaches, without sufficient data, may collapse to live up to their promise. Current literature suggests a few methods, to certain extent, effectively handle low data for better output from the AI models in the context of drug discovery. These are transferring learning (TL), active learning (AL), single or one-shot learning (OSL), multi-task learning (MTL), data augmentation (DA), data synthesis (DS), etc. One different method, which enables sharing of proprietary data on a common platform (without compromising data privacy) to train ML model, is federated learning (FL). In this review, we compare and discuss these methods, their recent applications, and limitations while modeling small molecule data to get the improved output of AI methods in drug discovery. Article also sums up some other novel methods to handle inadequate data.
Keywords: Artificial intelligence federated learning; Drug discovery; Machine learning; Data privacy; Deep learning; Active learning; Transfer learning; One-shot learning; Data augmentation; Multi-task learning; Data synthesis

Doraid Dalalah, Osama M.A. Dalalah,
The false positives and false negatives of generative AI detection tools in education and academic research: The case of ChatGPT,
The International Journal of Management Education,
Volume 21, Issue 2,
2023,
100822,
ISSN 1472-8117,
https://doi.org/10.1016/j.ijme.2023.100822.
(https://www.sciencedirect.com/science/article/pii/S1472811723000605)
Abstract: Generative Pre-trained Transformers like ChatGPT are examples of AI systems which produce human-like responses in different forms such as text or images that have demonstrated excellent performance in producing logical and contextually relevant answers. However, the false positive/negative detection of generative AI has been noted as a challenge. In this article, statistical experiments are conducted to test the chances of false positive and false negative detection of AI-generated text. It was found that the detected likelihoods of generative AI in articles’ abstracts is much lower than that found in paragraphs taken from the literature section of the selected articles. This means that literature parts have higher likelihoods to falsely demonstrate AI-generated text. On the other hand, when genuine texts are compared with AI-generated texts, it is observed that there is a noticeable margin of overlap between their distributions and therefore type I and type II errors fall within the realm of possibility. We show that despite these challenges, generative AI like ChatGPT continues to be a promising tool for communication and information retrieval. However, it is vital to address the concerns regarding false detection of AI generated text and ensure that these models are used in ethical and responsible conduct.
Keywords: Artificial intelligence; Generative pre-trained transformer; Machine generated contents (MGC); False positive/negative; Text generation; ChatGPT

Yin Liu, Kijin An, Eli Tilevich,
RT-Trust: Automated refactoring for different trusted execution environments under real-time constraints,
Journal of Computer Languages,
Volume 56,
2020,
100939,
ISSN 2590-1184,
https://doi.org/10.1016/j.cola.2019.100939.
(https://www.sciencedirect.com/science/article/pii/S2590118419300644)
Abstract: Real-time systems must meet strict timeliness requirements. These systems also often need to protect their critical program information (CPI) from adversarial interference and intellectual property theft. Trusted execution environments (TEE) execute CPI tasks on a special-purpose processor, thus providing hardware protection. However, adapting a system written to execute in environments without TEE requires partitioning the code into untrusted and trusted parts. This process involves complex manual program transformations that are not only laborious and intellectually tiresome, but also hard to validate and verify adherence to real-time constraints. To address these problems, this paper presents novel program analyses and transformation techniques, accessible to the developer via a declarative meta-programming model. The developer declaratively specifies the CPI portion of the system. A custom static analysis checks CPI specifications for validity, while probe-based profiling helps identify whether the transformed system would continue to meet the original real-time constraints, with a feedback loop suggesting how to modify the code, so its CPI can be isolated. Finally, an automated refactoring isolates the CPI portion for TEE-based execution, communicated with through generated calls to the TEE API. The reference implementation of our approach profiles and transforms real-time systems to isolate their CPI functions to execute on two different TEE platforms: OP-TEE and SGX. Although these platforms substantially differ in terms of their respective APIs and performance characteristics, our refactoring completely hides these differences from the developer by automatically synthesizing the correct CPI functionality required for these dissimilar TEE implementations. We have evaluated our approach by successfully enabling the trusted execution of the CPI portions of several microbenchmarks and a drone autopilot. Our approach shows the promise of declarative meta-programming in reducing the programmer effort required to adapt systems for trusted execution under real-time constraints.
Keywords: Trusted execution; Real-time systems; Declarative meta-programming; Software refactoring; Program analyses

Yang Liu, Jiuyu Dong, Liang Mei, Rui Shen,
Digital innovation and performance of manufacturing firms: An affordance perspective,
Technovation,
Volume 119,
2023,
102458,
ISSN 0166-4972,
https://doi.org/10.1016/j.technovation.2022.102458.
(https://www.sciencedirect.com/science/article/pii/S0166497222000050)
Abstract: We explore the underlying mechanisms and institutional conditions of profiting from digital innovation in the context of manufacturing firms. Building on affordance theory, we propose that digital innovations positively affect manufacturing firms’ performance via innovation speed and operational efficiency due to the affordance of digital technology. Moreover, the interactions between digital and institutional affordances suggest that the intellectual property rights (IPR) protection system (i.e. IPR regime and its enforcement) negatively moderates the relationship between digital innovation adoption and innovation speed, as well as operational efficiency. Results from a longitudinal sample of Chinese listed firms support our hypotheses. Our findings contribute to the emerging literature on profiting from digital innovation and provide managerial implications for manufacturing firms in emerging markets.
Keywords: Digital innovation; Profit from innovation; Intellectual property right; Chinese manufacturing firm

Swati Sachan, Xi Liu (Lisa),
Blockchain-based auditing of legal decisions supported by explainable AI and generative AI tools,
Engineering Applications of Artificial Intelligence,
Volume 129,
2024,
107666,
ISSN 0952-1976,
https://doi.org/10.1016/j.engappai.2023.107666.
(https://www.sciencedirect.com/science/article/pii/S095219762301850X)
Abstract: Generative AI tools powered by Large Language Models (LLMs) have demonstrated advanced capabilities in understanding and articulating legal facts closer to the level of legal practitioners. However, scholars hold contrasting views on the reliability of the reasoning behind a decision derived from LLMs due to its black-box nature. Law firms are vigilant in recognizing the potential risks of violating confidentiality and inappropriate exposure of sensitive legal data through the prompt sent to Generative AI. This research attempts to find an equilibrium between responsible usage and control of human legal professionals over content produced by Generative AI through regular audits. It investigates the potential of Generative AI in drafting correspondence for pre-litigation decisions derived from an eXplainable AI (XAI) algorithm. This research presents an end-to-end process of designing the architecture and methodology for a blockchain-based auditing system. It detects unauthorized alterations of data repositories containing the decisions by an XAI model and automated textual explanation by Generative AI. The automated auditing by blockchain facilitates responsible usage of AI technologies and reduces discrepancies in tracing the accountability of adversarial decisions. It conceptualizes the two algorithms. First, strategic on-chain (within blockchain) and off-chain (outside blockchain) data storage in compliance with the data protection laws and critical requirements of stakeholders in a legal firm. Second, auditing by comparison of the unique signature as Merkle roots of files stored off-chain with their immutable blockchain counterpart. A case study on liability cases under tort law demonstrates the system implementation results.
Keywords: Legal; Law; Explainable AI; Blockchain; Generative AI; Responsible AI

Rui Xu, Zhong Wang,
Generative artificial intelligence in healthcare from the perspective of digital media: Applications, opportunities and challenges,
Heliyon,
Volume 10, Issue 12,
2024,
e32364,
ISSN 2405-8440,
https://doi.org/10.1016/j.heliyon.2024.e32364.
(https://www.sciencedirect.com/science/article/pii/S2405844024083956)
Abstract: Introduction
The emergence and application of generative artificial intelligence/large language models (hereafter GenAI LLMs) have the potential for significant impact on the healthcare industry. However, there is currently a lack of systematic research on GenAI LLMs in healthcare based on reliable data. This article aims to conduct an exploratory study of the application of GenAI LLMs (i.e., ChatGPT) in healthcare from the perspective of digital media (i.e., online news), including the application scenarios, potential opportunities, and challenges.
Methods
This research used thematic qualitative text analysis in five steps: firstly, developing main topical categories based on relevant articles; secondly, encoding the search keywords using these categories; thirdly, conducting searches for news articles via Google ; fourthly, encoding the sub-categories using the elaborate category system; and finally, conducting category-based analysis and presenting the results. Natural language processing techniques, including the TermRaider and AntConc tool, were applied in the aforementioned steps to assist in text qualitative analysis. Additionally, this study built a framework, using for analyzing the above three topics, from the perspective of five different stakeholders, including healthcare demanders and providers.
Results
This study summarizes 26 applications (e.g., provide medical advice, provide diagnosis and triage recommendations, provide mental health support, etc.), 21 opportunities (e.g., make healthcare more accessible, reduce healthcare costs, improve patients care, etc.), and 17 challenges (e.g., generate inaccurate/misleading/wrong answers, raise privacy concerns, lack of transparency, etc.), and analyzes the reasons for the formation of these key items and the links between the three research topics.
Conclusions
The application of GenAI LLMs in healthcare is primarily focused on transforming the way healthcare demanders access medical services (i.e., making it more intelligent, refined, and humane) and optimizing the processes through which healthcare providers offer medical services (i.e., simplifying, ensuring timeliness, and reducing errors). As the application becomes more widespread and deepens, GenAI LLMs is expected to have a revolutionary impact on traditional healthcare service models, but it also inevitably raises ethical and security concerns. Furthermore, GenAI LLMs applied in healthcare is still in the initial stage, which can be accelerated from a specific healthcare field (e.g., mental health) or a specific mechanism (e.g., GenAI LLMs’ economic benefits allocation mechanism applied to healthcare) with empirical or clinical research.
Keywords: ChatGPT; Healthcare; Digital media; Applications; Opportunities; Challenges; Digital health; Generative artificial intelligence; Large language models; Artificial intelligence generated content

Rohit Gupta, Bhawana Rathore,
Exploring the generative AI adoption in service industry: A mixed-method analysis,
Journal of Retailing and Consumer Services,
Volume 81,
2024,
103997,
ISSN 0969-6989,
https://doi.org/10.1016/j.jretconser.2024.103997.
(https://www.sciencedirect.com/science/article/pii/S0969698924002935)
Abstract: In the last few years, many service organisations have been exploring the use of Generative Artificial Intelligence (GAI) tools for their businesses and upgrading their existing processes. These tools have the potential and capability to transform the business world in various aspects. However, serval service organisations are facing many challenges while adopting the GAI tools in their organisations. In a similar context, this study explores the adoption of GAI barriers through two studies by a mixed-method approach. The first study is based on YouTube datasets of selected videos where GAI adoption challenges, problems, and barriers were discussed. Further, these YouTube datasets were analysed through text mining and empirical modelling techniques. In the second study, an extensive literature review was done and critical barriers to GAI adoption were identified based on the extensive literature review. Further, these barriers were analysed through three theoretical lenses and a hybrid fuzzy multicriteria decision-making approach. In addition, the results from the first study were further matched and verified with our second study. This establishes the relevance of adopting a mixed-method approach. Our major findings are: (i) trust, anticipation, and surprise emerged as the strongest emotions of the viewers who posted their comments on the YouTube videos; (ii) Five major barriers are revealed through topic analysis of YouTube transcripts and these are ethical, technological, regulations & policies, cost, and human resources; (iii) Six major barriers are identified through second study are privacy & security, return on investment, running cost, misuse, over-reliance, and Lack of digital infrastructure.
Keywords: Generative AI; Service industry; Text mining; Topic modelling; FDM; Fuzzy AHP; Fuzzy DEMATEL

Yubin Qu, Song Huang, Peng Nie,
A review of backdoor attacks and defenses in code large language models: Implications for security measures,
Information and Software Technology,
Volume 182,
2025,
107707,
ISSN 0950-5849,
https://doi.org/10.1016/j.infsof.2025.107707.
(https://www.sciencedirect.com/science/article/pii/S0950584925000461)
Abstract: Context:
Large Language Models (LLMS) have revolutionized software engineering by bridging human language understanding and complex problem solving. However, resource constraints often lead users to rely on open-source models or third-party platforms for training and prompt engineering, introducing significant security vulnerabilities.
Objective:
This study provides a comprehensive analysis of backdoor attacks targeting LLMS in software engineering, with a particular focus on fine-tuning methods. Our work addresses a critical gap in existing literature by proposing a novel three-category framework for backdoor attacks: full-parameter fine-tuning, parameter-efficient fine-tuning, and no-tuning attacks.
Methods:
We systematically reviewed existing studies and analyzed attack success rates across different methods. Full-parameter fine-tuning generally achieves high success rates but requires significant computational resources. Parameter-efficient fine-tuning offers comparable success rates with lower resource demands, while no-tuning attacks exhibit variable success rates depending on prompt design, posing unique challenges due to their minimal resource requirements.
Results:
Our findings underscore the evolving landscape of backdoor attacks, highlighting the shift towards more resource-efficient and stealthy methods. These trends emphasize the need for advanced detection mechanisms and robust defense strategies.
Conclusion:
By focusing on code-specific threats, this study provides unique insights into securing LLMS in software engineering. Our work lays the foundation for future research on developing sophisticated defense mechanisms and understanding stealthy backdoor attacks.
Keywords: Deep neural networks; Triggers; Large language models for code; Backdoor attacks; Software engineering

Laura Egan,
Plagiarism: History, Culture, and Prevention,
Editor(s): David Baker, Lucy Ellis,
Encyclopedia of Libraries, Librarianship, and Information Science (First Edition),
Academic Press,
2025,
Pages 98-107,
ISBN 9780323956901,
https://doi.org/10.1016/B978-0-323-95689-5.00018-3.
(https://www.sciencedirect.com/science/article/pii/B9780323956895000183)
Abstract: This entry covers the origins of the word plagiarism, how it differentiates from common knowledge and copyright, and defines different types of plagiarism. It addresses how perceptions of plagiarism have changed over time, variations in how different cultures view plagiarism, and how technological advances have led to increases in plagiarism. One section focuses on plagiarism and academic integrity, providing examples of incidents and their consequences, as well as non-academic examples. Later sections identify how training and skillsets can reduce plagiarism, as well as providing a detailed list of plagiarism detection tools. Finally, the entry addresses plagiarism issues related to using artificial intelligence Large Language Models (LLMs), such as ChatGPT.
Keywords: Academic integrity; AI chatbots; Artificial intelligence; ChatGPT; Common knowledge; Contract cheating; Copyright; Cultural differences; Large language models; Plagiarism; Plagiarism consequences; Plagiarism detection tools; Self-plagiarism

Chaudhery Ghazanfar Hussain, Muhammad Qadeer, Rüstem Keçili, Chaudhery Mustansar Hussain,
12 - Additive manufacturing in the next world,
Editor(s): Shadpour Mallakpour, Chaudhery Mustansar Hussain,
In Additive Manufacturing Materials and Technologies,
Medical Additive Manufacturing,
Elsevier,
2024,
Pages 299-362,
ISBN 9780323953832,
https://doi.org/10.1016/B978-0-323-95383-2.00007-X.
(https://www.sciencedirect.com/science/article/pii/B978032395383200007X)
Abstract: Additive manufacturing (AM), also known as 3D printing, has revolutionized the manufacturing industry by enabling the creation of complex objects with intricate designs. This chapter explores the potential impact of AM in the next world, highlighting key advancements, challenges, and opportunities in various sectors. We discuss the transformative potential of additive manufacturing across industries such as healthcare, aerospace, automotive, consumer goods, and construction. Additionally, we examine the implications of AM for sustainability, supply chains, intellectual property, and societal implications. By analyzing current trends and prospects, this chapter aims to provide valuable insights into the next world's AM landscape.
Keywords: 3D printing; fused deposition modeling (FDM); selective laser sintering (SLS); stereolithography (SLA); digital light processing (DLP); PolyJet technology; multijet fusion (MJF); direct metal laser sintering (DMLS); computer-aided design (CAD); postprocessing; rapid prototyping; materials; support structures; layer height; build volume; infill; powder bed fusion (PBF); binder jetting

Diego Garcés, Matilde Santos, David Fernández-Llorca,
Leveraging language models for automated distribution of review notes in animated productions,
Neurocomputing,
Volume 626,
2025,
129620,
ISSN 0925-2312,
https://doi.org/10.1016/j.neucom.2025.129620.
(https://www.sciencedirect.com/science/article/pii/S0925231225002929)
Abstract: During the production of an animated film, professionals at the animation studio prepare thousands of notes. These notes describe improvements and corrections identified by supervisors and directors during daily meetings where the film’s progress is reviewed. After each meeting, these notes are manually distributed to the appropriate departments that need to address them. Due to the manual nature of this process, many notes are not assigned correctly, and the identified issues are not addressed, reducing the final quality of the film. This article describes and compares several approaches to automatically distribute notes using multi-label text classification with different language models (LM). Implemented methods include logistic regression models, encoder-only models such as the BERT family, and decoder-only models such as Llama 2 including fine-tuning and QLoRA techniques. Training and inference were conducted on a local RTX-3090. The results of the different techniques have been compared, achieving a maximum average accuracy of 0.83 and an f1-score of 0.89 with the fine-tuned Multilingual BERT model. This demonstrates the validity of these models for multi-label text classification, as well as their usefulness in a hitherto unexplored area such as animation studios.
Keywords: Movie production; Review notes; Text Classification; Large Language Models (LLM); Natural Language Processing

Alexander J. Carroll, Joshua Borycz,
Integrating large language models and generative artificial intelligence tools into information literacy instruction,
The Journal of Academic Librarianship,
Volume 50, Issue 4,
2024,
102899,
ISSN 0099-1333,
https://doi.org/10.1016/j.acalib.2024.102899.
(https://www.sciencedirect.com/science/article/pii/S0099133324000600)
Abstract: Generative artificial intelligence (AI) and large language models (LLMs) have induced a mixture of excitement and panic among educators. However, there is a lack of consensus over how much experience science and engineering students have with using these tools for research-related tasks. Likewise, it is not yet known how educators and information professionals can leverage these tools to teach students strategies for information retrieval and knowledge synthesis. This study assesses the extent of students' use of AI tools in research-related tasks and if information literacy instruction could impact their perception of these tools. Responses to Likert-scale questions indicate that many students did not have extensive experience using LLMs for research-related purposes prior to the information literacy sessions. However, after participating in a didactic lecture and discussion with an engineering librarian that explored how to use these tools effectively and responsibly, many students reported viewing these tools as potentially useful for future assignments. Student responses to open-response questions suggest that librarian-led information literacy training can assist students in developing more sophisticated understandings of the limitations and use cases for artificial intelligence in inquiry-based coursework.
Keywords: Generative artificial intelligence; Large language models; Information literacy; STEM education; Information retrieval; Critical thinking

Qiyang Nie, Tong Liu,
Large language models: Tools for new environmental decision-making,
Journal of Environmental Management,
Volume 375,
2025,
124373,
ISSN 0301-4797,
https://doi.org/10.1016/j.jenvman.2025.124373.
(https://www.sciencedirect.com/science/article/pii/S0301479725003494)
Abstract: This study represents the first exploration of Large Language Models (LLMs) in environmental decision-making, examining their potential benefits and limitations. To address environmental issues, we propose and compare two generalizable frameworks: an LLMs-assisted framework that leverages LLMs to augment human expertise and coding in traditional decision workflows, and an LLMs-driven framework that aims to automate optimization. Through a water engineering case study focusing on PFAS control, where Environmental Fluid Dynamics Code (EFDC) was used for water environment simulation, we illustrate how to instantiate these frameworks and assess their performance. The case study reveals generalizable insights about these frameworks. Results indicate that both frameworks can contribute to environmental decision-making optimization to varying degrees, though their applicability differs significantly when facing complex decision scenarios. The LLMs-assisted framework, which effectively regulates flow rates and achieves higher PFAS interception, demonstrates how AI can enhance human decision-making while preserving the essential role of domain expertise and professional judgment. In contrast, the LLMs-driven framework faces challenges in handling complex parameter optimization tasks due to constraints such as context window and maximum output length. The findings emphasize the advantages of integrating Artificial Intelligence (AI) with conventional environmental modeling and management practices. This work confirms a crucial principle: LLMs should enhance rather than replace human expertise, with the ultimate responsibility for environmental decisions remaining with humans. The originality of this research lies in its innovative methodological approach, which leverages process design and prompt engineering to integrate cutting-edge AI with conventional environmental models, establishing a foundation for responsible human-AI collaboration in environmental decision-making. While examining current strengths and limitations, this framework robustly generates optimized environmental decision strategies, marking a new exploration in the field.
Keywords: Large language models; GPT; PFAS; Environmental decisions; Water resources management; Multi-objective optimization

Runze Li, Yu Tian, Zhuyi Shen, Jin Li, Jun Li, Kefeng Ding, Jingsong Li,
Improving an Electronic Health Record–Based Clinical Prediction Model Under Label Deficiency: Network-Based Generative Adversarial Semisupervised Approach,
JMIR Medical Informatics,
Volume 11,
2023,
,
ISSN 2291-9694,
https://doi.org/10.2196/47862.
(https://www.sciencedirect.com/science/article/pii/S2291969423000273)
Abstract: Background
Observational biomedical studies facilitate a new strategy for large-scale electronic health record (EHR) utilization to support precision medicine. However, data label inaccessibility is an increasingly important issue in clinical prediction, despite the use of synthetic and semisupervised learning from data. Little research has aimed to uncover the underlying graphical structure of EHRs.
Objective
A network-based generative adversarial semisupervised method is proposed. The objective is to train clinical prediction models on label-deficient EHRs to achieve comparable learning performance to supervised methods.
Methods
Three public data sets and one colorectal cancer data set gathered from the Second Affiliated Hospital of Zhejiang University were selected as benchmarks. The proposed models were trained on 5% to 25% labeled data and evaluated on classification metrics against conventional semisupervised and supervised methods. The data quality, model security, and memory scalability were also evaluated.
Results
The proposed method for semisupervised classification outperforms related semisupervised methods under the same setup, with the average area under the receiver operating characteristics curve (AUC) reaching 0.945, 0.673, 0.611, and 0.588 for the four data sets, respectively, followed by graph-based semisupervised learning (0.450, 0.454, 0.425, and 0.5676, respectively) and label propagation (0.475,0.344, 0.440, and 0.477, respectively). The average classification AUCs with 10% labeled data were 0.929, 0.719, 0.652, and 0.650, respectively, comparable to that of the supervised learning methods logistic regression (0.601, 0.670, 0.731, and 0.710, respectively), support vector machines (0.733, 0.720, 0.720, and 0.721, respectively), and random forests (0.982, 0.750, 0.758, and 0.740, respectively). The concerns regarding the secondary use of data and data security are alleviated by realistic data synthesis and robust privacy preservation.
Conclusions
Training clinical prediction models on label-deficient EHRs is indispensable in data-driven research. The proposed method has great potential to exploit the intrinsic structure of EHRs and achieve comparable learning performance to supervised methods.
Keywords: semisupervised learning; generative adversarial network; network analysis; label deficiency; clinical prediction; electronic health record; EHR; clinical prediction; adversarial network; data set

Biying Fu, Abdenour Hadid, Naser Damer,
Generative AI in the context of assistive technologies: Trends, limitations and future directions,
Image and Vision Computing,
Volume 154,
2025,
105347,
ISSN 0262-8856,
https://doi.org/10.1016/j.imavis.2024.105347.
(https://www.sciencedirect.com/science/article/pii/S0262885624004529)
Abstract: With the tremendous successes of Large Language Models (LLMs) like ChatGPT for text generation and Dall-E for high-quality image generation, generative Artificial Intelligence (AI) models have shown a hype in our society. Generative AI seamlessly delved into different aspects of society ranging from economy, education, legislation, computer science, finance, and even healthcare. This article provides a comprehensive survey on the increased and promising use of generative AI in assistive technologies benefiting different parties, ranging from the assistive system developers, medical practitioners, care workforce, to the people who need the care and the comfort. Ethical concerns, biases, lack of transparency, insufficient explainability, and limited trustworthiness are major challenges when using generative AI in assistive technologies, particularly in systems that impact people directly. Key future research directions to address these issues include creating standardized rules, establishing commonly accepted evaluation metrics and benchmarks for explainability and reasoning processes, and making further advancements in understanding and reducing bias and its potential harms. Beyond showing the current trends of applying generative AI in the scope of assistive technologies in four identified key domains, which include care sectors, medical sectors, helping people in need, and co-working, the survey also discusses the current limitations and provides promising future research directions to foster better integration of generative AI in assistive technologies.
Keywords: Assistive AI; Generative AI; Generative models; Assistive systems; Assistive technologies and services

Daniel Lee, Matthew Arnold, Amit Srivastava, Katrina Plastow, Peter Strelan, Florian Ploeckl, Dimitra Lekkas, Edward Palmer,
The impact of generative AI on higher education learning and teaching: A study of educators’ perspectives,
Computers and Education: Artificial Intelligence,
Volume 6,
2024,
100221,
ISSN 2666-920X,
https://doi.org/10.1016/j.caeai.2024.100221.
(https://www.sciencedirect.com/science/article/pii/S2666920X24000225)
Abstract: In recent months, Artificial Intelligence (AI) has had, and will continue to have, a dramatic impact on Higher Education (HE). A study conducted by researchers at a leading university in Australia surveyed 30 of their teaching staff, drawn predominantly from their teaching academy, and interviewed eight of them regarding the impact of AI on HE. Data were analyzed using the procedures of Inductive Thematic Analysis and revealed a lack of any homogenous sentiment around AI in HE and much ambiguity regarding best practice regarding recent technological developments. The results indicate concerns exist around concepts relating to academic integrity, however, these concerns may be exaggerated. Almost half of the participants indicated they were using AI within their teaching roles with the most common design change being modifications to assessments. Less than a quarter of staff agreed the university has adequately equipped them for AI, and more than three quarters indicated they would like support. They unanimously assumed the technology will improve. Keeping in mind universities’ obligation to serve students by preparing them for industry, it is vitally important that the HE sector stays informed of developments in AI and commit to ongoing research and discussions regarding best practice in response to AI. However, anything regarding AI and future developments will be extremely difficult to predict.
Keywords: Artificial intelligence; Generative artificial intelligence; Higher education; ChatGPT; Learning and teaching

Ran Zhang, Hong-Wei Li, Xin-Yuan Qian, Wen-Bo Jiang, Han-Xiao Chen,
On large language models safety, security, and privacy: A survey,
Journal of Electronic Science and Technology,
Volume 23, Issue 1,
2025,
100301,
ISSN 1674-862X,
https://doi.org/10.1016/j.jnlest.2025.100301.
(https://www.sciencedirect.com/science/article/pii/S1674862X25000023)
Abstract: The integration of artificial intelligence (AI) technology, particularly large language models (LLMs), has become essential across various sectors due to their advanced language comprehension and generation capabilities. Despite their transformative impact in fields such as machine translation and intelligent dialogue systems, LLMs face significant challenges. These challenges include safety, security, and privacy concerns that undermine their trustworthiness and effectiveness, such as hallucinations, backdoor attacks, and privacy leakage. Previous works often conflated safety issues with security concerns. In contrast, our study provides clearer and more reasonable definitions for safety, security, and privacy within the context of LLMs. Building on these definitions, we provide a comprehensive overview of the vulnerabilities and defense mechanisms related to safety, security, and privacy in LLMs. Additionally, we explore the unique research challenges posed by LLMs and suggest potential avenues for future research, aiming to enhance the robustness and reliability of LLMs in the face of emerging threats.
Keywords: Large language models; Privacy issues; Safety issues; Security issues

Jian Feng, Zhenfeng Liu, Lijie Feng,
Identifying opportunities for sustainable business models in manufacturing: Application of patent analysis and generative topographic mapping,
Sustainable Production and Consumption,
Volume 27,
2021,
Pages 509-522,
ISSN 2352-5509,
https://doi.org/10.1016/j.spc.2021.01.021.
(https://www.sciencedirect.com/science/article/pii/S235255092100021X)
Abstract: Early identification of business opportunities is critical for technology-based manufacturers seeking to develop new sustainable business models (SBMs) for future competitive advantages. However, there exists an insufficiency of identifying business opportunities compared to previous studies which have focused mainly on technology opportunities and service opportunities. To fill this research gap, this study proposes a new systematic approach to identify business opportunities for new SBMs based on information relating to the manufacturers' technologies and patents. To illustrate, an example in the mining machinery industry was examined as a case study. The results demonstrated that 255 patent documents relating to the product were collected. Next, latent Dirichlet allocation was used to generate 26 business topics, which were categorized into the 9 building blocks of the business model canvas (BMC). Then, generative topographic mapping (GTM) was applied to identify 13 vacuums and related technology-driven business opportunities on the basis of BMC-based patent-business vectors. Finally, dynamic business modelling was conducted, which integrated sustainable BMCs and system dynamics in order to evaluate and rank these business opportunities. The proposed approach can promote consensus building between the technology and business planning departments on developing technology-driven SBMs in both public and private sectors.
Keywords: sustainable business model; business opportunity identification; patent analysis; business model canvas; generative topographic mapping; system dynamics; manufacturing

Anam Nazir, Ze Wang,
A comprehensive survey of ChatGPT: Advancements, applications, prospects, and challenges,
Meta-Radiology,
Volume 1, Issue 2,
2023,
100022,
ISSN 2950-1628,
https://doi.org/10.1016/j.metrad.2023.100022.
(https://www.sciencedirect.com/science/article/pii/S295016282300022X)
Abstract: Large Language Models (LLMs) especially when combined with Generative Pre-trained Transformers (GPT) represent a groundbreaking in natural language processing. In particular, ChatGPT, a state-of-the-art conversational language model with a user-friendly interface, has garnered substantial attention owing to its remarkable capability for generating human-like responses across a variety of conversational scenarios. This survey offers an overview of ChatGPT, delving into its inception, evolution, and key technology. We summarize the fundamental principles that underpin ChatGPT, encompassing its introduction in conjunction with GPT and LLMs. We also highlight the specific characteristics of GPT models with details of their impressive language understanding and generation capabilities. We then summarize applications of ChatGPT in a few representative domains. In parallel to the many advantages that ChatGPT can provide, we discuss the limitations and challenges along with potential mitigation strategies. Despite various controversial arguments and ethical concerns, ChatGPT has drawn significant attention from research industries and academia in a very short period. The survey concludes with an envision of promising avenues for future research in the field of ChatGPT. It is worth noting that knowing and addressing the challenges faced by ChatGPT will mount the way for more reliable and trustworthy conversational agents in the years to come.
Keywords: Large Language Models (LLMs); Generative Pre-trained Transformers (GPT); Natural language processing (NLP); Contextual learning; Trustworthy conversational agents; Human-computer interaction; ChatGPT

Feifeng Jiang, Jun Ma, Christopher John Webster, Alain J.F. Chiaradia, Yulun Zhou, Zhan Zhao, Xiaohu Zhang,
Generative urban design: A systematic review on problem formulation, design generation, and decision-making,
Progress in Planning,
Volume 180,
2024,
100795,
ISSN 0305-9006,
https://doi.org/10.1016/j.progress.2023.100795.
(https://www.sciencedirect.com/science/article/pii/S0305900623000569)
Abstract: Urban design is the process of designing and shaping the physical forms of cities, towns, and suburbs. It involves the arrangement and design of street systems, groups of buildings, public spaces, and landscapes, to make the urban environment performative and sustainable. The typical design process, reliant on manual work and expert experience has unavoidable low efficiency in generating high-performing design solutions due to the involvement of complex social, institutional, and economic contexts and the trade-off between conflicting preferences of different stakeholder groups. Taking advantage of artificial intelligence (AI) and computational capacity, generative urban design (GUD) has been developed as a trending technical direction to narrow the gaps and produce design solutions with high efficiency at early design stages. It uses computer-aided generative methods, such as evolutionary optimization and deep generative models, to efficiently explore complex solution spaces and automatically generate design options that satisfy conflicting objectives and various constraints. GUD experiments have attracted much attention from academia, practitioners, and public authorities in recent years. However, a systematic review of the current stage of GUD research is lacking. This study, therefore, reports on a systematic investigation of the existing literature according to the three key stages in the GUD process: (1) design problem formulation, (2) design option generation, and (3) decision-making. For each stage, current trends, findings, and limitations from GUD studies are examined. Future directions and potential challenges are discussed and presented. The review is highly interdisciplinary and involves articles from urban study, computer science, social science, management, and other fields. It reports what scholars have found in GUD experiments and organizes a diverse and complicated technical agenda into something accessible to all stakeholders. The results and discoveries will serve as a holistic reference for GUD developers and users in both academia and industry and form a baseline for the field of GUD development in the coming years.
Keywords: Generative urban design; Urban form generation; Generative method; AI-generated content (AIGC); Generative AI; Human-machine collaboration

Yanhui Zhang, Haolong Pei, Shihan Zhen, Qian Li, Fengchao Liang,
Chat Generative Pre-Trained Transformer (ChatGPT) usage in healthcare,
Gastroenterology & Endoscopy,
Volume 1, Issue 3,
2023,
Pages 139-143,
ISSN 2949-7523,
https://doi.org/10.1016/j.gande.2023.07.002.
(https://www.sciencedirect.com/science/article/pii/S2949752323000353)
Abstract: In recent years, with the rapid development of deep learning, artificial intelligence (AI) has gradually become a powerful assistant for humans in various aspects. The maturity and advancement of natural language models led to the birth of Chat Generative Pre-Trained Transformer (ChatGPT), generating great potential for healthcare applications. This review summarized the possible application prospects of ChatGPT in healthcare, assessed the limitations that still exist, and suggested possible improvements. In conclusion, the current version of ChatGPT can provide accurate and practical information for doctors and patients, help healthcare professionals to diagnose and treat, promote medical education, and improve the accuracy and efficiency of the healthcare system, etc. However, ChatGPT still has some limitations, as issues such as the accuracy of the information, privacy of data, ethical and moral sentiments still need to be solved. In the future, scientists still need to make efforts to achieve more accurate and effective medical applications with ChatGPT or other AI model by enhancing model performance, filling the data gap, and promoting the standardization of ethical and copyright issues.
Keywords: ChatGPT; Healthcare; AI; Artificial intelligence; Medicine; Application

Jussi T.S. Heikkilä, Mirva Peltoniemi,
The changing work of IPR attorneys: 30 years of institutional transitions,
Technological Forecasting and Social Change,
Volume 197,
2023,
122853,
ISSN 0040-1625,
https://doi.org/10.1016/j.techfore.2023.122853.
(https://www.sciencedirect.com/science/article/pii/S0040162523005383)
Abstract: Intellectual property rights (IPR) are at the core of innovation studies. Patent attorneys and other IPR experts play an important role in drafting and filing processes yet we know little of their work. We conduct an exploratory case study to shed light on how IPR attorneys adapt to changes in institutions and competitive environment that overturn the fundamentals of their business. We focus on the sector's evolution in Finland from 1990 to 2020, and analyse the impacts of globalization, European integration, and digitalization. EPC, EUTM, RCD and the London Agreement are identified as significant changes for the industry. IPR register data and expert interviews show that the business has shifted from serving foreign clients filing in Finland to serving Finnish clients filing internationally, increasing the knowledge requirements of local experts. The filing volume has increased due to globalization while billing per filing has decreased. This has triggered the development of consulting services relating to technology strategy. We contribute by analysing the sector's evolution in a small open economy where start-ups typically aim at the global market from the start. Our study also highlights the need to integrate IPR attorneys into the literatures on appropriability and propensity to file.
Keywords: Patent attorney; IPR; Small open economy; European integration; Industry dynamics; Institutional change
