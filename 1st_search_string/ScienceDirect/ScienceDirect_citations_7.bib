@article{MENG2023107201,
title = {Clinical applications of graph neural networks in computational histopathology: A review},
journal = {Computers in Biology and Medicine},
volume = {164},
pages = {107201},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107201},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523006662},
author = {Xiangyan Meng and Tonghui Zou},
keywords = {Deep learning, Graph neural network, Computational histopathology, Histopathological images},
abstract = {Pathological examination is the optimal approach for diagnosing cancer, and with the advancement of digital imaging technologies, it has spurred the emergence of computational histopathology. The objective of computational histopathology is to assist in clinical tasks through image processing and analysis techniques. In the early stages, the technique involved analyzing histopathology images by extracting mathematical features, but the performance of these models was unsatisfactory. With the development of artificial intelligence (AI) technologies, traditional machine learning methods were applied in this field. Although the performance of the models improved, there were issues such as poor model generalization and tedious manual feature extraction. Subsequently, the introduction of deep learning techniques effectively addressed these problems. However, models based on traditional convolutional architectures could not adequately capture the contextual information and deep biological features in histopathology images. Due to the special structure of graphs, they are highly suitable for feature extraction in tissue histopathology images and have achieved promising performance in numerous studies. In this article, we review existing graph-based methods in computational histopathology and propose a novel and more comprehensive graph construction approach. Additionally, we categorize the methods and techniques in computational histopathology according to different learning paradigms. We summarize the common clinical applications of graph-based methods in computational histopathology. Furthermore, we discuss the core concepts in this field and highlight the current challenges and future research directions.}
}
@article{TAUHID2023100114,
title = {A survey on security analysis of machine learning-oriented hardware and software intellectual property},
journal = {High-Confidence Computing},
volume = {3},
number = {2},
pages = {100114},
year = {2023},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2023.100114},
url = {https://www.sciencedirect.com/science/article/pii/S2667295223000120},
author = {Ashraful Tauhid and Lei Xu and Mostafizur Rahman and Emmett Tomai},
keywords = {Intellectual property, IP protection, Patent, Copyright, Trademark, Infringement, Machine learning, Integrated circuit},
abstract = {Intellectual Property (IP) includes ideas, innovations, methodologies, works of authorship (viz., literary and artistic works), emblems, brands, images, etc. This property is intangible since it is pertinent to the human intellect. Therefore, IP entities are indisputably vulnerable to infringements and modifications without the owner’s consent. IP protection regulations have been deployed and are still in practice, including patents, copyrights, contracts, trademarks, trade secrets, etc., to address these challenges. Unfortunately, these protections are insufficient to keep IP entities from being changed or stolen without permission. As for this, some IPs require hardware IP protection mechanisms, and others require software IP protection techniques. To secure these IPs, researchers have explored the domain of Intellectual Property Protection (IPP) using different approaches. In this paper, we discuss the existing IP rights and concurrent breakthroughs in the field of IPP research; provide discussions on hardware IP and software IP attacks and defense techniques; summarize different applications of IP protection; and lastly, identify the challenges and future research prospects in hardware and software IP security.}
}
@article{NAQVI2023103387,
title = {Mitigation strategies against the phishing attacks: A systematic literature review},
journal = {Computers & Security},
volume = {132},
pages = {103387},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103387},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823002973},
author = {Bilal Naqvi and Kseniia Perova and Ali Farooq and Imran Makhdoom and Shola Oyedeji and Jari Porras},
keywords = {Guidelines and recommendations, Mitigation strategies, Phishing attacks, Systematic, Literature review},
abstract = {Phishing attacks are among the most prevalent attack mechanisms employed by attackers. The consequences of successful phishing include (and are not limited to) financial losses, impact on reputation, and identity theft. The paper presents a systematic literature review featuring 248 articles (from the beginning of 2018 until March 2023) across the main digital libraries to identify, (1) the existing mitigation strategies against phishing attacks, and the underlying technologies considered in the development of these strategies; (2) the most considered phishing vectors in the development of the mitigation strategies; (3) anti-phishing guidelines and recommendations for organizations and end-users respectively; and (4) gaps and open issues that exist in the state of the art. The paper advocates for the need to consider the abilities of human users during the design and development of the mitigation strategies as only technology-centric solutions will not suffice to cater to the challenges posed by phishing attacks.}
}
@article{KAUR2023101804,
title = {Artificial intelligence for cybersecurity: Literature review and future research directions},
journal = {Information Fusion},
volume = {97},
pages = {101804},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101804},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523001136},
author = {Ramanpreet Kaur and Dušan Gabrijelčič and Tomaž Klobučar},
keywords = {Detection, Protection, Response, Recovery, Identify, Learning, Cyberattacks, Taxonomy},
abstract = {Artificial intelligence (AI) is a powerful technology that helps cybersecurity teams automate repetitive tasks, accelerate threat detection and response, and improve the accuracy of their actions to strengthen the security posture against various security issues and cyberattacks. This article presents a systematic literature review and a detailed analysis of AI use cases for cybersecurity provisioning. The review resulted in 2395 studies, of which 236 were identified as primary. This article classifies the identified AI use cases based on a NIST cybersecurity framework using a thematic analysis approach. This classification framework will provide readers with a comprehensive overview of the potential of AI to improve cybersecurity in different contexts. The review also identifies future research opportunities in emerging cybersecurity application areas, advanced AI methods, data representation, and the development of new infrastructures for the successful adoption of AI-based cybersecurity in today's era of digital transformation and polycrisis.}
}
@article{ZHAO2023102790,
title = {Quantity or quality: The roles of technology and science convergence on firm innovation performance},
journal = {Technovation},
volume = {126},
pages = {102790},
year = {2023},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2023.102790},
url = {https://www.sciencedirect.com/science/article/pii/S0166497223001013},
author = {Shengchao Zhao and Deming Zeng and Jian Li and Ke Feng and Yao Wang},
keywords = {Novel technology convergence, Reinforced technology convergence, Science convergence, Innovation quantity, Innovation quality},
abstract = {While firms in science-based sectors have multiple convergence opportunities, few empirical studies have explored how technology and science convergence can improve innovation performance. This study offered a novel approach by distinguishing between novel technology convergence (NTC) and reinforced technology convergence (RTC), investigated their different effects on firm innovation quantity and quality, and examined the moderating role of science convergence. We analyzed data on published papers, patents, and other panel data of firms in China's organic chemical industry between 1998 and 2015. The results show that (a) NTC and RTC positively affect firm innovation quantity, but NTC's effect is weaker than that of RTC; (b) NTC positively affects firm innovation quality, whereas RTC negatively affects it; (c) NTC's positive effect on firm innovation quantity is strengthened by science convergence, whereas RTC's positive effect on firm innovation quantity is weakened by it; and (d) NTC's positive effect on firm innovation quality is strengthened by science convergence, while RTC's negative effect on firm innovation quality is weakened by it. By shedding light on these relationships, this study contributes to the literature on technology convergence and recombinant innovation theory, and offers valuable management implications, particularly in terms of selecting appropriate convergence strategies for different innovation orientations.}
}
@article{VENKATACHALAM2023317,
title = {IMR – 7th INDAM CONFERENCE PAPER: Future of workplace design from a socio-technical perspective},
journal = {IIMB Management Review},
volume = {35},
number = {4},
pages = {317-332},
year = {2023},
issn = {0970-3896},
doi = {https://doi.org/10.1016/j.iimb.2022.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S097038962300054X},
author = {Parvathy Venkatachalam and Rajhans Mishra},
keywords = {Hybrid work, Workplace transition, Socio-technical, Future workplace, Pandemic},
abstract = {Post-COVID-19, there is an ongoing discussion worldwide on the future of workplaces across organisations. This is a socio-technical system in transition, and we use a multi-level perspective (MLP) to identify the changing elements. We perform discourse analysis on the social media platform to gather stakeholders’ perspectives on the workplace to identify (a) changing dimensions of the workplace; (b) emerging challenges; (c) the necessity for adoption of new technologies; and (d) learnings from the pandemic with respect to the workplace. We propose a conceptual model through theories of socio-technical perspectives that shall inform practice on the design of workplace policy.}
}
@article{DWIVEDI2023102642,
title = {Opinion Paper: “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy},
journal = {International Journal of Information Management},
volume = {71},
pages = {102642},
year = {2023},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102642},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223000233},
author = {Yogesh K. Dwivedi and Nir Kshetri and Laurie Hughes and Emma Louise Slade and Anand Jeyaraj and Arpan Kumar Kar and Abdullah M. Baabdullah and Alex Koohang and Vishnupriya Raghavan and Manju Ahuja and Hanaa Albanna and Mousa Ahmad Albashrawi and Adil S. Al-Busaidi and Janarthanan Balakrishnan and Yves Barlette and Sriparna Basu and Indranil Bose and Laurence Brooks and Dimitrios Buhalis and Lemuria Carter and Soumyadeb Chowdhury and Tom Crick and Scott W. Cunningham and Gareth H. Davies and Robert M. Davison and Rahul Dé and Denis Dennehy and Yanqing Duan and Rameshwar Dubey and Rohita Dwivedi and John S. Edwards and Carlos Flavián and Robin Gauld and Varun Grover and Mei-Chih Hu and Marijn Janssen and Paul Jones and Iris Junglas and Sangeeta Khorana and Sascha Kraus and Kai R. Larsen and Paul Latreille and Sven Laumer and F. Tegwen Malik and Abbas Mardani and Marcello Mariani and Sunil Mithas and Emmanuel Mogaji and Jeretta Horn Nord and Siobhan O’Connor and Fevzi Okumus and Margherita Pagani and Neeraj Pandey and Savvas Papagiannidis and Ilias O. Pappas and Nishith Pathak and Jan Pries-Heje and Ramakrishnan Raman and Nripendra P. Rana and Sven-Volker Rehm and Samuel Ribeiro-Navarrete and Alexander Richter and Frantz Rowe and Suprateek Sarker and Bernd Carsten Stahl and Manoj Kumar Tiwari and Wil {van der Aalst} and Viswanath Venkatesh and Giampaolo Viglia and Michael Wade and Paul Walton and Jochen Wirtz and Ryan Wright},
keywords = {Conversational agent, Generative artificial intelligence, Generative AI, ChatGPT, Large language models},
abstract = {Transformative artificially intelligent tools, such as ChatGPT, designed to generate sophisticated text indistinguishable from that produced by a human, are applicable across a wide range of contexts. The technology presents opportunities as well as, often ethical and legal, challenges, and has the potential for both positive and negative impacts for organisations, society, and individuals. Offering multi-disciplinary insight into some of these, this article brings together 43 contributions from experts in fields such as computer science, marketing, information systems, education, policy, hospitality and tourism, management, publishing, and nursing. The contributors acknowledge ChatGPT’s capabilities to enhance productivity and suggest that it is likely to offer significant gains in the banking, hospitality and tourism, and information technology industries, and enhance business activities, such as management and marketing. Nevertheless, they also consider its limitations, disruptions to practices, threats to privacy and security, and consequences of biases, misuse, and misinformation. However, opinion is split on whether ChatGPT’s use should be restricted or legislated. Drawing on these contributions, the article identifies questions requiring further research across three thematic areas: knowledge, transparency, and ethics; digital transformation of organisations and societies; and teaching, learning, and scholarly research. The avenues for further research include: identifying skills, resources, and capabilities needed to handle generative AI; examining biases of generative AI attributable to training datasets and processes; exploring business and societal contexts best suited for generative AI implementation; determining optimal combinations of human and generative AI for various tasks; identifying ways to assess accuracy of text produced by generative AI; and uncovering the ethical and legal issues in using generative AI across different contexts.}
}
@article{FRISTON202335,
title = {Path integrals, particular kinds, and strange things},
journal = {Physics of Life Reviews},
volume = {47},
pages = {35-62},
year = {2023},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2023.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S1571064523001094},
author = {Karl Friston and Lancelot {Da Costa} and Dalton A.R. Sakthivadivel and Conor Heins and Grigorios A. Pavliotis and Maxwell Ramstead and Thomas Parr},
keywords = {Self-organisation, Variational inference, Bayesian, Markov blanket, Active matter, Path integral},
abstract = {This paper describes a path integral formulation of the free energy principle. The ensuing account expresses the paths or trajectories that a particle takes as it evolves over time. The main results are a method or principle of least action that can be used to emulate the behaviour of particles in open exchange with their external milieu. Particles are defined by a particular partition, in which internal states are individuated from external states by active and sensory blanket states. The variational principle at hand allows one to interpret internal dynamics—of certain kinds of particles—as inferring external states that are hidden behind blanket states. We consider different kinds of particles, and to what extent they can be imbued with an elementary form of inference or sentience. Specifically, we consider the distinction between dissipative and conservative particles, inert and active particles and, finally, ordinary and strange particles. Strange particles can be described as inferring their own actions, endowing them with apparent autonomy or agency. In short—of the kinds of particles afforded by a particular partition—strange kinds may be apt for describing sentient behaviour.}
}
@article{RAMANI2023101463,
title = {Using field and quasi experiments and text-based analysis to advance international business theory},
journal = {Journal of World Business},
volume = {58},
number = {5},
pages = {101463},
year = {2023},
issn = {1090-9516},
doi = {https://doi.org/10.1016/j.jwb.2023.101463},
url = {https://www.sciencedirect.com/science/article/pii/S109095162300038X},
author = {Ravi S. Ramani and Herman Aguinis},
keywords = {Research methodology, Research design, Analytical methods, Topic models, Sentiment analysis, Experiments},
abstract = {Methodological developments are critical for driving theoretical advancements in international business (IB) due to the field's diversity regarding disciplinary, theoretical, and conceptual bases. We provide an accessible introduction to two methodological approaches—one related to design and one to analysis—that are currently underutilized in IB despite their great potential: (a) field and quasi experiments, and (b) text-based analysis. We describe each method and provide examples of how they can be used to make advancements in several IB domains and theories including internationalization process theory; ownership, location, internalization (OLI) paradigm; knowledge-based view of multinational enterprises; dynamic capability theory; and international entrepreneurship.}
}
@article{HOSSAIN2023109249,
title = {A skin lesion hair mask dataset with fine-grained annotations},
journal = {Data in Brief},
volume = {48},
pages = {109249},
year = {2023},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2023.109249},
url = {https://www.sciencedirect.com/science/article/pii/S2352340923003682},
author = {Sk Imran Hossain and Sudipta Singha Roy and Jocelyn {De Goër De Herve} and Robert E. Mercer and Engelbert {Mephu Nguifo}},
keywords = {Skin lesion, Hair mask, Hair segmentation, Hair augmentation, Deep learning},
abstract = {Occlusion of skin lesions in dermoscopic images due to hair affects the performance of computer-assisted lesion analysis algorithms. Lesion analysis can benefit from digital hair removal or realistic hair simulation techniques. To assist in that process, we have created the largest publicly available skin lesion hair segmentation mask dataset by carefully annotating 500 dermoscopic images. Compared to the existing datasets, our dataset is free of non-hair artifacts like ruler markers, bubbles, and ink marks. The dataset is also less prone to over and under segmentations because of fine-grained annotations and quality checks from multiple independent annotators. To create the dataset, first, we collected five hundred copyright-free CC0 licensed dermoscopic images covering different hair patterns. Second, we trained a deep learning hair segmentation model on a publicly available weakly annotated dataset. Third, we extracted hair masks for the selected five hundred images using the segmentation model. Finally, we manually corrected all the segmentation errors and verified the annotations by superimposing the annotated masks on top of the dermoscopic images. Multiple annotators were involved in the annotation and verification process to make the annotations as error-free as possible. The prepared dataset will be useful for benchmarking and training hair segmentation algorithms as well as creating realistic hair augmentation systems.}
}
@article{DEMMER2023107875,
title = {Does an emotional connection to art really require a human artist? Emotion and intentionality responses to AI- versus human-created art and impact on aesthetic experience},
journal = {Computers in Human Behavior},
volume = {148},
pages = {107875},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107875},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002261},
author = {Theresa Rahel Demmer and Corinna Kühnapfel and Joerg Fingerhut and Matthew Pelowski},
keywords = {Computer-art, Computer-human-interaction, Emotion-transmission, Empirical aesthetics, Intentionality, Anthropomorphizing},
abstract = {AI has captured the artworld, and, and, progressively, is reshaping the way humans interact with various forms of media. Computer-generated art sells for millions at auctions; artists routinely use algorithms to generate aesthetic materials. However, to capture the impact of such works and our relationships with them, we need to better understand the kinds of responses we make to AI/computer-generated images. Here, we consider whether and, if so, to what extent humans report feeling emotions when engaging computer-generated art, or even ascribe intentionality behind those feelings. These are emerging—and also long-standing—points of controversy, with critical arguments that this should not occur, thus marking potential distinctions between artificial and ‘real’ human productions. We tested this by employing visually similar abstract, black-and-white artworks, made by a computer (RNG) or by human artists intentionally aiming at transmitting emotions. In a 2 × 2 design, participants (N = 48) viewed the art, preceded by primes about human/computer provenance (true, 50% of cases). Contrary to critical suggestions, participants almost always not only reported emotions but also ascribed intentionality, independent of the prime given. Interestingly, they did report stronger emotions when the work actually was made by a human. We discuss implications for our understanding of art engagements and future developments regarding computer-generated digital interactions.}
}
@article{CHOI2023464167,
title = {Machine learning liquid chromatography retention time prediction model augments the dansylation strategy for metabolite analysis of urine samples},
journal = {Journal of Chromatography A},
volume = {1705},
pages = {464167},
year = {2023},
issn = {0021-9673},
doi = {https://doi.org/10.1016/j.chroma.2023.464167},
url = {https://www.sciencedirect.com/science/article/pii/S002196732300393X},
author = {Eunwoo Choi and Won Jun Yoo and Hwa-Yong Jang and Tae-Young Kim and Sung Ki Lee and Han Bin Oh},
keywords = {Artificial neural network, Dansylation, Liquid chromatography–mass spectrometry, Machine learning, Retention time},
abstract = {Herein, a standalone software equipped with a graphic user interface (GUI) is developed to predict liquid chromatography mass spectrometry (LC–MS) retention times (RTs) of dansylated metabolites. Dansylation metabolomics strategy developed by Li et al. narrows down a vast chemical space of metabolites into the metabolites containing amines and phenolic hydroxyls. Combined with differential isotope labeling, e.g., 12C-reagent labeled individual samples spiked with a 13C-reagent labeled reference or pooled sample, LC–MS analysis of the dansylated samples enables accurate relative quantification of all labeled metabolites. Herein, the LC–RTs for dansylated metabolites are predicted using an artificial neural network (ANN) machine-learning model. For the ANN modeling, 315 dansylated urine metabolites obtained from the DnsID database are used. The ANN LC–RT prediction model was reliable, with a mean absolute deviation of 0.74 min for the 30 min LC run. In the RT model, a deviation of more than 2 min was observed in only 3.2% of the total 315 metabolites, while a deviation of 1.5 min or more was observed in 11% of the metabolites. Furthermore, it was found that the LC–RT prediction was also reliable even for metabolites containing both amine and phenolic functional groups that can undergo dansylation on either one of the two functional groups, resulting in the generation of two isomeric forms. This RT-prediction model is embedded into a user-friendly GUI and can be used for identifying nontargeted dansylated metabolites with unknown RTs, along with accurate mass measurements. Furthermore, it is demonstrated that the developed software can help identify metabolites from a urine sample of an anonymous healthy pregnant woman.}
}
@article{ARORA2023100478,
title = {Risk and the future of AI: Algorithmic bias, data colonialism, and marginalization},
journal = {Information and Organization},
volume = {33},
number = {3},
pages = {100478},
year = {2023},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2023.100478},
url = {https://www.sciencedirect.com/science/article/pii/S1471772723000325},
author = {A. Arora and M. Barrett and E. Lee and E. Oborn and K. Prince}
}
@article{LIU2023798,
title = {Generative artificial intelligence and its applications in materials science: Current situation and future perspectives},
journal = {Journal of Materiomics},
volume = {9},
number = {4},
pages = {798-816},
year = {2023},
issn = {2352-8478},
doi = {https://doi.org/10.1016/j.jmat.2023.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352847823000771},
author = {Yue Liu and Zhengwei Yang and Zhenyao Yu and Zitu Liu and Dahui Liu and Hailong Lin and Mingqing Li and Shuchang Ma and Maxim Avdeev and Siqi Shi},
keywords = {Machine learning, Artificial intelligence, Generative artificial intelligence, Materials science, Novel materials discovery, Deep learning},
abstract = {Generative Artificial Intelligence (GAI) is attracting the increasing attention of materials community for its excellent capability of generating required contents. With the introduction of Prompt paradigm and reinforcement learning from human feedback (RLHF), GAI shifts from the task-specific to general pattern gradually, enabling to tackle multiple complicated tasks involved in resolving the structure-activity relationships. Here, we review the development status of GAI comprehensively and analyze pros and cons of various generative models in the view of methodology. The applications of task-specific generative models involving materials inverse design and data augmentation are also dissected. Taking ChatGPT as an example, we explore the potential applications of general GAI in generating multiple materials content, solving differential equation as well as querying materials FAQs. Furthermore, we summarize six challenges encountered for the use of GAI in materials science and provide the corresponding solutions. This work paves the way for providing effective and explainable materials data generation and analysis approaches to accelerate the materials research and development.}
}
@article{GANGWAR2023200,
title = {Triple-BigGAN: Semi-supervised generative adversarial networks for image synthesis and classification on sexual facial expression recognition},
journal = {Neurocomputing},
volume = {528},
pages = {200-216},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223000346},
author = {Abhishek Gangwar and Víctor González-Castro and Enrique Alegre and Eduardo Fidalgo},
keywords = {Facial expressions, Pornography, Not safe for work (NSFW), Obscene image retrieval, Deep learning, Emotion detection},
abstract = {Automatic recognition of facial images showing erotic expressions can help to understand our social interaction and to detect non-appropriate images even when there is no nakedness present in them. This paper contemplates, for the first time, to exploit facial cues applied to automatic Sexual Facial Expression Recognition (SFER). With this goal, we introduce a new dataset named Sexual Expression and Activity Faces (SEA-Faces-30k) for SFER, which contains 30k manually labeled images under three categories: erotic, suggestive-erotic, and non-erotic. Deep Convolutional Neural Networks require large-scale annotated image datasets with diversity and variations to be properly trained. Unfortunately, gathering such a massive amount of data is not feasible in this area. Therefore, we present a new semi-supervised GAN framework named Triple-BigGAN, which learns a generative model and a classifier simultaneously. It learns both tasks in an end-to-end fashion while using unlabeled or partially labeled data. The Triple-BigGAN framework shows promising classification performance for the SFER task (i.e., 93.59%) and other five benchmark datasets, i.e., FER-2013, CIFAR-10, Expression in-the-Wild (ExpW), Modified National Institute of Standards and Technology database (MNIST), and Street View House Numbers (SVHN). Next, we evaluated the quality of samples generated by Triple-BigGAN with a resolution of 256×256 pixels using Inception Score (IS) and Frechet Inception Distance (FID). Our approach obtained the best FID (i.e., 19.94%) and IS (i.e., 97.98%) scores on the SEA-Faces-30k dataset. Further, we empirically demonstrated that synthetic erotic face images generated by Triple-BigGAN could also help in improving the classification performance of deep supervised networks.}
}
@article{HUANG2023114147,
title = {Nudging corporate environmental responsibility through green finance? Quasi-natural experimental evidence from China},
journal = {Journal of Business Research},
volume = {167},
pages = {114147},
year = {2023},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2023.114147},
url = {https://www.sciencedirect.com/science/article/pii/S0148296323005064},
author = {Hongyun Huang and William Mbanyele and Fengrong Wang and Chenxi Zhang and Xin Zhao},
keywords = {Green finance, Corporate environmental responsibility, Financial constraints, Risk-taking, External governance, Difference-in-Difference-in-Difference},
abstract = {Green finance has drawn increased worldwide attention from policymakers as a financial mechanism that could potentially encourage corporations to actively engage in sustainable activities. However, despite a growing body of studies investigating the economic outcomes of green financial policies, there is still a lack of research that systematically quantifies the social welfare implications of green finance. Hence, this study aims to fill this research gap by establishing the causal effect of green finance on corporate environmental responsibility. Exploiting the “bottom-up” enforcement of the green finance pilots in 2017 in China as a quasi-natural experiment and the difference-in-difference-in-difference identification strategy, we find that green finance significantly enhances corporate environmental responsibility performance in high-polluting industries relative to their counterparts, and this evidence continues to survive a battery of robustness checks. Moreover, we explore three underlying mechanisms that possibly explain this beneficial effect: risk-taking, external governance and financing channels. Furthermore, we uncover that corporate environmental responsibility serves as a plausible non-economic channel that combines green finance with economic benefits by stimulating green innovation, promoting total factor productivity and expanding market share. Overall, our study offers new insights on both the economic and non-economic consequences of green finance on business performance.}
}
@article{SHAO202324,
title = {COVAD: Content-oriented video anomaly detection using a self attention-based deep learning model},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {1},
pages = {24-41},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000481},
author = {Wenhao Shao and Praboda Rajapaksha and Yanyan Wei and Dun Li and Noel Crespi and Zhigang Luo},
keywords = {Video surveillance, Video anomaly detection, Machine learning, Deep learning, Neural network, Coordinate attention},
abstract = {Background
Video anomaly detection has always been a hot topic and has attracted increasing attention. Many of the existing methods for video anomaly detection depend on processing the entire video rather than considering only the significant context.
Method
This paper proposes a novel video anomaly detection method called COVAD that mainly focuses on the region of interest in the video instead of the entire video. Our proposed COVAD method is based on an autoencoded convolutional neural network and a coordinated attention mechanism, which can effectively capture meaningful objects in the video and dependencies among different objects. Relying on the existing memory-guided video frame prediction network, our algorithm can significantly predict the future motion and appearance of objects in a video more effectively.
Result
The proposed algorithm obtained better experimental results on multiple datasets and outperformed the baseline models considered in our analysis. Simultaneously, we provide an improved visual test that can provide pixel-level anomaly explanations.}
}
@article{SINGH2023104020,
title = {A review of image fusion: Methods, applications and performance metrics},
journal = {Digital Signal Processing},
volume = {137},
pages = {104020},
year = {2023},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2023.104020},
url = {https://www.sciencedirect.com/science/article/pii/S105120042300115X},
author = {Simrandeep Singh and Harbinder Singh and Gloria Bueno and Oscar Deniz and Sartajvir Singh and Himanshu Monga and P.N. Hrisheekesha and Anibal Pedraza},
keywords = {Information fusion, Image decomposition, Quality metrics, Segmentation, Fusion criteria},
abstract = {The same sensor or a number of image sensors are used to take a series of photographs in order to gather as much data as possible about the scene. Several imaging techniques are used to retrieve entire information from the source under observation. Image fusion (IF) is used to create a new image that incorporates comprehensive information from many photographs. The various images may be captured from different viewpoints, different imaging sensors i.e., visible (VIS) and IR camera, different modalities i.e., computed tomography (CT) and magnetic resonance image (MRI), hyper spectral images i.e., panchromatic and multi-spectral satellite images, multi-exposure images and multi-focus images. Owing to the growing mandates and development of image enhancement schemes, numerous fusion methods were recently formulated. Consequentially, we are doing a survey study to document the methodological development in IF techniques. The outline of picture merging technologies is described in this article. Ultimately, latest state-of-the-art fusion techniques are also demonstrated. Readers will gain insights on current discoveries and their implications for the future through a review of diverse image fusion in various areas and fusion quality metrics.}
}
@article{EZZAMELI2023101847,
title = {Emotion recognition from unimodal to multimodal analysis: A review},
journal = {Information Fusion},
volume = {99},
pages = {101847},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101847},
url = {https://www.sciencedirect.com/science/article/pii/S156625352300163X},
author = {K. Ezzameli and H. Mahersia},
keywords = {Affective computing, Deep learning, Emotion recognition, Fusion, Modality, Multimodality},
abstract = {The omnipresence of numerous information sources in our daily life brings up new alternatives for emotion recognition in several domains including e-health, e-learning, robotics, and e-commerce. Due to the variety of data, the research area of multimodal machine learning poses special problems for computer scientists; how did the field of emotion recognition progress in each modality and what are the most common strategies for recognizing emotions? What part does deep learning play in this? What is multimodality? How did it progress? What are the methods of information fusion? What are the most used datasets in each modality and in multimodal recognition? We can understand and compare the various methods by answering these questions.}
}
@article{2023I,
title = {Full Issue PDF},
journal = {JACC: Asia},
volume = {3},
number = {1},
pages = {I-CLXXX},
year = {2023},
issn = {2772-3747},
doi = {https://doi.org/10.1016/S2772-3747(23)00011-X},
url = {https://www.sciencedirect.com/science/article/pii/S277237472300011X}
}
@article{NIROOMAND2023106848,
title = {Smart investigation of artificial intelligence in renewable energy system technologies by natural language processing: Insightful pattern for decision-makers},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106848},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106848},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623010321},
author = {Kamran Niroomand and Noori M. Cata Saady and Carlos Bazan and Sohrab Zendehboudi and Amilcar Soares and Talib M. Albayati},
keywords = {Natural language processing, Artificial intelligence, Text mining, Topic modeling, Pattern identification, Renewable energy},
abstract = {This study aims to provide a framework which enables decision-makers and researchers to identify AI technology patterns in renewable energy systems from a massive data set of textual data. However, the study was challenged by the Scopus database limitation that allows users to retrieve only 2000 documents per query. Therefore, we developed a search engine based on the Scopus Application Programming Interface (API) that enables us to download an unlimited number of documents per query based on our desirable settings. We extracted 5661 renewable energy systems-related publications from Scopus database and leveraged Natural Language Processing (NLP) and unsupervised algorithms to identify the most frequent computational science models and dense meta-topics and investigate their evolution throughout the period 2000-2021. Our findings showed 7 meta-topics based on the class-based Term Frequency-Inverse Document Frequency (c-TD-IDF) score and term score decline graph. Emerging advanced algorithms, such as different deep learning architectures, directly impacted growing meta-topics involving problems with uncertainty and dynamic conditions.}
}
@article{BAXTER2023104811,
title = {Reconceptualising innovation failure},
journal = {Research Policy},
volume = {52},
number = {7},
pages = {104811},
year = {2023},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2023.104811},
url = {https://www.sciencedirect.com/science/article/pii/S0048733323000951},
author = {David Baxter and Paul Trott and Paul Ellwood},
keywords = {Innovation failure, Systematic literature review},
abstract = {This study examines the concept of innovation failure. It is a problematic subject without an accepted definition. For different stakeholders the same innovation can be both a success and a failure at the same time. The academic literature has concentrated on the determinants of innovation success. Yet, there is a notable lack of academic literature that deals with innovation failure as a topic in its own right. As a result, there is limited attention to, and little consensus on, the meaning of innovation failure. Existing definitions imply a highly contingent conceptualisation of innovation failure informed by the different theoretical framings and disciplinary interests of the researchers. We adopt a systematic literature review methodology that examines the concept of innovation failure at the level of the firm and from an innovation management perspective. The findings of this review are based on a total of 69 peer-reviewed articles from 1977 to 2021. We find the concept is widely used yet poorly defined and frequently lacks any theoretical underpinning. By means of a theory-building inductive synthesis our findings contribute to research by reconceptualising the concept of innovation failure along three processual dimensions: failure-as-experimentation; −judgement and -event.}
}
@incollection{PELLICELLI2023101,
title = {Chapter five - Managing the supply chain: technologies for digitalization solutions},
editor = {Michela Pellicelli},
booktitle = {The Digital Transformation of Supply Chain Management},
publisher = {Elsevier},
pages = {101-152},
year = {2023},
isbn = {978-0-323-85532-7},
doi = {https://doi.org/10.1016/B978-0-323-85532-7.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323855327000025},
author = {Michela Pellicelli},
keywords = {Additive Manufaturing, Artificial Intelligence, Autonomous Vehicles, Big Data, Blockchain, Cloud computing, Co-creation, Digital technologies, Digital Value Chain, Emergent computing, Internet of Things, Managing supply chain, Operations management, Robotics, Technology management},
abstract = {In recent years, technology has profoundly impacted strategies as well as their implementation. Digital technology, by which everything is going to be connected, is forcing companies across all industries to rethink their operations. The development of the internet and mobile technologies has led to a fundamental impact of new technologies on economics and business. Every technology has distinctive elements that account for its employment in specific domains. The present chapter presents the principal innovative solutions for managing and increasing the efficiency of supply chains. Cloud computing, Big Data, Internet of Things (IoT), Blockchain, Robotics, Additive Manufacturing (AM), Autonomous Vehicles (AV), Artificial Intelligence (AI), Co-creation, and Digital Value Chain (DVC) are some of the many innovations that digital technology has made possible. According to Kumar et al., the ‘new-age technologies' - in particular, the Internet of Things (IoT), Artificial Intelligence (AI), Machine Learning and Blockchain - are widely considered the way of the future.}
}
@article{HUA2023109844,
title = {Deep fidelity in DNN watermarking: A study of backdoor watermarking for classification models},
journal = {Pattern Recognition},
volume = {144},
pages = {109844},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109844},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323005423},
author = {Guang Hua and Andrew Beng Jin Teoh},
keywords = {Deep fidelity, Backdoor watermarking, Backdoor fidelity, Deep learning security, Neural network watermarking, Intellectual property protection, Ownership verification},
abstract = {Backdoor watermarking is a promising paradigm to protect the copyright of deep neural network (DNN) models. In the existing works on this subject, researchers have intensively focused on watermarking robustness, while the concept of fidelity, which is concerned with the preservation of the model’s original functionality, has received less attention. In this paper, focusing on deep image classification models, we show that the existing shared notion of the sole measurement of learning accuracy is inadequate to characterize backdoor fidelity. Meanwhile, we show that the analogous concept of embedding distortion in multimedia watermarking, interpreted as the total weight loss (TWL) in DNN backdoor watermarking, is also problematic for fidelity measurement. To address this challenge, we propose the concept of deep fidelity, which states that the backdoor watermarked DNN model should preserve both the feature representation and decision boundary of the unwatermarked host model. To achieve deep fidelity, we propose two loss functions termed penultimate feature loss (PFL) and softmax probability-distribution loss (SPL) to preserve feature representation, while the decision boundary is preserved by the proposed fix last layer (FixLL) treatment, inspired by the recent discovery that deep learning with a fixed classifier causes no loss of learning accuracy. With the above designs, both embedding from scratch and fine-tuning strategies are implemented to evaluate the deep fidelity of backdoor embedding, whose advantages over the existing methods are verified via experiments using ResNet18 for MNIST and CIFAR-10 classifications, and wide residual network (i.e., WRN28_10) for CIFAR-100 task. PyTorch codes are available at https://github.com/ghua-ac/dnn_watermark.}
}
@article{R2023103209,
title = {A hybrid deep learning framework for privacy preservation in edge computing},
journal = {Computers & Security},
volume = {129},
pages = {103209},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103209},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823001190},
author = {Harine Rajashree R and Sundarakantham K and Sivasankar E and Mercy Shalinie S},
keywords = {Internet of things, Privacy preservation, Evolutionary algorithm, Adversarial training, Deep learning},
abstract = {The number of connected devices in our world is continuously increasing at a rapid rate. The Internet of Things(IoT) civilization has resulted in the generation of enormous amounts of data. Analytics on this data pays off in various sectors like health care, manufacturing, and transportation. However, the data generated in the IoT environment is often sensitive, and hence, the need to address the privacy concerns of the data owners. Existing approaches incur a huge computation cost and there is also a gap between privacy preservation and data utility. In this work, a genetic algorithm is coupled with a deep learning network based on adversarial training to build a utility-privacy balanced, low computation solution. The proposal aims to prevent inference of implicit privacy labels present in the data while maintaining data utility. The first part of the proposed work leverages an optimized encoder architecture to learn latent space representation of the input and the second part is the incorporation of adversary for training the framework to prevent unintended sensitive inference. Both parts are governed by a genetic algorithm to output a fitting encoder. Numerical results carried on a benchmark dataset exhibit the capability to protect sensitive data by keeping the accuracy level of the adversary within 23%, and producing a maximum inference accuracy of 95% for the intended task.}
}
@article{TODESCATO2024121116,
title = {Multiscale patch-based feature graphs for image classification},
journal = {Expert Systems with Applications},
volume = {235},
pages = {121116},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121116},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423016184},
author = {Matheus V. Todescato and Luan F. Garcia and Dennis G. Balreira and Joel L. Carbonera},
keywords = {Image classification, Transfer learning, Feature extraction, Multiscale},
abstract = {Deep learning architectures have demonstrated outstanding results in image classification in the last few years. However, applying sophisticated neural network architectures in small datasets remains challenging. In this context, transfer learning is a promising approach for dealing with this scenario. Generally, the available pre-trained architectures adopt a standard fixed input, which usually implies resizing and cropping the input images in the preprocessing phase, causing information loss. Besides, images present visual features in different scales in real-world scenarios, and most common approaches do not consider this fact. In this paper, we propose an approach that applies transfer learning for dealing with small datasets and leverages visual features extracted by pre-trained models from different scales. We based our approach on graph convolutional networks (GCN) that take graphs representing the images in different scales as input and whose nodes are characterized by features extracted by pre-trained models from regular image patches of different scales. Since GCN can deal with graphs with different numbers of nodes, our approach can deal naturally with images of heterogeneous sizes without discarding relevant information. We evaluated our approach in two datasets: a set of geological images and a publicly available dataset, both presenting characteristics that challenge traditional approaches. We tested our approach by adopting three different pre-trained models as feature extractors: two efficient pre-trained CNN models (DenseNet and ResNeXt) and one Vision Transformer (CLIP). We compared our approach with two conventional approaches for dealing with image classification. The experiments show that our approach achieves better results than the conventional approaches for this task.}
}
@article{SMITS202329,
title = {Responsible design and implementation of technologies for the prevention of infectious diseases: towards a values-based assessment framework for the Dutch government},
journal = {Public Health},
volume = {222},
pages = {29-36},
year = {2023},
issn = {0033-3506},
doi = {https://doi.org/10.1016/j.puhe.2023.06.027},
url = {https://www.sciencedirect.com/science/article/pii/S0033350623002172},
author = {M. Smits and N. Back and W. Ebbers},
keywords = {Assessment framework, Contact tracing, COVID-19, Technologies, Values},
abstract = {Objectives
The Dutch government implemented the apps ‘CoronaMelder’ and ‘CoronaCheck’ to prevent the transmission of SARS-CoV-2. They faced many questions on how to responsibly implement such technologies. Here, we aim to develop an assessment framework to support the Dutch national government with the responsible design and implementation of technologies for the prevention of future infectious diseases.
Study design
Three-stage web-based Delphi process.
Methods
The assessment framework was developed through two research phases. During the Initial Design phase, a conceptual version of the assessment framework was developed through a scoping review and semistructured interviews with a scientific board. The Consensus phase involved a three-stage web-based Delphi process with an expert community.
Results
The final assessment framework consists of five development phases, 10 values, and a total of 152 questions.
Conclusions
Technology assessment frameworks help policymakers to make informed decisions and contribute to the responsible implementation of technologies in society. The framework is now available for the Dutch government and other stakeholders to use in future pandemics. We discuss the possibilities of using the framework transnationally.}
}
@article{GIANOLA2023101090,
title = {Advances and opportunities in high-throughput small-scale mechanical testing},
journal = {Current Opinion in Solid State and Materials Science},
volume = {27},
number = {4},
pages = {101090},
year = {2023},
issn = {1359-0286},
doi = {https://doi.org/10.1016/j.cossms.2023.101090},
url = {https://www.sciencedirect.com/science/article/pii/S1359028623000359},
author = {Daniel S. Gianola and Nicolò Maria {della Ventura} and Glenn H. Balbus and Patrick Ziemke and McLean P. Echlin and Matthew R. Begley},
keywords = {Mechanical testing, High-throughput, Small-scale testing, Automation, Fidelity, Down-selection, Machine learning, In situ, Laser ablation},
abstract = {The quest for novel materials used in technologies demanding extreme performance has been accelerated by advances in computational materials screening, additive manufacturing routes, and characterization probes. Despite tremendous progress, the pace of adoption of new materials has still not met the promise of global initiatives in materials discovery. This challenge is particularly acute for structural materials with thermomechanical and environmental demands whose performance depends on microstructure as well as material composition. In this prospective article, we review advances in high-throughput mechanical testing, and the associated specimen fabrication, materials characterization, and modeling tasks that show promise for acceleration of the materials development cycle. We identify a critical need to develop rapid testing and characterization strategies that faithfully reproduce design-relevant properties and circumvent the time and expense of conventional high fidelity testing. We identify small-scale mechanical testing workflows that can incorporate real-time decision making based on feedback from multimodal characterization and computational modeling. These workflows will require site-specific specimen fabrication procedures that are agnostic to the synthesis route and have the ability to modulate microstructure and defect characteristics. We close our review by conceptualizing a fully integrated high-throughput testing platform that addresses the speed-fidelity tradeoff in pursuit of a design-relevant suite of properties for new materials.}
}
@article{DAR2023107311,
title = {Lung anomaly detection from respiratory sound database (sound signals)},
journal = {Computers in Biology and Medicine},
volume = {164},
pages = {107311},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107311},
url = {https://www.sciencedirect.com/science/article/pii/S001048252300776X},
author = {Jawad Ahmad Dar and Kamal Kr Srivastava and Alok Mishra},
abstract = {Chest or upper body auscultation has long been considered a useful part of the physical examination going back to the time of Hippocrates. However, it did not become a prevalent practice until the invention of the stethoscope by Rene Laennec in 1816, which made the practice suitable and hygienic. Pulmonary disease is a kind of sickness that affects the lungs and various parts of the respiratory system. Lung diseases are the third largest cause of death in the world. According to the World Health Organization (WHO), the five major respiratory diseases, namely chronic obstructive pulmonary disease (COPD), tuberculosis, acute lower respiratory tract infection (LRTI), asthma, and lung cancer, cause the death of more than 3 million people each year worldwide. Respiratory sounds disclose significant information regarding the lungs of patients. Numerous methods are developed for analyzing the lung sounds. However, clinical approaches require qualified pulmonologists to diagnose such kind of signals appropriately and are also time consuming. Hence, an efficient Fractional Water Cycle Swarm Optimizer-based Deep Residual Network (Fr-WCSO-based DRN) is developed in this research for detecting the pulmonary abnormalities using respiratory sounds signals. The proposed Fr-WCSO is newly designed by the incorporation of Fractional Calculus (FC) and Water Cycle Swarm Optimizer WCSO. Meanwhile, WCSO is the combination of Water Cycle Algorithm (WCA) with Competitive Swarm Optimizer (CSO). The respiratory input sound signals are pre-processed and the important features needed for the further processing are effectively extracted. With the extracted features, data augmentation is carried out for minimizing the over fitting issues for improving the overall detection performance. Once data augmentation is done, feature selection is performed using proposed Fr-WCSO algorithm. Finally, pulmonary abnormality detection is performed using DRN where the training procedure of DRN is performed using the developed Fr-WCSO algorithm. The developed method achieved superior performance by considering the evaluation measures, namely True Positive Rate (TPR), True Negative Rate (TNR) and testing accuracy with the values of 0.963(96.3%), 0.932,(93.2%) and 0.948(94.8%), respectively.}
}
@article{PRIYA2023163913,
title = {Artificial intelligence enabled carbon capture: A review},
journal = {Science of The Total Environment},
volume = {886},
pages = {163913},
year = {2023},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2023.163913},
url = {https://www.sciencedirect.com/science/article/pii/S0048969723025342},
author = {A.K. Priya and Balaji Devarajan and Avinash Alagumalai and Hua Song},
keywords = {AI in carbon capture, Machine learning, Prediction capability, Patent landscape},
abstract = {Carbon capturing is imperative to fight climate change as much carbon emissions are liberated into the atmosphere, leading to adversely negative environmental impacts. Today's world addresses all the issues with the aid of digital technologies like data pooling and artificial intelligence (AI). Accordingly, this study is articulated based on AI-assisted carbon capturing. Techniques including machine learning (ML), deep learning (DL), and hybrid techniques being adopted in carbon capture are discussed. The role of AI tools, frameworks, and mathematical models are also discussed herein. Furthermore, the confluence of AI in carbon capture patent landscape is explored. This study would allow researchers to envision the growth of AI-assisted carbon capture in mitigating climate change and meeting SDG 13 - climate action.}
}
@article{MORI2023101484,
title = {COMPASS: A creative support system that alerts novelists to the unnoticed missing contents},
journal = {Computer Speech & Language},
volume = {80},
pages = {101484},
year = {2023},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2023.101484},
url = {https://www.sciencedirect.com/science/article/pii/S0885230823000037},
author = {Yusuke Mori and Hiroaki Yamane and Ryohei Shimizu and Yusuke Mukuta and Tatsuya Harada},
keywords = {Storytelling, Story completion, Story understanding, Creative support, Natural language processing},
abstract = {Writing a story is never easy. Even experienced writers sometimes unintentionally omit information from their writings, and it makes the stories not understandable from others. Complementing such unintentionally omitted information using a computer is helpful in providing writing support. Recently, in the field of story understanding and generation, story completion (SC) was proposed to generate the missing parts of an incomplete story. Although its applicability is limited because it requires that the user have prior knowledge of the missing part of a story, missing position prediction (MPP) can be used to compensate for this problem. MPP aims to predict the position of the missing part, but the prerequisite knowledge that “one sentence is missing” is still required. In this study, we propose Variable Number MPP (VN-MPP), a new MPP task that removes this restriction; that is, the task to predict multiple missing sentences or to judge whether there are no missing sentences in the first place. We also propose two methods for this new MPP task. One solves this task end-to-end, while the other learns the two modules separately. The latter allows the writer more flexibility in using the information by making the intermediate outputs between the modules explicit. Based on the novel task, we developed a creative writing support system, COMPASS. The results of a user experiment involving professional creators who write texts in Japanese confirm the efficacy and utility of the developed system. This study aimed to propose a creation support system and, at the same time, to build a relationship of trust between creators and researchers to lay the groundwork for future research and development of creation support AI.}
}
@article{DIAZRODRIGUEZ2023101896,
title = {Connecting the dots in trustworthy Artificial Intelligence: From AI principles, ethics, and key requirements to responsible AI systems and regulation},
journal = {Information Fusion},
volume = {99},
pages = {101896},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101896},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523002129},
author = {Natalia Díaz-Rodríguez and Javier {Del Ser} and Mark Coeckelbergh and Marcos {López de Prado} and Enrique Herrera-Viedma and Francisco Herrera},
keywords = {Trustworthy AI, AI ethics, Responsible AI systems, AI regulation, Regulatory sandbox},
abstract = {Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system’s entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system’s life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental wellbeing; and accountability) are analyzed from a triple perspective: What each requirement for trustworthy AI is, Why it is needed, and How each requirement can be implemented in practice. On the other hand, a practical approach to implement trustworthy AI systems allows defining the concept of responsibility of AI-based systems facing the law, through a given auditing process. Therefore, a responsible AI system is the resulting notion we introduce in this work, and a concept of utmost necessity that can be realized through auditing processes, subject to the challenges posed by the use of regulatory sandboxes. Our multidisciplinary vision of trustworthy AI culminates in a debate on the diverging views published lately about the future of AI. Our reflections in this matter conclude that regulation is a key for reaching a consensus among these views, and that trustworthy and responsible AI systems will be crucial for the present and future of our society.}
}
@article{KALPOKIENE2023102197,
title = {Creative encounters of a posthuman kind – anthropocentric law, artificial intelligence, and art},
journal = {Technology in Society},
volume = {72},
pages = {102197},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102197},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23000027},
author = {Julija Kalpokiene and Ignas Kalpokas},
keywords = {Anthropocentrism, Artificial intelligence, Creativity, Copyright},
abstract = {Artificial Intelligence (AI) is becoming an increasingly transformative force in human life. Crucially, its impact is already extending beyond automation of routine tasks and encroaching on creativity – a domain once seen as exclusively human. Hence, this article first surveys the discriminatory and exploitative underpinnings of the anthropocentric thinking that lies beyond attempts at sidelining the creative capacities of AI. Next, four different approaches to creativity and art are analyzed, ultimately conceptualizing art-ness as externally ascribed. Ultimately, the article moves to one way of such ascription – copyrightability – demonstrating the anthropocentric thinking behind attempts to both deny and award copyright protection to AI-generated content. Moreover, it transpires that human authors are under threat whichever of such strategies ends up dominant.}
}
@article{2023A5,
title = {Guide for Authors},
journal = {Journal of the American Society of Echocardiography},
volume = {36},
number = {7},
pages = {A5-A13},
year = {2023},
note = {34th ASE Annual Scientific Sessions},
issn = {0894-7317},
doi = {https://doi.org/10.1016/S0894-7317(23)00276-6},
url = {https://www.sciencedirect.com/science/article/pii/S0894731723002766}
}
@article{NG2023449,
title = {Machine learning-inspired battery material innovation},
journal = {Energy Advances},
volume = {2},
number = {4},
pages = {449-464},
year = {2023},
issn = {2753-1457},
doi = {https://doi.org/10.1039/d3ya00040k},
url = {https://www.sciencedirect.com/science/article/pii/S2753145723000629},
author = {Man-Fai Ng and Yongming Sun and Zhi Wei Seh},
abstract = {ABSTRACT
Machine learning (ML) techniques have been a powerful tool responsible for many new discoveries in materials science in recent years. In the field of energy storage materials, particularly battery materials, ML techniques have been widely utilized to predict and discover materials’ properties. In this review, we first discuss the key properties of the most common electrode and electrolyte materials. We then summarize recent progress in battery material advancement using ML techniques, through the three main strategies of direct property predictions, machine learning potentials, and inverse design. The major challenges, advantages and limitations of these techniques are also discussed. Finally, we conclude this review with a perspective on sustainable battery development using ML.}
}
@article{LU2023383,
title = {Learning invariant and uniformly distributed feature space for multi-view generation},
journal = {Information Fusion},
volume = {93},
pages = {383-395},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523000192},
author = {Yuqin Lu and Jiangzhong Cao and Shengfeng He and Jiangtao Guo and Qiliang Zhou and Qingyun Dai},
keywords = {Multi-view generation, Generative adversarial networks, Contrastive learning},
abstract = {Multi-view generation from a given single view is a significant, yet challenging problem with broad applications in the field of virtual reality and robotics. Existing methods mainly utilize the basic GAN-based structure to help directly learn a mapping between two different views. Although they can produce plausible results, they still struggle to recover faithful details and fail to generalize to unseen data. In this paper, we propose to learn invariant and uniformly distributed representations for multi-view generation with an “Alignment” and a “Uniformity” constraint (AU-GAN). Our method is inspired by the idea of contrastive learning to learn a well-regulated feature space for multi-view generation. Specifically, our feature extractor is supposed to extract view-invariant representation that captures intrinsic and essential knowledge of the input, and distribute all representations evenly throughout the space to enable the network to “explore” the entire feature space, thus avoiding poor generative ability on unseen data. Extensive experiments on multi-view generation for both faces and objects demonstrate the generative capability of our proposed method on generating realistic and high-quality views, especially for unseen data in wild conditions.}
}
@article{JANES2023111793,
title = {Open tracing tools: Overview and critical comparison},
journal = {Journal of Systems and Software},
volume = {204},
pages = {111793},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111793},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001887},
author = {Andrea Janes and Xiaozhou Li and Valentina Lenarduzzi},
keywords = {Open tracing tool, Telemetry, Multivocal literature review, ChatGPT},
abstract = {Background:
Coping with the rapid growing complexity in contemporary software architecture, tracing has become an increasingly critical practice and been adopted widely by software engineers. By adopting tracing tools, practitioners are able to monitor, debug, and optimize distributed software architectures easily. However, with excessive number of valid candidates, researchers and practitioners have a hard time finding and selecting the suitable tracing tools by systematically considering their features and advantages.
Objective:
To such a purpose, this paper aims to provide an overview of popular Open tracing tools via comparison.
Methods:
Herein, we first identified 30 tools in an objective, systematic, and reproducible manner adopting the Systematic Multivocal Literature Review protocol. Then, we characterized each tool looking at the 1) measured features, 2) popularity both in peer-reviewed literature and online media, and 3) benefits and issues. We used topic modeling and sentiment analysis to extract and summarize the benefits and issues. Specially, we adopted ChatGPT to support the topic interpretation.
Results:
As a result, this paper presents a systematic comparison amongst the selected tracing tools in terms of their features, popularity, benefits and issues.
Conclusion:
The result mainly shows that each tracing tool provides a unique combination of features with also different pros and cons. The contribution of this paper is to provide the practitioners better understanding of the tracing tools facilitating their adoption.}
}
@article{HARBINJA2023105791,
title = {Governing ghostbots},
journal = {Computer Law & Security Review},
volume = {48},
pages = {105791},
year = {2023},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2023.105791},
url = {https://www.sciencedirect.com/science/article/pii/S026736492300002X},
author = {Edina Harbinja and Lilian Edwards and Marisa McVey},
keywords = {Ghostbots, Deepfakes, Post-mortem privacy, Digital legacy, Digital remains},
abstract = {This article discusses the legal implications of a novel phenomenon, namely, digital reincarnations of deceased persons, sometimes known as post-mortem avatars, deepfakes, replicas, holographs, or chatbots. To elide these multiple names, we use the term 'ghostbots'. The piece is an early attempt to discuss the potential social and individual harms, roughly grouped around notions of privacy (including post-mortem privacy), property, personal data and reputation, arising from ghostbots, how they are regulated and whether they need to be adequately regulated further. For reasons of space and focus, the article does not deal with copyright implications, fraud, consumer protection, tort, product liability, and pornography laws, including the non-consensual use of intimate images (‘revenge porn’). This paper focuses on law, although we fully acknowledge and refer to the role of philosophy and ethics in this domain. We canvas two interesting legal developments with implications for ghostbots, namely, the proposed EU Artificial Intelligence (AI) Act and the 2021 New York law amending publicity rights to protect the rights of celebrities whose personality is used in post-mortem ‘replicas’. The latter especially evidences a remarkable shift from the norm we have chronicled in previous articles of no respect for post-mortem privacy to a growing recognition that personality rights do need protection post-mortem in a world where pop stars and actors are routinely re-created using AI. While the legislative motivation here may still be primarily to protect economic interests, we argue it also shows a concern for dignitary and privacy interests. Given the apparent concern for the appropriation of personality post-mortem, possibly in defiance or ignorance of what the deceased would have wished, we propose an early solution to regulate the rise of ghostbots, namely an enforceable ‘do not bot me’ clause in analogue or digital wills.}
}
@article{FENG202310,
title = {Detecting contradictions from IoT protocol specification documents based on neural generated knowledge graph},
journal = {ISA Transactions},
volume = {141},
pages = {10-19},
year = {2023},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2023.04.025},
url = {https://www.sciencedirect.com/science/article/pii/S0019057823001945},
author = {Xinguo Feng and Yanjun Zhang and Mark Huasong Meng and Yansong Li and Chegne Eu Joe and Zhe Wang and Guangdong Bai},
keywords = {Internet of things, Natural language processing, Web protocol, Contradiction detection, Large language models},
abstract = {Due to the boom of Internet of Things (IoT) in recent years, various IoT devices are connected to the Internet and communicate with each other through network protocols such as the Constrained Application Protocol (CoAP). These protocols are typically defined and described in specification documents, such as Request for Comments (RFC), which are written in natural or semi-formal languages. Since developers largely follow the specification documents when implementing web protocols, they have become the de facto protocol specifications. Therefore, it must be ensured that the descriptions in them are consistent to avoid technological issues, incompatibility, security risks, or even legal concerns. In this work, we propose Neural RFC Knowledge Graph (NRFCKG), a neural network-generated knowledge graph based contradictions detection tool for IoT protocol specification documents. Our approach can automatically parse the specification documents and construct knowledge graphs from them through entity extraction, relation extraction, and rule extraction with large language models. It then conducts an intra-entity and inter-entity contradiction detection over the generated knowledge graph. We implement NRFCKG and apply it to the most extensively used messaging protocols in IoT, including the main RFC (RFC7252) of CoAP, the specification document of MQTT, and the specification document of AMQP. Our evaluation shows that NRFCKG generalizes well to other specification documents and it manages to detect contradictions from these IoT protocol specification documents.}
}
@article{TU2023226,
title = {Predictive chemistry: machine learning for reaction deployment, reaction development, and reaction discovery},
journal = {Chemical Science},
volume = {14},
number = {2},
pages = {226-244},
year = {2023},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d2sc05089g},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023061746},
author = {Zhengkai Tu and Thijs Stuyver and Connor W. Coley},
abstract = {The field of predictive chemistry relates to the development of models able to describe how molecules interact and react. It encompasses the long-standing task of computer-aided retrosynthesis, but is far more reaching and ambitious in its goals. In this review, we summarize several areas where predictive chemistry models hold the potential to accelerate the deployment, development, and discovery of organic reactions and advance synthetic chemistry.}
}
@article{AMINIZADEH2023107745,
title = {The applications of machine learning techniques in medical data processing based on distributed computing and the Internet of Things},
journal = {Computer Methods and Programs in Biomedicine},
volume = {241},
pages = {107745},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107745},
url = {https://www.sciencedirect.com/science/article/pii/S016926072300411X},
author = {Sarina Aminizadeh and Arash Heidari and Shiva Toumaj and Mehdi Darbandi and Nima Jafari Navimipour and Mahsa Rezaei and Samira Talebi and Poupak Azad and Mehmet Unal},
keywords = {Medical data processing, Healthcare data analysis, Deep learning, Distributed computing},
abstract = {Medical data processing has grown into a prominent topic in the latest decades with the primary goal of maintaining patient data via new information technologies, including the Internet of Things (IoT) and sensor technologies, which generate patient indexes in hospital data networks. Innovations like distributed computing, Machine Learning (ML), blockchain, chatbots, wearables, and pattern recognition can adequately enable the collection and processing of medical data for decision-making in the healthcare era. Particularly, to assist experts in the disease diagnostic process, distributed computing is beneficial by digesting huge volumes of data swiftly and producing personalized smart suggestions. On the other side, the current globe is confronting an outbreak of COVID-19, so an early diagnosis technique is crucial to lowering the fatality rate. ML systems are beneficial in aiding radiologists in examining the incredible amount of medical images. Nevertheless, they demand a huge quantity of training data that must be unified for processing. Hence, developing Deep Learning (DL) confronts multiple issues, such as conventional data collection, quality assurance, knowledge exchange, privacy preservation, administrative laws, and ethical considerations. In this research, we intend to convey an inclusive analysis of the most recent studies in distributed computing platform applications based on five categorized platforms, including cloud computing, edge, fog, IoT, and hybrid platforms. So, we evaluated 27 articles regarding the usage of the proposed framework, deployed methods, and applications, noting the advantages, drawbacks, and the applied dataset and screening the security mechanism and the presence of the Transfer Learning (TL) method. As a result, it was proved that most recent research (about 43%) used the IoT platform as the environment for the proposed architecture, and most of the studies (about 46%) were done in 2021. In addition, the most popular utilized DL algorithm was the Convolutional Neural Network (CNN), with a percentage of 19.4%. Hence, despite how technology changes, delivering appropriate therapy for patients is the primary aim of healthcare-associated departments. Therefore, further studies are recommended to develop more functional architectures based on DL and distributed environments and better evaluate the present healthcare data analysis models.}
}
@article{LIN2023101802,
title = {Machine learning accelerates the investigation of targeted MOFs: Performance prediction, rational design and intelligent synthesis},
journal = {Nano Today},
volume = {49},
pages = {101802},
year = {2023},
issn = {1748-0132},
doi = {https://doi.org/10.1016/j.nantod.2023.101802},
url = {https://www.sciencedirect.com/science/article/pii/S1748013223000518},
author = {Jing Lin and Zhimeng Liu and Yujie Guo and Shulin Wang and Zhang Tao and Xiangdong Xue and Rushuo Li and Shihao Feng and Linmeng Wang and Jiangtao Liu and Hongyi Gao and Ge Wang and Yanjing Su},
keywords = {Metal-organic frameworks, Machine learning, Nanoporous materials, Performance prediction, Rational design, Intelligent synthesis},
abstract = {Metal-organic frameworks (MOFs) are a new class of nanoporous materials that are widely used in various emerging fields due to their large specific surface area, high porosity and tunable pore size. Its excellent chemical tunability provides a wide material space, in which tens of thousands of MOFs have been synthesized. However, it is impossible to explore such a vast chemical space through trial-and-error methods, making it difficult to achieve custom design of high-performance MOFs for specific applications. Machine learning (ML) is a powerful tool for guiding materials design and preparation by mining the hidden knowledge in data, and can even make prediction of material properties in seconds. This review aims to provide readers with a new perspective on how ML has been changing the research and development paradigm of MOFs. The four main data sources for MOFs and how to select the suitable features (descriptors) are firstly presented to enable the reader to quickly acquire data and carry out machine learning. Moreover, the application of ML in the development of MOFs is highlighted from the perspectives of performance prediction, rational design and intelligent synthesis. Finally, the future challenges and opportunities of combining ML with MOFs from the points of view of data and algorithms are proposed. This review will provide instructive guidance for ML-assisted MOFs research.}
}
@article{AGATHOKLEOUS2023164154,
title = {Use of ChatGPT: What does it mean for biology and environmental science?},
journal = {Science of The Total Environment},
volume = {888},
pages = {164154},
year = {2023},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2023.164154},
url = {https://www.sciencedirect.com/science/article/pii/S0048969723027754},
author = {Evgenios Agathokleous and Costas J. Saitanis and Chao Fang and Zhen Yu},
keywords = {Artificial intelligence, ChatGPT, Biology, Environmental science, Generative Pre-trained Transformer, Large language model},
abstract = {Artificial intelligence (AI) large language models (LLMs) have emerged as important technologies. Recently, ChatGPT (Generative Pre-trained Transformer) has been released and attracted massive interest from the public, owing to its unique capabilities to simplify many daily tasks of people from diverse backgrounds and social statuses. Here, we discuss how ChatGPT (and similar AI technologies) can impact biology and environmental science, providing examples obtained through interactive sessions with ChatGPT. The benefits that ChatGPT offers are ample and can impact many aspects of biology and environmental science, including education, research, scientific publishing, outreach, and societal translation. Among others, ChatGPT can simplify and expedite highly complex and challenging tasks. As an example to illustrate this, we provide 100 important questions for biology and 100 important questions for environmental science. Although ChatGPT offers a plethora of benefits, there are several risks and potential harms associated with its use, which we analyze herein. Awareness of risks and potential harms should be raised. However, understanding and overcoming the current limitations could lead these recent technological advances to push biology and environmental science to their limits.}
}
@article{MA2023103792,
title = {Perceptions and experiences of exercise among pregnant women},
journal = {Midwifery},
volume = {125},
pages = {103792},
year = {2023},
issn = {0266-6138},
doi = {https://doi.org/10.1016/j.midw.2023.103792},
url = {https://www.sciencedirect.com/science/article/pii/S026661382300195X},
author = {Nan Ma and Janita Pak Chun Chau and Yuli Zang and Yongfang Deng and Cho Lee Wong and David R Thompson},
keywords = {Exercise, Perceptions, Experiences, Pregnancy},
abstract = {Introduction
Though exercise during pregnancy can yield important maternal benefits, most pregnant women in China do less aerobic exercise than is currently recommended. This qualitative study aimed to explore the perceptions and experiences of physical exercise among pregnant women and to identify perceived barriers to and facilitators of exercise participation.
Methods
Purposive sampling was used to recruit 40 pregnant women attending prenatal visits at an obstetrics outpatient department of a tertiary general hospital in Southern China. Individual semi-structured telephone interviews were conducted with the verbatim transcripts analyzed through content analysis.
Results
Three main themes emerged from the data: perceptions and patterns of exercise; concerns and hesitations about participating in exercise; and determinants of adoption and maintenance of exercise participation. Though pregnant women recognised their need for physical exercise instruction, their demands remained unmet due to a combination of factors such as lack of knowledge, confidence, and support, and concerns about safety.
Discussion
The findings of this study suggest that the provision of tailored exercise programs for pregnant women, which include education, reassurance, motivational strategies, and lay and professional support, may help improve knowledge, allay concerns, boost confidence, and bolster support when doing physical exercise.}
}
@article{KAZEROUNI2023102846,
title = {Diffusion models in medical imaging: A comprehensive survey},
journal = {Medical Image Analysis},
volume = {88},
pages = {102846},
year = {2023},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.102846},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523001068},
author = {Amirhossein Kazerouni and Ehsan Khodapanah Aghdam and Moein Heidari and Reza Azad and Mohsen Fayyaz and Ilker Hacihaliloglu and Dorit Merhof},
keywords = {Generative models, Diffusion models, Denoising diffusion models, Noise conditioned score networks, Score-based models, Medical imaging, Medical applications, Survey},
abstract = {Denoising diffusion models, a class of generative models, have garnered immense interest lately in various deep-learning problems. A diffusion probabilistic model defines a forward diffusion stage where the input data is gradually perturbed over several steps by adding Gaussian noise and then learns to reverse the diffusion process to retrieve the desired noise-free data from noisy data samples. Diffusion models are widely appreciated for their strong mode coverage and quality of the generated samples in spite of their known computational burdens. Capitalizing on the advances in computer vision, the field of medical imaging has also observed a growing interest in diffusion models. With the aim of helping the researcher navigate this profusion, this survey intends to provide a comprehensive overview of diffusion models in the discipline of medical imaging. Specifically, we start with an introduction to the solid theoretical foundation and fundamental concepts behind diffusion models and the three generic diffusion modeling frameworks, namely, diffusion probabilistic models, noise-conditioned score networks, and stochastic differential equations. Then, we provide a systematic taxonomy of diffusion models in the medical domain and propose a multi-perspective categorization based on their application, imaging modality, organ of interest, and algorithms. To this end, we cover extensive applications of diffusion models in the medical domain, including image-to-image translation, reconstruction, registration, classification, segmentation, denoising, 2/3D generation, anomaly detection, and other medically-related challenges. Furthermore, we emphasize the practical use case of some selected approaches, and then we discuss the limitations of the diffusion models in the medical domain and propose several directions to fulfill the demands of this field. Finally, we gather the overviewed studies with their available open-source implementations at our GitHub.11https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging. We aim to update the relevant latest papers within it regularly.}
}
@article{ALI2023101805,
title = {Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence},
journal = {Information Fusion},
volume = {99},
pages = {101805},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101805},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523001148},
author = {Sajid Ali and Tamer Abuhmed and Shaker El-Sappagh and Khan Muhammad and Jose M. Alonso-Moral and Roberto Confalonieri and Riccardo Guidotti and Javier {Del Ser} and Natalia Díaz-Rodríguez and Francisco Herrera},
keywords = {Explainable Artificial Intelligence, Interpretable machine learning, Trustworthy AI, AI principles, Post-hoc explainability, XAI assessment, Data Fusion, Deep Learning},
abstract = {Artificial intelligence (AI) is currently being utilized in a wide range of sophisticated applications, but the outcomes of many AI models are challenging to comprehend and trust due to their black-box nature. Usually, it is essential to understand the reasoning behind an AI model’s decision-making. Thus, the need for eXplainable AI (XAI) methods for improving trust in AI models has arisen. XAI has become a popular research subject within the AI field in recent years. Existing survey papers have tackled the concepts of XAI, its general terms, and post-hoc explainability methods but there have not been any reviews that have looked at the assessment methods, available tools, XAI datasets, and other related aspects. Therefore, in this comprehensive study, we provide readers with an overview of the current research and trends in this rapidly emerging area with a case study example. The study starts by explaining the background of XAI, common definitions, and summarizing recently proposed techniques in XAI for supervised machine learning. The review divides XAI techniques into four axes using a hierarchical categorization system: (i) data explainability, (ii) model explainability, (iii) post-hoc explainability, and (iv) assessment of explanations. We also introduce available evaluation metrics as well as open-source packages and datasets with future research directions. Then, the significance of explainability in terms of legal demands, user viewpoints, and application orientation is outlined, termed as XAI concerns. This paper advocates for tailoring explanation content to specific user types. An examination of XAI techniques and evaluation was conducted by looking at 410 critical articles, published between January 2016 and October 2022, in reputed journals and using a wide range of research databases as a source of information. The article is aimed at XAI researchers who are interested in making their AI models more trustworthy, as well as towards researchers from other disciplines who are looking for effective XAI methods to complete tasks with confidence while communicating meaning from data.}
}
@article{MARIC2023122605,
title = {Innovation management of three-dimensional printing (3DP) technology: Disclosing insights from existing literature and determining future research streams},
journal = {Technological Forecasting and Social Change},
volume = {193},
pages = {122605},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122605},
url = {https://www.sciencedirect.com/science/article/pii/S0040162523002901},
author = {Josip Marić and Marco Opazo-Basáez and Božidar Vlačić and Marina Dabić},
keywords = {3D print, Additive manufacturing, Innovation management, Systematic literature review, Multi correspondence analysis},
abstract = {Three-dimensional printing (3DP) is a technological innovation that has been receiving an increased amount of attention – both in the media and among scholars – as a result of its profound implications for business, industry, and society. Although it has existed since the 1980s, literature reviews covering major aspects of 3DP technology through the lenses of business and management studies remain limiting with regards to their scope and insights. Through a systematic literature review of 192 manuscripts published in top-tier journals indexed in Scopus and Web of Science scholarly databases, this study combines the results of a Multiple Correspondence Analysis and a content analysis to holistically elaborate principal research themes, theoretical frameworks, and future research trends. Major research themes, summarized respectively as: industrial revolution; strategy; technology adoption and governance; performance; risk and uncertainty; human resources; innovation; and sustainability and circular economy, are closely analyzed, and research predictions are provided with regards to the topics of Industry 5.0, future of governance and 3DP adoption, operations performance and supply chain management, and sustainable development and circular economy. Theoretical contributions explore and consolidate the most relevant theoretical foundations of 3DP as a research field and offer guidelines for scholars to consider in future projects.}
}
@article{AGNIHOTRI202398,
title = {Challenges, opportunities, and advances related to COVID-19 classification based on deep learning},
journal = {Data Science and Management},
volume = {6},
number = {2},
pages = {98-109},
year = {2023},
issn = {2666-7649},
doi = {https://doi.org/10.1016/j.dsm.2023.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2666764923000188},
author = {Abhishek Agnihotri and Narendra Kohli},
keywords = {Classification, COVID-19, Coronavirus, Deep learning, CAD system},
abstract = {The novel coronavirus disease, or COVID-19, is a hazardous disease. It is endangering the lives of many people living in more than two hundred countries. It directly affects the lungs. In general, two main imaging modalities, i.e., computed tomography (CT) and chest x-ray (CXR) are used to achieve a speedy and reliable medical diagnosis. Identifying the coronavirus in medical images is exceedingly difficult for diagnosis, assessment, and treatment. It is demanding, time-consuming, and subject to human mistakes. In biological disciplines, excellent performance can be achieved by employing artificial intelligence (AI) models. As a subfield of AI, deep learning (DL) networks have drawn considerable attention than standard machine learning (ML) methods. DL models automatically carry out all the steps of feature extraction, feature selection, and classification. This study has performed comprehensive analysis of coronavirus classification using CXR and CT imaging modalities using DL architectures. Additionally, we have discussed how transfer learning is helpful in this regard. Finally, the problem of designing and implementing a system using computer-aided diagnostic (CAD) to find COVID-19 using DL approaches highlighted a future research possibility.}
}
@article{MUFFOLETTO202380,
title = {Combining generative modelling and semi-supervised domain adaptation for whole heart cardiovascular magnetic resonance angiography segmentation},
journal = {Journal of Cardiovascular Magnetic Resonance},
volume = {25},
number = {1},
pages = {80},
year = {2023},
issn = {1097-6647},
doi = {https://doi.org/10.1186/s12968-023-00981-6},
url = {https://www.sciencedirect.com/science/article/pii/S1097664724010548},
author = {Marica Muffoletto and Hao Xu and Karl P. Kunze and Radhouene Neji and René Botnar and Claudia Prieto and Daniel Rückert and Alistair A. Young},
keywords = {Deep learning, Whole-heart segmentation, Domain adaptation, Generative adversarial networks, Variational auto-encoders},
abstract = {Background
Quantification of three-dimensional (3D) cardiac anatomy is important for the evaluation of cardiovascular diseases. Changes in anatomy are indicative of remodeling processes as the heart tissue adapts to disease. Although robust segmentation methods exist for computed tomography angiography (CTA), few methods exist for whole-heart cardiovascular magnetic resonance angiograms (CMRA) which are more challenging due to variable contrast, lower signal to noise ratio and a limited amount of labeled data.
Methods
Two state-of-the-art unsupervised generative deep learning domain adaptation architectures, generative adversarial networks and variational auto-encoders, were applied to 3D whole heart segmentation of both conventional (n = 20) and high-resolution (n = 45) CMRA (target) images, given segmented CTA (source) images for training. An additional supervised loss function was implemented to improve performance given 10%, 20% and 30% segmented CMRA cases. A fully supervised nn-UNet trained on the given CMRA segmentations was used as the benchmark.
Results
The addition of a small number of segmented CMRA training cases substantially improved performance in both generative architectures in both standard and high-resolution datasets. Compared with the nn-UNet benchmark, the generative methods showed substantially better performance in the case of limited labelled cases. On the standard CMRA dataset, an average 12% (adversarial method) and 10% (variational method) improvement in Dice score was obtained.
Conclusions
Unsupervised domain-adaptation methods for CMRA segmentation can be boosted by the addition of a small number of supervised target training cases. When only few labelled cases are available, semi-supervised generative modelling is superior to supervised methods.}
}
@article{ZHANG2023536,
title = {Data driven recurrent generative adversarial network for generalized zero shot image classification},
journal = {Information Sciences},
volume = {625},
pages = {536-552},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.01.039},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523000403},
author = {Jie Zhang and Shengbin Liao and Haofeng Zhang and Yang Long and Zheng Zhang and Li Liu},
keywords = {Generalized zero-shot learning, Data-driven sampling, Prototype synthesis, Recurrent adversarial network},
abstract = {Traditional Generative Adversarial Network (GAN) based Generalized Zero Shot Learning (GZSL) methods usually suffer from a problem that these methods ignore the differences between classes when using the standard normal distribution to fit the true distribution of each category, and the incompleteness of a single adversarial training makes the model unable to capture all the characteristics of the samples. To address this problem, a data-driven recurrent adversarial generative network is proposed in this paper. We first synthesize visual prototypes for unseen classes using the transformation from semantic attributes to visual prototypes learned on seen classes. Then, some noise is generated from these prototypes to synthesize the unseen samples according to the corresponding semantic attributes. During the sample generation process, a recurrent generative adversarial network is designed to facilitate the generated visual features to be more representative. Extensive experiments on five popular datasets as well as detailed ablation studies demonstrate the effectiveness and superiority of the proposed method.}
}
@article{LAU2023105828,
title = {(Let's) playing by the rules: A choice of law rule for communication of copyright material from video games to the public, through Let's Plays},
journal = {Computer Law & Security Review},
volume = {49},
pages = {105828},
year = {2023},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2023.105828},
url = {https://www.sciencedirect.com/science/article/pii/S0267364923000389},
author = {Joseph Lau},
keywords = {Copyright, Internet, Let's Plays, Video games, Private international law},
abstract = {Several proposals have been made regarding a choice of law rule for ‘ubiquitous infringements’ (the unauthorised dissemination of copyright material online) but none have been implemented by national courts, which continue to struggle with the issue of what law determines whether ubiquitous infringements have occurred. This article explores fresh solutions to that issue, focusing on the scenario where copyright material from video games is communicated to the public, through its inclusion in Let's Plays (playthroughs of video games streamed from platforms like YouTube), or where such use of that material, under the terms of a license, is contemplated. In this scenario, the issue of infringement should be governed by the law of the place of the video game developer's incorporation, as a proxy for laws qualifying as the lex loci protectionis (law of the country where protection is sought (Fawcett & Torremans (2011)), abbreviated as the LLP). Where any party can prove specific differences between the law of the place of the developer's incorporation and a law qualifying as the LLP (called State A's law for ease of reference), in aspects essential for deciding whether infringement has occurred, the forum court must issue separate rulings as to whether (i) the claimant's copyrights under State A's laws have been infringed; and (ii) the claimant's copyrights under laws besides those of State A have been infringed. Courts should also adopt, as a mandatory rule of their domestic law, a rule precluding de facto infringements of copyrights in video games and/or their constituent elements from giving rise to liability for infringement.}
}
@article{2023I,
title = {Full issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {16},
number = {5},
pages = {I-CLXI},
year = {2023},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(23)00177-8},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X23001778}
}
@article{CHEN2023343,
title = {A Core Genome Multilocus Sequence Typing Scheme for Proteus mirabilis},
journal = {Biomedical and Environmental Sciences},
volume = {36},
number = {4},
pages = {343-352},
year = {2023},
issn = {0895-3988},
doi = {https://doi.org/10.3967/bes2023.040},
url = {https://www.sciencedirect.com/science/article/pii/S0895398823000594},
author = {Sheng Lin CHEN and Yu Tong KANG and Yi He LIANG and Xiao Tong QIU and Zhen Jun LI},
keywords = {, CgMLST, Genotyping, Clonal evolution, ChewBBACA},
abstract = {Objective
A core genome multilocus sequence typing (cgMLST) scheme to genotype and identify potential risk clonal groups (CGs) in Proteus mirabilis.
Methods
In this work, we propose a publicly available cgMLST scheme for P. mirabilis using chewBBACA. In total 72 complete P. mirabilis genomes, representing the diversity of this species, were used to set up a cgMLST scheme targeting 1,842 genes, 635 unfinished (contig, chromosome, and scaffold) genomes were used for its validation.
Results
We identified a total of 205 CGs from 695 P. mirabilis strains with regional distribution characteristics. Of these, 159 unique CGs were distributed in 16 countries. CG20 and CG3 carried large numbers of shared and unique antibiotic resistance genes. Nine virulence genes (papC, papD, papE, papF, papG, papH, papI, papJ, and papK) related to the P fimbrial operon that cause severe urinary tract infections were only found in CG20. These CGs require attention due to potential risks.
Conclusion
This research innovatively performs high-resolution molecular typing of P. mirabilis using whole-genome sequencing technology combined with a bioinformatics pipeline (chewBBACA). We found that the CGs of P. mirabilis showed regional distribution differences. We expect that our research will contribute to the establishment of cgMLST for P. mirabilis.}
}
@article{DOO2023877,
title = {Exploring the Clinical Translation of Generative Models Like ChatGPT: Promise and Pitfalls in Radiology, From Patients to Population Health},
journal = {Journal of the American College of Radiology},
volume = {20},
number = {9},
pages = {877-885},
year = {2023},
issn = {1546-1440},
doi = {https://doi.org/10.1016/j.jacr.2023.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S1546144023005161},
author = {Florence X. Doo and Tessa S. Cook and Eliot L. Siegel and Anupam Joshi and Vishwa Parekh and Ameena Elahi and Paul H. Yi},
keywords = {generative artificial intelligence, radiology, limitations, large language models, ChatGPT},
abstract = {Generative artificial intelligence (AI) tools such as GPT-4, and the chatbot interface ChatGPT, show promise for a variety of applications in radiology and health care. However, like other AI tools, ChatGPT has limitations and potential pitfalls that must be considered before adopting it for teaching, clinical practice, and beyond. We summarize five major emerging use cases for ChatGPT and generative AI in radiology across the levels of increasing data complexity, along with pitfalls associated with each. As the use of AI in health care continues to grow, it is crucial for radiologists (and all physicians) to stay informed and ensure the safe translation of these new technologies.}
}
@article{COMPANY2023103486,
title = {A Functional Classification of Text Annotations for Engineering Design},
journal = {Computer-Aided Design},
volume = {158},
pages = {103486},
year = {2023},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2023.103486},
url = {https://www.sciencedirect.com/science/article/pii/S0010448523000180},
author = {Pedro Company and Jorge D. Camba and Stanislao Patalano and Ferdinando Vitolo and Antonio Lanzotti},
keywords = {Annotations, Model-based definition, Text annotations},
abstract = {Describing and supplementing geometric shapes (parts) and layouts (assemblies) with relevant information is key for successful product design communication. 3D annotation tools are widely available in commercial systems, but they are generally used in the same manner as 2D annotations in traditional engineering drawings. The gap between technology and practices is particularly evident in plain text annotations. In this paper, we introduce a functional classification of text annotations to provide an information framework for shifting traditional annotation practices towards the Model-Based Definition (MBD) paradigm. In our view, the current classification of dimensions, tolerances, symbols, notes, and text does not stress the inherent properties of two broader categories: symbols and text. Symbol-based annotations use a symbolic language (mostly standardized) such as Geometric Dimensioning and Tolerancing (GD&T) to provide precise information about the implications of geometric imperfections in manufacturing, whereas notes and text are based on non-standardized and unstructured plain text, and can be used to convey design information. We advocate that text annotations can be characterized in four different functional types (objectives, requirements, rationale, and intent), which should be classified as such when annotations are added to a model. The identification and definition of a formalized structure and syntax can enable the management of the annotations as separate entities, thus leveraging their individual features, or as a group to gain a global and collective view of the design problem. The proposed classification was tested with a group of users in a redesign task that involved a series of geometric changes to an annotated assembly model.}
}
@article{2023A14,
title = {Instructions for authors},
journal = {Gastrointestinal Endoscopy},
volume = {98},
number = {1},
pages = {A14-A20},
year = {2023},
issn = {0016-5107},
doi = {https://doi.org/10.1016/j.gie.2023.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0016510723026196}
}
@article{WANG20231831,
title = {Artificial-intelligence-led revolution of construction materials: From molecules to Industry 4.0},
journal = {Matter},
volume = {6},
number = {6},
pages = {1831-1859},
year = {2023},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2023.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S2590238523002023},
author = {Xing Quan Wang and Pengguang Chen and Cheuk Lun Chow and Denvid Lau},
keywords = {construction materials, artificial intelligence, machine learning, Industry 4.0},
abstract = {Summary
Industry 4.0 promotes the transformation of manufacturing industry to intelligence, which demands advances in materials, devices, and systems of the construction industry. Researchers of construction materials have incorporated artificial intelligence technology to accelerate these advances. From this perspective, we evaluate the latest advances in applying machine learning to the development of concrete, fiber-reinforced composites, and metals in improving their durability, sustainability, safety, and recyclability. We highlight how artificial intelligence addresses the challenges of material research, emphasizing the peculiarities of the construction industry under the Industry 4.0 framework. Based on the advances in artificial intelligence, we envision integration with Industry 4.0, starting with digitization of construction materials, progressing to advanced manufacturing, and eventually aiming to the level of intelligent application and operation of buildings. A revolutionary future can be envisaged in which design, manufacturing, and application of construction materials involve the meticulous integration of artificial intelligence, big data with all theory, experiments, and computations.}
}
@article{HAN2023262,
title = {Multiscale progressive text prompt network for medical image segmentation},
journal = {Computers & Graphics},
volume = {116},
pages = {262-274},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.08.030},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323002170},
author = {Xianjun Han and Qianqian Chen and Zhaoyang Xie and Xuejun Li and Hongyu Yang},
keywords = {Medical image segmentation, Multiscale progressive network, Text prompt, Multimodal image segmentation},
abstract = {The accurate segmentation of medical images is a crucial step in obtaining reliable morphological statistics. However, training a deep neural network for this task requires a large amount of labeled data to ensure high-accuracy results. To address this issue, we propose using progressive text prompts as prior knowledge to guide the segmentation process. Our model consists of two stages. In the first stage, we perform contrastive learning on natural images to pretrain a powerful prior prompt encoder (PPE). This PPE leverages text prior prompts to generate multimodality features. In the second stage, medical image and text prior prompts are sent into the PPE inherited from the first stage to achieve the downstream medical image segmentation task. A multiscale feature fusion block (MSFF) combines the features from the PPE to produce multiscale multimodality features. These two progressive features not only bridge the semantic gap but also improve prediction accuracy. Finally, an UpAttention block refines the predicted results by merging the image and text features. This design provides a simple and accurate way to leverage multiscale progressive text prior prompts for medical image segmentation. Compared with using only images, our model achieves high-quality results with low data annotation costs. Moreover, our model not only has excellent reliability and validity on medical images but also performs well on natural images. The experimental results on different image datasets demonstrate that our model is effective and robust for image segmentation.}
}
@article{AN2023104509,
title = {Current state and future directions for deep learning based automatic seismic fault interpretation: A systematic review},
journal = {Earth-Science Reviews},
volume = {243},
pages = {104509},
year = {2023},
issn = {0012-8252},
doi = {https://doi.org/10.1016/j.earscirev.2023.104509},
url = {https://www.sciencedirect.com/science/article/pii/S0012825223001988},
author = {Yu An and Haiwen Du and Siteng Ma and Yingjie Niu and Dairui Liu and Jing Wang and Yuhan Du and Conrad Childs and John Walsh and Ruihai Dong},
keywords = {Systematic literature review, Deep learning, DL, Seismic fault interpretation, Artificial intelligence, Convolutional neural network},
abstract = {Automated seismic fault interpretation has been an active area of research. Since 2018, Deep learning (DL) based seismic fault interpretation methods have emerged and shown promising results. However, to date, these methods have not been reasonably summarised, making it difficult for those involved to make sense of the current development process. To close this gap, we systematically reviewed the DL-based fault interpretation literature published between 2012 and 2022, and searched seven digital libraries. Fault interpretation has been considered an image-processing task using only convolutional neural networks (CNN)-based DL methods, and most of them have been trained in a supervised manner. U-Net and its variants designed for the image segmentation task are the most commonly used network structures. A total of 73 seismic datasets were summarised from the 56 articles included, of which only three field datasets and four synthetic datasets were publicly available benchmarks. The study reported benefits of using DL, such as its outstanding learning and generalisation capabilities or predicting faults in a fast, cheap and repeatable manner, which ultimately led to an increase in the acceptability of these methods and the potential to incorporate them into oil and industry workflows. However, we identified 12 challenges that hinder its integration into industrial workflows, including the most discussed lack of sufficient annotated data. We conclude with an in-depth discussion of current research trends and potential future research directions to promote research on less studied areas and collaboration between computer scientists and geoscientists.}
}
@article{2023228,
title = {Guide for Authors},
journal = {Intelligent Medicine},
volume = {3},
number = {3},
pages = {228-234},
year = {2023},
issn = {2667-1026},
doi = {https://doi.org/10.1016/S2667-1026(23)00055-4},
url = {https://www.sciencedirect.com/science/article/pii/S2667102623000554}
}
@article{MIRIKHARAJI2023102863,
title = {A survey on deep learning for skin lesion segmentation},
journal = {Medical Image Analysis},
volume = {88},
pages = {102863},
year = {2023},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.102863},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523001238},
author = {Zahra Mirikharaji and Kumar Abhishek and Alceu Bissoto and Catarina Barata and Sandra Avila and Eduardo Valle and M. Emre Celebi and Ghassan Hamarneh},
keywords = {Skin lesion, Deep learning, Segmentation, Survey},
abstract = {Skin cancer is a major public health problem that could benefit from computer-aided diagnosis to reduce the burden of this common disease. Skin lesion segmentation from images is an important step toward achieving this goal. However, the presence of natural and artificial artifacts (e.g., hair and air bubbles), intrinsic factors (e.g., lesion shape and contrast), and variations in image acquisition conditions make skin lesion segmentation a challenging task. Recently, various researchers have explored the applicability of deep learning models to skin lesion segmentation. In this survey, we cross-examine 177 research papers that deal with deep learning-based segmentation of skin lesions. We analyze these works along several dimensions, including input data (datasets, preprocessing, and synthetic data generation), model design (architecture, modules, and losses), and evaluation aspects (data annotation requirements and segmentation performance). We discuss these dimensions both from the viewpoint of select seminal works, and from a systematic viewpoint, examining how those choices have influenced current trends, and how their limitations should be addressed. To facilitate comparisons, we summarize all examined works in a comprehensive table as well as an interactive table available online33https://github.com/sfu-mial/skin-lesion-segmentation-survey..}
}
@article{SAHEB2023102316,
title = {Topical review of artificial intelligence national policies: A mixed method analysis},
journal = {Technology in Society},
volume = {74},
pages = {102316},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102316},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23001215},
author = {Tahereh Saheb and Tayebeh Saheb},
keywords = {Artificial intelligence, National policy, Topic modeling, Qualitative content analysis, Ethic, Responsible, Open data},
abstract = {A number of countries have adopted national policies and directives to balance the advantages and disadvantages of innovative technologies. The purpose of this paper is to identify the most prominent topics addressed by national AI policies, as well as their relative importance across nations. This paper integrates the results of a topic modeling analysis of 30 national AI policies with a qualitative content analysis of the policies. Based on this analysis, fourteen main common themes have been identified among national AI policies, which predominantly relate to educational, technological, government, ethical/legal, and social good concerns. Following this, we conducted a co-occurrence analysis of topics across countries to determine the extent of topic prioritization in each country. In this investigation, several marginalized AI policy topics were also identified. In general, the challenges and concerns of the majority of policies pertain to education, technology, and the government. Governments refer to real-world projects and investments in AI technologies without developing shared digital governance platforms that promote responsible and sustainable AI among technology titans and mitigate the negative effects of surveillance capitalism. Although governments acknowledge the ethical and legal aspects of AI development and frequently cite the GDPR, they limit their discussion to the data level, particularly data sharing, and marginalize ethical algorithms and other phases of data and AI management and design. In addition, government policies marginalize AI startups and the API economy, even though they play a crucial role in fostering the AI ecosystem. The paper contributes to the existing literature on AI policy and will serve as a guide for AI policymakers to help them better understand the topical similarities across countries and the neglected or marginalized challenges that require further attention.}
}
@article{HASHEM2023102974,
title = {Speech emotion recognition approaches: A systematic review},
journal = {Speech Communication},
volume = {154},
pages = {102974},
year = {2023},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2023.102974},
url = {https://www.sciencedirect.com/science/article/pii/S0167639323001085},
author = {Ahlam Hashem and Muhammad Arif and Manal Alghamdi},
keywords = {Speech emotion recognition, Emotional speech database, Classification of emotion, Speech features, Systematic review},
abstract = {The speech emotion recognition (SER) field has been active since it became a crucial feature in advanced Human–Computer Interaction (HCI), and wide real-life applications use it. In recent years, numerous SER systems have been covered by researchers, including the availability of appropriate emotional databases, selecting robustness features, and applying suitable classifiers using Machine Learning (ML) and Deep Learning (DL). Deep models proved to perform more accurately for SER than conventional ML techniques. Nevertheless, SER is yet challenging for classification where to separate similar emotional patterns; it needs a highly discriminative feature representation. For this purpose, this survey aims to critically analyze what is being done in this field of research in light of previous studies that aim to recognize emotions using speech audio in different aspects and review the current state of SER using DL. Through a systematic literature review whereby searching selected keywords from 2012–2022, 96 papers were extracted and covered the most current findings and directions. Specifically, we covered the database (acted, evoked, and natural) and features (prosodic, spectral, voice quality, and teager energy operator), the necessary preprocessing steps. Furthermore, different DL models and their performance are examined in depth. Based on our review, we also suggested SER aspects that could be considered in the future.}
}
@article{DEDELOUDI2023122818,
title = {Machine learning in additive manufacturing & Microfluidics for smarter and safer drug delivery systems},
journal = {International Journal of Pharmaceutics},
volume = {636},
pages = {122818},
year = {2023},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2023.122818},
url = {https://www.sciencedirect.com/science/article/pii/S0378517323002387},
author = {Aikaterini Dedeloudi and Edward Weaver and Dimitrios A. Lamprou},
keywords = {Machine learning, Quality by design, Additive manufacturing, 3D printing, Microfluidics, Algorithms},
abstract = {A new technological passage has emerged in the pharmaceutical field, concerning the management, application, and transfer of knowledge from humans to machines, as well as the implementation of advanced manufacturing and product optimisation processes. Machine Learning (ML) methods have been introduced to Additive Manufacturing (AM) and Microfluidics (MFs) to predict and generate learning patterns for precise fabrication of tailor-made pharmaceutical treatments. Moreover, regarding the diversity and complexity of personalised medicine, ML has been part of quality by design strategy, targeting towards the development of safe and effective drug delivery systems. The utilisation of different and novel ML techniques along with Internet of Things sensors in AM and MFs, have shown promising aspects regarding the development of well-defined automated procedures towards the production of sustainable and quality-based therapeutic systems. Thus, the effective data utilisation, prospects on a flexible and broader production of “on demand” treatments. In this study, a thorough overview has been achieved, concerning scientific achievements of the past decade, which aims to trigger the research interest on incorporating different types of ML in AM and MFs, as essential techniques for the enhancement of quality standards of customised medicinal applications, as well as the reduction of variability potency, throughout a pharmaceutical process.}
}
@article{MCKAY2023100321,
title = {Inalienable data: Ethical imaginaries of de-identified health data ownership},
journal = {SSM - Qualitative Research in Health},
volume = {4},
pages = {100321},
year = {2023},
issn = {2667-3215},
doi = {https://doi.org/10.1016/j.ssmqr.2023.100321},
url = {https://www.sciencedirect.com/science/article/pii/S2667321523001051},
author = {Francis McKay and Darren Treanor and Nina Hallowell},
keywords = {Artificial intelligence, Health data, Ownership, De-identification, Anonymity, Consent, Public involvement},
abstract = {Many legal, ethical, and regulatory frameworks allow de-identified health data to be shared for research without patients’ opt-in consent. However, there may be public concerns about this practice, as people may feel they should have some say in how such data is used. This paper introduces the concept of the “inalienability of de-identified data,” to describe a key assumption underlying that public concern and preference. The assumption, derived from ethnographic research with public and professional stakeholders in AI driven medical image analysis over the past two years, refers to a sense of felt ownership over de-identified health data, even where the subject has been obscured as referent and no clear legal rights of data ownership otherwise exist. The concept is important to medical ethics because it underpins public expectations regarding the rights people should have over the sharing of medical data (including expectations for consent). We note that where those expectations go counter to current legal and bioethical frameworks for de-identified data sharing, they provide a challenge for public support of big data and artificial intelligence driven health research.}
}
@article{RAZI2023102981,
title = {Emergency remote teaching adaptation of the anonymous multi–mediated writing model},
journal = {System},
volume = {113},
pages = {102981},
year = {2023},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2023.102981},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X23000039},
author = {Salim Razı},
keywords = {Anonymous multi–mediated writing model, Academic integrity, Preventing plagiarism, Asynchronous online peer feedback, Asynchronous online teacher feedback, ERT (Emergency remote teaching)},
abstract = {Covid-19 related transfer of instruction to digital platforms has heightened the complications involved in teaching writing, including assessment problems regarding the increased risk of academic misconduct incidents. This study aimed at scrutinizing how the revised anonymous multi–mediated writing model fits emergency remote teaching (ERT), ensuring the promotion of academic integrity. The revised model was implemented throughout a two–semester freshmen “Writing Skills” course via a mixed methods triangulation research design in the ELT department of a university in Türkiye. Quantitative data came from writing assignments and peer feedback analyses, whereas qualitative data were retrieved through reflection papers and interviews. Students' ERT scores were compared to pre-Covid face-to-face (F2F) learning scores, revealing no significant differences; confirming that students’ performances were similar in F2F or ERT without any increase in academic misconduct in ERT. The AMMW model worked well in ERT by enabling scaffolding through asymmetrical and symmetrical asynchronous online feedback, with the integration of a rubric as the learning tool. Qualitative findings revealed the limitations of online teaching, especially regarding the importance of teacher–student(s) interaction. As an anthology of L2 writing practice amid the Covid-19 outbreak, this study may help other academics to cope with cases resembling those presented here.}
}
@article{MORADIDAKHEL2023111734,
title = {GitHub Copilot AI pair programmer: Asset or Liability?},
journal = {Journal of Systems and Software},
volume = {203},
pages = {111734},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111734},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001292},
author = {Arghavan {Moradi Dakhel} and Vahid Majdinasab and Amin Nikanjam and Foutse Khomh and Michel C. Desmarais and Zhen Ming (Jack) Jiang},
keywords = {Code completion, Language model, GitHub copilot, Testing},
abstract = {Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by OpenAI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot’s proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of humans’ solutions is greater than Copilot’s suggestions, while the buggy solutions generated by Copilot require less effort to be repaired. Based on our findings, if Copilot is used by expert developers in software projects, it can become an asset since its suggestions could be comparable to humans’ contributions in terms of quality. However, Copilot can become a liability if it is used by novice developers who may fail to filter its buggy or non-optimal solutions due to a lack of expertise.}
}
@article{KANG2023102326,
title = {Delineating development trends of nanotechnology in the semiconductor industry: Focusing on the relationship between science and technology by employing structural topic model},
journal = {Technology in Society},
volume = {74},
pages = {102326},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102326},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23001318},
author = {Inje Kang and Jiseong Yang and Wonjae Lee and Eun-Yeong Seo and Duk Hee Lee},
keywords = {Nanotechnology, Science-oriented nanotechnology, Technology-oriented nanotechnology, S&T interrelationships, Development opportunities, Structural topic model},
abstract = {The bibliometrics research on nanotechnology highlights close interrelationships between scientific and technological activities (S&T) in the field of nanotechnology. Notwithstanding abundant empirical evidence on the mutual relations between S&T, the dynamics of the relationship from a contextual perspective have gained relatively little attention. Accordingly, our understanding of how science- and technology-oriented nanotechnology identifies development opportunities from each other is still at a nascent stage. To address this gap, by focusing on nanotechnology in the semiconductor industry, we use structural topic model to empirically explore the dynamic interrelationships between science- and technology-oriented nanotechnology. We empirically delineate the dynamic development trends in the context of the interrelationships between S&T and demonstrate how development opportunities are identified from each other. These findings show a new window of opportunities for how state-of-the-art models for semantic analysis can be used in the literature on S&T interrelationships.}
}
@article{SMITH2023102672,
title = {Designing and implementing an instructional triptych for a digital future},
journal = {The Journal of Academic Librarianship},
volume = {49},
number = {2},
pages = {102672},
year = {2023},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2023.102672},
url = {https://www.sciencedirect.com/science/article/pii/S0099133323000113},
author = {Erin Sweeney Smith and Amanda Koziura and Elizabeth Meinke and Evan Meszaros},
keywords = {Canvas, LibGuides, Instruction, Asynchronous, Course design},
abstract = {In response to the evolving needs of students and faculty, a small team of librarians rebuilt their library instruction program from the ground-up in 2020. The new approach consisted of three aspects: shifting introductory lessons to easily-accessible Canvas LMS modules, revamping LibGuides, and introducing a credit-bearing course. Together, they allowed librarians to move beyond traditional one-shot instruction, form deeper partnerships with faculty, and make the expertise of librarians more accessible. While the impetus for change was the onset of the COVID-19 pandemic, the new program laid the groundwork for rethinking traditional approaches to instruction and finding better ways to meet faculty and students at the point of need.}
}
@article{HEISS2023107908,
title = {Social media information literacy: Conceptualization and associations with information overload, news avoidance and conspiracy mentality},
journal = {Computers in Human Behavior},
volume = {148},
pages = {107908},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107908},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002595},
author = {Raffael Heiss and Andreas Nanz and Jörg Matthes},
keywords = {Social media, Media literacy, Information literacy, Information overload, News avoidance, Conspiracy mentality},
abstract = {In this study, we present a novel scale for measuring social media information literacy (SMIL) that encompasses six sub-dimensions: navigation, curation, appraisal, comprehension, creation, and interaction. We also examine antecedents of SMIL, its association with information overload, and possible indirect consequences such as news avoidance and conspiracy thinking. Relying on a two-wave panel dataset (n = 901), we first used factor analysis to test the proposed measurement. The results showed that the six dimensions were empirically distinct and loaded on a higher order SMIL factor. In a second step, we explored antecedents and outcomes of SMIL and its sub-dimensions. We found that not age, but education and frequency of social media use were positively associated with gains in SMIL. Furthermore, SMIL was associated with a decrease in information overload. Information overload, in turn, was associated with a decrease in news avoidance and an increase in conspiracy mentality. Taken together, our results lend support that SMIL may support positive civic outcomes by its potential role in lowering information overload. Helping citizens to acquire SMIL may be one valuable measure to foster democratic resilience.}
}
@article{KAPLANWEINGER2023102669,
title = {Thirty years on: Planetary climate planning and the Intergovernmental Negotiating Committee},
journal = {Global Environmental Change},
volume = {80},
pages = {102669},
year = {2023},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2023.102669},
url = {https://www.sciencedirect.com/science/article/pii/S0959378023000353},
author = {Benjamin {Kaplan Weinger}},
keywords = {Political geography, North-South relations, Climate governance, Climate planning, Climate justice},
abstract = {On the occasion of the thirtieth anniversary of the United Nations Framework Convention on Climate Change, this principal supra-national institution remains paramount to the project of planetary climate planning and governance. Reflections on this anniversary should serve to recall the contestations through which this foundational institution was formed, and the delegate dynamics that continue to be reproduced in its wake. The contentious debates and political dynamics that afflicted the Intergovernmental Negotiating Committee tasked with crafting the Framework Convention on Climate Change, as well as dissension in the periphery, remain as relevant today as they were three decades ago. Reprising these dynamics through detailed historical and archival analysis, this article excavates the negotiations of the 1992 Framework Convention on Climate Change by the Intergovernmental Negotiating Committee, which met in 5 sessions during 1991–1992. The aim is to identify key fault-lines and conflicts in the lead-up to the finalization of the 1992 Convention, in order to demonstrate whose epistemic and normative commitments came to be reflected in the final outcome and to show how the legacy of this process endures to date. I seek to render visible actors and proposals peripheralized in the formation of planetary climate governance to extrapolate normative boundaries and proffer heterodox lessons from the margins.}
}
@article{LI2023126720,
title = {Artificial intelligence accelerates multi-modal biomedical process: A Survey},
journal = {Neurocomputing},
volume = {558},
pages = {126720},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126720},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223008433},
author = {Jiajia Li and Xue Han and Yiming Qin and Feng Tan and Yulong Chen and Zikai Wang and Haitao Song and Xi Zhou and Yuan Zhang and Lun Hu and Pengwei Hu},
keywords = {Multi-modal biomedicine, Artificial intelligence, Deep learning, Neural network},
abstract = {The abundance of artificial intelligence AI algorithms and growing computing power has brought a disruptive revolution to the smart medical industry. Its powerful data abstraction and representation capabilities enable the modeling of hundreds of millions of medical data, such as sub-Computed Tomography tumor identification, retinal lesion screening, and survival curve analysis. However, all of these applications demonstrate AI’s use of unimodal data for specific tasks. In contrast, clinicians deal with multi-modal data from multiple sources when diagnosing, performing prognostic assessments, and deciding on treatment plans. These requirements have facilitated the development of multi-modal AI solutions and improved the performance of AI models in handling complex medical scenarios and data. In this paper, we provide an overview of the current state of the art and research in multi-modal biomedical AI, including applications, data, methods, and analytics. Additionally, we summarize potential research directions for multi-modal AI technologies in the future of healthcare.}
}
@article{CHO2023102212,
title = {Text mining method to identify artificial intelligence technologies for the semiconductor industry in Korea},
journal = {World Patent Information},
volume = {74},
pages = {102212},
year = {2023},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2023.102212},
url = {https://www.sciencedirect.com/science/article/pii/S017221902300042X},
author = {Insu Cho and Yonghan Ju},
keywords = {Semiconductor, Patent, International patent classification, Network analysis, Text mining},
abstract = {Semiconductors are among the most important core technologies contributing to the Fourth Industrial Revolution. The United States, Taiwan, and China have been investing heavily in semiconductor research and development. To achieve international competitiveness in the semiconductor industry, Korea needs to establish a research and development (R&D) roadmap for small- and medium-sized enterprises (SMEs). Our study identified trends in the semiconductor industry by analyzing the characteristics of core technologies based on patents that disclose technologies instead of holding exclusive ownership. Specifically, we analyzed registered patents concerned with artificial intelligence and machine learning pertaining to the semiconductor industry, which are attracting considerable attention. Using the Korea Intellectual Property Rights Information Service database, we identified 3569 patent specifications related to AI technology and the semiconductor industry. The text mining and network analysis results indicated that the application of deep neural networks is the most important and affects various aspects of R&D. Particularly, AI technology is actively studied for monitoring manufacturing and etch processes. Additionally, technology convergence among virtual reality, visualization, smart factories, and etching technology was identified. The analysis results identify promising technologies related to semiconductors and provide insights that would enable SMEs in the Korean semiconductor industry to establish a technology roadmap.}
}
@incollection{HAMILTON2023371,
title = {Chapter 16 - Natural Language Processing},
editor = {Julien Delarue and J. Ben Lawlor},
booktitle = {Rapid Sensory Profiling Techniques (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {371-410},
year = {2023},
series = {Woodhead Publishing Series in Food Science, Technology and Nutrition},
isbn = {978-0-12-821936-2},
doi = {https://doi.org/10.1016/B978-0-12-821936-2.00004-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219362000042},
author = {Leah Marie Hamilton and Jacob Lahne},
keywords = {Natural Language Processing, Machine learning, Deep learning, Text analysis, Computational linguistics, Sensory evaluation, Descriptive analysis},
abstract = {Sensory evaluation is predicated on the use and interpretation of human language. We ask our subjects to describe their sensory experiences and affective responses, which we cannot directly observe. This formulation of sensory science encourages direct engagement with linguistics and in particular, a recent subfield of linguistics, computer science, and artificial intelligence called “Natural Language Processing” (NLP, sometimes “computational linguistics”). In this chapter we will provide an introduction to Natural Language Processing (NLP) for sensory scientists who wish to employ NLP as a rapid method for sensory evaluation. Because NLP is a large, diverse, and rapidly evolving field, we will begin with a brief, pragmatic overview of the discipline, with an emphasis on key historical and current methods and applications. We will then briefly discuss the linguistic perspective and its application to sensory evaluation, with an aim to motivating the remaining chapter. Following that, we will discuss key areas of NLP, from data collection to processing to analysis to advanced applications. Throughout the chapter, we will use a consistent case study of natural-language descriptions for a food product to provide examples and illustrate NLP methods.}
}
@article{GUO2023329,
title = {AIGC challenges and opportunities related to public safety: A case study of ChatGPT},
journal = {Journal of Safety Science and Resilience},
volume = {4},
number = {4},
pages = {329-339},
year = {2023},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2023.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666449623000397},
author = {Danhuai Guo and Huixuan Chen and Ruoling Wu and Yangang Wang},
keywords = {Generative artificial intelligence， Artificial intelligence generated content, ChatGPT, Public safety, Strong artificial intelligence},
abstract = {Artificial intelligence generated content (AIGC) is a production method based on artificial intelligence (AI) technology that finds rules through data and automatically generates content. In contrast to computational intelligence, generative AI, as exemplified by ChatGPT, exhibits characteristics that increasingly resemble human-level comprehension and creation processes. This paper provides a detailed technical framework and history of ChatGPT, followed by an examination of the challenges posed to political security, military security, economic security, cultural security, social security, ethical security, legal security, machine escape problems, and information leakage. Finally, this paper discusses the potential opportunities that AIGC presents in the realms of politics, military, cybersecurity, society, and public safety education.}
}
@article{HARRER2023104512,
title = {Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine},
journal = {eBioMedicine},
volume = {90},
pages = {104512},
year = {2023},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2023.104512},
url = {https://www.sciencedirect.com/science/article/pii/S2352396423000774},
author = {Stefan Harrer},
keywords = {Generative artificial intelligence, Large language models, Foundation models, AI ethics, Augmented human intelligence, Information management, AI trustworthiness},
abstract = {Summary
Large Language Models (LLMs) are a key component of generative artificial intelligence (AI) applications for creating new content including text, imagery, audio, code, and videos in response to textual instructions. Without human oversight, guidance and responsible design and operation, such generative AI applications will remain a party trick with substantial potential for creating and spreading misinformation or harmful and inaccurate content at unprecedented scale. However, if positioned and developed responsibly as companions to humans augmenting but not replacing their role in decision making, knowledge retrieval and other cognitive processes, they could evolve into highly efficient, trustworthy, assistive tools for information management. This perspective describes how such tools could transform data management workflows in healthcare and medicine, explains how the underlying technology works, provides an assessment of risks and limitations, and proposes an ethical, technical, and cultural framework for responsible design, development, and deployment. It seeks to incentivise users, developers, providers, and regulators of generative AI that utilises LLMs to collectively prepare for the transformational role this technology could play in evidence-based sectors.}
}
@article{ADAMS2023113660,
title = {Competence and enterprise of management as drivers of early foreign listing of medium-sized emerging market multinationals (EMNEs) from Africa},
journal = {Journal of Business Research},
volume = {158},
pages = {113660},
year = {2023},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2023.113660},
url = {https://www.sciencedirect.com/science/article/pii/S0148296323000188},
author = {Kweku Adams and Rexford Attah-Boakye and Honglan Yu and Irene Chu and Maria Ishaque},
keywords = {Early internationalisation, Managerial competence, Foreign listing, Oil and gas multinationals, Africa},
abstract = {EMNEs from Africa are missing in global places and spaces, and Africapitalism is also meagrely represented within the capillaries of international investments, relative to the opportunities offered by globalisation and Africa’s rich natural resource endowment. Using the Penrosian MNE growth theory, we investigate how African firms' managerial competence and entrepreneurial behaviours can be enhanced by engaging foreign executive directors during pre, early and post-internationalisation. We conduct our analysis by using data from 157 companies domiciled in 17 African countries. Our results show that whilst access to liquidity, foreign managerial know-how, and experience are key drivers of early foreign listing of African EMNEs, these factors have less effect on corporate outcomes during the 3rd and 5th year without the moderating effect of foreign executive directors. We contribute to the international business and international entrepreneurship literature by showing that African EMNEs can succeed in global spaces if they leverage the expertise of foreign executive directors as they bring idiosyncratic industry and market knowledge during early internationalisation. EMNEs intending to internationalise must use a polycentric governing board structure to reflect the intended destination country. Our results imply that early listing on the international stock markets is among the key strategies latecomers use to enter a global game they are just learning to play.}
}
@article{WU2023108102,
title = {Additively manufactured materials and structures: A state-of-the-art review on their mechanical characteristics and energy absorption},
journal = {International Journal of Mechanical Sciences},
volume = {246},
pages = {108102},
year = {2023},
issn = {0020-7403},
doi = {https://doi.org/10.1016/j.ijmecsci.2023.108102},
url = {https://www.sciencedirect.com/science/article/pii/S0020740323000048},
author = {Yaozhong Wu and Jianguang Fang and Chi Wu and Cunyi Li and Guangyong Sun and Qing Li},
keywords = {Additive manufacturing, Mechanical property, Energy absorption, Cellular material, Lattice structure, Machine learning, Optimization, Defect},
abstract = {Lightweight materials and structures have been extensively studied for a wide range of applications in design and manufacturing of more environment-friendly and more sustainable products, such as less materials and lower energy consumption, while maintaining proper mechanical and energy absorption characteristics. Additive manufacturing (AM) or 3D printing techniques offer more freedom to realize some new designs of novel lightweight materials and structures in an efficient way. However, the rational design for desired mechanical properties of these materials and structures remains a demanding topic. This paper provides a comprehensive review on the recent advances in additively manufactured materials and structures as well as their mechanical properties with an emphasis on energy absorption applications. First, the additive manufacturing techniques used for fabricating various materials and structures are briefly reviewed. Then, a variety of lightweight AM materials and structures are discussed, together with their mechanical properties and energy-absorption characteristics. Next, the AM-induced defects, their impacts on mechanical properties and energy absorption, as well as the methods for minimizing the effects are discussed. After that, numerical modeling approaches for AM materials and structures are outlined. Furthermore, design optimization techniques are reviewed, including parametric optimization, topology optimization, and nondeterministic optimization with fabrication-induced uncertainties. Notably, data-driven and machine learning-based techniques exhibit compelling potential in design for additive manufacturing, process-property relations, and in-situ monitoring. Finally, significant challenges and future directions in this area are highlighted. This review is anticipated to provide a deep understanding of the state-of-the-art additively manufactured materials and structures, aiming to improve the future design for desired mechanical properties and energy absorption.}
}
@article{WANG20232106,
title = {Bringing down the heat in methanol synthesis},
journal = {Matter},
volume = {6},
number = {7},
pages = {2106-2135},
year = {2023},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2023.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S2590238523002400},
author = {Andrew Wang and Athanasios A. Tountas and Alán Aspuru-Guzik and Geoffrey A. Ozin},
abstract = {Summary
The methanol economy envisioned by Nobel laureate George Olah is growing by leaps and bounds. This growth is spurred by its burgeoning use not only as a major feedstock for a vast range of commodity chemicals and fuels but as a high-capacity and secure hydrogen (H2) storage, transportation, and delivery medium to power the hydrogen economy these days. Methanol is currently produced industrially at more than 100 million metric ton scales from fossil-sourced syngas, CO-H2, and by conventional fossil-powered heterogeneous catalysis, operated under high temperature and pressure conditions. The synthesis process is enabled by a ternary copper-zinc oxide-alumina composite (CZA), the performance metrics of which have remained pre-eminent for the past two decades. There are, however, incentives to lower the energy, economic, and environmental impact of the commercial methanol process, which functions at 250°C and 8 MPa, and to reduce its carbon footprint by switching to a CO2-H2 feedstock rather than continuing the use of CO-H2. Herein, to go beyond CZA, a surface coordination materials chemistry perspective is presented for the thermally enabled adsorption, activation, reaction, and desorption steps of CO2-H2 that ensue on metal oxide catalysts as a function of temperature and pressure. The objective is to identify periodic trends in the chemical and physical properties of the metal oxide that determine its activity and selectivity toward methanol synthesis versus the competitive reverse water gas shift carbon monoxide product. Armed with this materials chemistry perspective of methanol synthesis enabled by CZA, an enquiry is launched into how to rationally envision and design, by human-to-artificial intelligence, low-temperature metal oxide catalysts able to enhance methanol yield and reduce the energy requirement of the reaction. This chimie douce investigation culminates with an exploration of what it will take to power the methanol synthesis reaction directly with light rather than heat to ultimately reduce the dream of solar methanol refineries to practice and realize the solar advantage.}
}
@article{SOHAIL2023101675,
title = {Decoding ChatGPT: A taxonomy of existing research, current challenges, and possible future directions},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {8},
pages = {101675},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101675},
url = {https://www.sciencedirect.com/science/article/pii/S131915782300229X},
author = {Shahab Saquib Sohail and Faiza Farhat and Yassine Himeur and Mohammad Nadeem and Dag Øivind Madsen and Yashbir Singh and Shadi Atalla and Wathiq Mansoor},
keywords = {ChatGPT, Large language models (LLMs), Generative Pre-trained Transformer (GPT), AI Generated Content (AIGC), Systematic review, Trustworthy AI},
abstract = {Chat Generative Pre-trained Transformer (ChatGPT) has gained significant interest and attention since its launch in November 2022. It has shown impressive performance in various domains, including passing exams and creative writing. However, challenges and concerns related to biases and trust persist. In this work, we present a comprehensive review of over 100 Scopus-indexed publications on ChatGPT, aiming to provide a taxonomy of ChatGPT research and explore its applications. We critically analyze the existing literature, identifying common approaches employed in the studies. Additionally, we investigate diverse application areas where ChatGPT has found utility, such as healthcare, marketing and financial services, software engineering, academic and scientific writing, research and education, environmental science, and natural language processing. Through examining these applications, we gain valuable insights into the potential of ChatGPT in addressing real-world challenges. We also discuss crucial issues related to ChatGPT, including biases and trustworthiness, emphasizing the need for further research and development in these areas. Furthermore, we identify potential future directions for ChatGPT research, proposing solutions to current challenges and speculating on expected advancements. By fully leveraging the capabilities of ChatGPT, we can unlock its potential across various domains, leading to advancements in conversational AI and transformative impacts in society.}
}
@article{RAJ2023101395,
title = {FDT: A python toolkit for fake image and video detection},
journal = {SoftwareX},
volume = {22},
pages = {101395},
year = {2023},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2023.101395},
url = {https://www.sciencedirect.com/science/article/pii/S2352711023000912},
author = {Surbhi Raj and Jimson Mathew and Arijit Mondal},
keywords = {Deepfake video, Generative Adversarial Networks (GANs), Fake Detection Tool (FDT), Copy-move, Splicing, Twitter},
abstract = {With the advent of readily and widely available applications based on deepfake technology, several cybersecurity threats are on the rise. It is challenging to curtail these threats as deepfakes are realistic and very difficult to detect. The present work proposes a Fake Detection Tool (FDT) that streamlines the procedure of fake detection by incorporating various manipulation techniques and aids users in detecting and visualizing the same. The tool is also integrated with Twitter for streaming facial image posts based on hashtags. It provides an output dataframe and presents statistics of virality, sentiments, etc, using pie charts for better visualization. The proposed tool uses a wide variety of large-scale datasets for training to deal with the fakes in the wild and deploys models that are at par with the cutting-edge models. It is an efficient, user-friendly, and freely available software for fake detection. The source code of the FDT package toolkit is available at https://github.com/surbhiraj786/GUI_Fake-Detection.}
}
@article{LIU202334,
title = {Comparison of administrative and regulatory green technologies development between China and the U.S. based on patent analysis},
journal = {Data Science and Management},
volume = {6},
number = {1},
pages = {34-45},
year = {2023},
issn = {2666-7649},
doi = {https://doi.org/10.1016/j.dsm.2023.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666764923000012},
author = {Mengshu Liu and Ju’e Guo and Dan Bi},
keywords = {Administrative, regulatory or design (ARD), Green technologies, Patent analysis, Text mining, Latent dirichlet allocation (LDA), Topic evolution},
abstract = {With the increasing importance of computer intelligence in the new round of the industrial revolution, administrative, regulatory, or design (ARD) green technology contributes to improving national technological competitiveness and promoting the transformation of green technology, which is becoming an important field under sustainable development goals. The U.S. and China ranked top two in terms of paper influence and patent applications in the field of ARD green technology. However, few comparative studies have been conducted in these two countries. This study presents the evolution and landscapes of ARD green technology between China and the U.S., focusing on comparing development priorities and technical layouts in each five-year plan period. According to the “International Patent Classification (IPC) Green Inventory” launched by the World Intellectual Property Organization (WIPO), we retrieved 69,412 patents published between 2001 and 2020 from the PatSnap database. Descriptive, content, and thematic network analyses were conducted using latent dirichlet allocation (LDA) and community detection algorithms. The results show that both China and the U.S. strategically focus on ARD green technology development. The technical topics in this field can be divided into three themes: data processing systems, traffic control systems, and building designs. The emphasis on technology research and development (R&D) differs between China and the U.S. There is also evidence that the U.S. has advantages in terms of technological innovation and capabilities. However, China has an advantage in terms of data volume, and the gap between China and the U.S. is gradually narrowing. We also highlight the contributions and limitations of this study.}
}
@article{LUPI2023103996,
title = {Automatic definition of engineer archetypes: A text mining approach},
journal = {Computers in Industry},
volume = {152},
pages = {103996},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103996},
url = {https://www.sciencedirect.com/science/article/pii/S016636152300146X},
author = {Francesco Lupi and Mohammed M. Mabkhot and Eleonora Boffa and Pedro Ferreira and Dario Antonelli and Antonio Maffei and Niels Lohse and Michele Lanzetta},
keywords = {Text mining, Engineering, Professional profile, Archetype, Latent dirichlet allocation, Industry 4.0},
abstract = {With the rapid and continuous advancements in technology, as well as the constantly evolving competences required in the field of engineering, there is a critical need for the harmonization and unification of engineering professional figures or archetypes. The current limitations in tymely defining and updating engineers' archetypes are attributed to the absence of a structured and automated approach for processing educational and occupational data sources that evolve over time. This study aims to enhance the definition of professional figures in engineering by automating archetype definitions through text mining and adopting a more objective and structured methodology based on topic modeling. This will expand the use of archetypes as a common language, bridging the gap between educational and occupational frameworks by providing a unified and up-to-date engineering professional figure tailored to a specific period, specialization type, and level. We validate the automatically defined industrial engineer archetype against our previously manually defined profile.}
}
@incollection{CHANG20243,
title = {Chapter 1 - Introduction to artificial intelligence for cardiovascular clinicians},
editor = {Anthony C. Chang and Alfonso Limon},
booktitle = {Intelligence-Based Cardiology and Cardiac Surgery},
publisher = {Academic Press},
pages = {3-120},
year = {2024},
series = {Intelligence-Based Medicine: Subspecialty Series},
isbn = {978-0-323-90534-3},
doi = {https://doi.org/10.1016/B978-0-323-90534-3.00010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390534300010X},
author = {Anthony C. Chang and Alfonso Limon},
keywords = {Artificial intelligence, Cardiovascular clinicians, Deep learning technology, Human-machine intelligence continuum, Machine learning, Neuroscience},
abstract = {The impressive gains in deep learning (DL) started in 2012 and its successful utilization in image interpretation have led to the current momentum for artificial intelligence (AI) awareness and adoption. In 2016, Google DeepMind's AlphaGo software soundly defeated the best human Go champion Lee Sedol to introduce the capability of DL outside of image interpretation. More recently, there have been impressive exponential advances in natural language processing with transformer tools such as GPT-3, GPT-4, and now ChatGPT. DeepMind and its AlphaFold AI tool has been able to predict the three-dimensional (3D) structure of proteins since 2021 and was Science magazine's “Breakthrough of the Year.” All of these AI accomplishments heralded the recent new era in AI. Major universities with AI departments (such as Stanford, MIT, and Carnegie Mellon) and technology giants (such as IBM, Apple, Facebook, and Microsoft in the United States as well as other large companies such as Baidu, Alibaba, and Tencent [BAT] in China) are all fervidly exploring real-life applications of AI. There is also a movement to democratize AI so that “no-code platforms” can accommodate people who do not know how to code [1].}
}
@article{QINQIN2023100415,
title = {The effects of enterprises' attention to digital economy on innovation and cost control: Evidence from A-stock market of China},
journal = {Journal of Innovation & Knowledge},
volume = {8},
number = {4},
pages = {100415},
year = {2023},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2023.100415},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X23001117},
author = {Wu Qinqin and Sikandar Ali Qalati and Rana Yassir Hussain and Hira Irshad and Kayhan Tajeddini and Faiza Siddique and Thilini Chathurika Gamage},
keywords = {Attention, Digital economy, Innovation, Patent applications, Cost control},
abstract = {China's digital economy has made amazing achievements, which brings deep impacts on enterprise innovation. Based on unbalance panel dataset covering more than two thousand manufacturing listed companies in A-stock market of China during the 2011 to 2018 period, this paper employs two-way fixed effects (TWFE) model to examine the effects of attention to digital economy on enterprise innovation. The primary explanatory variable in this research is attention degree that enterprises pay to the digital economy measured by Python technology and text analysis. Additionally, the intermediate effect model is adopted to check the underlying mechanisms of cost control in enterprises, which is also impacted by the digital economy. Several novel findings emerge. First, the number of patent applications increase as enterprises pay more attention to the digital economy. Digital economy has positive impacts on different innovation processes, not only promotes invention, but also promotes appearance design. Second, digital technology and business model as two aspects of digital economy have different effects on innovation. The attention to digital technology has positive impacts on invention patents and design patents, while business model only has a positive impact on design patents. Third, enterprises that pay attention to the digital economy are more likely to increase their R&D expenditure and decrease their sales and finance expenses, which encourages the innovation output. This paper explains these findings in the context of China and makes some specific suggestions for enterprises to promote digital transformation and innovation.}
}
@article{DALALAH2023100822,
title = {The false positives and false negatives of generative AI detection tools in education and academic research: The case of ChatGPT},
journal = {The International Journal of Management Education},
volume = {21},
number = {2},
pages = {100822},
year = {2023},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2023.100822},
url = {https://www.sciencedirect.com/science/article/pii/S1472811723000605},
author = {Doraid Dalalah and Osama M.A. Dalalah},
keywords = {Artificial intelligence, Generative pre-trained transformer, Machine generated contents (MGC), False positive/negative, Text generation, ChatGPT},
abstract = {Generative Pre-trained Transformers like ChatGPT are examples of AI systems which produce human-like responses in different forms such as text or images that have demonstrated excellent performance in producing logical and contextually relevant answers. However, the false positive/negative detection of generative AI has been noted as a challenge. In this article, statistical experiments are conducted to test the chances of false positive and false negative detection of AI-generated text. It was found that the detected likelihoods of generative AI in articles’ abstracts is much lower than that found in paragraphs taken from the literature section of the selected articles. This means that literature parts have higher likelihoods to falsely demonstrate AI-generated text. On the other hand, when genuine texts are compared with AI-generated texts, it is observed that there is a noticeable margin of overlap between their distributions and therefore type I and type II errors fall within the realm of possibility. We show that despite these challenges, generative AI like ChatGPT continues to be a promising tool for communication and information retrieval. However, it is vital to address the concerns regarding false detection of AI generated text and ensure that these models are used in ethical and responsible conduct.}
}
@article{2023100639,
title = {Full Issue PDF},
journal = {JACC: Advances},
volume = {2},
number = {7},
pages = {100639},
year = {2023},
issn = {2772-963X},
doi = {https://doi.org/10.1016/S2772-963X(23)00615-4},
url = {https://www.sciencedirect.com/science/article/pii/S2772963X23006154}
}
@article{NICHELINI2023103166,
title = {CANova: A hybrid intrusion detection framework based on automatic signal classification for CAN},
journal = {Computers & Security},
volume = {128},
pages = {103166},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103166},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823000767},
author = {Alessandro Nichelini and Carlo Alberto Pozzoli and Stefano Longari and Michele Carminati and Stefano Zanero},
keywords = {Automotive Security, Intrusion Detection, Signal Classification, Controller Area Network, Flow and Payload based Detection},
abstract = {Over the years, vehicles have become increasingly complex and an attractive target for malicious adversaries. This raised the need for effective and efficient Intrusion Detection Systemss (IDSs) for onboard networks able to work with the stringent requirements and the heterogeneity of information transmitted on the Controller Area Network. While state-of-the-art solutions are effective in detecting specific types of anomalies and work on a subset of the CAN signals, no single method can perform better than the others on all types of attacks, particularly if they need to provide predictions to comply with the domain’s real-time constraints. In this paper, we present CANova, a modular framework that exploits the characteristics of the different Controller Area Network (CAN) packets to select the Intrusion Detection Systemss (IDSs) that better fits them. In particular, it uses flow- and payload-based IDSs to analyze the packets’ content and arrival time. We evaluate CANova by comparing its performance against state-of-the-art Intrusion Detection Systemss (IDSs) for in-vehicle network and a comprehensive set of synthetic and real attacks in real-world CAN datasets. We demonstrate that our approach can achieve good performances in terms of detection, false positive rates, and temporal performances.}
}
@article{AROMIWURA202354,
title = {Artificial intelligence in cardiac computed tomography},
journal = {Progress in Cardiovascular Diseases},
volume = {81},
pages = {54-77},
year = {2023},
issn = {0033-0620},
doi = {https://doi.org/10.1016/j.pcad.2023.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0033062023000920},
author = {Afolasayo A. Aromiwura and Tyler Settle and Muhammad Umer and Jonathan Joshi and Matthew Shotwell and Jishanth Mattumpuram and Mounica Vorla and Maryta Sztukowska and Sohail Contractor and Amir Amini and Dinesh K. Kalra},
keywords = {Artificial intelligence, Computed tomography, Deep learning, Machine learning, Cardiovascular disease},
abstract = {Artificial Intelligence (AI) is a broad discipline of computer science and engineering. Modern application of AI encompasses intelligent models and algorithms for automated data analysis and processing, data generation, and prediction with applications in visual perception, speech understanding, and language translation. AI in healthcare uses machine learning (ML) and other predictive analytical techniques to help sort through vast amounts of data and generate outputs that aid in diagnosis, clinical decision support, workflow automation, and prognostication. Coronary computed tomography angiography (CCTA) is an ideal union for these applications due to vast amounts of data generation and analysis during cardiac segmentation, coronary calcium scoring, plaque quantification, adipose tissue quantification, peri-operative planning, fractional flow reserve quantification, and cardiac event prediction. In the past 5 years, there has been an exponential increase in the number of studies exploring the use of AI for cardiac computed tomography (CT) image acquisition, de-noising, analysis, and prognosis. Beyond image processing, AI has also been applied to improve the imaging workflow in areas such as patient scheduling, urgent result notification, report generation, and report communication. In this review, we discuss algorithms applicable to AI and radiomic analysis; we then present a summary of current and emerging clinical applications of AI in cardiac CT. We conclude with AI's advantages and limitations in this new field.}
}
@article{XU202362,
title = {Artificial intelligence-aided optical imaging for cancer theranostics},
journal = {Seminars in Cancer Biology},
volume = {94},
pages = {62-80},
year = {2023},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2023.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X23000949},
author = {Mengze Xu and Zhiyi Chen and Junxiao Zheng and Qi Zhao and Zhen Yuan},
keywords = {Optical imaging, Artificial intelligence, Cancer theranostics, Precision oncology},
abstract = {The use of artificial intelligence (AI) to assist biomedical imaging have demonstrated its high accuracy and high efficiency in medical decision-making for individualized cancer medicine. In particular, optical imaging methods are able to visualize both the structural and functional information of tumors tissues with high contrast, low cost, and noninvasive property. However, no systematic work has been performed to inspect the recent advances on AI-aided optical imaging for cancer theranostics. In this review, we demonstrated how AI can guide optical imaging methods to improve the accuracy on tumor detection, automated analysis and prediction of its histopathological section, its monitoring during treatment, and its prognosis by using computer vision, deep learning and natural language processing. By contrast, the optical imaging techniques involved mainly consisted of various tomography and microscopy imaging methods such as optical endoscopy imaging, optical coherence tomography, photoacoustic imaging, diffuse optical tomography, optical microscopy imaging, Raman imaging, and fluorescent imaging. Meanwhile, existing problems, possible challenges and future prospects for AI-aided optical imaging protocol for cancer theranostics were also discussed. It is expected that the present work can open a new avenue for precision oncology by using AI and optical imaging tools.}
}
@article{JIANG2023293,
title = {Review of intelligent diagnosis methods for imaging gland cancer based on machine learning},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {4},
pages = {293-316},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000985},
author = {Han Jiang and Wenjia Sun and Hanfei Guo and Jiayuan Zeng and Xin Xue and Shuai Li},
keywords = {Gland cancer, Intelligent diagnosis, Machine learning, Deep learning, Multimodal medical images},
abstract = {Gland cancer is a high-incidence disease that endangers human health, and its early detection and treatment require efficient, accurate, and objective intelligent diagnosis methods. In recent years, the advent of machine learning techniques has yielded satisfactory results in intelligent gland cancer diagnosis based on clinical images, significantly improving the accuracy and efficiency of medical image interpretation while reducing the workload of doctors. The focus of this study is to review, classify, and analyze intelligent diagnosis methods for imaging gland cancer based on machine learning and deep learning. This paper briefly introduces some basic imaging principles of multimodal medical images, such as the commonly used computed tomography (CT), magnetic resonance imaging (MRI), ultrasound (US), positron emission tomography (PET), and pathology. In addition, the intelligent diagnosis methods for imaging gland cancer were further classified into supervised learning and weakly supervised learning. Supervised learning consists of traditional machine learning methods, such as Knearest neighbor algorithm (KNN), support vector machine (SVM), and multilayer perceptron, and deep learning methods evolving from convolutional neural network (CNN). By contrast, weakly supervised learning can be further categorized into active learning, semisupervised learning, and transfer learning. State-of-the-art methods are illustrated with implementation details, including image segmentation, feature extraction, and optimization of classifiers. Their performances are evaluated through indicators, such as accuracy, precision, and sensitivity. In conclusion, the challenges and development trends of intelligent diagnosis methods for imaging gland cancer were addressed and discussed.}
}
@article{ADDISON2023100295,
title = {Relational stigma as a social determinant of health: “I'm not what you _____see me as”},
journal = {SSM - Qualitative Research in Health},
volume = {4},
pages = {100295},
year = {2023},
issn = {2667-3215},
doi = {https://doi.org/10.1016/j.ssmqr.2023.100295},
url = {https://www.sciencedirect.com/science/article/pii/S2667321523000793},
author = {Michelle Addison and Monique Lhussier and Clare Bambra},
keywords = {Health, Inequality, Drugs, Stigma, Harm, Marginalization, Bourdieu},
abstract = {Aim
The aim of the paper is to understand how people who use drugs (PWUD) experience stigma. To examine this issue, this paper draws on Bourdieu's logic of practice to understand how social harm emerges relationally between people via ‘mechanisms of stigma’.
Methods
This paper draws on 24 qualitative semi-structured interviews with people who use drugs (heroin, crack/cocaine, amphetamine, ecstasy; 11 men/12 women/1 transgender) living in the northeast of England. Thematic analysis of data was undertaken and coded in Nvivo.
Findings
PWUD experienced stigmatisation relationally with family, employers, health workers, Criminal Justice System, and the public for reasons linked to (but not limited to) their drug use, social class position, and their appearance. Stigmatisation shaped how participants saw themselves as a person ‘lacking’ in a valued or worthy identity. Social relations had detrimental effects on mental and physical health, and how participants accessed health services.
Conclusions
Models of Social Determinants of Health (SDoH) currently focus almost entirely on a positivist, material ‘reality’ in which a person lives (housing, employment, food insecurity, healthcare, education, access to services), overlooking the ways in which social relations and a practical ‘mastery’ of social space contribute to health and inequalities. Furthermore, relational stigma shapes our experience of a healthy life; as such, stigma should be regarded as a SDoH as it contributes to a widening of health inequalities and unfairly impacts marginalised people in society.}
}
@incollection{RUNCO2023455,
title = {Chapter 14 - Conclusion: What Creativity Is and What It Is Not},
editor = {Mark A. Runco},
booktitle = {Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {455-508},
year = {2023},
isbn = {978-0-08-102617-5},
doi = {https://doi.org/10.1016/B978-0-08-102617-5.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026175000011},
author = {Mark A. Runco},
keywords = {Adaptability, Discovery, Evolution, Imagination, Innovation, Intelligence, Intentions/intentionality, Invention, Originality, Serendipity},
abstract = {This chapter explores how creativity is related to, but at the same time distinct from, innovation, imagination, intelligence, originality, and problems solving. Imagination is, for example, frequently associated with creativity. Imaginary worlds are sometimes known as paracosms. These are probably most common around nine years of age and typically fade in the teenage years. Creativity and imagination are also both apparent when an individual has an imaginary friend. Originality is more difficult to separate from creativity. This is because creative things are always original. Originality may take the form novelty, uniqueness or unusualness, or unconventionality. Creative things are always original, but originality is not sufficient for creativity. Then there is the fact that some creative efforts solve a problem or have some utility of some sort. Other creative efforts are self-expressive and intrinsically motivated. Innovation certainly requires some level of originality, but not maximal novelty, whereas other creative performances may benefit from extreme originality. Many cultural differences support the idea that invention, innovation, and creativity are distinguishable and extricable. This chapter includes discussions of the discovery and development of chaos theories have involved professional marginality of the sort that is useful for individuals and their insights. Towards the end of the chapter the question of “what is creativity” is roached by examining computer creativity and animal creativity.}
}
@article{VIANNA2023111744,
title = {A Grey Literature Review on Data Stream Processing applications testing},
journal = {Journal of Systems and Software},
volume = {203},
pages = {111744},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111744},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001395},
author = {Alexandre Vianna and Fernando Kenji Kamei and Kiev Gama and Carlos Zimmerle and João Alexandre Neto},
keywords = {Data streams, Grey literature, Software testing},
abstract = {Context:
The Data Stream Processing (DSP) approach focuses on real-time data processing by applying specific techniques for capturing and processing relevant data for on-the-fly results, i.e. without necessarily requiring prior storage. Like in any other software, testing plays a vital role in the quality assurance of DSP applications. However, testing such kind of software is not a simple task. In this context, some factors that make challenging testing are message temporality, parallelism, data volume, complex infrastructure, variability, and speed of messages.
Objective:
This work aims to map and synthesize industry knowledge and experience regarding DSP application testing. Specifically, we want to know about challenges, test purposes, test approaches, test data sources, and adopted tools.
Method:
To achieve the objective, we performed a Grey Literature Review (e.g., blog posts, white papers, discussion lists, lecture themes at technical events, professional social networks, software repositories, and other web-published) on testing DSP applications. We searched the grey literature using Google’s regular search engine in addition to specific searches on technical software development content websites. The selected studies were analyzed using qualitative and quantitative techniques.
Results:
Results are based on evidence from 154 selected sources. The challenges for testing DSP applications are the complexity of DSP applications, test infrastructure complexity, timing, and data acquisition issues. The main test objectives identified are functional suitability, performance efficiency, reliability, and maintainability. The main test approaches reported: Performance Testing, Regression Testing, Property-Based Testing, Chaos Testing, and Contract/Schema Testing. The strategies adopted by practitioners to obtain test data: Historical Data, Production Data Mirroring, Semi-Synthetic Data, and Synthetic Data. We also report 50 tools used in various testing activities, which are used for: automating infrastructure, generating test data, test utilities, dealing with timing issues, load generation, simulation, and others. Furthermore, we identified gaps and opportunities for future scientific work.
Conclusion:
This work selected and summarized content produced by practitioners regarding DSP application testing. We identified that knowledge, techniques, and tools intrinsic to the practice were not present in the formal literature, so this study helps reduce the gap between industry and academia on this topic. The document has delivered benefits to industry practitioners and academic researchers.}
}
@article{NAZIR2023100022,
title = {A comprehensive survey of ChatGPT: Advancements, applications, prospects, and challenges},
journal = {Meta-Radiology},
volume = {1},
number = {2},
pages = {100022},
year = {2023},
issn = {2950-1628},
doi = {https://doi.org/10.1016/j.metrad.2023.100022},
url = {https://www.sciencedirect.com/science/article/pii/S295016282300022X},
author = {Anam Nazir and Ze Wang},
keywords = {Large Language Models (LLMs), Generative Pre-trained Transformers (GPT), Natural language processing (NLP), Contextual learning, Trustworthy conversational agents, Human-computer interaction, ChatGPT},
abstract = {Large Language Models (LLMs) especially when combined with Generative Pre-trained Transformers (GPT) represent a groundbreaking in natural language processing. In particular, ChatGPT, a state-of-the-art conversational language model with a user-friendly interface, has garnered substantial attention owing to its remarkable capability for generating human-like responses across a variety of conversational scenarios. This survey offers an overview of ChatGPT, delving into its inception, evolution, and key technology. We summarize the fundamental principles that underpin ChatGPT, encompassing its introduction in conjunction with GPT and LLMs. We also highlight the specific characteristics of GPT models with details of their impressive language understanding and generation capabilities. We then summarize applications of ChatGPT in a few representative domains. In parallel to the many advantages that ChatGPT can provide, we discuss the limitations and challenges along with potential mitigation strategies. Despite various controversial arguments and ethical concerns, ChatGPT has drawn significant attention from research industries and academia in a very short period. The survey concludes with an envision of promising avenues for future research in the field of ChatGPT. It is worth noting that knowing and addressing the challenges faced by ChatGPT will mount the way for more reliable and trustworthy conversational agents in the years to come.}
}
@article{JIANG2024100795,
title = {Generative urban design: A systematic review on problem formulation, design generation, and decision-making},
journal = {Progress in Planning},
volume = {180},
pages = {100795},
year = {2024},
note = {Generative urban design: A systematic review on problem formulation, design generation, and decision-making},
issn = {0305-9006},
doi = {https://doi.org/10.1016/j.progress.2023.100795},
url = {https://www.sciencedirect.com/science/article/pii/S0305900623000569},
author = {Feifeng Jiang and Jun Ma and Christopher John Webster and Alain J.F. Chiaradia and Yulun Zhou and Zhan Zhao and Xiaohu Zhang},
keywords = {Generative urban design, Urban form generation, Generative method, AI-generated content (AIGC), Generative AI, Human-machine collaboration},
abstract = {Urban design is the process of designing and shaping the physical forms of cities, towns, and suburbs. It involves the arrangement and design of street systems, groups of buildings, public spaces, and landscapes, to make the urban environment performative and sustainable. The typical design process, reliant on manual work and expert experience has unavoidable low efficiency in generating high-performing design solutions due to the involvement of complex social, institutional, and economic contexts and the trade-off between conflicting preferences of different stakeholder groups. Taking advantage of artificial intelligence (AI) and computational capacity, generative urban design (GUD) has been developed as a trending technical direction to narrow the gaps and produce design solutions with high efficiency at early design stages. It uses computer-aided generative methods, such as evolutionary optimization and deep generative models, to efficiently explore complex solution spaces and automatically generate design options that satisfy conflicting objectives and various constraints. GUD experiments have attracted much attention from academia, practitioners, and public authorities in recent years. However, a systematic review of the current stage of GUD research is lacking. This study, therefore, reports on a systematic investigation of the existing literature according to the three key stages in the GUD process: (1) design problem formulation, (2) design option generation, and (3) decision-making. For each stage, current trends, findings, and limitations from GUD studies are examined. Future directions and potential challenges are discussed and presented. The review is highly interdisciplinary and involves articles from urban study, computer science, social science, management, and other fields. It reports what scholars have found in GUD experiments and organizes a diverse and complicated technical agenda into something accessible to all stakeholders. The results and discoveries will serve as a holistic reference for GUD developers and users in both academia and industry and form a baseline for the field of GUD development in the coming years.}
}
@article{GHAFFARI2023101912,
title = {Toward domain adaptation with open-set target data: Review of theory and computer vision applications},
journal = {Information Fusion},
volume = {100},
pages = {101912},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101912},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523002282},
author = {Reyhane Ghaffari and Mohammad Sadegh Helfroush and Abbas Khosravi and Kamran Kazemi and Habibollah Danyali and Leszek Rutkowski},
keywords = {Open-set domain adaptation, Deep learning, Transfer learning, Negative knowledge, Domain gap, Machine vision},
abstract = {Open-set domain adaptation is a developing and practical solution to training deep networks using unlabeled data which have been collected among unknown data and are under domain shift with other labeled data. This scenario transfers knowledge from a source domain enriched with labeled data to the unlabeled target domain, meanwhile, unknown target samples which are not present in the source domain are separated. Existing methods aim to bridge the domain gap of shared classes between source and target domains in a trustworthy manner and avoid negative transfer learning using keeping away unknown data from the domain alignment step. In this review article, we present a unified framework of theory advances for network risk and a new categorization of open-set domain adaptation along with listing evaluation metrics and popular datasets. Then we accentuate challenges and gaps in existing studies to organize a road map for future research using detailed analysis of investigations. To bring things full circle, we also point out different assumptions and outlooks in the settings of this research area.}
}
@article{2023I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {16},
number = {9},
pages = {I-CXXXIII},
year = {2023},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(23)00375-3},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X23003753}
}
@article{ZHANG2023139,
title = {Chat Generative Pre-Trained Transformer (ChatGPT) usage in healthcare},
journal = {Gastroenterology & Endoscopy},
volume = {1},
number = {3},
pages = {139-143},
year = {2023},
issn = {2949-7523},
doi = {https://doi.org/10.1016/j.gande.2023.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2949752323000353},
author = {Yanhui Zhang and Haolong Pei and Shihan Zhen and Qian Li and Fengchao Liang},
keywords = {ChatGPT, Healthcare, AI, Artificial intelligence, Medicine, Application},
abstract = {In recent years, with the rapid development of deep learning, artificial intelligence (AI) has gradually become a powerful assistant for humans in various aspects. The maturity and advancement of natural language models led to the birth of Chat Generative Pre-Trained Transformer (ChatGPT), generating great potential for healthcare applications. This review summarized the possible application prospects of ChatGPT in healthcare, assessed the limitations that still exist, and suggested possible improvements. In conclusion, the current version of ChatGPT can provide accurate and practical information for doctors and patients, help healthcare professionals to diagnose and treat, promote medical education, and improve the accuracy and efficiency of the healthcare system, etc. However, ChatGPT still has some limitations, as issues such as the accuracy of the information, privacy of data, ethical and moral sentiments still need to be solved. In the future, scientists still need to make efforts to achieve more accurate and effective medical applications with ChatGPT or other AI model by enhancing model performance, filling the data gap, and promoting the standardization of ethical and copyright issues.}
}
@article{RUSTNGUYEN2023103098,
title = {Darknet traffic classification and adversarial attacks using machine learning},
journal = {Computers & Security},
volume = {127},
pages = {103098},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103098},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823000081},
author = {Nhien Rust-Nguyen and Shruti Sharma and Mark Stamp},
keywords = {Darknet, Classification, Adversarial attacks, Convolutional neural network, Auxiliary-Classifier generative adversarial network, Random forest},
abstract = {The anonymous nature of darknets is commonly exploited for illegal activities. Previous research has employed machine learning and deep learning techniques to automate the detection of darknet traffic in an attempt to block these criminal activities. This research aims to improve darknet traffic detection by assessing a wide variety of machine learning and deep learning techniques for the classification of such traffic and for classification of the underlying application types. We find that a Random Forest model outperforms other state-of-the-art machine learning techniques used in prior work with the CIC-Darknet2020 dataset. To evaluate the robustness of our Random Forest classifier, we obfuscate select application type classes to simulate realistic adversarial attack scenarios. We demonstrate that our best-performing classifier can be degraded by such attacks, and we consider ways to effectively deal with such adversarial attacks.}
}