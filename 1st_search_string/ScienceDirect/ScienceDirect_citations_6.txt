Zhaoyang Sun, Shengwu Xiong, Yaxiong Chen, Yi Rong,
A Fine Rendering High-Resolution Makeup Transfer network via inversion-editing strategy,
Engineering Applications of Artificial Intelligence,
Volume 138, Part A,
2024,
109346,
ISSN 0952-1976,
https://doi.org/10.1016/j.engappai.2024.109346.
(https://www.sciencedirect.com/science/article/pii/S0952197624015045)
Abstract: While current makeup transfer methods have made progress in realism and color fidelity, they struggle with capturing texture details and producing high-resolution images, limiting their practical utility. To address these challenges, we propose a Fine Rendering High-resolution Makeup Transfer (FRHMT) network, which leverages a powerful style-based generator and introduces a novel inversion-editing strategy tailored for makeup transfer. Concretely, in the inversion phase, considering the semantic decoupling properties in the latent space, we design a Hierarchical Residual Inversion (HRI), which projects the image onto high-dimensional feature maps in coarse layers and low-dimensional style codes in fine layers. This design effectively restores the content information of the image while maintaining flexibility in editing the makeup styles. In the editing phase, the Makeup Modulation Module (MMM) learns two mapping networks to adjust the latent variables of the source image based on those of the reference image. This modification occurs in fine layers to transfer the makeup information and preserve the content information. With new network structures and customized loss functions, our training eliminates cumbersome pseudo-paired data synthesis and unstable adversarial learning. Extensive experiments have demonstrated that our method outperforms existing methods in both image quality and makeup similarity through quantitative and qualitative analysis. Additionally, we address the lack of high-resolution data by collecting a dataset of 9716 face images with a resolution of 1024 × 1024. In conclusion, our framework offers a novel artificial intelligence (AI) implementation of makeup transfer in engineering, with the collected dataset holding substantial value for further advancements in AI research.
Keywords: Makeup transfer; Inversion-editing strategy; Image translation; Artificial intelligence application

Luca Bortolussi, Francesca Cairoli, Ginevra Carbone, Francesco Franchina, Enrico Regolin,
Adversarial Learning of Robust and Safe Controllers for Cyber-Physical Systems,
IFAC-PapersOnLine,
Volume 54, Issue 5,
2021,
Pages 223-228,
ISSN 2405-8963,
https://doi.org/10.1016/j.ifacol.2021.08.502.
(https://www.sciencedirect.com/science/article/pii/S2405896321012775)
Abstract: We introduce a novel learning-based approach to synthesize safe and robust controllers for autonomous Cyber-Physical Systems and, at the same time, to generate challenging tests. This procedure combines formal methods for model verification with Generative Adversarial Networks. The method learns two Neural Networks: the first one aims at generating troubling scenarios for the controller, while the second one aims at enforcing the safety constraints. We test the proposed method on a variety of case studies.
Keywords: Robust control; Signal Temporal Logic; Adversarial Learning; Data-based Control; Test generation; Safe control

Monu Singh, Naman Baranwal, K.N. Singh, A.K. Singh, Huiyu Zhou,
Deep learning-based biometric image feature extraction for securing medical images through data hiding and joint encryption–compression,
Journal of Information Security and Applications,
Volume 79,
2023,
103628,
ISSN 2214-2126,
https://doi.org/10.1016/j.jisa.2023.103628.
(https://www.sciencedirect.com/science/article/pii/S2214212623002120)
Abstract: Images are promising information carriers when compared to other media documents in the healthcare domain. However, digital data transmission over unprotected wired or wireless networks poses a threat to the security of healthcare systems. As a result, the issue of copyright violation and identity theft can occur due to the unauthorised use of these data. This paper proposes a new secure method under a framework that embeds biometric fingerprint image features in a medical image without any perceptual distortion. This paper uses ResNet152 for biometric image feature extraction in the first stage and features to generate a secret key for embedding in the second stage. The method combines encryption and compression scheme based on a generated key, novel chaotic map and Huffman coding to enhance the security of medical images while reducing the storage consumption or bandwidth requirements if images are transmitted to remote servers. Experimental results show that the proposed method presents superior security with high imperceptibility and compression performance, ensuring its effectiveness as an image protection mechanism for medical applications. Extensive experimental results show that the proposed method achieves an average peak signal-to-noise ratio (PSNR) that is above 54 dB, a structural similarity index measure (SSIM) close to 1, a bit error rate (BER) of 0 and a normalised correlation (NC) of 1. Moreover, this method compresses the images up to 70% when tested on three standard datasets.
Keywords: Biometric images; Encryption; Data hiding; Feature extraction; Compression; Security; Attacks

Geeta Rani, Ankit Misra, Vijaypal Singh Dhaka, Deepak Buddhi, Ravindra Kumar Sharma, Ester Zumpano, Eugenio Vocaturo,
A multi-modal bone suppression, lung segmentation, and classification approach for accurate COVID-19 detection using chest radiographs,
Intelligent Systems with Applications,
Volume 16,
2022,
200148,
ISSN 2667-3053,
https://doi.org/10.1016/j.iswa.2022.200148.
(https://www.sciencedirect.com/science/article/pii/S2667305322000850)
Abstract: The high transmission rate of COVID-19 and the lack of quick, robust, and intelligent systems for its detection have become a point of concern for the public, Government, and health experts worldwide. The study of radiological images is one of the fastest ways to comprehend the infectious spread and diagnose a patient. However, it is difficult to differentiate COVID-19 from other pneumonic infections. The purpose of this research is to provide an automatic, precise, reliable, robust, and intelligent assisting system ‘Covid Scanner’ for mass screening of COVID-19, Non-COVID Viral Pneumonia, and Bacterial Pneumonia from healthy chest radiographs. To train the proposed system, the authors of this research prepared novel a dataset called, “COVID-Pneumonia CXR”. The system is a coherent integration of bone suppression, lung segmentation, and the proposed classifier, ‘EXP-Net’. The system reported an AUC of 96.58% on the validation dataset and 96.48% on the testing dataset comprising chest radiographs. The results from the ablation study prove the efficacy and generalizability of the proposed integrated pipeline of models. To prove the system's reliability, the feature heatmaps visualized in the lung region were validated by radiology experts. Moreover, a comparison with the state-of-the-art models and existing approaches shows that the proposed system finds clearer demarcation between the highly similar chest radiographs of COVID-19 and Non-COVID viral pneumonia. The copyright of “Covid Scanner” is protected with registration number SW-13625/2020. The code for the models used in this research is publicly available at: https://github.com/Ankit-Misra/multi_modal_covid_detection/.
Keywords: SARS-CoV-2; COVID-19; Deep learning; Biomedical; Medical imaging

Alice Melocchi, Francesco Briatico-Vangosa, Marco Uboldi, Federico Parietti, Maximilian Turchi, Didier von Zeppelin, Alessandra Maroni, Lucia Zema, Andrea Gazzaniga, Ahmed Zidan,
Quality considerations on the pharmaceutical applications of fused deposition modeling 3D printing,
International Journal of Pharmaceutics,
Volume 592,
2021,
119901,
ISSN 0378-5173,
https://doi.org/10.1016/j.ijpharm.2020.119901.
(https://www.sciencedirect.com/science/article/pii/S0378517320308863)
Abstract: 3D printing, and particularly fused deposition modeling (FDM), has rapidly brought the possibility of personalizing drug therapies to the forefront of pharmaceutical research and media attention. Applications for this technology, described in published articles, are expected to grow significantly in 2020. Where are we on this path, and what needs to be done to develop a FDM 2.0 process and make personalized medicines available to patients? Based on literature analysis, this manuscript aims to answer these questions and highlight the critical technical aspects of FDM as an emerging technology for manufacturing safe, high-quality personalized oral drug products. In this collaborative paper, experts from different fields contribute strategies for ensuring the quality of starting materials and discuss the design phase, printer hardware and software, the process, the environment and the resulting products, from the perspectives of both patients and operators.
Keywords: 3D printing; Fused deposition modeling; Drug product fabrication; Quality; Safety

Sk Imran Hossain, Sudipta Singha Roy, Jocelyn De Goër De Herve, Robert E. Mercer, Engelbert Mephu Nguifo,
A skin lesion hair mask dataset with fine-grained annotations,
Data in Brief,
Volume 48,
2023,
109249,
ISSN 2352-3409,
https://doi.org/10.1016/j.dib.2023.109249.
(https://www.sciencedirect.com/science/article/pii/S2352340923003682)
Abstract: Occlusion of skin lesions in dermoscopic images due to hair affects the performance of computer-assisted lesion analysis algorithms. Lesion analysis can benefit from digital hair removal or realistic hair simulation techniques. To assist in that process, we have created the largest publicly available skin lesion hair segmentation mask dataset by carefully annotating 500 dermoscopic images. Compared to the existing datasets, our dataset is free of non-hair artifacts like ruler markers, bubbles, and ink marks. The dataset is also less prone to over and under segmentations because of fine-grained annotations and quality checks from multiple independent annotators. To create the dataset, first, we collected five hundred copyright-free CC0 licensed dermoscopic images covering different hair patterns. Second, we trained a deep learning hair segmentation model on a publicly available weakly annotated dataset. Third, we extracted hair masks for the selected five hundred images using the segmentation model. Finally, we manually corrected all the segmentation errors and verified the annotations by superimposing the annotated masks on top of the dermoscopic images. Multiple annotators were involved in the annotation and verification process to make the annotations as error-free as possible. The prepared dataset will be useful for benchmarking and training hair segmentation algorithms as well as creating realistic hair augmentation systems.
Keywords: Skin lesion; Hair mask; Hair segmentation; Hair augmentation; Deep learning

Muna Al-Hawawreh, Mamoun Alazab, Mohamed Amine Ferrag, M. Shamim Hossain,
Securing the Industrial Internet of Things against ransomware attacks: A comprehensive analysis of the emerging threat landscape and detection mechanisms,
Journal of Network and Computer Applications,
Volume 223,
2024,
103809,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103809.
(https://www.sciencedirect.com/science/article/pii/S108480452300228X)
Abstract: Due to the complexity and diversity of Industrial Internet of Things (IIoT) systems, which include heterogeneous devices, legacy and new connectivity protocols and systems, and distributed networks, sophisticated attacks like ransomware will likely target these systems in the near future. Researchers have focused on studying and addressing ransomware attacks against various platforms in recent years. However, to the best of our knowledge, no existing study investigates the new trends of ransomware tactics and techniques and provides a comprehensive analysis of ransomware attacks and their detection techniques for IIoT systems. Therefore, this paper investigates this attack and its associated detection techniques in IIoT systems in various aspects, including recent ransomware tactics, types, infected operating systems, and platforms. Specifically, we initially discuss the evolution of the IIoT system and its common architecture. Then, we provide an in-depth examination of the development of ransomware attacks and their constituent blocks, outline recent tactics and types of ransomware, and provide an extensive overview of the latest research on detection models. We also summarize numerous significant issues that have yet to be addressed and require further research. We conclude that offensive and defensive research is urgently needed to protect IIoT against ransomware attacks.
Keywords: IIoT; Ransomware attacks; Artificial intelligence; Detection; IT; OT

Hongyun Huang, William Mbanyele, Fengrong Wang, Chenxi Zhang, Xin Zhao,
Nudging corporate environmental responsibility through green finance? Quasi-natural experimental evidence from China,
Journal of Business Research,
Volume 167,
2023,
114147,
ISSN 0148-2963,
https://doi.org/10.1016/j.jbusres.2023.114147.
(https://www.sciencedirect.com/science/article/pii/S0148296323005064)
Abstract: Green finance has drawn increased worldwide attention from policymakers as a financial mechanism that could potentially encourage corporations to actively engage in sustainable activities. However, despite a growing body of studies investigating the economic outcomes of green financial policies, there is still a lack of research that systematically quantifies the social welfare implications of green finance. Hence, this study aims to fill this research gap by establishing the causal effect of green finance on corporate environmental responsibility. Exploiting the “bottom-up” enforcement of the green finance pilots in 2017 in China as a quasi-natural experiment and the difference-in-difference-in-difference identification strategy, we find that green finance significantly enhances corporate environmental responsibility performance in high-polluting industries relative to their counterparts, and this evidence continues to survive a battery of robustness checks. Moreover, we explore three underlying mechanisms that possibly explain this beneficial effect: risk-taking, external governance and financing channels. Furthermore, we uncover that corporate environmental responsibility serves as a plausible non-economic channel that combines green finance with economic benefits by stimulating green innovation, promoting total factor productivity and expanding market share. Overall, our study offers new insights on both the economic and non-economic consequences of green finance on business performance.
Keywords: Green finance; Corporate environmental responsibility; Financial constraints; Risk-taking; External governance; Difference-in-Difference-in-Difference

Charles Mondal, Robert B. Mellor,
Investigating the effect of state support on innovation pathways by tracking the legacy performance of firms involved in academic co-operations,
Journal of Innovation & Knowledge,
Volume 10, Issue 2,
2025,
100679,
ISSN 2444-569X,
https://doi.org/10.1016/j.jik.2025.100679.
(https://www.sciencedirect.com/science/article/pii/S2444569X25000290)
Abstract: The performance of firms involved in projects from 2 UK research councils was investigated; firms in Innovate UK projects receive co-funding while firms in Arts & Humanities Research Council (AHRC) projects do not. Firms in 266 projects 2009–2012 were tracked for Standard Industrial Code (SIC), location and year-on-year financial performance 2012–22. The results show that firms (un- and co-funded) were mainly not local to universities. The growth performance of non-funded firms was steady in the majority of SIC codes, but some SIC codes performed very well, while for co-funded firms, many SICs performed under control but losses were made up for on average by exceptionally high performance in other SIC codes. Overall, non-funded firms achieved average growth of ∼29 % above control while co-funded firms only achieved an average growth of ∼18 % above control. Firms (both co- and un-funded) associated with 21 universities perform consistently well, while other firms (co- and un-funded) associated with 24 other universities perform consistently poorly. This difference in performance was better correlated to degree of business ambidexterity in the tech transfer function, rather than with university reputation.
Keywords: Entrepreneurial universities; Geographical distribution; Knowledge spillovers; State funding; Technology transfer

Christian Tyrchan, Eva Nittinger, Dea Gogishvili, Atanas Patronov, Thierry Kogej,
Chapter 4 - Approaches using AI in medicinal chemistry,
Editor(s): Takashiro Akitsu,
Computational and Data-Driven Chemistry Using Artificial Intelligence,
Elsevier,
2022,
Pages 111-159,
ISBN 9780128222492,
https://doi.org/10.1016/B978-0-12-822249-2.00002-5.
(https://www.sciencedirect.com/science/article/pii/B9780128222492000025)
Abstract: The challenge of pharmacological effect prediction and its relation to analog design consists of the decision of which molecule to make next on the basis of the available data, medicinal chemistry knowledge, experience, and intuition. In the second half of the 1900s century, attempts were made to relate narcotics pharmacology to their physicochemical properties by specifically using distribution and partition coefficients. This was shortly followed by Paul Ehrlich's observation to attribute the pharmacological effect of a compound to a specific functional group. However, it was only in 1971 that this observation was called pharmacophore. About 30years after Ehrlich, Hammett related the effect of changes in structure on reaction mechanisms, specifically based on resonance interaction of an aromatic ring. This model was later extended by Taft by separating the inductive effects from the steric properties of substituents. This concept was formalized with the work of Hansch, Free, and Wilson which reasoned that the biological activity for a set of analogs could be described by the contributions that substituents or structural elements make to the activity of a parent structure. This led to the analytical description of general quantitative-structure–activity relationship studies (QSAR). If the question “What to synthesize next?” is answered then “How to synthesize it?” follows up. The prediction of chemical reactions starting from educts or the product and educing input reactants and reaction conditions is a fundamental scientific problem. As QSAR computer-aided synthesis planning (CASP) has a long history starting in the 1960s with LHASA a rule-based approach to retrosynthesis planning.
Keywords: QSAR; Machine learning; De novo design; Synthesis prediction; AI; Active learning; Inverse QSAR; Nonadditivity

Muhammad Hamza Zafar, Even Falkenberg Langås, Filippo Sanfilippo,
Exploring the synergies between collaborative robotics, digital twins, augmentation, and industry 5.0 for smart manufacturing: A state-of-the-art review,
Robotics and Computer-Integrated Manufacturing,
Volume 89,
2024,
102769,
ISSN 0736-5845,
https://doi.org/10.1016/j.rcim.2024.102769.
(https://www.sciencedirect.com/science/article/pii/S0736584524000553)
Abstract: Industry 5.0 aims at establishing an inclusive, smart and sustainable production process that encourages human creativity and expertise by leveraging enhanced automation and machine intelligence. Collaborative robotics, or “cobotics”,is a major enabling technology of Industry 5.0, which aspires at improving human dexterity by elevating robots to extensions of human capabilities and, ultimately, even as team members. A pivotal element that has the potential to operate as an interface for the teaming aspiration of Industry 5.0 is the adoption of novel technologies such as virtual reality (VR), augmented reality (AR), mixed reality (MR) and haptics, together known as “augmentation”. Industry 5.0 also benefit from Digital Twins (DTs), which are digital representations of a physical assets that serves as their counterpart — or twins. Another essential component of Industry 5.0 is artificial intelligence (AI), which has the potential to create a more intelligent and efficient manufacturing process. In this study, a systematic review of the state of the art is presented to explore the synergies between cobots, DTs, augmentation, and Industry 5.0 for smart manufacturing. To the best of the author’s knowledge, this is the first attempt in the literature to provide a comprehensive review of the synergies between the various components of Industry 5.0. This work aims at increasing the global efforts to realize the large variety of application possibilities offered by Industry 5.0 and to provide an up-to-date reference as a stepping-stone for new research and development within this field.
Keywords: Digital twins; Industry 5.0; Deep learning; Augmentation; HRC

Xiuli Chai, Zongwei Tang, Zhihua Gan, Yang Lu, Binjie Wang, Yushu Zhang,
SE-NDEND: A novel symmetric watermarking framework with neural network-based chaotic encryption for Internet of Medical Things,
Biomedical Signal Processing and Control,
Volume 90,
2024,
105877,
ISSN 1746-8094,
https://doi.org/10.1016/j.bspc.2023.105877.
(https://www.sciencedirect.com/science/article/pii/S1746809423013101)
Abstract: The development of the Internet of Medical Things heavily relies on big data, and data security based on medical images has become a growing concern in society. Digital watermarking serves as a crucial technique for protecting and tracing medical image data copyright, as well as enabling forensic analysis. However, existing deep watermarking methods often neglect the protection of watermarks after extraction, leading to potential copyright disputes. To address this issue, this paper proposes SE-NDEND, a novel symmetric watermarking framework with neural network-based chaotic encryption for the Internet of Medical Things that significantly enhances the effectiveness and security of watermarking while maintaining robustness. Specifically, the SE-NDEND leverages neural networks to simulate chaotic systems and generate chaotic sequences, mitigating the complexity and high cost of implementing chaotic systems using hardware circuits. Moreover, we introduce a new noise layer with Moiré distortion that interacts with the decoder, forming a symmetric network structure that bolsters the robustness of watermarking. Parameters are jointly trained and shared during the training process to counteract potential interference from the noise layer. Experimental results validate the effectiveness of SE-NDEND in enhancing copyright protection, traceability, and forensic capabilities, surpassing existing deep learning methods in terms of visual quality (with PSNR of 45.8492 dB and SSIM of 0.9874), security, and robustness. The proposed framework can find application in protecting medical image data in the Internet of Medical Things.
Keywords: Medical image data; Watermarking; Effectiveness; Moiré distortion; Copyright protection

Pablo Quijano Velasco, Kedar Hippalgaonkar, Balamurugan Ramalingam,
Emerging trends in the optimization of organic synthesis through high-throughput tools and machine learning,
Beilstein Journal of Organic Chemistry,
Volume 21,
2025,
Pages 10-38,
ISSN 1860-5397,
https://doi.org/10.3762/bjoc.21.3.
(https://www.sciencedirect.com/science/article/pii/S1860539725000088)
Abstract: The discovery of the optimal conditions for chemical reactions is a labor-intensive, time-consuming task that requires exploring a high-dimensional parametric space. Historically, the optimization of chemical reactions has been performed by manual experimentation guided by human intuition and through the design of experiments where reaction variables are modified one at a time to find the optimal conditions for a specific reaction outcome. Recently, a paradigm change in chemical reaction optimization has been enabled by advances in lab automation and the introduction of machine learning algorithms. Therein, multiple reaction variables can be synchronously optimized to obtain the optimal reaction conditions, requiring a shorter experimentation time and minimal human intervention. Herein, we review the currently used state-of-the-art high-throughput automated chemical reaction platforms and machine learning algorithms that drive the optimization of chemical reactions, highlighting the limitations and future opportunities of this new field of research.
Keywords: autonomous reactors; data processing; high-throughput experimentation; machine learning; reaction optimization

Michela Pellicelli,
Chapter five - Managing the supply chain: technologies for digitalization solutions,
Editor(s): Michela Pellicelli,
The Digital Transformation of Supply Chain Management,
Elsevier,
2023,
Pages 101-152,
ISBN 9780323855327,
https://doi.org/10.1016/B978-0-323-85532-7.00002-5.
(https://www.sciencedirect.com/science/article/pii/B9780323855327000025)
Abstract: In recent years, technology has profoundly impacted strategies as well as their implementation. Digital technology, by which everything is going to be connected, is forcing companies across all industries to rethink their operations. The development of the internet and mobile technologies has led to a fundamental impact of new technologies on economics and business. Every technology has distinctive elements that account for its employment in specific domains. The present chapter presents the principal innovative solutions for managing and increasing the efficiency of supply chains. Cloud computing, Big Data, Internet of Things (IoT), Blockchain, Robotics, Additive Manufacturing (AM), Autonomous Vehicles (AV), Artificial Intelligence (AI), Co-creation, and Digital Value Chain (DVC) are some of the many innovations that digital technology has made possible. According to Kumar et al., the ‘new-age technologies' - in particular, the Internet of Things (IoT), Artificial Intelligence (AI), Machine Learning and Blockchain - are widely considered the way of the future.
Keywords: Additive Manufaturing; Artificial Intelligence; Autonomous Vehicles; Big Data; Blockchain; Cloud computing; Co-creation; Digital technologies; Digital Value Chain; Emergent computing; Internet of Things; Managing supply chain; Operations management; Robotics; Technology management

Olga Beatrice Carcassi, Lola Ben-Alon,
Additive manufacturing of natural materials,
Automation in Construction,
Volume 167,
2024,
105703,
ISSN 0926-5805,
https://doi.org/10.1016/j.autcon.2024.105703.
(https://www.sciencedirect.com/science/article/pii/S0926580524004394)
Abstract: As additive manufacturing (AM) technology continues to advance for computer-aided design and engineering applications, a parallel imperative emerges — a conscientious shift towards more responsible material practices, aligning with ethical, environmental, and social sustainability considerations. The present systematic review analyzes the state-of-the-art developments in relation to AM using natural, low-carbon, and readily available material practices. The results show that published work is situated at the intersection of material science, digital fabrication, and construction, with an array of geo-, bio-, and living mix designs, and different properties analyzed. Under certain conditions, a move towards more use of natural materials could be the solution to source more responsible materials while contributing to the quality of the built environment and the planet Earth itself. The long-term contribution is to provide leading guidance for future research aimed at developing novel and bespoke natural materials in digital fabrication and advanced manufacturing.
Keywords: Natural materials; Earth materials; Biobased-materials; Living materials; Additive manufacturing; Mix design; Systematic review

Ioannis Makris, Aikaterini Karampasi, Panagiotis Radoglou-Grammatikis, Nikolaos Episkopos, Eider Iturbe, Erkuden Rios, Nikos Piperigkos, Aris Lalos, Christos Xenakis, Thomas Lagkas, Vasileios Argyriou, Panagiotis Sarigiannidis,
A comprehensive survey of Federated Intrusion Detection Systems: Techniques, challenges and solutions,
Computer Science Review,
Volume 56,
2025,
100717,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2024.100717.
(https://www.sciencedirect.com/science/article/pii/S157401372400100X)
Abstract: Cyberattacks have increased radically over the last years, while the exploitation of Artificial Intelligence (AI) leads to the implementation of even smarter attacks which subsequently require solutions that will efficiently confront them. This need is indulged by incorporating Federated Intrusion Detection Systems (FIDS), which have been widely employed in multiple scenarios involving communication in cyber–physical systems. These include, but are not limited to, the Internet of Things (IoT) devices, Industrial IoT (IIoT), healthcare systems (Internet of Medical Things/IoMT), Internet of Vehicles (IoV), Smart Manufacturing (SM), Supervisory Control and Data Acquisition (SCADA) systems, Multi-access Edge Computing (MEC) devices, among others. Tackling the challenge of cyberthreats in all the aforementioned scenarios is of utmost importance for assuring the safety and continuous functionality of the operations, crucial for maintaining proper procedures in all Critical Infrastructures (CIs). For this purpose, pertinent knowledge of the current status in state-of-the-art (SOTA) federated intrusion detection methods is mandatory, towards encompassing while simultaneously evolving them in order to timely detect and mitigate cyberattack incidents. In this study, we address this challenge and provide the readers with an overview of FL implementations regarding Intrusion Detection in several CIs. Additionally, the distinct communication protocols, attack types and datasets utilized are thoroughly discussed. Finally, the latest Machine Learning (ML) and Deep Learning (DL) frameworks and libraries to implement such methods are also provided.
Keywords: Cybersecurity; Federated Learning; Intrusion detection; Intrusion prevention

Yen-Lin Chen, Shu-Wei Chang,
Recent advances in the integration of protein mechanics and machine learning,
Extreme Mechanics Letters,
Volume 72,
2024,
102236,
ISSN 2352-4316,
https://doi.org/10.1016/j.eml.2024.102236.
(https://www.sciencedirect.com/science/article/pii/S2352431624001160)
Abstract: Mechanics underlies protein properties and behavior. From a theoretical standpoint, it is possible to derive these based on physical rules. This is appealing because they provide insights into physiology and disease, as well as aid in protein engineering; however, the convoluted nature of the biological system and current computational speeds limit its feasibility. Machine learning (ML) architectures are known for their ability to make inferences on complex data, such as the relationship between protein mechanics, properties, and behavior. Substantial efforts have been made to learn such correlations in tasks such as the prediction of structure, stability, natural frequency, mechanical strength, folding rate, solubility, and function. Each of these properties is interconnected through protein mechanics, and it is not surprising that the methods used in these tasks overlap highly in model input and architecture. In this review, we evaluate ML methods for the seven aforementioned prediction tasks to identify current trends in ML research in the field of protein sciences, focusing on the input and model architecture of each method. A short overview of de novo protein design is also provided. Finally, we highlight trends in the application of ML methods in the field of protein science, as well as directions for future improvements.
Keywords: Machine learning; Protein mechanics; Protein property prediction

Qing Ma, Peter Crosthwaite, Daner Sun, Di Zou,
Exploring ChatGPT literacy in language education: A global perspective and comprehensive approach,
Computers and Education: Artificial Intelligence,
Volume 7,
2024,
100278,
ISSN 2666-920X,
https://doi.org/10.1016/j.caeai.2024.100278.
(https://www.sciencedirect.com/science/article/pii/S2666920X2400081X)
Abstract: With the widespread use of Generative AI in education, effectively utilizing and integrating it into teaching have become key focal points and challenges in education. Different subjects and target audiences require varied norms and strategies for implementing Generative AI, such as ChatGPT. These differences directly impact the educational integration of Generative AI in various educational contexts. To address these disparities and establish common ground, we propose the concept of ChatGPT literacy to bridge research gaps. In this study, we tailor the concept of ChatGPT literacy specifically for language teachers, aiming to delineate the essential competencies needed to proficiently and ethically use ChatGPT as a language learning and teaching tool. We propose a theoretical framework encompassing six fundamental constructs: benefits, limitations, prompts, evaluation (of ChatGPT responses), assessment (assisted by ChatGPT), and ethics, to comprehensively conceptualise and evaluate ChatGPT literacy. Drawing on both quantitative and qualitative survey data from 492 language teachers across 41 countries, we validate the proposed ChatGPT literacy framework by examining teachers' practices and challenges associated with ChatGPT usage. Our analysis of Likert-scale data, utilizing item and confirmatory techniques, confirms the effectiveness of the six-construct framework in defining ChatGPT literacy. In addition, we collected qualitative data through open questions and conducted thematic analysis, demonstrating that ChatGPT has been integrated throughout the instructional cycle, from material preparation to formative and summative assessment phases. These quantitative and qualitative findings have significant implications for a range of stakeholders, including language educators, learners, AI technology developers, and policymakers, providing valuable insights to inform decisions regarding ChatGPT integration in language education. Ultimately, our study equips relevant stakeholders with the necessary competencies to responsibly exploiting ChatGPT's potential in language and other subject areas.
Keywords: ChatGPT literacy; Framework; Language teachers; Model validation; Confirmatory factor analysis

Dominique J. Monlezun,
5 - Framework part II: artificial intelligence + political economics,
Editor(s): Dominique J. Monlezun,
Responsible Artificial Intelligence Re-engineering the Global Public Health Ecosystem,
Morgan Kaufmann,
2024,
Pages 133-184,
ISBN 9780443215971,
https://doi.org/10.1016/B978-0-443-21597-1.00005-6.
(https://www.sciencedirect.com/science/article/pii/B9780443215971000056)
Abstract: This chapter considers the political economic or meta-determinants of health for the global public health ecosystem, critical for the scale, scope, and speed of coordinated actions (including in consensus-based governance and financing) to generate equitable and effective global health solutions to urgent shared challenges. Rising international separation and tensions between democracies and autocracies in addition to the Global North and the Global South undermines the health of these regimes and regions and that of humanity. This chapter thus considers global health and artificial intelligence (AI) in their political economic context in the strategic competition of dominant power players, particularly with the governments, militaries, and corporations of the United States and China which account for most of the global health financing and programs along with that of AI’s development and deployment. Failures in managed strategic competition can not only undermine the cooperation required for the AI-driven global public health ecosystem, but they may even imperil it through accelerated and even catastrophic conflicts. This chapter therefore considers the history and foreseeable future of the global public health ecosystem from the structural perspective of political economics, including the underlying values that may provide a durable foundation for coordinated health action. It additionally considers emergent solutions and advances for the health ecosystem toward this including with human security and data sovereignty within Political Liberalism articulating a bridge between the above blocs, while addressing health determinants integrally and globally: social determinants of health, political determinants of health, economic determinants of health, commercial determinants of health, and digital determinants of health. Specific advances include shared global governance, affordable clean energy transition, and affordable AI digital transformation for sustainable development (with deference and deterrence guardrails maximizing cooperation, managing strategic competition, and minimizing conflict). The chapter additionally considers medical diplomacy, multilateral development, deep medicine, large language models (including ChatGPT), commercial fusion, and digital supply chain resilience (with diversification and de-risking), in the context of moving away from an imperial ideological values-driven ruler-based world order to a more sovereign integral values-driven rules-based world order.
Keywords: Political economics; determinants of health; managed strategic competition; medical diplomacy; supply chain resilience; large language models; clean energy; de-risking; diversification; multilateralism; deterrence; defense; development

Graham Collins,
Appendix 2 - Artificial Intelligence (AI) and Big Data,
Editor(s): Albert Lester,
Project Management, Planning and Control (Eighth Edition),
Butterworth-Heinemann,
2021,
Pages 571-589,
ISBN 9780128243398,
https://doi.org/10.1016/B978-0-12-824339-8.15002-3.
(https://www.sciencedirect.com/science/article/pii/B9780128243398150023)

Zofia Bednarz, Kayleen Manwaring,
Hidden depths: The effects of extrinsic data collection on consumer insurance contracts,
Computer Law & Security Review,
Volume 45,
2022,
105667,
ISSN 2212-473X,
https://doi.org/10.1016/j.clsr.2022.105667.
(https://www.sciencedirect.com/science/article/pii/S0267364922000152)
Abstract: ABSTRACT
Commentators have predicted that the insurance industry will soon benefit from technological advancements, such as developments in Artificial Intelligence (‘AI’) and Big Data. The application of AI- and Big Data-powered tools promises cost reduction, the creation of innovative products, and the potential to offer more efficient and tailored services to consumers. However, these new opportunities are mirrored by new legal and regulatory challenges. This article discusses challenges facing Australian data protection law, focusing on (potential) collection of consumers' data by insurers from non-traditional sources. In particular, we examine situations in which consumers may not be aware that the data collected could end up being used to price insurance. In our analysis, we discuss two useful examples of such non-traditional data sources: customer loyalty schemes and social media. These may give rise to several concerning data practices, including a significant increase in the collection of consumers' data by insurers. We argue that datafication of insurer processes may fuel excessive data collection in the context of insurance contracts, generating a substantial risk of harm to consumers, especially in terms of discrimination, exclusion, and unaffordability of insurance. We complement our analysis with the discussion of Australian insurance-specific provisions, asking if, and how, the harms examined could be adequately addressed.
Keywords: Artificial intelligence; Big data; Consumer insurance; Privacy; Data protection

Yiwei Wang, Binyou Wang, Jun Zou, Anguo Wu, Yuan Liu, Ying Wan, Jiesi Luo, Jianming Wu,
Capsule neural network and its applications in drug discovery,
iScience,
2025,
112217,
ISSN 2589-0042,
https://doi.org/10.1016/j.isci.2025.112217.
(https://www.sciencedirect.com/science/article/pii/S258900422500478X)
Abstract: SUMMARY
Deep learning holds great promise in drug discovery, yet its application is hindered by high labeling costs and limited datasets. Developing algorithms that effectively learn from sparsely labeled data is crucial. Capsule networks (CapsNet), introduced in 2017, solve the spatial information loss in traditional neural networks and excel in handling small datasets by capturing spatial hierarchical relationships among features. This capability makes CapsNet particularly promising for drug discovery, where data scarcity is a common challenge. Various modified CapsNet architectures have been successfully applied to drug design and discovery tasks. This review provides a comprehensive analysis of CapsNet's theoretical foundations, its current applications in drug discovery, and its performance in addressing key challenges in the field. Additionally, the study highlights the limitations of CapsNet and outlines potential future research directions to further enhance its utility in drug discovery, offering valuable insights for researchers in both computational and pharmaceutical sciences.
Keywords: Capsule neural network; Drug discovery application; Small dataset challenges; Future research directions

André Ferreira, Jianning Li, Kelsey L. Pomykala, Jens Kleesiek, Victor Alves, Jan Egger,
GAN-based generation of realistic 3D volumetric data: A systematic review and taxonomy,
Medical Image Analysis,
Volume 93,
2024,
103100,
ISSN 1361-8415,
https://doi.org/10.1016/j.media.2024.103100.
(https://www.sciencedirect.com/science/article/pii/S1361841524000252)
Abstract: With the massive proliferation of data-driven algorithms, such as deep learning-based approaches, the availability of high-quality data is of great interest. Volumetric data is very important in medicine, as it ranges from disease diagnoses to therapy monitoring. When the dataset is sufficient, models can be trained to help doctors with these tasks. Unfortunately, there are scenarios where large amounts of data is unavailable. For example, rare diseases and privacy issues can lead to restricted data availability. In non-medical fields, the high cost of obtaining enough high-quality data can also be a concern. A solution to these problems can be the generation of realistic synthetic data using Generative Adversarial Networks (GANs). The existence of these mechanisms is a good asset, especially in healthcare, as the data must be of good quality, realistic, and without privacy issues. Therefore, most of the publications on volumetric GANs are within the medical domain. In this review, we provide a summary of works that generate realistic volumetric synthetic data using GANs. We therefore outline GAN-based methods in these areas with common architectures, loss functions and evaluation metrics, including their advantages and disadvantages. We present a novel taxonomy, evaluations, challenges, and research opportunities to provide a holistic overview of the current state of volumetric GANs.
Keywords: Synthetic volumetric data; Generative adversarial network; Systematic review; Volumetric GANs taxonomy

Ed Pizzi, Giorgos Kordopatis-Zilos, Hiral Patel, Gheorghe Postelnicu, Sugosh Nagavara Ravindra, Akshay Gupta, Symeon Papadopoulos, Giorgos Tolias, Matthijs Douze,
The 2023 video similarity dataset and challenge,
Computer Vision and Image Understanding,
Volume 243,
2024,
103997,
ISSN 1077-3142,
https://doi.org/10.1016/j.cviu.2024.103997.
(https://www.sciencedirect.com/science/article/pii/S107731422400078X)
Abstract: This work introduces a dataset, benchmark, and challenge for the problem of video copy tracing. There are two related tasks: determining whether a query video shares content with a reference video (“detection”) and temporally localizing the shared content within each video (“localization”). The benchmark is designed to evaluate methods on these two tasks. It simulates a realistic needle-in-haystack setting, where the majority of both query and reference videos are “distractors” containing no copied content. We propose an accuracy metric for both tasks. The associated challenge imposes computing resource restrictions that reflect real-world settings. We also analyze the results and methods of the top submissions to the challenge. The dataset, baseline methods, and evaluation code are publicly available and were discussed at the Visual Copy Detection Workshop (VCDW) at CVPR’23. We provide reference code for evaluation and baselines at: https://github.com/facebookresearch/vsc2022.
Keywords: Video similarity challenge; Video copy detection; Video copy localization; Video dataset

Saikat Sinha Ray, Pranav R.T. Peddinti, Rohit Kumar Verma, Harish Puppala, Byungmin Kim, Ashutosh Singh, Young-Nam Kwon,
Leveraging ChatGPT and Bard: What does it convey for water treatment/desalination and harvesting sectors?,
Desalination,
Volume 570,
2024,
117085,
ISSN 0011-9164,
https://doi.org/10.1016/j.desal.2023.117085.
(https://www.sciencedirect.com/science/article/pii/S0011916423007178)
Abstract: Artificial intelligence (AI) has emerged as a prominent tool in the modern day. The utilization of AI and advanced language models such as chat generative pre-trained transformer (ChatGPT) and Bard is not only innovative but also crucial for handling challenges related to water research. ChatGPT is an AI chatbot that uses natural language processing to create humanlike conversations. ChatGPT has recently gained considerable public interest, owing to its unique ability to simplify tasks from various backgrounds. Similarly, Google introduced Bard, an AI-powered chatbot to simulate human conversations. Herein, we investigated how ChatGPT and Bard (AI powdered chatbots) tools can impact water research through interactive sessions. Typically, ChatGPT and Bard offer significant benefits to various fields, including research, education, scientific publications, and outreach. ChatGPT and Bard simplify complex and challenging tasks. For instance, 50 important questions about water treatment/desalination techniques and 50 questions about water harvesting techniques were provided to both chatbots. Time analytics was performed by ChatGPT 3.5, and Bard was used to generate full responses. In particular, the effectiveness of this emerging tool for research purposes in the field of conventional water treatment techniques, advanced water treatment techniques, membrane technology and seawater desalination has been thoroughly demonstrated. Moreover, potential pitfalls and challenges were also highlighted. Thus, sharing these experiences may encourage the effective and responsible use of Bard and ChatGPT in research purposes. Finally, the responses were compared from the perspective of an expert. Although ChatGPT and Bard possess huge benefits, there are several issues, which are discussed in this study. Based on this study, we can compare the abilities of artificial intelligence and human intelligence in water sector research.
Keywords: ChatGPT; Bard; Water treatment; Desalination; Water harvesting and artificial intelligence (AI)

Iman Salahshoori, Majid Namayandeh Jorabchi, Morteza Asghari, Sebastian Wohlrab, Mehdi Golriz, Hossein Ali Khonakdar,
Comprehensive insights into molecular simulation-driven advances in functional materials for pollutant mitigation,
Coordination Chemistry Reviews,
Volume 534,
2025,
216580,
ISSN 0010-8545,
https://doi.org/10.1016/j.ccr.2025.216580.
(https://www.sciencedirect.com/science/article/pii/S001085452500150X)
Abstract: Removing gaseous pollutants from the environment is a pressing global concern due to their detrimental effects on human health and ecosystems. Adsorbents, materials capable of capturing and retaining gaseous molecules, play a crucial role in addressing this issue. While laboratory experiments are indispensable for adsorbent development, they can be time-consuming and resource-intensive. On the other hand, molecular simulation methods offer a powerful alternative by providing insights into the adsorption process at the molecular level. This review article explores the application of molecular simulation in designing and optimizing functional materials for gaseous pollutant mitigation. It discusses the fundamental principles of molecular simulation techniques and their advantages over traditional laboratory methods. A wide range of adsorbent materials, including polymers, carbon nanotubes, graphene oxide, zeolites, metal-organic frameworks, zeolitic imidazolate frameworks, and covalent organic frameworks, are examined in detail. Numerous practical examples illustrate how molecular simulation can predict adsorption capacities, selectivity, and kinetics. This review aims to empower researchers to create more efficient and sustainable solutions for gaseous pollutant removal by providing a comprehensive overview of molecular simulation methods and their applications in adsorbent development. The insights gained from molecular simulation can accelerate the development of innovative adsorbents, ultimately contributing to a cleaner and healthier environment.
Keywords: Air pollutants; Advanced functional materials; Computational methods; Gas separation; Materials design; Molecular simulations

Abid Haleem, Mohd Javaid, Ravi Pratap Singh,
Exploring the competence of ChatGPT for customer and patient service management,
Intelligent Pharmacy,
Volume 2, Issue 3,
2024,
Pages 392-414,
ISSN 2949-866X,
https://doi.org/10.1016/j.ipha.2024.03.002.
(https://www.sciencedirect.com/science/article/pii/S2949866X24000480)
Abstract: The modern language generation model ChatGPT, created by Open Artificial Intelligence (AI), is recognised for its capacity to comprehend context and produce pertinent content. This model is built on the transformer architecture, which enables it to process massive volumes of data and produce text that is both cohesive and illuminating. Service is a crucial component everywhere as it provides the basis for establishing client rapport and offering aid and support. In healthcare, the application of ChatGPT for patient service support has been one of the most significant advances in recent years. ChatGPT can help overcome language obstacles and improve patient satisfaction by facilitating communication with healthcare personnel and understanding of care. It can assist in enhancing the entire patient experience by offering personalised information and support to patients and making it more straightforward for them to communicate with healthcare professionals. Its goal can be to expedite and streamline service by promptly and accurately responding to customers. Businesses of all sizes increasingly use ChatGPT since it allows them to provide 24/7 customer support without requiring human contact. This paper briefly discusses ChatGPT and the need for better services. Various perspectives on improving customer and patient services through ChatGPT are discussed. The article also discussed the major key enablers of ChatGPT for refining customer and patient assistance. Further, the paper identifies and discusses the critical application areas of ChatGPT for customer and patient service. With its ability to handle several requests simultaneously, respond quickly and accurately to client questions, and gain knowledge from every interaction, ChatGPT is revolutionising customer and patient service. Its accessibility and compatibility with various communication channels make it a desirable solution for businesses looking to improve support. As technology advances, ChatGPT is positioned to become an essential tool for businesses wishing to provide speedy and customised service. Although ChatGPT may give convincing solutions, the chance of providing accurate and updated information poses a problem for its usage in service jobs that need accurate and up-to-date information. In future, various services will become better and more efficient due to ChatGPT and AI.
Keywords: Artificial intelligence (AI); ChatGPT; Applications; Healthcare; Customer; Patient

Alexandre Pólvora, Susana Nascimento, Joana S. Lourenço, Fabiana Scapolo,
Blockchain for industrial transformations: A forward-looking approach with multi-stakeholder engagement for policy advice,
Technological Forecasting and Social Change,
Volume 157,
2020,
120091,
ISSN 0040-1625,
https://doi.org/10.1016/j.techfore.2020.120091.
(https://www.sciencedirect.com/science/article/pii/S0040162520309173)
Abstract: Beyond more recognized financial applications of Blockchain, its potential for other sectors has increasingly come to the foreground. Yet, its development still faces questions over impact, added value, or concrete paths for widespread deployment. In this paper we argue for a transdisciplinary forward-looking approach to address such uncertainties, based on the processes and findings of the research project #Blockchain4EU: Blockchain for Industrial Transformations, which was developed inside the European Commission with a focus on multi-stakeholder engagement and co-creation. We invested in a mix of desk research with qualitative methods, including interviews, surveys, or ethnographic explorations, together with participatory workshops for collective vision building and speculative prototyping for policy. Our main findings underline key sociotechnical challenges and opportunities for the development and uptake of Blockchain in specific European industrial and business contexts, taking into account the complexity of policy, economic, social, technical, legal and environmental elements. But, aiming to push the frontiers of what's common practice in advice for policy when looking into early-stage technologies as Blockchain, we also strive to emphasize how our approach can benefit decision-makers through robust methodological and conceptual processes that are simultaneously evidence-based and experimental in their delivery and impact.
Keywords: Blockchain; Industry; Innovation policy; Technology uptake; Foresight; Stakeholder engagement

Yves Gendron, Jane Andrew, Christine Cooper, Helen Tregidga,
On the juggernaut of artificial intelligence in organizations, research and society,
Critical Perspectives on Accounting,
Volume 100,
2024,
102759,
ISSN 1045-2354,
https://doi.org/10.1016/j.cpa.2024.102759.
(https://www.sciencedirect.com/science/article/pii/S1045235424000583)
Abstract: Capitalizing on what we currently know about artificial intelligence (AI), the editorial of this special issue, entitled “Artificial Intelligence in the Spotlight”, adds our voice to a call to order in the face of the unbridled enthusiasm we often encounter regarding the benefits of AI. In short, we maintain that there is a crucial need for skepticism about the all-out colonization project vigorously pursued by AI and its sustaining infrastructure. We draw on our own analysis and that of the contributors to this special issue to consider what we see as a bold agenda for colonizing our communities, our ways of doing, and our minds – so that we become fundamentally dependent on technologies whose reliability is dubious and whose algorithms are secretly maintained behind the safety of corporate walls. Our thesis is that the cacophony of aberrations, disorder, and worries that emerge in the wake of AI can be meaningfully viewed as a juggernaut, an inexorable force that is ready to unsettle all things in its tedious path. The juggernaut metaphor constitutes our way of putting “artificial intelligence in the spotlight”. We call for researchers from all disciplines to engage in the study of the AI juggernaut and speak out as much as they can, in public and in academic spheres, about its dangers.
Keywords: Artificial intelligence; Colonization; Dangers; Juggernaut; Research; Trust in science

C. Wang, X.P. Tan, S.B. Tor, C.S. Lim,
Machine learning in additive manufacturing: State-of-the-art and perspectives,
Additive Manufacturing,
Volume 36,
2020,
101538,
ISSN 2214-8604,
https://doi.org/10.1016/j.addma.2020.101538.
(https://www.sciencedirect.com/science/article/pii/S2214860420309106)
Abstract: Additive manufacturing (AM) has emerged as a disruptive digital manufacturing technology. However, its broad adoption in industry is still hindered by high entry barriers of design for additive manufacturing (DfAM), limited materials library, various processing defects, and inconsistent product quality. In recent years, machine learning (ML) has gained increasing attention in AM due to its unprecedented performance in data tasks such as classification, regression and clustering. This article provides a comprehensive review on the state-of-the-art of ML applications in a variety of AM domains. In the DfAM, ML can be leveraged to output new high-performance metamaterials and optimized topological designs. In AM processing, contemporary ML algorithms can help to optimize process parameters, and conduct examination of powder spreading and in-process defect monitoring. On the production of AM, ML is able to assist practitioners in pre-manufacturing planning, and product quality assessment and control. Moreover, there has been an increasing concern about data security in AM as data breaches could occur with the aid of ML techniques. Lastly, it concludes with a section summarizing the main findings from the literature and providing perspectives on some selected interesting applications of ML in research and development of AM.
Keywords: Additive manufacturing; Process; Machine learning; Production; Design

Chamara Sandeepa, Bartlomiej Siniarski, Nicolas Kourtellis, Shen Wang, Madhusanka Liyanage,
A survey on privacy for B5G/6G: New privacy challenges, and research directions,
Journal of Industrial Information Integration,
Volume 30,
2022,
100405,
ISSN 2452-414X,
https://doi.org/10.1016/j.jii.2022.100405.
(https://www.sciencedirect.com/science/article/pii/S2452414X22000723)
Abstract: Massive developments in mobile wireless telecommunication networks have been made during the last few decades. At present, mobile users are getting familiar with the latest 5G networks, and the discussion for the next generation of Beyond 5G (B5G)/6G networks has already been initiated. It is expected that B5G/6G will push the existing network capabilities to the next level, with higher speeds, enhanced reliability and seamless connectivity. To make these expectations a reality, research is progressing on new technologies, architectures, and intelligence-based decision-making processes related to B5G/6G. Privacy considerations are a crucial aspect that requires further attention in such developments, as billions of people and devices will be transmitting data through the upcoming network. However, the main recognition remains biased towards the network security. A discussion focused on privacy of B5G/6G is lacking at the moment. To address the gap, this paper provides a comprehensive survey on privacy-related aspects of B5G/6G networks. First, it discusses a taxonomy of different privacy perspectives. Based on the taxonomy, the paper then conceptualizes a set of challenges that appear as barriers to reach privacy preservation. Next, this work provides a set of solutions applicable to the proposed architecture of B5G/6G networks to mitigate the challenges. It also provides an overview of standardization initiatives for privacy preservation. Finally, the paper concludes with a roadmap of future directions, which will be an arena for new research towards privacy-enhanced B5G/6G networks. This work provides a basis for privacy aspects that will significantly impact peoples’ daily lives when using these future networks.
Keywords: Beyond 5G; 6G; Privacy issues; Privacy solutions; Artificial intelligence; Machine learning; Explainable AI; Survey

Lisa Cipolotti, Joe Mole, James K. Ruffle, Amy Nelson, Robert Gray, Parashkev Nachev,
Cognitive control & the anterior cingulate cortex: Necessity & coherence,
Cortex,
Volume 182,
2025,
Pages 87-99,
ISSN 0010-9452,
https://doi.org/10.1016/j.cortex.2024.11.010.
(https://www.sciencedirect.com/science/article/pii/S001094522400323X)
Abstract: Influential theories of complex behaviour invoke the notion of cognitive control modulated by conflict between counterfactual actions. Medial frontal cortex, notably the anterior cingulate cortex, has been variously posited as critical to such conflict detection, resolution, or monitoring, largely based on correlative data from functional imaging. Examining performance on the most widely used “conflict” task—Stroop—in a large cohort of patients with focal brain injury (N = 176), we compare anatomical patterns of lesion-inferred neural substrate dependence to those derived from functional imaging, meta-analytically summarised. Our results show that whereas performance is sensitive to the integrity of left lateral frontal regions implicated by functional imaging, it does not depend on medial frontal cortex, despite sampling adequate to reveal robust medial effects in the context of phonemic fluency. We suggest that medial frontal cortex is not critically invoked by Stroop and proceed to review the conceptual grounds for rejecting the core notion of conflict-driven cognitive control.
Keywords: Cognitive control; Conflict detection; Stroop; Executive functions; Network lesion-deficit mapping; Conceptual analysis; Anterior cingulate cortex

Neris Özen, Wenjuan Mu, Esther D. van Asselt, Leonieke M. van den Bulk,
Extracting chemical food safety hazards from the scientific literature automatically using large language models,
Applied Food Research,
Volume 5, Issue 1,
2025,
100679,
ISSN 2772-5022,
https://doi.org/10.1016/j.afres.2024.100679.
(https://www.sciencedirect.com/science/article/pii/S2772502224002890)
Abstract: The number of scientific articles published in the domain of food safety has consistently been increasing over the last few decades. It has therefore become unfeasible for food safety experts to read all relevant literature related to food safety and the occurrence of hazards in the food chain. However, it is important that food safety experts are aware of the newest findings and can access this information in an easy and concise way. In this study, an approach is presented to automate the extraction of chemical hazards from the scientific literature through large language models. The large language model was used out-of-the-box and applied on scientific abstracts; no extra training of the models or a large computing cluster was required. Three different styles of prompting the model were tested to assess which was the most optimal for the task at hand. The prompts were optimized with two validation foods (leafy greens and shellfish) and the final performance of the best prompt was evaluated using three test foods (dairy, maize and salmon). The specific wording of the prompt was found to have a considerable effect on the results. A prompt breaking the task down into smaller steps performed best overall. This prompt reached an average accuracy of 93 % and contained many chemical contaminants already included in food monitoring programs, validating the successful retrieval of relevant hazards for the food safety domain. The results showcase how valuable large language models can be for the task of automatic information extraction from the scientific literature.
Keywords: Chemical contamination; Food safety; Information extraction; Prompt engineering; Natural language processing; Artificial intelligence

Elijah W. Riddle, Divya Kewalramani, Mayur Narayan, Daniel B. Jones,
Surgical Simulation: Virtual Reality to Artificial Intelligence,
Current Problems in Surgery,
Volume 61, Issue 11,
2024,
101625,
ISSN 0011-3840,
https://doi.org/10.1016/j.cpsurg.2024.101625.
(https://www.sciencedirect.com/science/article/pii/S0011384024001862)

Frederik von Briel, Jan Recker, Per Davidsson,
Not all digital venture ideas are created equal: Implications for venture creation processes,
The Journal of Strategic Information Systems,
Volume 27, Issue 4,
2018,
Pages 278-295,
ISSN 0963-8687,
https://doi.org/10.1016/j.jsis.2018.06.002.
(https://www.sciencedirect.com/science/article/pii/S0963868717301002)
Abstract: Digital ventures are formed around ideas that have digital artifacts at their core. We develop theory that explains how the composition of digital artifacts influences venture creation processes. First, we develop propositions that link differences in the embodiment and coupling of digital artifact components to tensions in venture creation process inputs, behaviors, and outputs. Second, we link compositional differences in digital artifacts to differences in venture creation process initiation, duration, and outcome. Our theorizing establishes a foundation for future research on digital artifacts within and beyond entrepreneurship contexts, and for future research on entrepreneurship within and beyond digital artifact contexts.
Keywords: Digital entrepreneurship; Digital ventures; New venture ideas; Venture creation process; Hardware ventures; Software ventures

Margo Van Poucke,
ChatGPT, the perfect virtual teaching assistant? Ideological bias in learner-chatbot interactions,
Computers and Composition,
Volume 73,
2024,
102871,
ISSN 8755-4615,
https://doi.org/10.1016/j.compcom.2024.102871.
(https://www.sciencedirect.com/science/article/pii/S8755461524000471)
Abstract: This paper examines ChatGPT's use of evaluative language and engagement strategies while addressing information-seeking queries. It assesses the chatbot's role as a virtual teaching assistant (VTA) across various educational settings. By employing Appraisal theory, the analysis contrasts responses generated by ChatGPT and those added by humans, focusing on the interactants’ attitude, deployment of interpersonal metaphors and evaluations of entities, revealing their views on Australian cultural practice. Two datasets were analysed: the first sample (15,909 words) was retrieved from the subreddit r/AskAnAustralian and the second (10,696 words) was obtained by prompting ChatGPT with the same questions. The findings show that, while human experts mainly opt for subjective explicit formulations to express personal viewpoints, the chatbot's preference goes out to incongruent ‘it is’-constructions to share pre-programmed perspectives, which may reflect ideological bias. Even though ChatGPT displays promising socio-communicative capabilities (SCs), its lack of contextual awareness, required to function cross-culturally as a VTA, may lead to considerable ethical issues. The study's novel contribution lies in the in-depth investigation of how the chatbot's SCs and lexicogrammatical selections may impact its role as a VTA, highlighting the need to develop students’ critical digital literacy skills while using AI learning tools.
Keywords: Appraisal theory; Systemic functional linguistics; ChatGPT, Human-chatbot interaction; Interpersonal metaphors; Ethical considerations; Bias; AI learning tools; Education

Mike Thelwall, Andrew Cox,
Estimating the quality of academic books from their descriptions with ChatGPT,
The Journal of Academic Librarianship,
Volume 51, Issue 2,
2025,
103023,
ISSN 0099-1333,
https://doi.org/10.1016/j.acalib.2025.103023.
(https://www.sciencedirect.com/science/article/pii/S0099133325000199)
Abstract: Although indicators based on scholarly citations are widely used to support the evaluation of academic journals, alternatives are needed for scholarly book acquisitions. This article assesses the value of research quality scores from ChatGPT 4o-mini for 9830 social sciences, arts, and humanities books from 2019 indexed in Scopus, based on their titles and descriptions but not their full texts. Although most books scored the same (3* on a 1* to 4* scale), the citation rates correlate positively but weakly with ChatGPT 4o-mini research quality scores in both the social sciences and the arts and humanities. Part of the reason for the differences was the inclusion of textbooks, short books, and edited collections, all of which tended to be less cited and lower scoring. Some topics also tend to attract many/few citations and/or high/low ChatGPT scores. Descriptions explicitly mentioning theory and/or some methods also associated with higher scores and more citations. Overall, the results provide some evidence that both ChatGPT scores and citation counts are weak indicators of the research quality of books. Whilst not strong enough to support individual book quality judgements, they may help academic librarians seeking to evaluate new book collections, series, or publishers for potential acquisition.
Keywords: Large language models; Collection acquisition; Scientometrics, bibliometrics

Houcemeddine Turki, Khalil Chebil, Bonaventure F.P. Dossou, Chris Chinenye Emezue, Abraham Toluwase Owodunni, Mohamed Ali Hadj Taieb, Mohamed Ben Aouicha,
A framework for integrating biomedical knowledge in Wikidata with open biological and biomedical ontologies and MeSH keywords,
Heliyon,
Volume 10, Issue 19,
2024,
e38448,
ISSN 2405-8440,
https://doi.org/10.1016/j.heliyon.2024.e38448.
(https://www.sciencedirect.com/science/article/pii/S2405844024144799)
Abstract: This study presents a comprehensive framework to enhance Wikidata as an open and collaborative knowledge graph by integrating Open Biological and Biomedical Ontologies (OBO) and Medical Subject Headings (MeSH) keywords from PubMed publications. The primary data sources include OBO ontologies and MeSH keywords, which were collected and classified using SPARQL queries for RDF knowledge graphs. The semantic alignment between OBO ontologies and Wikidata was evaluated, revealing significant gaps and distorted representations that necessitate both automated and manual interventions for improvement. We employed pointwise mutual information to extract biomedical relations among the 5000 most common MeSH keywords in PubMed, achieving an accuracy of 89.40 % for superclass-based classification and 75.32 % for relation type-based classification. Additionally, Integrated Gradients were utilized to refine the classification by removing irrelevant MeSH qualifiers, enhancing overall efficiency. The framework also explored the use of MeSH keywords to identify PubMed reviews supporting unsupported Wikidata relations, finding that 45.8 % of these relations were not present in PubMed, indicating potential inconsistencies in Wikidata. The contributions of this study include improved methodologies for enriching Wikidata with biomedical information, validated semantic alignments, and efficient classification processes. This work enhances the interoperability and multilingual capabilities of biomedical ontologies and demonstrates the critical role of MeSH keywords in verifying semantic relations, thereby contributing to the robustness and accuracy of collaborative biomedical knowledge graphs.
Keywords: Wikidata; Open biological and biomedical ontologies; MeSH keywords; Biomedical relation identification; Crowdsourcing; PubMed

Iman Salahshoori, Marcos A.L. Nobre, Amirhosein Yazdanbakhsh, Rahime Eshaghi Malekshah, Morteza Asghari, Hossein Ali Khonakdar, Amir H. Mohammadi,
Navigating the molecular landscape of environmental science and heavy metal removal: A simulation-based approach,
Journal of Molecular Liquids,
Volume 410,
2024,
125592,
ISSN 0167-7322,
https://doi.org/10.1016/j.molliq.2024.125592.
(https://www.sciencedirect.com/science/article/pii/S0167732224016519)
Abstract: Heavy metals pose a significant threat to ecosystems and human health because of their toxic properties and their ability to bioaccumulate in living organisms. Traditional removal methods often fall short in terms of cost, energy efficiency, and minimizing secondary pollutant generation, especially in complex environmental settings. In contrast, molecular simulation methods offer a promising solution by providing in-depth insights into atomic and molecular interactions between heavy metals and potential adsorbents. This review highlights the potential of molecular simulation methods for removing types of pollutants in environmental science, specifically heavy metals. These methods offer a powerful tool for predicting and designing materials and processes for environmental remediation. We focus on removing specific heavy metals like lead, Cadmium, and mercury, utilizing cutting-edge simulation techniques such as Molecular Dynamics (MD), Monte Carlo (MC) simulations, Quantum Chemical Calculations (QCC), and Artificial Intelligence (AI). By leveraging these methods, we aim to develop highly efficient and selective materials and processes for environmental remediation. By unravelling the underlying mechanisms, these techniques pave the way for developing more efficient and selective removal technologies. This comprehensive review addresses a critical gap in the scientific literature, providing valuable insights for researchers in environmental protection and human health. Molecular modelling methods hold significant promise for revolutionizing the prediction and removal of heavy metals, ultimately contributing to sustainable solutions for a cleaner and healthier future.
Keywords: Computational methods; Environmental pollutants; Heavy metals removal; Molecular simulations, wastewater treatments

Kamalakkannan Ravi, Adan Ernesto Vela,
RICo: Reddit ideological communities,
Online Social Networks and Media,
Volume 42,
2024,
100279,
ISSN 2468-6964,
https://doi.org/10.1016/j.osnem.2024.100279.
(https://www.sciencedirect.com/science/article/pii/S2468696424000041)
Abstract: The main objective of our research is to gain a comprehensive understanding of the relationship between language usage within different communities and delineating the ideological narratives. We focus specifically on utilizing Natural Language Processing techniques to identify underlying narratives in the coded or suggestive language employed by non-normative communities associated with targeted violence. Earlier studies addressed the detection of ideological affiliation through surveys, user studies, and a limited number based on the content of text articles, which still require label curation. Previous work addressed label curation by using ideological subreddits (r/Liberal and r/Conservative for Liberal and Conservative classes) to label the articles shared on those subreddits according to their prescribed ideologies, albeit with a limited dataset. Building upon previous work, we use subreddit ideologies to categorize shared articles. In addition to the conservative and liberal classes, we introduce a new category called “Restricted” which encompasses text articles shared in subreddits that are restricted, privatized, or banned, such as r/TheDonald. The “Restricted” class encompasses posts tied to violence, regardless of conservative or liberal affiliations. Additionally, we augment our dataset with text articles from self-identified subreddits like r/progressive and r/askaconservative for the liberal and conservative classes, respectively. This results in an expanded dataset of 377,144 text articles, consisting of 72,488 liberal, 79,573 conservative, and 225,083 restricted class articles. Our goal is to analyze language variances in different ideological communities, investigate keyword relevance in labeling article orientations, especially in unseen cases (922,522 text articles), and delve into radicalized communities, conducting thorough analysis and interpretation of the results.
Keywords: Social networking (online); Learning (artificial intelligence); Predictive models; Transformers; Support vector machines; Text analysis; Context modeling; Natural language processing

Seung-Lee Lee, Minjae Kang, Jong-Uk Hou,
Localization of diffusion model-based inpainting through the inter-intra similarity of frequency features,
Image and Vision Computing,
Volume 148,
2024,
105138,
ISSN 0262-8856,
https://doi.org/10.1016/j.imavis.2024.105138.
(https://www.sciencedirect.com/science/article/pii/S0262885624002427)
Abstract: Recently, the enhanced abilities of diffusion models have led to more realistic inpainting results, which raises the potential for criminal activity through image forgery. In this study, we explore the detection of inpainted images generated by a diffusion model. We propose a method for inpainting localization using an inter-intra similarity (IIS) module based on image frequency features. The proposed IIS module learns the inter-patch relationship through the learnable frequency filter and subsequently covers the intra-patch relationship through the self-similarity operation. We provide the Diffusion Model Inpainting Dataset (DMID), a benchmark dataset comprising inpainted images using four different diffusion models and three types of masks. Additionally, a test dataset that includes three sampling steps is provided. We validated the effectiveness of our proposed approach by conducting comparative tests with existing forgery detectors using our dataset and testing the robustness of JPEG compression. Additionally, we tested our proposed method on datasets with different sampling step sizes. Our work provides a starting point for research on the detection of inpainting-based forgery using diffusion models. Additionally, by openly releasing the dataset, we offer an opportunity to advance future in-depth research related to forensics.
Keywords: Image forensics; Image inpainting; Generative model; Diffusion model; Localization

Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein Heidari, Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu, Dorit Merhof,
Diffusion models in medical imaging: A comprehensive survey,
Medical Image Analysis,
Volume 88,
2023,
102846,
ISSN 1361-8415,
https://doi.org/10.1016/j.media.2023.102846.
(https://www.sciencedirect.com/science/article/pii/S1361841523001068)
Abstract: Denoising diffusion models, a class of generative models, have garnered immense interest lately in various deep-learning problems. A diffusion probabilistic model defines a forward diffusion stage where the input data is gradually perturbed over several steps by adding Gaussian noise and then learns to reverse the diffusion process to retrieve the desired noise-free data from noisy data samples. Diffusion models are widely appreciated for their strong mode coverage and quality of the generated samples in spite of their known computational burdens. Capitalizing on the advances in computer vision, the field of medical imaging has also observed a growing interest in diffusion models. With the aim of helping the researcher navigate this profusion, this survey intends to provide a comprehensive overview of diffusion models in the discipline of medical imaging. Specifically, we start with an introduction to the solid theoretical foundation and fundamental concepts behind diffusion models and the three generic diffusion modeling frameworks, namely, diffusion probabilistic models, noise-conditioned score networks, and stochastic differential equations. Then, we provide a systematic taxonomy of diffusion models in the medical domain and propose a multi-perspective categorization based on their application, imaging modality, organ of interest, and algorithms. To this end, we cover extensive applications of diffusion models in the medical domain, including image-to-image translation, reconstruction, registration, classification, segmentation, denoising, 2/3D generation, anomaly detection, and other medically-related challenges. Furthermore, we emphasize the practical use case of some selected approaches, and then we discuss the limitations of the diffusion models in the medical domain and propose several directions to fulfill the demands of this field. Finally, we gather the overviewed studies with their available open-source implementations at our GitHub.11https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging. We aim to update the relevant latest papers within it regularly.
Keywords: Generative models; Diffusion models; Denoising diffusion models; Noise conditioned score networks; Score-based models; Medical imaging; Medical applications; Survey

An Tang, Roger Tam, Alexandre Cadrin-Chênevert, Will Guest, Jaron Chong, Joseph Barfett, Leonid Chepelev, Robyn Cairns, J. Ross Mitchell, Mark D. Cicero, Manuel Gaudreau Poudrette, Jacob L. Jaremko, Caroline Reinhold, Benoit Gallix, Bruce Gray, Raym Geis, Timothy O'Connell, Paul Babyn, David Koff, Darren Ferguson, Sheldon Derkatch, Alexander Bilbily, Wael Shabana,
Canadian Association of Radiologists White Paper on Artificial Intelligence in Radiology,
Canadian Association of Radiologists Journal,
Volume 69, Issue 2,
2018,
Pages 120-135,
ISSN 0846-5371,
https://doi.org/10.1016/j.carj.2018.02.002.
(https://www.sciencedirect.com/science/article/pii/S0846537118300305)
Abstract: Artificial intelligence (AI) is rapidly moving from an experimental phase to an implementation phase in many fields, including medicine. The combination of improved availability of large datasets, increasing computing power, and advances in learning algorithms has created major performance breakthroughs in the development of AI applications. In the last 5 years, AI techniques known as deep learning have delivered rapidly improving performance in image recognition, caption generation, and speech recognition. Radiology, in particular, is a prime candidate for early adoption of these techniques. It is anticipated that the implementation of AI in radiology over the next decade will significantly improve the quality, value, and depth of radiology's contribution to patient care and population health, and will revolutionize radiologists' workflows. The Canadian Association of Radiologists (CAR) is the national voice of radiology committed to promoting the highest standards in patient-centered imaging, lifelong learning, and research. The CAR has created an AI working group with the mandate to discuss and deliberate on practice, policy, and patient care issues related to the introduction and implementation of AI in imaging. This white paper provides recommendations for the CAR derived from deliberations between members of the AI working group. This white paper on AI in radiology will inform CAR members and policymakers on key terminology, educational needs of members, research and development, partnerships, potential clinical applications, implementation, structure and governance, role of radiologists, and potential impact of AI on radiology in Canada.
Résumé
L'intelligence artificielle progresse rapidement de la phase expérimentale à la phase de mise en œuvre dans de nombreux domaines, notamment la médecine. L'accès à de grands ensembles de données, la puissance croissante des ordinateurs et les avancées en matière d'algorithmes d'apprentissage ont permis de faire des pas de géant au chapitre du développement des applications d'intelligence artificielle. Au cours des cinq dernières années, des techniques comme l'apprentissage profond ont permis d'améliorer rapidement les capacités de reconnaissance d'images, de production de légendes d'images et de reconnaissance vocale. La radiologie est un domaine tout indiqué pour l'adoption précoce de ces techniques. L'intégration d'applications d'intelligence artificielle en radiologie au cours de la prochaine décennie devrait grandement améliorer la qualité, la valeur et la portée de la contribution de la radiologie aux soins des patients et à la santé de la population, en plus de révolutionner le travail des radiologistes. En sa qualité de porte-parole de la profession au Canada, l’Association canadienne des radiologistes (CAR) défend des normes de pratique élevées en imagerie centrée sur les patients, en apprentissage continu et en recherche. La CAR a mis sur pied un groupe de travail sur l'intelligence artificielle qui a pour mandat de discuter des enjeux liés à la pratique, aux politiques et à la prestation de soins relativement à l'introduction et à la mise en œuvre d'outils d'intelligence artificielle en radiologie. Le présent livre blanc formule à l'intention de la CAR des recommandations issues des délibérations des membres du groupe de travail. Il renseigne les membres de la CAR et les responsables de l’élaboration des politiques sur la terminologie à employer, les besoins en matière de formation, la recherche-développement, les partenariats, les applications cliniques potentielles, la mise en œuvre, la structure et la gouvernance, le rôle des radiologistes et sur les retombées potentielles de l'intelligence artificielle en radiologie au Canada.
Keywords: Artificial intelligence; Machine learning; Deep learning; Radiology; Imaging; Medicine; Healthcare; Quality improvement

Xin Liao, Jing Peng, Yun Cao,
GIFMarking: The robust watermarking for animated GIF based deep learning,
Journal of Visual Communication and Image Representation,
Volume 79,
2021,
103244,
ISSN 1047-3203,
https://doi.org/10.1016/j.jvcir.2021.103244.
(https://www.sciencedirect.com/science/article/pii/S1047320321001590)
Abstract: Animated GIF has become a key communication tool in contemporary social platforms thanks to highly compatible with affective performance, and it is gradually adopted in commercial applications. Therefore, the copyright protection of the animated GIF requires more attention. Digital watermarking is an effective method to embed invisible data into a digital medium that can identify the creator or authorized users. However, few works have been devoted to robust watermarking for the animated GIF. One of the main challenges is that the animated image also contains time frame dimension information compare with still images. This paper proposes a robust blind watermarking framework based 3D convolutional neural networks for the animated GIF image, which achieves watermark image embedding and extraction for the animated GIF. Also, noise simulation is developed in frame-level to ensure robustness for the attack of the temporal dimension in this framework. Furthermore, the invisibility of the watermarked animated image is optimized by adversarial learning. Experimental results provide the effectiveness of the proposed framework and show advantages over existing works.
Keywords: Animated GIF images; Robust watermarking; 3D convolutional neural networks; Adversarial network

Yuval Cohen, Hila Chalutz–Ben Gal,
Digital, Technological and AI Skills for Smart Production Work Environment,
IFAC-PapersOnLine,
Volume 58, Issue 19,
2024,
Pages 545-550,
ISSN 2405-8963,
https://doi.org/10.1016/j.ifacol.2024.09.269.
(https://www.sciencedirect.com/science/article/pii/S2405896324016987)
Abstract: This paper analyses the past and anticipated developments in collaborative smart production work environment and points at the required skills to best utilize and flourish in this newly formed work environment. The paper identifies the new work requirements using the job type and its related required technologies and maps the work requirements to the set of skills that may fulfill these requirements. An important notion in this paper is that a shopfloor usually involves several different work environments, each with its unique set of work requirements and associated skills. Thus, tailoring a subset of skills to these set of requirements is the suggested strategy. We use a small example of assembly shopfloor for illustrating the proposed approach. Finally, we propose future research related to this study.
Keywords: Skills; Collaborative Work; Smart Manufacturing; Industry 5.0; Human in the loop; Cobot

Byungun Yoon, Yujin Jeong, Keeeun Lee, Sungjoo Lee,
A systematic approach to prioritizing R&D projects based on customer-perceived value using opinion mining,
Technovation,
Volume 98,
2020,
102164,
ISSN 0166-4972,
https://doi.org/10.1016/j.technovation.2020.102164.
(https://www.sciencedirect.com/science/article/pii/S0166497218306874)
Abstract: As product development has recently emphasized user innovation, it should necessarily reflect customer-perceived value, as well as technological value itself. However, while previous studies for technology planning have focused on analyzing the potential of technology, they have not considered the customer-perceived value that technology can create in a new product. Therefore, this research suggests a new approach to assessing the level of technology and evaluating R&D projects, by investigating customer-perceived value on technology through the use of the structural equation model and opinion mining. For this, the assessment framework is developed in terms of technology, product quality, and customer satisfaction, respectively, by investigating a variety of databases on each factor. The relationship between technology level and customer satisfaction is analyzed, using structural equation model and opinion mining. Based on the results, a strategy for technology development is established through gap analysis and simulation, after selecting and evaluating technologies that need to be developed. The proposed approach is applied to the real case of a moving system, in particular an automobile door, and we obtained that an R&D project for hinge-related technology would be promising, enhancing the customer satisfaction. It can suggest a future direction for new technology development. This paper contributes to enhancing the efficiency of technology planning based on the customer-perceived value, enabling the launch of new R&D projects.
Keywords: Technology assessment; Customer-perceived value; Structural equation model; Opinion mining; Technology strategy

Mamoru Mimura, Yui Tajiri,
Static detection of malicious PowerShell based on word embeddings,
Internet of Things,
Volume 15,
2021,
100404,
ISSN 2542-6605,
https://doi.org/10.1016/j.iot.2021.100404.
(https://www.sciencedirect.com/science/article/pii/S2542660521000482)
Abstract: While traditional malware relies on executables to function, fileless malware resides in memory to evade traditional detection methods. PowerShell which is a legitimate management tool used by system administrators provides an ideal cover for attackers. Many studies attempted to detect unknown malware with machine learning techniques. However, there are a few studies for detecting malicious PowerShell. Previous studies proposed methods of detecting malicious PowerShell with deep neural networks. Previous methods require decoding obfuscated samples for dynamic code evaluation. Decoding obfuscated samples is a troublesome task and is often time consuming. Security devices such as intrusion detection system (IDS) or sandbox are located at a point that can monitor all inbound traffic. In general, this traffic contains too massive samples to analyze by dynamic analysis. Therefore, a light-weight static method is desirable. In addition, some studies use their private dataset to evaluate their methods. In this paper, we propose a static method of detecting malicious PowerShell based on word embeddings. In our method, PowerShell scripts are separated into words, and these words are used as features for machine learning techniques. We improved the feature extraction method by selecting frequent words. To provide reproducibility, we obtained thousands of samples from multiple websites which are publicly available. The best F1 score achieves 0.995 in practical environment, and achieves 0.985 in 5-fold cross-validation. Furthermore, we identified their malware families, and confirmed our method is effective to new ones.
Keywords: PowerShell; Latent Semantic Indexing; Doc2vec; XGBoost

Wen Xie, Gaini Ma, Feng Zhao, Hanqiang Liu, Lu Zhang,
PolSAR image classification via a novel semi-supervised recurrent complex-valued convolution neural network,
Neurocomputing,
Volume 388,
2020,
Pages 255-268,
ISSN 0925-2312,
https://doi.org/10.1016/j.neucom.2020.01.020.
(https://www.sciencedirect.com/science/article/pii/S0925231220300497)
Abstract: Due to that polarimetric synthetic aperture radar (PolSAR) data suffers from missing labeled samples and complex-valued data, this article presents a novel semi-supervised PolSAR terrain classification method named recurrent complex-valued convolution neural network (RCV-CNN) which combines semi-supervised learning and complex-valued convolution neural network (CV-CNN). The proposed method only needs a small number of labeled samples to achieve good classification results. First, a Wishart classifier is used to select some reliable PolSAR samples. Then, two new semi-supervised deep classification model RCV-CNN1 and RCV-CNN2 have been proposed to improve PolSAR image classification accuracy. Moreover, our proposed methods could solve the problem of network overfitting phenomenon to some extend when the number of training samples is too small. Finally, three real PolSAR dataset are applied to verify the effectiveness of our algorithms. Compared with the other five state-of-the-art methods, the proposed RCV-CNN1 and RCV-CNN2 classification models show good performance in accuracy and generalization.
Keywords: PolSAR terrain classification; Network overfitting; RCV-CNN1; RCV-CNN2

Pendukeni Phalaagae, Adamu Murtala Zungeru, Boyce Sigweni, Selvaraj Rajalakshmi, Herbet Batte, Odongo S. Eyobu,
An energy efficient authentication scheme for cluster-based wireless IoT sensor networks,
Scientific African,
Volume 25,
2024,
e02287,
ISSN 2468-2276,
https://doi.org/10.1016/j.sciaf.2024.e02287.
(https://www.sciencedirect.com/science/article/pii/S2468227624002321)
Abstract: Wireless Internet of Things (IoT) sensor networks (WITSN) play a pivotal role in modern society, facilitating a myriad of applications ranging from smart homes to industrial automation. Therefore, safeguarding these networks against security threats is paramount to ensure the integrity, confidentiality, and availability of data transmitted within them. However, WITSNs face escalating security threats due to their diverse structures and platforms. Existing literature has identified these vulnerabilities but lacks comprehensive solutions to address them effectively. To bridge this gap, this paper proposes a novel security approach termed Randomized Bi-Phase Authentication Scheme (RBAS), which integrates digital watermarking techniques to fortify both external and internal network security. RBAS not only tackles data availability, confidentiality, and authenticity challenges prevalent in WITSNs but also strives to maintain a delicate equilibrium between robust security measures and energy efficiency. Key contributions of this work include the meticulous examination and validation of the integration of cyclic redundancy check (CRC) codes in IoT sensor network authentication, demonstrating their efficacy in error detection through simulations. Furthermore, RBAS employs advanced hashing, cryptography, and dynamic verification codes to ensure strong error detection and data authenticity, leveraging robust CRC codes and randomization to thwart potential attacks. The scheme's complexity acts as a deterrent against manipulation, while its cluster awareness enhances adaptability, and cryptographic principles bolster overall security. Extensive performance evaluation using the Network Simulator-2 (NS2) reveals significant benefits of RBAS, including an 8 % reduction in power consumption and a 7 % increase in network longevity. Notably, RBAS surpasses existing solutions with a 14 % reduction in dropping ratio and an 8 % decrease in latency. The success of RBAS stems from its innovative utilization of lightweight watermarking techniques and cluster-based routing, enabling proactive identification of data tampering and thereby enhancing the network's overall security posture. This pioneering work not only advances the state of the art in WITSN security but also holds profound implications for the practical deployment of secure and efficient IoT sensor networks in real-world scenarios.
Keywords: Bi-phase authentication scheme; Digital watermarking; Energy adaptive clustering hierarchy; Internet of Things; Randomized Bi-phase authentication scheme; Wireless Internet of Things sensor network

Sparsh Mittal, Himanshi Gupta, Srishti Srivastava,
A survey on hardware security of DNN models and accelerators,
Journal of Systems Architecture,
Volume 117,
2021,
102163,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2021.102163.
(https://www.sciencedirect.com/science/article/pii/S1383762121001168)
Abstract: As “deep neural networks” (DNNs) achieve increasing accuracy, they are getting employed in increasingly diverse applications, including security-critical applications such as medical and defense. This immense use of DNNs has motivated the researchers to scrutinizingly study their security vulnerability and propose countermeasures, especially in the context of hardware security. In this paper, we present a survey of techniques for the hardware security of DNNs. For the research works, we highlight the threat-model, key idea for launching attack and defense strategies. We organize the works on salient categories to highlight their strengths and limitations. This paper aims to equip researchers with the knowledge of recent advances in DNN security and motivate them to think of security as the first principle.
Keywords: Hardware security; Trojan; Fault-injection attack; Side-channel attack; Encryption; Deep neural network

Joshua Pantanowitz, Christopher D. Manko, Liron Pantanowitz, Hooman H. Rashidi,
Synthetic Data and Its Utility in Pathology and Laboratory Medicine,
Laboratory Investigation,
Volume 104, Issue 8,
2024,
102095,
ISSN 0023-6837,
https://doi.org/10.1016/j.labinv.2024.102095.
(https://www.sciencedirect.com/science/article/pii/S0023683724017732)
Abstract: In our rapidly expanding landscape of artificial intelligence, synthetic data have become a topic of great promise and also some concern. This review aimed to provide pathologists and laboratory professionals with a primer on the role of synthetic data and how it may soon shape the landscape within our field. Using synthetic data presents many advantages but also introduces a milieu of new obstacles and limitations. This review aimed to provide pathologists and laboratory professionals with a primer on the general concept of synthetic data and its potential to transform our field. By leveraging synthetic data, we can help accelerate the development of various machine learning models and enhance our medical education and research/quality study needs. This review explored the methods for generating synthetic data, including rule-based, machine learning model-based and hybrid approaches, as they apply to applications within pathology and laboratory medicine. We also discussed the limitations and challenges associated with such synthetic data, including data quality, malicious use, and ethical bias/concerns and challenges. By understanding the potential benefits (ie, medical education, training artificial intelligence programs, and proficiency testing, etc) and limitations of this new data realm, we can not only harness its power to improve patient outcomes, advance research, and enhance the practice of pathology but also become readily aware of their intrinsic limitations.
Keywords: artificial intelligence; data simulation; generative AI; laboratory medicine; machine learning models; pathology artificial intelligence; pathology education; synthetic data

José Luis Cabrera-Ponce, Eliana Valencia-Lozano, Diana Lilia Trejo-Saavedra,
Chapter 3 - Genetic Modifications of Corn,
Editor(s): Sergio O. Serna-Saldivar,
Corn (Third Edition),
AACC International Press,
2019,
Pages 43-85,
ISBN 9780128119716,
https://doi.org/10.1016/B978-0-12-811971-6.00003-6.
(https://www.sciencedirect.com/science/article/pii/B9780128119716000036)
Abstract: Corn, Zea mays L., is one of the humankind's earliest innovations. The large productivity gains in corn production have come primarily from advanced plant breeding techniques and improved corn management. Plant breeding has gotten technologically savvy in the last century. Realizing that natural mutants often introduce valuable traits, scientists turned to chemicals and irradiation to speed the creation of mutants. Later, plant tissue culture evolved (1970s), then the use of molecular markers to identify interesting hereditary traits; during the 1980s, genetic engineering by means of making transgenic plants and, more recently, genome editing as a new tool to do more precise specific mutations for specific traits. Since 1996, corn products with biotechnological traits and associated agronomic practices have contributed to the steady increase in corn production by reducing pest and environmental stress on highly productive new corn genetics. The generation of transgenic plants is the crucial step in the development of new biotech trait products. In this chapter, we will review the importance of several methods to create mutants in corn; also, the technology of genetic transformation mainly by particle bombardment and Agrobacterium and the next generation of biotech products; genome editing corn.
Keywords: Corn; Mutation; Transgenic plant; Agrobacterium; Particle bombardment; Embryogenic calli; Insect resistance; Herbicide tolerance; Genome editing

R. Baranski, I. Goldman, T. Nothnagel, H. Budahn, J.W. Scott,
Chapter 22 - Improving color sources by plant breeding and cultivation,
Editor(s): Ralf Schweiggert,
In Woodhead Publishing Series in Food Science, Technology and Nutrition,
Handbook on Natural Pigments in Food and Beverages (Second Edition),
Woodhead Publishing,
2024,
Pages 507-553,
ISBN 9780323996082,
https://doi.org/10.1016/B978-0-323-99608-2.00012-4.
(https://www.sciencedirect.com/science/article/pii/B9780323996082000124)
Abstract: Profitable natural pigment production in the form of pure compounds or concentrates requires high-quality plant sources. Plant breeding is a time-consuming process of crop improvement resulting in new plant cultivars of desired characteristics that are suitable for agricultural production. Demands in the pigment industry for high-quality plant materials must be combined with the needs of farmers for high yield. Progress in cultivar development depends on many factors, including plant reproductive biology, trait heritability, existing genetic variation, agrotechnical practices, and environmental conditions. In this chapter, we discuss these and related aspects of breeding plants for improved sources of natural pigments. We have focused our chapter on three separate case studies of vegetable crops: tomato, beetroot, and carrot, representing the most important industrial sources of the three major pigment classes for carotenoids (lycopene), betalain, and anthocyanin production, respectively. These fruit and root crops differ in their life cycle and reproductive biology, and they exhibit diverse biosynthetic pathways for pigment production. The depth of knowledge on genes and biosynthetic pathways involved in pigment production varies for these crops. In addition, the biennial life cycle of carrot and beetroot makes progress in cultivar development more challenging. Advances in new molecular techniques facilitate conventional breeding for pigment production and new avenues of inquiry have been opened from genome sequencing and related approaches.
Keywords: Anthocyanin; beetroot; Beta vulgaris; betalain; breeding; carrot; cultivation; Daucus carota; lycopene; Solanum lycopersicum; tomato


Full issue PDF,
JACC: Cardiovascular Imaging,
Volume 17, Issue 3,
2024,
Pages I-CXVII,
ISSN 1936-878X,
https://doi.org/10.1016/S1936-878X(24)00057-3.
(https://www.sciencedirect.com/science/article/pii/S1936878X24000573)

Abigail Chiu Mei Lim, Lynnette Hui Xian Ng, Araz Taeihagh,
Biometric data landscape in Southeast Asia: Challenges and opportunities for effective regulation,
Computer Law & Security Review,
Volume 56,
2025,
106095,
ISSN 2212-473X,
https://doi.org/10.1016/j.clsr.2024.106095.
(https://www.sciencedirect.com/science/article/pii/S0267364924001602)
Abstract: Technology evolves at a breakneck pace. As a result, legislatures are often unable to enact laws that can keep pace with technological changes. The dissonance between the state of the law and the state of technology intensifies with respect to biometric data because the purposes of biometric data use evolve, the types of biometric data expand, and its collection, processing and use have shifted from conventional biometric systems to online platforms. This dissonance is exemplified in the Association of Southeast Asian Nations, where no regional legal instrument regulates biometric data even though governmental agencies, private entities and social media platforms actively employ biometric data and artificial intelligence systems. At national level, only five countries, Malaysia, Singapore, Indonesia, Thailand, and the Philippines, have enacted omnibus data protection legislations that afford some protection to biometric data and govern its use. This article analyses these data protection legislations and assesses their suitability in protecting and governing biometric data in the contemporary era. It identifies common trends amongst the five countries and concludes that more needs to be done to protect biometric data and rights of data subjects. Thereafter, it makes recommendations for changes to improve the state of biometric regulation in Southeast Asia.
Keywords: Biometrics; Biometrics regulation; ASEAN; Data protection; Southeast Asia

Javaneh Ramezani, Luis M. Camarinha-Matos,
Approaches for resilience and antifragility in collaborative business ecosystems,
Technological Forecasting and Social Change,
Volume 151,
2020,
119846,
ISSN 0040-1625,
https://doi.org/10.1016/j.techfore.2019.119846.
(https://www.sciencedirect.com/science/article/pii/S0040162519304494)
Abstract: Contemporary business ecosystems are continuously challenged by unexpected disruptive events, which are increasing in their frequency and effects. A critical question is why do some organizations collapse in face of extreme events, while others not? On the other hand, current engineering and socio-technical systems were designed to operate in “mostly stable” situations; sporadic instability and disturbances are at best captured by exception handling mechanisms, focusing on reliability and robustness. Recent and more ambitious design goals, however, aim at building systems that are expected to cope with severe disruptions, and survive or even thrive in a context of volatility and uncertainty. This led to an increasing attention to the concepts of resilience and antifragility. As such, this article introduces the findings of a comprehensive literature survey aimed at shedding light on emerging concepts and approaches to handle disruptions in business ecosystems. Main contributions include a clarification of related concepts, identification and classification of disruption sources and drivers, and extensive lists of strategies and underlying capabilities to cope with disruptions. Related perspectives and approaches developed in multiple knowledge areas are also analysed and synthesized. Finally, a collection of engineered systems implementing promising approaches to increase resilience and antifragility are presented.
Keywords: Business ecosystem; Collaboration; Disruptions; Resilience; Antifragility

Simeng Yao, Xunhui Zhang, Yang Zhang, Tao Wang,
Open source oriented cross-platform survey,
Information and Software Technology,
2025,
107704,
ISSN 0950-5849,
https://doi.org/10.1016/j.infsof.2025.107704.
(https://www.sciencedirect.com/science/article/pii/S0950584925000436)
Abstract: Context:
Open-source software development has become a widely adopted approach to software creation. However, developers’ activities extend beyond social coding platforms (e.g., GitHub), encompassing social Q&A platforms (e.g., StackOverflow) and social media platforms (e.g., Twitter). Therefore, cross-platform research is essential for a deeper understanding of the nature of software development activities.
Objective:
This paper focuses on open-source platforms and systematically summarizes relevant cross-platform research. It aims to assess the current state of cross-platform research and provide insights into the challenges and future developments in this field.
Method:
This paper reviews 69 cross-platform research papers related to open-source software from 2013 to 2024, with a focus on several key areas, including platform interconnections, research themes, experimental design methods, challenges and rersearch opportunities.
Results:
Through the analysis of 69 papers, we found that cross-platform research primarily involves platforms such as social coding, social Q&A, and social media. Researchers typically rely on information traces, including user personal info, technical info, project/post/bug report metadata, interaction info, to facilitate connections between platforms. Cross-platform research in the open-source domain mainly focuses on problem classification and feature extraction. The predominant research methods include data-driven approaches, qualitative studies, modeling and machine learning, and tool development and implementation. Despite these advancements, common challenges remain, such as subjective evaluation bias in manual data classification, insufficient data source coverage, and inaccurate data recognition. Future research opportunities may focus on increasing the diversity of data sources, improving data recognition accuracy, optimizing data classification methods, and clarifying user skill requirements.
Conclusions:
Based on our findings, we propose six future directions for cross-platform research in the open-source domain and provide corresponding recommendations for developers, researchers, and service/tool providers.
Keywords: Open source; Cross-platform; Systematic literature review; GitHub; StackOverflow; Twitter

Carlos Castillo, Jorge López-Moreno, Carlos Aliaga,
Recent advances in fabric appearance reproduction,
Computers & Graphics,
Volume 84,
2019,
Pages 103-121,
ISSN 0097-8493,
https://doi.org/10.1016/j.cag.2019.07.007.
(https://www.sciencedirect.com/science/article/pii/S0097849319301256)
Abstract: Reproducing the appearance of real world materials has been a long standing problem in computer graphics. Among them, fibrous materials such as cloth, remain as some of the most challenging to recreate. This is due to the intrinsic complexity of fabrics; their overall look is determined by both the anisotropic light scattering behavior exhibited at the fiber level, usually at the micron scale, and the weaving structure that constrains the alignment of those fibers. Despite the increasing research efforts in the different areas involved, from capturing to modeling, rendering and filtering, there is no single survey nowadays that collects and discusses the benefits, drawbacks and practical considerations of the available techniques that aim to reproduce the appearance of fabrics. In this review, we provide a comprehensive survey of the existing techniques involved at each of the different stages of fabric appearance reproduction. We aim to provide guidelines for practitioners to select among existing options in crucial aspects such as scattering models or fabric representations depending on each particular context, also discussing future lines of research and most promising paths in the direction of accurately representing virtual fabrics.
Keywords: Fiber scattering; Fabric modeling; Appearance matching; Fabric rendering; Virtual fabrics

Shivdutt Sharma,
Distance distributions and runtime analysis of perceptual hashing algorithms,
Journal of Visual Communication and Image Representation,
Volume 104,
2024,
104310,
ISSN 1047-3203,
https://doi.org/10.1016/j.jvcir.2024.104310.
(https://www.sciencedirect.com/science/article/pii/S1047320324002669)
Abstract: Perceptual image hashing refers to a class of algorithms that produce content-based image hashes. These systems use specialized perceptual hash algorithms like Phash, Microsoft’s PhotoDNA, or Facebook’s PDQ to generate a compact digest of an image file that can be roughly compared to a database of known illicit-content digests. Time taken by perceptual hashing algorithms to generate hash code has been computed. Then, we evaluated perceptual hashing algorithms on two million dataset of images. The produced nine variants of the original images were computed and then several distances were calculated. There have been several studies in the past, but in the existing literature size of the data is small and there are very few studies with hash code computation time and robustness tradeoff. This work shows that existing perceptual hashing algorithms are robust for most of the content-preserving operations and there is a tradeoff between computation time and robustness.
Keywords: Perceptual hashing; Distance distributions; Image similarity

Bahman Zohuri, Farhang Mossavar-Rahmani, Farahnaz Behgounia,
Chapter 27 - Artificial intelligence, machine learning, and deep learning driving big data,
Editor(s): Bahman Zohuri, Farhang Mossavar-Rahmani, Farahnaz Behgounia,
Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm,
Academic Press,
2022,
Pages 837-888,
ISBN 9780323951128,
https://doi.org/10.1016/B978-0-323-95112-8.00027-1.
(https://www.sciencedirect.com/science/article/pii/B9780323951128000271)
Abstract: Artificial Intelligence (AI) is one of those technologies that seem to be expanding outward in every direction, and this expansion is driven by sheer volume of data that we are encountering in our daily routine life in these days, where technology is deviating from its tradition format to the more modern world of electronic gadget. In today's modern technological world, we are accumulating so much data in real time with speed of electron through the world of Internet of Things (IoT), and it comes to us in Omni-Direction space. Dealing with these data in the form of structured and unstructured, we have no choice except to turn to AI for assistant, in order to stay on top of all the information that is collected from these data for us to have a knowledge that allows us to consequently have power of right and accurate decision-making in real time in our daily routine as stakeholder. However, comes with AI needs its two other integrated components, namely Machine Learning (ML) and Deep Learning (DL) to be more effective to us as human being, where we are relying on Artificial Intelligence to run our day-to-day operation in very resilience form.
Keywords: Artificial intelligence; Deep learning; Internet of things; Machine learning

Muhammad Raies Abdullah, Zhen Peng, Vignesh babu Rajendren, Farooq Ahmad, Syed Sohail Ahmed Shah, Abdul Wasy Zia, Amjad Ali, Guanjun Qiao, Khurram Shehzad,
Comprehensive review of 3D/4D printing of soft materials, methods and applications,
Applied Materials Today,
Volume 43,
2025,
102667,
ISSN 2352-9407,
https://doi.org/10.1016/j.apmt.2025.102667.
(https://www.sciencedirect.com/science/article/pii/S2352940725000861)
Abstract: This article extensively reviews the transforming landscape of 3D and 4D printing of soft materials, their printing methods, and their applications in wider engineering and technological sectors. Soft materials are known for their suppleness and plasticity and have a wider scope in rapidly growing additive manufacturing sector. The contents of this article are appropriately designed to provide detailed subject knowledge to young or new researchers and also critically analysing several enabling strategies on manufacturing process, material design and formulation, and multifunctional application for subject experts. This article reviews the high potential 3D/4D printing methods for materials such as extrusion-based printing, inkjet-based printing, stereolithography, selective laser sintering, direct ink writing, vat photopolymerization, and comparison of techniques and compares their scope and limitations by process and materials. Further, the article reviews the printability of emerging soft materials such as elastomers, hydrogels, biopolymers, conductive and flexible materials, and biomimetic materials. The application of 3D/4D printed soft materials in wider engineering sectors are discussed such as biomedical and healthcare applications, soft robotics, sensors, and actuators, wearable devices and smart textiles, food technology, and pharmaceutical products. The later sections describe design considerations, challenges, and limitations of 3D/4D printing of soft materials in terms of geometric complexity, support structures, printing resolution, accuracy, cost, and fabrication time, material compatibility, scalability, and safety issues. The article concludes by summarizing advancements in printing technologies, material innovations and developments and reflecting on future challenges such as integration of 3D/4D printing of soft materials with other technologies and compounds and their regulatory and ethical considerations.
Keywords: 3D/4D Printing; Smart Materials; Bioprinting; Soft materials; Hydrogels; Stimuli-Responsive Materials

Riordan Alfredo, Vanessa Echeverria, Yueqiao Jin, Lixiang Yan, Zachari Swiecki, Dragan Gašević, Roberto Martinez-Maldonado,
Human-centred learning analytics and AI in education: A systematic literature review,
Computers and Education: Artificial Intelligence,
Volume 6,
2024,
100215,
ISSN 2666-920X,
https://doi.org/10.1016/j.caeai.2024.100215.
(https://www.sciencedirect.com/science/article/pii/S2666920X2400016X)
Abstract: The rapid expansion of Learning Analytics (LA) and Artificial Intelligence in Education (AIED) offers new scalable, data-intensive systems but raises concerns about data privacy and agency. Excluding stakeholders—like students and teachers—from the design process can potentially lead to mistrust and inadequately aligned tools. Despite a shift towards human-centred design in recent LA and AIED research, there remain gaps in our understanding of the importance of human control, safety, reliability, and trustworthiness in the design and implementation of these systems. We conducted a systematic literature review to explore these concerns and gaps. We analysed 108 papers to provide insights about i) the current state of human-centred LA/AIED research; ii) the extent to which educational stakeholders have contributed to the design process of human-centred LA/AIED systems; iii) the current balance between human control and computer automation of such systems; and iv) the extent to which safety, reliability and trustworthiness have been considered in the literature. Results indicate some consideration of human control in LA/AIED system design, but limited end-user involvement in actual design. Based on these findings, we recommend: 1) carefully balancing stakeholders' involvement in designing and deploying LA/AIED systems throughout all design phases 2) actively involving target end-users, especially students, to delineate the balance between human control and automation, and 3) exploring safety, reliability, and trustworthiness as principles in future human-centred LA/AIED systems.
Keywords: Human-centered AI; Human-centered learning analytics; AI in education; Stakeholders involvement; Education technology; Ethical considerations

Anuj Sharma, Nripendra P. Rana, Robin Nunkoo,
Fifty years of information management research: A conceptual structure analysis using structural topic modeling,
International Journal of Information Management,
Volume 58,
2021,
102316,
ISSN 0268-4012,
https://doi.org/10.1016/j.ijinfomgt.2021.102316.
(https://www.sciencedirect.com/science/article/pii/S0268401221000098)
Abstract: Information management is the management of organizational processes, technologies, and people which collectively create, acquire, integrate, organize, process, store, disseminate, access, and dispose of the information. Information management is a vast, multi-disciplinary domain that syndicates various subdomains and perfectly intermingles with other domains. This study aims to provide a comprehensive overview of the information management domain from 1970 to 2019. Drawing upon the methodology from statistical text analysis research, this study summarizes the evolution of knowledge in this domain by examining the publication trends as per authors, institutions, countries, etc. Further, this study proposes a probabilistic generative model based on structural topic modeling to understand and extract the latent themes from the research articles related to information management. Furthermore, this study graphically visualizes the variations in the topic prevalences over the period of 1970 to 2019. The results highlight that the most common themes are data management, knowledge management, environmental management, project management, service management, and mobile and web management. The findings also identify themes such as knowledge management, environmental management, project management, and social communication as academic hotspots for future research.
Keywords: Information management; Structural topic models; Topic modeling; Generative models; Text analytics

Weng Marc Lim, Satish Kumar, Naveen Donthu,
How to combine and clean bibliometric data and use bibliometric tools synergistically: Guidelines using metaverse research,
Journal of Business Research,
Volume 182,
2024,
114760,
ISSN 0148-2963,
https://doi.org/10.1016/j.jbusres.2024.114760.
(https://www.sciencedirect.com/science/article/pii/S0148296324002649)
Abstract: Bibliometrics (or scientometrics) is a powerful technique to assess the trajectory of scientific research. Building on the Journal of Business Research’s seminal guides for bibliometric analysis—i.e., “How to conduct a bibliometric analysis: An overview and guidelines” and “Guidelines for advancing theory and practice through bibliometric research”—and using metaverse research as a case, this article presents in-depth procedural guidelines for (i) combing and cleaning bibliometric data from multiple databases (Scopus and Web of Science) and (ii) conducting bibliometric analysis using multiple tools (bibliometrix and VOSviewer). Besides serving as a guide to harness the potential of bibliometrics for insightful assessments of scientific research, this article provides noteworthy insights into various features of the metaverse. This includes an examination of decentralized systems and the integration of digital assets, alongside innovations, the influence of industrial revolutions, and ethical and sustainable development. The dynamics of digital identity, ownership, and business models are explored in tandem with engagement strategies and multi-disciplinary perspectives of the metaverse. This comprehensive analysis also addresses metaverse challenges, market behaviors, and marketing strategies. Collectively, these insights offer a robust foundation for scholars, practitioners, and policymakers to shape the future of the metaverse with clarity, purpose, and impact.
Keywords: Bibliometrics; Bibliometric analysis; Scientometrics; Scientometric analysis; Performance analysis; Science mapping; Bibliographic coupling; Co-occurrence analysis; Co-citation analysis; Trend analysis; Bibliometrix; Biblioshiny; R; RStudio; VOSviewer; Scopus; Web of Science; Metaverse

Solene Bechelli, Jerome Delhommelle,
AI's role in pharmaceuticals: Assisting drug design from protein interactions to drug development,
Artificial Intelligence Chemistry,
Volume 2, Issue 1,
2024,
100038,
ISSN 2949-7477,
https://doi.org/10.1016/j.aichem.2023.100038.
(https://www.sciencedirect.com/science/article/pii/S2949747723000386)
Abstract: Developing new pharmaceutical compounds is a lengthy, costly, and intensive process. In recent years, the development of Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) models has drawn considerable interest in drug discovery. In this review, we discuss recent advances in the field and show how these methods can be leveraged to assist each stage of the drug discovery process. After discussing recent technical progress in the encoding of chemical information via fingerprinting and the emergence of graph-based and generative models, we examine all types of interactions, including drug-target interactions, protein-protein interactions, protein-peptide interactions, and nucleic acid-based interactions. Furthermore, we discuss recent advances enabled by DL models for the prediction of ADMET (Absorption, Distribution, Metabolism, Elimination, Toxicity) properties and of solubility. We also review applications that have emerged in the past two years with the development of models, for instance, on SARS-CoV-2 inhibitors and highlight outstanding challenges.
Keywords: Artificial intelligence; Property prediction; Molecular docking; Drug discovery; Deep learning


16th Clinical Trials on Alzheimer's Disease (CTAD) Boston, MA (USA) October 24–27, 2023: Posters,
The Journal of Prevention of Alzheimer's Disease,
Volume 10, Supplement 1,
2023,
Pages 56-240,
ISSN 2274-5807,
https://doi.org/10.14283/jpad.2022.130.
(https://www.sciencedirect.com/science/article/pii/S2274580724001365)

Peter R. Wiecha,
Deep learning for nano-photonic materials – The solution to everything!?,
Current Opinion in Solid State and Materials Science,
Volume 28,
2024,
101129,
ISSN 1359-0286,
https://doi.org/10.1016/j.cossms.2023.101129.
(https://www.sciencedirect.com/science/article/pii/S1359028623000748)
Abstract: Deep learning is currently being hyped as an almost magical tool for solving all kinds of difficult problems that computers have not been able to solve in the past. Particularly in the fields of computer vision and natural language processing, spectacular results have been achieved. The hype has now infiltrated several scientific communities. In (nano-) photonics, researchers are trying to apply deep learning to all kinds of forward and inverse problems. A particularly challenging problem is for instance the rational design of nanophotonic materials and devices. In this opinion article, I will first discuss the public expectations of deep learning and give an overview of the quite different scales at which actors from industry and research are operating their deep learning models. I then examine the weaknesses and dangers associated with deep learning. Finally, I’ll discuss the key strengths that make this new set of statistical methods so attractive, and review a personal selection of opportunities that shouldn’t be missed in the current developments.

Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh, Michel C. Desmarais, Zhen Ming (Jack) Jiang,
GitHub Copilot AI pair programmer: Asset or Liability?,
Journal of Systems and Software,
Volume 203,
2023,
111734,
ISSN 0164-1212,
https://doi.org/10.1016/j.jss.2023.111734.
(https://www.sciencedirect.com/science/article/pii/S0164121223001292)
Abstract: Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by OpenAI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot’s proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of humans’ solutions is greater than Copilot’s suggestions, while the buggy solutions generated by Copilot require less effort to be repaired. Based on our findings, if Copilot is used by expert developers in software projects, it can become an asset since its suggestions could be comparable to humans’ contributions in terms of quality. However, Copilot can become a liability if it is used by novice developers who may fail to filter its buggy or non-optimal solutions due to a lack of expertise.
Keywords: Code completion; Language model; GitHub copilot; Testing

Beatriz Kira,
When non-consensual intimate deepfakes go viral: The insufficiency of the UK Online Safety Act,
Computer Law & Security Review,
Volume 54,
2024,
106024,
ISSN 2212-473X,
https://doi.org/10.1016/j.clsr.2024.106024.
(https://www.sciencedirect.com/science/article/pii/S0267364924000906)
Abstract: Advancements in artificial intelligence (AI) have drastically simplified the creation of synthetic media. While concerns often focus on potential misinformation harms, ‘non-consensual intimate deepfakes’ (NCID) – a form of image-based sexual abuse – pose a current, severe, and growing threat, disproportionately impacting women and girls. This article examines the measures implemented with the recently adopted Online Safety Act 2023 (OSA) and argues that the new criminal offences and the ‘systems and processes’ approach the law adopts are insufficient to counter NCID in the UK. This is because the OSA relies on platform policies that often lack consistency regarding synthetic media and on platforms’ content removal mechanisms which offer limited redress to victim-survivors after the harm has already occurred. The article argues that stronger prevention mechanisms are necessary and proposes that the law should mandate all AI-powered deepfake creation tools to ban the generation of intimate synthetic content and require the implementation of comprehensive and enforceable content moderation systems.
Keywords: Deepfakes; Synthetic media; Image-based sexual abuse; Content moderation; AI regulation; Online Safety Act; Social media platforms; United Kingdom

Asfak Ali, Ram Sarkar, Sheli Sinha Chaudhuri,
Wavelet-based Auto-Encoder for simultaneous haze and rain removal from images,
Pattern Recognition,
Volume 150,
2024,
110370,
ISSN 0031-3203,
https://doi.org/10.1016/j.patcog.2024.110370.
(https://www.sciencedirect.com/science/article/pii/S0031320324001213)
Abstract: Noise introduced due to weather can reduce the efficiency of computer vision applications as the visibility of the objects in images is greatly affected. Haze and rain are the most common weather conditions seen in nature. However, most of the algorithms found in the literature apply rain and haze removal approaches separately. To this end, in this paper, we propose a novel Wavelet-based deep Auto-encoder, called WAE, for simultaneously removing the haze and rain effects from images. The proposed network uses wavelet transformation and inverse wavelet transformation as an alternative to down-sampling and up-sampling operations, respectively, in order to add sparsity to the network. By training the model on both spatial and frequency domains, it learns non-stationary features that are found to be useful to remove haze and rain effects from images. The proposed model is tested on several rain and haze-affected image datasets, and it performs well in terms of standard evaluation metrics like structural similarity index measure and peak signal-to-noise ratio. The code can be found at : https://github.com/asfakali/WAE.git.
Keywords: Dehaze; Derain; Auto encoder; U-Net; Wavelet; Noise removal

Hong Zhang, Sumeet Gupta, Wei Sun, Yi Zou,
How social-media-enabled co-creation between customers and the firm drives business value? The perspective of organizational learning and social Capital,
Information & Management,
Volume 57, Issue 3,
2020,
103200,
ISSN 0378-7206,
https://doi.org/10.1016/j.im.2019.103200.
(https://www.sciencedirect.com/science/article/pii/S0378720618305901)
Abstract: Contemporary business organizations are increasingly turning their attention to value co-creation using social media between individual customers and business organizations in the process of new product development (NPD). However, little is known about the mechanisms underlying social-media-based customer-firm co-creation and their implications for business value in NPD. To address this knowledge gap, this study develops a model from the perspective of organizational learning and social capital to examine how the social-media-based customer-firm co-creation mechanism conceptualized as the structural, cognitive, and relational dimension of social capital influences the first-order knowledge outcome (knowledge transfer effectiveness) and second-order dynamic capability outcome (absorptive capacity), and how these co-creation outcomes ultimately influence organizational performance. The model is tested using survey data from 149 Chinese mobile application developers. The results indicate that social-media-based structural, cognitive, and relational linkage, in particular the structural linkage, is an important co-creation mechanism to improve organizational performance. Knowledge transfer effectiveness and absorptive capacity have significant mediating effects in this co-creation mechanism-outcomes-performance framework. Further, the moderating effects of social media use level on the relationships between co-creation mechanism and outcomes are largely supported. The study contributes to theory and practice by shedding light on the social-media-based customer-firm co-creation in NPD at a process level.
Keywords: Co-creation; Social media; Organizational learning; Social capital; Business value; Organizational performance

Reza Forghani, Peter Savadjiev, Avishek Chatterjee, Nikesh Muthukrishnan, Caroline Reinhold, Behzad Forghani,
Radiomics and Artificial Intelligence for Biomarker and Prediction Model Development in Oncology,
Computational and Structural Biotechnology Journal,
Volume 17,
2019,
Pages 995-1008,
ISSN 2001-0370,
https://doi.org/10.1016/j.csbj.2019.07.001.
(https://www.sciencedirect.com/science/article/pii/S2001037019301382)
Keywords: Artificial intelligence; Texture analysis; Radiomics; Machine learning; Precision oncology; Biomarker

Seokhyun Choung, Wongyu Park, Jinuk Moon, Jeong Woo Han,
Rise of machine learning potentials in heterogeneous catalysis: Developments, applications, and prospects,
Chemical Engineering Journal,
Volume 494,
2024,
152757,
ISSN 1385-8947,
https://doi.org/10.1016/j.cej.2024.152757.
(https://www.sciencedirect.com/science/article/pii/S138589472404244X)
Abstract: The urgency of tackling climate change is driving a global shift towards renewable sources of energy, with a growing contribution from alternative energy sources such as solar, wind and hydroelectric power. With the global push for the sustainable energy, the demand for effective catalysts for sustainable chemical production and energy storage has been rapidly increasing. Computational simulations have contributed to the rational design of catalysts by allowing profound analysis of catalyst properties. Machine learning potential (MLP) has emerged as a potential tool to bridge the gap between quantum mechanical accuracy and computational efficiency, overcoming the computational cost limitations of quantum chemistry-based simulations. This review discusses the development and application of MLP in multiscale simulations of heterogeneous catalysis. It covers the basic concepts of computational catalysis, the construction of MLP focusing on efficient datasets, atomic structure representations, and the process of training and evaluating of ML models. Furthermore, the potential applications of MLP are discussed in addressing computational challenges within the field, as MLP has potential to overcome limitations in simulation time and length scale. Lastly, the prospects for MLP are presented, taking advantage of the rapid advancements in artificial intelligence architectures. It is expected that the integration of MLP will accelerate progress within the catalyst research community and will bridge the gap between theoretical and experimental approaches in catalytic research.
Keywords: Catalysis; Energy materials; Quantum calculations; Density functional theory; Molecular dynamics; Machine learning potential; Machine learning; Artificial intelligence

Jingyuan Zhao, Xuebing Han, Yuyan Wu, Zhenghong Wang, Andrew F. Burke,
Opportunities and challenges in transformer neural networks for battery state estimation: Charge, health, lifetime, and safety,
Journal of Energy Chemistry,
Volume 102,
2025,
Pages 463-496,
ISSN 2095-4956,
https://doi.org/10.1016/j.jechem.2024.11.011.
(https://www.sciencedirect.com/science/article/pii/S209549562400771X)
Abstract: Battery technology plays a crucial role across various sectors, powering devices from smartphones to electric vehicles and supporting grid-scale energy storage. To ensure their safety and efficiency, batteries must be evaluated under diverse operating conditions. Traditional modeling techniques, which often rely on first principles and atomic-level calculations, struggle with practical applications due to incomplete or noisy data. Furthermore, the complexity of battery dynamics, shaped by physical, chemical, and electrochemical interactions, presents substantial challenges for precise and efficient modeling. The Transformer model, originally designed for natural language processing, has proven effective in time-series analysis and forecasting. It adeptly handles the extensive, complex datasets produced during battery cycles, efficiently filtering out noise and identifying critical features without extensive preprocessing. This capability positions Transformers as potent tools for tackling the intricacies of battery data. This review explores the application of customized Transformers in battery state estimation, emphasizing crucial aspects such as charging, health assessment, lifetime prediction, and safety monitoring. It highlights the distinct advantages of Transformer-based models and addresses ongoing challenges and future opportunities in the field. By combining data-driven AI techniques with empirical insights from battery analysis, these pre-trained models can deliver precise diagnostics and comprehensive monitoring, enhancing performance metrics like health monitoring, anomaly detection, and early-warning systems. This integrated approach promises significant improvements in battery technology management and application.
Keywords: Transformer; Battery; Health; Lifetime; Safety; SOC; SOH; RUL; Deep learning; Artificial general intelligence

Xueye Yan, Peng Peng, Yuting Liu,
Optimal design feature of computer-assisted reading instruction for students with reading difficulties? A Bayesian network meta-analysis,
Computers in Human Behavior,
Volume 152,
2024,
108062,
ISSN 0747-5632,
https://doi.org/10.1016/j.chb.2023.108062.
(https://www.sciencedirect.com/science/article/pii/S0747563223004132)
Abstract: Mayer (2017, 2020) proposed three major design features of computer-assisted instructions (CAI) within the Cognitive Theory of Multimedia Learning: reducing extraneous processing (i.e., excluding irrelevant content), managing essential processing (i.e., focusing on the complex but essential learning materials), and fostering generative processing (i.e., maximizing learner motivation with multimedia features). No study so far has systematically evaluated each design feature, or their combinations for students with reading difficulties. The present study is the first meta-analysis to investigate the optimal design features of CAIs for students with reading difficulties on their decoding/word reading and reading comprehension performance. A total of 49 experimental studies were reviewed with Bayesian network meta-analysis. Results showed that CAI programs with features that reduced extraneous processing were the most effective (g = 0.65) followed by programs that combined reducing extraneous processing, managing essential processing, and fostering generative processing (g = 0.29) as well as programs that combined reducing extraneous processing and managing essential processing (g = 0.27). CAI programs yielded larger effects when they were designed for younger learners with reading difficulties compared to older learners. No significant moderation effects were observed for students' reading difficulty status, reading content, reading outcomes, instruction dosages, control group types, measures, and fidelity checks. These findings suggest that different combinations of design features of CAI programs may generate different effects. Lowering students’ cognitive loads by excluding irrelevant content may be the foundation for designing effective computer instructions for students with reading difficulties.
Keywords: Computer-assisted instruction; Design feature; Reading difficulty; Bayesian network meta-analysis

Sarah A. Chauncey, H. Patricia McKenna,
A framework and exemplars for ethical and responsible use of AI Chatbot technology to support teaching and learning,
Computers and Education: Artificial Intelligence,
Volume 5,
2023,
100182,
ISSN 2666-920X,
https://doi.org/10.1016/j.caeai.2023.100182.
(https://www.sciencedirect.com/science/article/pii/S2666920X23000619)
Abstract: The aim of this paper is to investigate the ethical and responsible use of AI chatbots in education in support of critical thinking, cognitive flexibility and self-regulation in terms of their potential to enhance and motivate teaching and learning in contemporary education environments. AI chatbots such as ChatGPT by OpenAI appear to be improving in conversational and other capabilities and this paper explores such advances using version 4. Based on a review of the research literature, a conceptual framework is formulated for responsible use of AI chatbots in education supporting cognitive flexibility in AI-rich learning environments. The framework is then operationalized for use in this paper through the development of exemplars for math, english language arts (ELA), and studying with ChatGPT to close learning gaps in an effort to foster more ethical and responsible approaches to the design and development of AI chatbots for application and use in teaching and learning environments. This paper extends earlier foundational work on cognitive flexibility and AI chatbots as well as work on cognitive flexibility in support of creativity and innovation with AI chatbots in urban civic spaces.
Keywords: AI ethics; AI responsibility; AI-Rich learning environments; Cognitive flexibility; Critical thinking; Self-regulation

Shannon Harger Payen, Kayla Andrada, Evelyn Tara, Juli Petereit, Subhash C. Verma, Cyprian C. Rossetto,
The cellular paraspeckle component SFPQ associates with the viral processivity factor ORF59 during lytic replication of Kaposi's Sarcoma-associated herpesvirus (KSHV),
Virus Research,
Volume 349,
2024,
199456,
ISSN 0168-1702,
https://doi.org/10.1016/j.virusres.2024.199456.
(https://www.sciencedirect.com/science/article/pii/S0168170224001497)
Abstract: Kaposi's sarcoma-associated herpesvirus (KSHV) relies on many cellular proteins to complete replication and generate new virions. Paraspeckle nuclear bodies consisting of core ribonucleoproteins splicing factor proline/glutamine-rich (SFPQ), Non-POU domain-containing octamer-binding protein (NONO), and paraspeckle protein component 1 (PSPC1) along with the long non-coding RNA NEAT1, form a complex that has been speculated to play an important role in viral replication. Paraspeckle bodies are multifunctional and involved in various processes including gene expression, mRNA splicing, and anti-viral defenses. To better understand the role of SFPQ during KSHV replication, we performed SFPQ immunoprecipitation followed by mass spectrometry from KSHV-infected cells. Proteomic analysis showed that during lytic reactivation, SFPQ associates with viral proteins, including ORF10, ORF59, and ORF61. These results are consistent with a previously reported ORF59 proteomics assay identifying SFPQ. To test if the association between ORF59 and SFPQ is important for replication, we first identified the region of ORF59 that associates with SFPQ using a series of 50 amino acid deletion mutants of ORF59 in the KSHV BACmid system. By performing co-immunoprecipitations, we identified the region spanning amino acids 101–150 of ORF59 as the association domain with SFPQ. Using this information, we generated a dominant negative polypeptide of ORF59 encompassing amino acids 101–150, that disrupted the association between SFPQ and full-length ORF59, and decreased virus production. Interestingly, when we tested other human herpesvirus processivity factors (EBV BMRF1, HSV-1 UL42, and HCMV UL44) by transfection of each expression plasmid followed by co-immunoprecipitation, we found a conserved association with SFPQ. These are limited studies that remain to be done in the context of infection but suggest a potential association of SFPQ with processivity factors across multiple herpesviruses.
Keywords: Kaposi's sarcoma-associated herpesvirus (KSHV); Splicing factor proline and glutamine rich (SFPQ); Non-POU domain-containing octamer-binding protein (NONO); Paraspeckles, DNA polymerase processivity factor

Qianling Jiang, Yuzhuo Zhang, Wei Wei, Chao Gu,
Evaluating technological and instructional factors influencing the acceptance of AIGC-assisted design courses,
Computers and Education: Artificial Intelligence,
Volume 7,
2024,
100287,
ISSN 2666-920X,
https://doi.org/10.1016/j.caeai.2024.100287.
(https://www.sciencedirect.com/science/article/pii/S2666920X24000900)
Abstract: Purpose
This study aims to explore the key factors influencing design students' acceptance of AIGC-assisted design courses, providing specific strategies for course design to help students better learn this new technology and enhance their competitiveness in the design industry. The research focuses on evaluating technological and course-level factors, providing actionable insights for course developers.
Design/methodology/approach
The research establishes and validates evaluation dimensions and indicators affecting acceptance using structured questionnaires to collect data and employs factor analysis and weight analysis to determine the importance of each factor.
Findings
The results of the study reveal that the main dimensions influencing student acceptance include technology application and innovation, teaching content and methods, and extracurricular learning support and resources. Regarding indicators, data privacy, timeliness of extracurricular learning support, and availability of extracurricular learning resources are identified as the most critical factors.
Originality
The uniqueness of this study lies in providing specific course design strategies for AIGC-assisted design courses based on the weight analysis results for different dimensions and indicators. These strategies aim to help students better adapt to these courses and enhance their acceptance. Furthermore, the conclusions and recommendations of this study offer valuable insights for educational institutions and instructors, promoting further optimization and development of AIGC-assisted design courses.
Keywords: Artificial intelligence assisted design; Course acceptance; Factor analysis; Entropy method

Junghwan Yun, Youngjung Geum,
Automated classification of patents: A topic modeling approach,
Computers & Industrial Engineering,
Volume 147,
2020,
106636,
ISSN 0360-8352,
https://doi.org/10.1016/j.cie.2020.106636.
(https://www.sciencedirect.com/science/article/pii/S0360835220303703)
Abstract: Due to the rapid increase in technological innovation and corresponding increase in patent applications, automatic patent classification systems are very helpful for both individual inventors and patent attorneys in classifying patents. However, previous studies have neglected the question of what content patents include and how to represent patent content effectively in a structured form to predict the patent class. In response, this study suggests a topic model based on support vector machine (SVM) prediction for automatic patent classification. This study considers two important issues for patent classification: text representation and class prediction. For text representation, we use the topic modeling technique and employ latent Dirichlet allocation (LDA). The result of LDA is then used as the input for the second aspect: class prediction. We use SVM prediction for automatic patent classification. We also suggest potential improvement strategies to enhance the prediction performance of our suggested approach. This study contributes to the field in that it can lead to the automatic classification of patents without the need for any expert judgment during the process.
Keywords: Automatic patent classification; Latent Dirichlet allocation; LDA; Support vector machine; SVM

Yuri Alexeev, Maximilian Amsler, Marco Antonio Barroca, Sanzio Bassini, Torey Battelle, Daan Camps, David Casanova, Young Jay Choi, Frederic T. Chong, Charles Chung, Christopher Codella, Antonio D. Córcoles, James Cruise, Alberto Di Meglio, Ivan Duran, Thomas Eckl, Sophia Economou, Stephan Eidenbenz, Bruce Elmegreen, Clyde Fare, Ismael Faro, Cristina Sanz Fernández, Rodrigo Neumann Barros Ferreira, Keisuke Fuji, Bryce Fuller, Laura Gagliardi, Giulia Galli, Jennifer R. Glick, Isacco Gobbi, Pranav Gokhale, Salvador de la Puente Gonzalez, Johannes Greiner, Bill Gropp, Michele Grossi, Emanuel Gull, Burns Healy, Matthew R. Hermes, Benchen Huang, Travis S. Humble, Nobuyasu Ito, Artur F. Izmaylov, Ali Javadi-Abhari, Douglas Jennewein, Shantenu Jha, Liang Jiang, Barbara Jones, Wibe Albert de Jong, Petar Jurcevic, William Kirby, Stefan Kister, Masahiro Kitagawa, Joel Klassen, Katherine Klymko, Kwangwon Koh, Masaaki Kondo, Dog̃a Murat Kürkçüog̃lu, Krzysztof Kurowski, Teodoro Laino, Ryan Landfield, Matt Leininger, Vicente Leyton-Ortega, Ang Li, Meifeng Lin, Junyu Liu, Nicolas Lorente, Andre Luckow, Simon Martiel, Francisco Martin-Fernandez, Margaret Martonosi, Claire Marvinney, Arcesio Castaneda Medina, Dirk Merten, Antonio Mezzacapo, Kristel Michielsen, Abhishek Mitra, Tushar Mittal, Kyungsun Moon, Joel Moore, Sarah Mostame, Mario Motta, Young-Hye Na, Yunseong Nam, Prineha Narang, Yu-ya Ohnishi, Daniele Ottaviani, Matthew Otten, Scott Pakin, Vincent R. Pascuzzi, Edwin Pednault, Tomasz Piontek, Jed Pitera, Patrick Rall, Gokul Subramanian Ravi, Niall Robertson, Matteo A.C. Rossi, Piotr Rydlichowski, Hoon Ryu, Georgy Samsonidze, Mitsuhisa Sato, Nishant Saurabh, Vidushi Sharma, Kunal Sharma, Soyoung Shin, George Slessman, Mathias Steiner, Iskandar Sitdikov, In-Saeng Suh, Eric D. Switzer, Wei Tang, Joel Thompson, Synge Todo, Minh C. Tran, Dimitar Trenev, Christian Trott, Huan-Hsin Tseng, Norm M. Tubman, Esin Tureci, David García Valiñas, Sofia Vallecorsa, Christopher Wever, Konrad Wojciechowski, Xiaodi Wu, Shinjae Yoo, Nobuyuki Yoshioka, Victor Wen-zhe Yu, Seiji Yunoki, Sergiy Zhuk, Dmitry Zubarev,
Quantum-centric supercomputing for materials science: A perspective on challenges and future directions,
Future Generation Computer Systems,
Volume 160,
2024,
Pages 666-710,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.04.060.
(https://www.sciencedirect.com/science/article/pii/S0167739X24002012)
Abstract: Computational models are an essential tool for the design, characterization, and discovery of novel materials. Computationally hard tasks in materials science stretch the limits of existing high-performance supercomputing centers, consuming much of their resources for simulation, analysis, and data processing. Quantum computing, on the other hand, is an emerging technology with the potential to accelerate many of the computational tasks needed for materials science. In order to do that, the quantum technology must interact with conventional high-performance computing in several ways: approximate results validation, identification of hard problems, and synergies in quantum-centric supercomputing. In this paper, we provide a perspective on how quantum-centric supercomputing can help address critical computational problems in materials science, the challenges to face in order to solve representative use cases, and new suggested directions.
Keywords: Quantum-centric supercomputing; Quantum computing; Materials science; High-performance computing

Xin Feng, Youni Jiang, Xuejiao Yang, Ming Du, Xin Li,
Computer vision algorithms and hardware implementations: A survey,
Integration,
Volume 69,
2019,
Pages 309-320,
ISSN 0167-9260,
https://doi.org/10.1016/j.vlsi.2019.07.005.
(https://www.sciencedirect.com/science/article/pii/S0167926019301762)
Abstract: The field of computer vision is experiencing a great-leap-forward development today. This paper aims at providing a comprehensive survey of the recent progress on computer vision algorithms and their corresponding hardware implementations. In particular, the prominent achievements in computer vision tasks such as image classification, object detection and image segmentation brought by deep learning techniques are highlighted. On the other hand, review of techniques for implementing and optimizing deep-learning-based computer vision algorithms on GPU, FPGA and other new generations of hardware accelerators are presented to facilitate real-time and/or energy-efficient operations. Finally, several promising directions for future research are presented to motivate further development in the field.
Keywords: Computer vision; Hardware accelerator; Deep convolutional neural network; Artificial intelligence

Brooke Kidmose,
A review of smart vehicles in smart cities: Dangers, impacts, and the threat landscape,
Vehicular Communications,
Volume 51,
2025,
100871,
ISSN 2214-2096,
https://doi.org/10.1016/j.vehcom.2024.100871.
(https://www.sciencedirect.com/science/article/pii/S2214209624001463)
Abstract: The humble, mechanical automobile has gradually evolved into our modern connected and autonomous vehicles (CAVs)—also known as “smart vehicles.” Similarly, our cities are gradually developing into “smart cities,” where municipal services from transportation networks to utilities to recycling to law enforcement are integrated. The idea, with both smart vehicles and smart cities, is that more data leads to better, more informed decisions. Smart vehicles and smart cities would acquire data from their own equipment (e.g., cameras, sensors) and from their connections—e.g., connections to fellow smart vehicles, to road-side infrastructure, to smart transportation systems (STSs), etc. Unfortunately, the paradigm of smart vehicles in smart cities is rife with danger and ripe for misuse. One vulnerable system or service could become an attacker's entry point, facilitating access to every connected vehicle, device, etc. Worse, smart vehicles and smart cities are inherently cyber-physical; a cyberattack can have physical consequences, including destruction of infrastructure and loss of life. Lastly, to leverage all the benefits of smart vehicles in smart cities, we would need to accept exorbitant levels of data collection and surveillance, which, in the absence of ironclad privacy protections, could lead to total lack of privacy. In this work, we define the automotive context—i.e., smart vehicles—within the larger context of smart cities as our threat landscape. Then, we enumerate and describe all of the (1) threats, (2) attack surfaces & targets, (3) areas of concern (indirect vulnerabilities & threats), and (4) impacts of smart vehicles in smart cities. Our objective is to demonstrate that the dangers are real and imminent—in the hope that they will be addressed before an attack on the “smart vehicles in smart cities” paradigm results in loss of life.
Keywords: Automotive; Connected and autonomous vehicle; Smart transportation system; Smart city; Cybersecurity; Threat landscape

Omar Ali, Peter A. Murray, Mujtaba Momin, Yogesh K. Dwivedi, Tegwen Malik,
The effects of artificial intelligence applications in educational settings: Challenges and strategies,
Technological Forecasting and Social Change,
Volume 199,
2024,
123076,
ISSN 0040-1625,
https://doi.org/10.1016/j.techfore.2023.123076.
(https://www.sciencedirect.com/science/article/pii/S0040162523007618)
Abstract: With the continuous intervention of AI tools in the education sector, new research is required to evaluate the viability and feasibility of extant AI platforms to inform various pedagogical methods of instruction. The current manuscript explores the cumulative published literature to date in order to evaluate the key challenges that influence the implications of adopting AI models in the Education Sector. The researchers' present works both in favour and against AI-based applications within the Academic milieu. A total of 69 articles from a 618-article population was selected from diverse academic journals between 2018 and 2023. After a careful review of selected articles, the manuscript presents a classification structure based on five distinct dimensions: user, operational, environmental, technological, and ethical challenges. The current review recommends the use of ChatGPT as a complementary teaching-learning aid including the need to afford customized and optimized versions of the tool for the teaching fraternity. The study addresses an important knowledge gap as to how AI models enhance knowledge within educational settings. For instance, the review discusses interalia a range of AI-related effects on learning from the need for creative prompts, training on diverse datasets and genres, incorporation of human input and data confidentiality and elimination of bias. The study concludes by recommending strategic solutions to the emerging challenges identified while summarizing ways to encourage wider adoption of ChatGPT and other AI tools within the education sector. The insights presented in this review can act as a reference for policymakers, teachers, technology experts and stakeholders, and facilitate the means for wider adoption of ChatGPT in the Education sector more generally. Moreover, the review provides an important foundation for future research.
Keywords: ChatGPT; Artificial intelligence; Challenges; Strategies; Education sector

Wolfgang Maass, Veda C. Storey,
Pairing conceptual modeling with machine learning,
Data & Knowledge Engineering,
Volume 134,
2021,
101909,
ISSN 0169-023X,
https://doi.org/10.1016/j.datak.2021.101909.
(https://www.sciencedirect.com/science/article/pii/S0169023X21000367)
Abstract: Both conceptual modeling and machine learning have long been recognized as important areas of research. With the increasing emphasis on digitizing and processing large amounts of data for business and other applications, it would be helpful to consider how these areas of research can complement each other. To understand how they can be paired, we provide an overview of machine learning foundations and development cycle. We then examine how conceptual modeling can be applied to machine learning and propose a framework for incorporating conceptual modeling into data science projects. The framework is illustrated by applying it to a healthcare application. For the inverse pairing, machine learning can impact conceptual modeling through text and rule mining, as well as knowledge graphs. The pairing of conceptual modeling and machine learning in this way should help lay the foundations for future research.
Keywords: Conceptual modeling; Machine learning; Methodologies and tools; Models; Database management; Framework for incorporating conceptual modeling into data science projects; Artificial intelligence

Gabriele Gattiglia,
Managing Artificial Intelligence in Archeology. An overview,
Journal of Cultural Heritage,
Volume 71,
2025,
Pages 225-233,
ISSN 1296-2074,
https://doi.org/10.1016/j.culher.2024.11.020.
(https://www.sciencedirect.com/science/article/pii/S1296207424002516)
Abstract: The integration of AI in archaeology poses several risks due to the oversimplification of complex archaeological data for computational ease. This reductionist approach fosters a deterministic view, treating provisional classifications as definitive truths and influencing subsequent interpretations. The reliance on legacy data and Big Data for AI training risks perpetuating outdated ideas and frameworks. As AI expands from automating tasks to interpreting and creating reconstructions, archaeologists must adopt a critical approach to avoid biased and harmful outputs. The deterministic view of AI hinders informed debate. Archaeologists should engage in discussions that address the classificatory, and ethical aspects as well as the materiality of AI. The accumulation of data in AI mimics storytelling but lacks the interpretative depth needed to understand historical human perspectives. Developing theories and narrative practices is essential to making archaeological data meaningful. The shift from a representational to a co-creative view of data is necessary to understand its re-use and the power dynamics involved. Finally, to normalise AI in archaeology, a critical and sceptical approach is needed to integrate AI into the real world and understand its implications and ethical considerations.
Keywords: Archaeology; Artificial intelligence; Big Data; Theory; Ethics

Joe Williams,
Assembling the water factory: Seawater desalination and the techno-politics of water privatisation in the San Diego–Tijuana metropolitan region,
Geoforum,
Volume 93,
2018,
Pages 32-39,
ISSN 0016-7185,
https://doi.org/10.1016/j.geoforum.2018.04.022.
(https://www.sciencedirect.com/science/article/pii/S0016718518301313)
Abstract: This paper is about the peculiar particularities of the dual trends towards urban water privatization and commodification. It uses as its analytical entry point the extraordinary emergence of large-scale seawater desalination, delivered through public-private partnerships, as an alternative municipal water supply for the San Diego–Tijuana metropolitan region. The paper engages and extends Karen Bakker’s work on water as an ‘uncooperative commodity’. Interrogating the neoliberalization of water through desalination, it is argued, requires reference to the socio-technical relations drawn together under the ‘desalination assemblage’. Such water treatment technologies –and the social relations that flow through them– are, in other words, efficacious in the market-disciplining of water. The paper presents an understanding of privatization and commodification as diffuse, and as unfolding through multiple and contradictory materially heterogeneous relationships. Drawing on both urban political ecology (UPE) and assemblage thinking, the paper calls for a more constructive dialogue between different concepts of socio-material relationality. The empirical case studies of two large seawater desalination plants (one in Southern California, one in Baja California) and the re-configuring relations of public/private water governance associated with these projects, provides a pertinent imperative for greater attention to be paid to contingency and heterogeneity in our understanding of the ecology of capitalism.
Keywords: Assemblage; Urban political ecology; Infrastructure; Neoliberalism; Water; Desalination

Julian Frattini, Lloyd Montgomery, Davide Fucci, Michael Unterkalmsteiner, Daniel Mendez, Jannik Fischbach,
Requirements quality research artifacts: Recovery, analysis, and management guideline,
Journal of Systems and Software,
Volume 216,
2024,
112120,
ISSN 0164-1212,
https://doi.org/10.1016/j.jss.2024.112120.
(https://www.sciencedirect.com/science/article/pii/S0164121224001651)
Abstract: Requirements quality research, which is dedicated to assessing and improving the quality of requirements specifications, is dependent on research artifacts like data sets (containing information about quality defects) and implementations (automatically detecting and removing these defects). However, recent research exposed that the majority of these research artifacts have become unavailable or have never been disclosed, which inhibits progress in the research domain. In this work, we aim to improve the availability of research artifacts in requirements quality research. To this end, we (1) extend an artifact recovery initiative, (2) empirically evaluate the reasons for artifact unavailability using Bayesian data analysis, and (3) compile a concise guideline for open science artifact disclosure. Our results include 10 recovered data sets and 7 recovered implementations, empirical support for artifact availability improving over time and the positive effect of public hosting services, and a pragmatic artifact management guideline open for community comments. With this work, we hope to encourage and support adherence to open science principles and improve the availability of research artifacts for the requirements research quality community.
Keywords: Requirements engineering; Artifact; Availability; Bayesian data analysis; Guideline

Jagadish Shivamurthy, Deepti Vidyarthi, Tarun Uppal,
Natural Language Processing based Auto Generation of Proof Obligations for Formal Verification of Control Requirements in Safety-Critical Systems,
IFAC-PapersOnLine,
Volume 57,
2024,
Pages 1-6,
ISSN 2405-8963,
https://doi.org/10.1016/j.ifacol.2024.05.001.
(https://www.sciencedirect.com/science/article/pii/S2405896324000016)
Abstract: Formal verification uses mathematically rigorous techniques to establish the correctness of an algorithm or model. While traditional testing shows the presence of defects, it cannot guarantee the absence of defects in a design. Formal verification, on the other hand, can guarantee the absence of defects concerning a set of desirable properties, or provide counter-examples where the properties do not hold. Despite its value, it is not commonly used due to various reasons. This paper discusses two major reasons and proposes solutions for them. The first reason is the difficulty in deriving the proof obligations, the properties to be proved, from the textual requirements. The second hindrance is the additional effort in developing the infrastructure for formal verification. The paper proposes a Natural Language Processing (NLP) based approach to automatically suggest the proof obligations from the textual requirements to remove the first hindrance. They are expressed in propositional, Linear-time Temporal Logic (LTL), and a few customized expressions. The paper also provides methods for converting these obligations into verification subsystems which enable model checking, a method of formal verification to be invoked on the design model, thereby alleviating the second hindrance. The approach and methods are explained in the context of a flight control system's fault handling and safety requirements.
Keywords: Safety-critical systems; flight control system; formal verification; proof obligations; NLP; Propositional logic; LTL

Daniel S. Zahm,
Chapter six - Macrosystems in motion, representation, value, emotion, and neuropsychiatric illness,
Editor(s): Daniel S. Zahm,
Anatomy of Neuropsychiatry (Second Edition),
Academic Press,
2024,
Pages 241-301,
ISBN 9780443155963,
https://doi.org/10.1016/B978-0-443-15596-3.00006-0.
(https://www.sciencedirect.com/science/article/pii/B9780443155963000060)
Abstract: Neuroanatomical organization of basal forebrain macrosystems as developed in the preceding chapters is integrated with current models of mechanisms thought to underlie locomotion, intentional movements. A brief epistemological and historical background is then given as an introduction to the sensorimotor origins, substrates, and manipulation of CNS images, a.k.a., representations. The unsatisfactorily introspective nature of much of the inquiry into mental representations but general inadequacy of conventional physiological approaches to address them experimentally are discussed. Contributions of macrosystems to subjective valuation and consequent macrosystem interactions with cerebral cortex in the genesis of emotion are then considered followed by an analysis of vulnerabilities in these neuroanatomical and functional relationships that contribute to neuropsychiatric illness.
Keywords: Active inference; Categorization; Image; Inferential processing; Locomotion; Movement; Predictive coding; Predictive processing; Representation; Valuation; Value assessment

Shu Huang, Jacqueline M. Cole,
ChemDataWriter: a transformer-based toolkit for auto-generating books that summarise research††Electronic supplementary information (ESI) available: An example book that ChemDataWriter auto-generates: literature summary of recent research about Na-ion, Li–S, and Li–O2 battery materials. Results from applying a plagiarism checker to chapter 4 of this example book. See DOI: https://doi.org/10.1039/d3dd00159h,
Digital Discovery,
Volume 2, Issue 6,
2023,
Pages 1710-1720,
ISSN 2635-098X,
https://doi.org/10.1039/d3dd00159h.
(https://www.sciencedirect.com/science/article/pii/S2635098X23001237)
Abstract: ABSTRACT
Since the number of scientific papers has grown substantially over recent years, scientists spend much time searching, screening, and reading papers to follow the latest research trends. With the development of advanced natural-language-processing (NLP) models, transformer-based text-generation algorithms have the potential to summarise scientific papers and automatically write a literature review from numerous scientific publications. In this paper, we introduce a Python-based toolkit, ChemDataWriter, which auto-generates books about research in a completely unsupervised fashion. ChemDataWriter adopts a conservative book-generation pipeline to automatically write the book by suggesting potential book content, retrieving and re-ranking the relevant papers, and then summarising and paraphrasing the text within the paper. To the best of our knowledge, ChemDataWriter is the first open-source toolkit in the area of chemistry to be able to compose a literature review entirely via artificial intelligence once one has suggested a broad topic. We also provide an example of a book that ChemDataWriter has auto-generated about battery-materials research. To aid the use of ChemDataWriter, its code is provided with associated documentation to serve as a user guide.

Nour Moustafa, Jiankun Hu, Jill Slay,
A holistic review of Network Anomaly Detection Systems: A comprehensive survey,
Journal of Network and Computer Applications,
Volume 128,
2019,
Pages 33-55,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2018.12.006.
(https://www.sciencedirect.com/science/article/pii/S1084804518303886)
Abstract: Network Anomaly Detection Systems (NADSs) are gaining a more important role in most network defense systems for detecting and preventing potential threats. The paper discusses various aspects of anomaly-based Network Intrusion Detection Systems (NIDSs). The paper explains cyber kill chain models and cyber-attacks that compromise network systems. Moreover, the paper describes various Decision Engine (DE) approaches, including new ensemble learning and deep learning approaches. The paper also provides more details about benchmark datasets for training and validating DE approaches. Most of NADSs’ applications, such as Data Centers, Internet of Things (IoT), as well as Fog and Cloud Computing, are also discussed. Finally, we present several experimental explanations which we follow by revealing various promising research directions.
Keywords: Intrusion Detection System (IDS); Network Anomaly Detection Systems (NADS); Data pre-processing; Decision Engine (DE)

Robert X. Gao, Lihui Wang, Moneer Helu, Roberto Teti,
Big data analytics for smart factories of the future,
CIRP Annals,
Volume 69, Issue 2,
2020,
Pages 668-692,
ISSN 0007-8506,
https://doi.org/10.1016/j.cirp.2020.05.002.
(https://www.sciencedirect.com/science/article/pii/S0007850620301359)
Abstract: Continued advancement of sensors has led to an ever-increasing amount of data of various physical nature to be acquired from production lines. As rich information relevant to the machines and processes are embedded within these “big data”, how to effectively and efficiently discover patterns in the big data to enhance productivity and economy has become both a challenge and an opportunity. This paper discusses essential elements of and promising solutions enabled by data science that are critical to processing data of high volume, velocity, variety, and low veracity, towards the creation of added-value in smart factories of the future.
Keywords: Digital manufacturing system; Information; Learning

Jessica A.F. Thompson, Hannah Sheahan, Tsvetomira Dumbalska, Julian D. Sandbrink, Manuela Piazza, Christopher Summerfield,
Zero-shot counting with a dual-stream neural network model,
Neuron,
Volume 112, Issue 24,
2024,
Pages 4147-4158.e5,
ISSN 0896-6273,
https://doi.org/10.1016/j.neuron.2024.10.008.
(https://www.sciencedirect.com/science/article/pii/S0896627324007293)
Abstract: Summary
To understand a visual scene, observers need to both recognize objects and encode relational structure. For example, a scene comprising three apples requires the observer to encode concepts of “apple” and “three.” In the primate brain, these functions rely on dual (ventral and dorsal) processing streams. Object recognition in primates has been successfully modeled with deep neural networks, but how scene structure (including numerosity) is encoded remains poorly understood. Here, we built a deep learning model, based on the dual-stream architecture of the primate brain, which is able to count items “zero-shot”—even if the objects themselves are unfamiliar. Our dual-stream network forms spatial response fields and lognormal number codes that resemble those observed in the macaque posterior parietal cortex. The dual-stream network also makes successful predictions about human counting behavior. Our results provide evidence for an enactive theory of the role of the posterior parietal cortex in visual scene understanding.
Keywords: enumeration; visual reasoning; PPC; numerical cognition; dorsal stream; enactive cognition; neural networks; zero-shot generalization; attention; structure learning

Kai Meng, Guiyin Xu, Xianghui Peng, Kamal Youcef-Toumi, Ju Li,
Intelligent disassembly of electric-vehicle batteries: a forward-looking overview,
Resources, Conservation and Recycling,
Volume 182,
2022,
106207,
ISSN 0921-3449,
https://doi.org/10.1016/j.resconrec.2022.106207.
(https://www.sciencedirect.com/science/article/pii/S0921344922000556)
Abstract: Retired electric-vehicle lithium-ion battery (EV-LIB) packs pose severe environmental hazards. Efficient recovery of these spent batteries is a significant way to achieve closed-loop lifecycle management and a green circular economy. It is crucial for carbon neutralization, and for coping with the environmental and resource challenges associated with the energy transition. EV-LIB disassembly is recognized as a critical bottleneck for mass-scale recycling. Automated disassembly of EV-LIBs is extremely challenging due to the large variety and uncertainty of retired EV-LIBs. Recent advances in artificial intelligence (AI) machine learning (ML) provide new ways for addressing these problems. This study aims to provide a systematic review and forward-looking perspective on how AI/ML methodology can significantly boost EV-LIB intelligent disassembly for achieving sustainable recovery. This work examines the key advances and research opportunities of emerging intelligent technologies for EV-LIB disassembly, and recycling and reuse of industrial products in general. We show that AI could benefit the whole disassembly process, particularly addressing the uncertainty and safety issues. Currently, EV-LIB state prognostics, disassembly decision-making as well as target detection are indicated as promising areas to realize intelligence. The challenges still exist for extensive autonomy due to present AI's inherent limitations, mechanical and chemical complexities, and sustainable benefits concerns. This paper provides the practical map to direct how to implement EV-LIB intelligent disassembly as well as forward-looking perspectives for addressing these challenges.
Keywords: Electric vehicle battery; disassembly; recycling; artificial intelligence; machine learning; sustainability

Jenna N. Whitrock, Catherine G. Pratt, Michela M. Carter, Ryan C. Chae, Adam D. Price, Carla F. Justiniano, Robert M. Van Haren, Latifa S. Silski, Ralph C. Quillin, Shimul A. Shah,
Does using artificial intelligence take the person out of personal statements? We can't tell,
Surgery,
Volume 176, Issue 6,
2024,
Pages 1610-1616,
ISSN 0039-6060,
https://doi.org/10.1016/j.surg.2024.08.018.
(https://www.sciencedirect.com/science/article/pii/S0039606024005907)
Abstract: Background
Use of artificial intelligence to generate personal statements for residency is currently not permitted but is difficult to monitor. This study sought to evaluate the ability of surgical residency application reviewers to identify artificial intelligence–generated personal statements and to understand perceptions of this practice.
Methods
Three personal statements were generated using ChatGPT, and 3 were written by medical students who previously matched into surgery residency. Blinded participants at a single institution were instructed to read all personal statements and identify which were generated by artificial intelligence; they then completed a survey exploring their opinions regarding artificial intelligence use.
Results
Of the 30 participants, 50% were faculty (n = 15) and 50% were residents (n = 15). Overall, experience ranged from 0 to 20 years (median, 2 years; interquartile range, 1–6.25 years). Artificial intelligence–derived personal statements were identified correctly only 59% of the time, with 3 (10%) participants identifying all the artificial intelligence–derived personal statements correctly. Artificial intelligence–generated personal statements were labeled as the best 60% of the time and the worst 43.3% of the time. When asked whether artificial intelligence use should be allowed in personal statements writing, 66.7% (n = 20) said no and 30% (n = 9) said yes. When asked if the use of artificial intelligence would impact their opinion of an applicant, 80% (n = 24) said yes, and 20% (n = 6) said no. When survey questions and ability to identify artificial intelligence–generated personal statements were evaluated by faculty/resident status and experience, no differences were noted (P > .05).
Conclusion
This study shows that surgical faculty and residents cannot reliably identify artificial intelligence–generated personal statements and that concerns exist regarding the impact of artificial intelligence on the application process.

Jorge Echeverría, Jaime Font, Francisca Pérez, Carlos Cetina,
Comparison of search strategies for feature location in software models,
Journal of Systems and Software,
Volume 181,
2021,
111037,
ISSN 0164-1212,
https://doi.org/10.1016/j.jss.2021.111037.
(https://www.sciencedirect.com/science/article/pii/S0164121221001345)
Abstract: Search-based model-driven engineering is the application of search-based techniques to specific problems that are related to software engineering that is driven using software models. In this work, we make use of measures from the literature to report feature location problems in models (size and volume of the model and density, multiplicity, and dispersion of the feature being located) and a set of search strategies (random search, iterated local search, hill climbing, an evolutionary algorithm, and a hybrid between an evolutionary algorithm and hill climbing). The goal is to analyze of the impact of different values that are used to describe the feature location problems and the performance obtained by the different search strategies. We apply the search strategies to 1895 feature location problems that are obtained from 40 industrial software product lines. This work shows that: 1) the best results overall are obtained by a hybrid between evolutionary algorithm and hill climbing; 2) the size of the search space has the greatest impact on the results obtained by the search strategies; and 3) the impact of each of the measures is not the same in the five search strategies. This work highlights the use of the search strategy that produces the best results. In addition, we provide recommendations on when to use each search strategy.
Keywords: Feature location in models; Search strategies

Jože M. Rožanec, Patrik Zajec, Spyros Theodoropoulos, Erik Koehorst, Blaž Fortuna, Dunja Mladenić,
Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection,
IFAC-PapersOnLine,
Volume 56, Issue 2,
2023,
Pages 11094-11099,
ISSN 2405-8963,
https://doi.org/10.1016/j.ifacol.2023.10.817.
(https://www.sciencedirect.com/science/article/pii/S2405896323011941)
Abstract: Quality control is a crucial activity manufacturing companies perform to ensure their products conform to the requirements and specifications. The introduction of artificial intelligence models enables to automate the visual quality inspection, speeding up the inspection process and ensuring all products are evaluated under the same criteria. In this research, we compare supervised and unsupervised defect detection techniques and explore data augmentation techniques to mitigate the data imbalance in the context of automated visual inspection. Furthermore, we use Generative Adversarial Networks for data augmentation to enhance the classifiers’ discriminative performance. Our results show that state-of-the-art unsupervised defect detection does not match the performance of supervised models but can reduce the labeling workload if tolerating some labeling errors. Furthermore, the best classification performance was achieved considering GAN-based data generation with AUC ROC scores equal to or higher than 0,9898. We performed the research with real-world data provided by Philips Consumer Lifestyle BV.
Keywords: Manufacturing plant control; Intelligent manufacturing systems; Advanced manufacturing; Industry 4.0; Smart Manufacturing; Visual Inspection; Quality Inspection; Data Augmentation

Anastasiya V. Kulikova, Jennifer K. Parker, Bryan W. Davies, Claus O. Wilke,
Semantic search using protein large language models detects class II microcins in bacterial genomes,
mSystems,
Volume 9, Issue 10,
2024,
,
ISSN 2379-5077,
https://doi.org/10.1128/msystems.01044-24.
(https://www.sciencedirect.com/science/article/pii/S2379507724002952)
Abstract: ABSTRACT

Class II microcins are antimicrobial peptides that have shown some potential as novel antibiotics. However, to date, only 10 class II microcins have been described, and the discovery of novel microcins has been hampered by their short length and high sequence divergence. Here, we ask if we can use numerical embeddings generated by protein large language models to detect microcins in bacterial genome assemblies and whether this method can outperform sequence-based methods such as BLAST. We find that embeddings detect known class II microcins much more reliably than does BLAST and that any two microcins tend to have a small distance in embedding space even though they typically are highly diverged at the sequence level. In data sets of Escherichia coli, Klebsiella spp., and Enterobacter spp. genomes, we further find novel putative microcins that were previously missed by sequence-based search methods.
IMPORTANCE
Antibiotic resistance is becoming an increasingly serious problem in modern medicine, but the development pipeline for conventional antibiotics is not promising. Therefore, alternative approaches to combat bacterial infections are urgently needed. One such approach may be to employ naturally occurring antibacterial peptides produced by bacteria to kill competing bacteria. A promising class of such peptides are class II microcins. However, only a small number of class II microcins have been discovered to date, and the discovery of further such microcins has been hampered by their high sequence divergence and short length, which can cause sequence-based search methods to fail. Here, we demonstrate that a more robust method for microcin discovery can be built on the basis of a protein large language model, and we use this method to identify several putative novel class II microcins.
Antibiotic resistance is becoming an increasingly serious problem in modern medicine, but the development pipeline for conventional antibiotics is not promising. Therefore, alternative approaches to combat bacterial infections are urgently needed. One such approach may be to employ naturally occurring antibacterial peptides produced by bacteria to kill competing bacteria. A promising class of such peptides are class II microcins. However, only a small number of class II microcins have been discovered to date, and the discovery of further such microcins has been hampered by their high sequence divergence and short length, which can cause sequence-based search methods to fail. Here, we demonstrate that a more robust method for microcin discovery can be built on the basis of a protein large language model, and we use this method to identify several putative novel class II microcins.
Keywords: class II microcin; protein large language model; embedding

Nhien Rust-Nguyen, Shruti Sharma, Mark Stamp,
Darknet traffic classification and adversarial attacks using machine learning,
Computers & Security,
Volume 127,
2023,
103098,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2023.103098.
(https://www.sciencedirect.com/science/article/pii/S0167404823000081)
Abstract: The anonymous nature of darknets is commonly exploited for illegal activities. Previous research has employed machine learning and deep learning techniques to automate the detection of darknet traffic in an attempt to block these criminal activities. This research aims to improve darknet traffic detection by assessing a wide variety of machine learning and deep learning techniques for the classification of such traffic and for classification of the underlying application types. We find that a Random Forest model outperforms other state-of-the-art machine learning techniques used in prior work with the CIC-Darknet2020 dataset. To evaluate the robustness of our Random Forest classifier, we obfuscate select application type classes to simulate realistic adversarial attack scenarios. We demonstrate that our best-performing classifier can be degraded by such attacks, and we consider ways to effectively deal with such adversarial attacks.
Keywords: Darknet; Classification; Adversarial attacks; Convolutional neural network; Auxiliary-Classifier generative adversarial network; Random forest

Jonathan Dekhtiar, Alexandre Durupt, Matthieu Bricogne, Benoit Eynard, Harvey Rowson, Dimitris Kiritsis,
Deep learning for big data applications in CAD and PLM – Research review, opportunities and case study,
Computers in Industry,
Volume 100,
2018,
Pages 227-243,
ISSN 0166-3615,
https://doi.org/10.1016/j.compind.2018.04.005.
(https://www.sciencedirect.com/science/article/pii/S0166361517305560)
Abstract: With the increasing amount of available data, computing power and network speed for a decreasing cost, the manufacturing industry is facing an unprecedented amount of data to process, understand and exploit. Phenomena such as Big Data, the Internet-of-Things, Closed-Loop Product Lifecycle Management, and the advances of Smart Factories tend to produce humanly unmanageable quantities of data. The paper approaches the aforesaid context by assuming that any data processing automation is not only desirable but rather necessary in order to prevent prohibitive data analytics costs. This study focuses on highlighting the major specificities of engineering data and the data-processing difficulties which are inherent to data coming from the manufacturing industry. The artificial intelligence field of research is able to provide methods and tools to address some of the identified issues. A special attention was paid to provide a literature review of the most recent (in 2017) applications, that could present a high potential for the manufacturing industry, in the fields of machine learning and deep learning. In order to illustrate the proposed work, a case study was conducted on the challenging research question of object recognition in heterogeneous formats (3D models, photos and videos) with deep learning techniques. The DICE project – DMU Imagery Comparison Engine – is presented and has been completely open-sourced in order to encourage reuse and improvements of the proposed case-study. This project also leads to the development of an open-source research dataset of 2000 CAD Models, called DMU-Net available at: https://www.dmu-net.org.
Keywords: Deep learning; Machine learning; Computer vision; Product Lifecycle Management; Digital mock-up; Shape retrieval
