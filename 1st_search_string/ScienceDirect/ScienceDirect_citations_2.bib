@article{AMJAD2025e41835,
title = {Context aware machine learning techniques for brain tumor classification and detection – A review},
journal = {Heliyon},
volume = {11},
number = {2},
pages = {e41835},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2025.e41835},
url = {https://www.sciencedirect.com/science/article/pii/S2405844025002154},
author = {Usman Amjad and Asif Raza and Muhammad Fahad and Doaa Farid and Adnan Akhunzada and Muhammad Abubakar and Hira Beenish},
keywords = {CNN, MRI, Survival prediction, Tumor segmentation, Histology, Deep learning, Machine learning, K-MEANS clustering},
abstract = {Background
Machine learning has tremendous potential in acute medical care, particularly in the field of precise medical diagnosis, prediction, and classification of brain tumors. Malignant gliomas, due to their aggressive growth and dismal prognosis, stand out among various brain tumor types. Recent advancements in understanding the genetic abnormalities that underlie these tumors have shed light on their histo-pathological and biological characteristics, which support in better classification and prognosis.
Objectives
This review aims to predict gene alterations and establish structured correlations among various tumor types, extending the prediction of genetic mutations and structures using the latest machine learning techniques. Specifically, it focuses on multi-modalities of Magnetic Resonance Imaging (MRI) and histopathology, utilizing Convolutional Neural Networks (CNN) for image processing and analysis.
Methods
The review encompasses the most recent developments in MRI, and histology image processing methods across multiple tumor classes, including Glioma, Meningioma, Pituitary, Oligodendroglioma, and Astrocytoma. It identifies challenges in tumor classification, segmentation, datasets, and modalities, employing various neural network architectures. A competitive analysis assesses the performance of CNN. Furthermore it also implies K-MEANS clustering to predict Genetic structure, Genes Clusters prediction and Molecular Alteration of various types and grades of tumors e.g. Glioma, Meningioma, Pituitary, Oligodendroglioma, and Astrocytoma.
Results
CNN and KNN structures, with their ability to extract highlights in image-based information, prove effective in tumor classification and segmentation, surmounting challenges in image analysis. Competitive analysis reveals that CNN and outperform others algorithms on publicly available datasets, suggesting their potential for precise tumor diagnosis and treatment planning.
Conclusion
Machine learning, especially through CNN and SVM algorithms, demonstrates significant potential in the accurate diagnosis and classification of brain tumors based on imaging and histo-pathological data. Further advancements in this area hold promise for improving the accuracy and efficiency of intra-operative tumor diagnosis and treatment.}
}
@article{CAI2025103969,
title = {DWTAT-DASIS: Fusion of discrete wavelet transform and access tree for distributed authentication in secret image sharing},
journal = {Computer Standards & Interfaces},
volume = {93},
pages = {103969},
year = {2025},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2024.103969},
url = {https://www.sciencedirect.com/science/article/pii/S0920548924001387},
author = {Chuanda Cai and Changgen Peng and Hanlin Tang and Bin Xiao and Weijie Tan},
keywords = {Distributed storage, Discrete wavelet transform, Secret image sharing, Signature certification},
abstract = {Secret sharing of distributed data in cloud environments prevents unauthorized and wanton access and misuse by malicious participants. However, when applying secret sharing to image formats, the fixed range of pixel values in images presents unique challenges for share recovery, often resulting in recovery algorithms that reconstruct images in a lossy manner. Moreover, one-way authentication methods for participants in cloud environments are insufficient to address the heightened security demands of high-trust scenarios. This paper presents a secret image-sharing scheme with distributed authentication (DWTAT-DASIS) designed for cloud storage environments. By leveraging Discrete Wavelet Transform and an Access Tree structure, the scheme addresses the limitations of existing approaches (such as compression-based secret sharing and visual cryptography), which fail to provide lossless image recovery and efficient performance. Additionally, DWTAT-DASIS enhances security through fine-grained access control and two-way authentication. Finally, security analysis and experiments were conducted on the proposed protocol, demonstrating its ability to resist common attack methods as well as some deeper-level attack methods, meeting the security requirements for image storage in cloud systems. Experimental analysis shows that compared with similar protocols, this protocol can ensure the integrity of image restoration and greatly reduce the probability of image transmission being attacked.}
}
@article{2025A5,
title = {Instructions for authors},
journal = {Journal of Hand Therapy},
volume = {38},
number = {1},
pages = {A5-A12},
year = {2025},
issn = {0894-1130},
doi = {https://doi.org/10.1016/S0894-1130(25)00020-1},
url = {https://www.sciencedirect.com/science/article/pii/S0894113025000201}
}
@article{HAGIU2025103134,
title = {Artificial intelligence and competition policy},
journal = {International Journal of Industrial Organization},
pages = {103134},
year = {2025},
issn = {0167-7187},
doi = {https://doi.org/10.1016/j.ijindorg.2025.103134},
url = {https://www.sciencedirect.com/science/article/pii/S0167718725000013},
author = {Andrei Hagiu and Julian Wright},
keywords = {Antitrust, Data, Feedback loops, Generative AI, Network effects},
abstract = {This paper examines competition policy implications of the rapidly expanding Artificial Intelligence (AI) sector. We analyze the vertical AI technology stack and data feedback loops to address three key questions: the potential for market concentration in core AI services, AI's likely impact on existing market structures, and emerging competition policy challenges. We identify key risks to competition in the AI sector, ways in which AI may disrupt some existing platforms, how AI could lead to new types of gatekeepers, and some novel competition policy concerns raised by AI.}
}
@article{SINGH2025104231,
title = {Unveiling the veiled: An early stage detection of fileless malware},
journal = {Computers & Security},
volume = {150},
pages = {104231},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104231},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005376},
author = {Narendra Singh and Somanath Tripathy},
keywords = {Fileless malware attack, Memory analysis, Early stage detection, MITRE ATT&CK enterprise matrix},
abstract = {The threat actors continuously evolve their tactics and techniques in a novel form to evade traditional security solutions. Fileless malware attacks are one such advancement, which operates directly within system memory, leaving no footprint on the disk, so became challenging to detect. Meanwhile, the current state-of-the-art approaches detect fileless attacks at the final (post-infection) stage, although, detecting attacks at an early-stage is crucial to prevent potential damage and data breaches. In this work, we propose an early-stage detection system named Argus to detect fileless malware at early-stage. Argus extracts key features from acquired memory dumps of suspicious processes in real-time and generates explained features. It then correlates the explained features with the MITRE ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) framework to identify fileless malware attacks before their operational stage. The experimental results show that Argus could successfully identify, 4356 fileless malware samples (out of 5026 samples) during the operational stage. Specifically, 2978 samples are detected in the pre-operational phase, while 1378 samples are detected in the operational phase.}
}
@article{LAI202573,
title = {Enhancing Deepfake Detection: Proactive Forensics Techniques Using Digital Watermarking},
journal = {Computers, Materials and Continua},
volume = {82},
number = {1},
pages = {73-102},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.059370},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825000311},
author = {Zhimao Lai and Saad Arif and Cong Feng and Guangjun Liao and Chuntao Wang},
keywords = {Deepfake, proactive forensics, digital watermarking, traceability, detection techniques},
abstract = {With the rapid advancement of visual generative models such as Generative Adversarial Networks (GANs) and stable Diffusion, the creation of highly realistic Deepfake through automated forgery has significantly progressed. This paper examines the advancements in Deepfake detection and defense technologies, emphasizing the shift from passive detection methods to proactive digital watermarking techniques. Passive detection methods, which involve extracting features from images or videos to identify forgeries, encounter challenges such as poor performance against unknown manipulation techniques and susceptibility to counter-forensic tactics. In contrast, proactive digital watermarking techniques embed specific markers into images or videos, facilitating real-time detection and traceability, thereby providing a preemptive defense against Deepfake content. We offer a comprehensive analysis of digital watermarking-based forensic techniques, discussing their advantages over passive methods and highlighting four key benefits: real-time detection, embedded defense, resistance to tampering, and provision of legal evidence. Additionally, the paper identifies gaps in the literature concerning proactive forensic techniques and suggests future research directions, including cross-domain watermarking and adaptive watermarking strategies. By systematically classifying and comparing existing techniques, this review aims to contribute valuable insights for the development of more effective proactive defense strategies in Deepfake forensics.}
}
@article{KO2025105936,
title = {Graph neural networks for classification and error detection in 2D architectural detail drawings},
journal = {Automation in Construction},
volume = {170},
pages = {105936},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105936},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006721},
author = {Jaechang Ko and Donghyuk Lee},
keywords = {Architectural detail drawings, Graph neural networks (GNN), Drawing classification, Error detection, Explainable AI (XAI)},
abstract = {The assessment and classification of architectural sectional drawings is critical in the architecture, engineering, and construction (AEC) field, where the accurate representation of complex structures and the extraction of meaningful patterns are key challenges. This paper established a framework for standardizing different forms of architectural drawings into a consistent graph format, and evaluated different Graph Neural Networks (GNNs) architectures, pooling methods, node features, and masking techniques. This paper demonstrates that GNNs can be practically applied in the design and review process, particularly for categorizing details and detecting errors in architectural drawings. The potential for visual explanations of model decisions using Explainable AI (XAI) is also explored to enhance the reliability and user understanding of AI models in architecture. This paper highlights the potential of GNNs in architectural data analysis and outlines the challenges and future directions for broader application in the AEC field.}
}
@article{TIAN2025107723,
title = {Agent-based modeling in solid waste management: Advantages, progress, challenges and prospects},
journal = {Environmental Impact Assessment Review},
volume = {110},
pages = {107723},
year = {2025},
issn = {0195-9255},
doi = {https://doi.org/10.1016/j.eiar.2024.107723},
url = {https://www.sciencedirect.com/science/article/pii/S019592552400310X},
author = {Xi Tian and Fei Peng and Guoen Wei and Chong Xiao and Qingyuan Ma and Zhikang Hu and Yaobin Liu},
keywords = {Solid waste management, Agent-based modeling, Sustainable development, Complex systems simulation, Systematic review},
abstract = {The growing issue of solid waste management (SWM) is recognized as a significant challenge to ecosystem preservation. Agent-based modeling (ABM) has received significant attention for its capability to address complex systems and simulate the outcomes of strategic implementation. This review compares ABM with other methods and provides a comprehensive overview of research on ABM in SWM from 2000 to 2023, emphasizing its advantages, progress, challenges, and future directions. Results indicate that: 1) ABM possesses 8 key advantages in simulating individual behavior, responses to environmental changes across time and spatial scales, and decision-making processes, namely interactivity, heterogeneity, dynamism, traceability, spatiality, scalability, complexity, and adaptability. 2) Current research primarily focuses on simulating behavioral and strategic effects of SWM (accounting for 45.5 %), while multi-model coupling is becoming a new trend. 3) ABM encounters challenges in its research, including a lack of standardized research steps, high data dependency, limited computing resources, and difficulties in algorithm explanation. Therefore, this study introduces a set of normative steps that provide clear guidance for research and help ensure the reproducibility and accuracy of studies. Future research should incorporate big data and emerging technologies to enhance computational efficiency and processing capabilities of models. To better utilize ABM for achieving environmental protection and sustainable development goals, prioritizing integration of multi-model coupling, interdisciplinary collaboration, visualization, and open-source code sharing as key strategies is essential.}
}
@article{ESTEVEZ2025119850,
title = {High-performance carbonaceous absorbers: From heterogeneous absorbents to data-driven metamaterials},
journal = {Carbon},
volume = {233},
pages = {119850},
year = {2025},
issn = {0008-6223},
doi = {https://doi.org/10.1016/j.carbon.2024.119850},
url = {https://www.sciencedirect.com/science/article/pii/S0008622324010698},
author = {Diana Estevez and Faxiang Qin},
keywords = {Microwave absorption, Carbon composites absorber, Metamaterials absorber, Effective medium theory},
abstract = {Carbon-based materials are a key focus in the advancement of “on-demand” microwave absorbers due to their adjustable electrical conductivity and structure, as well as the presence of surface functional groups and defects that promote various loss mechanisms. Bottom-up strategies to optimize the carbon absorbent phase rely primarily on component hybridization, atomic doping, interface engineering, and the construction of hierarchical structures. However, while these strategies constitute important advancements, they do not extend beyond laboratory settings and remain restricted in scope. Compared to a composite absorber incorporating carbon inclusions within a matrix, greater flexibility in design and property control is achieved, as its adoption has triggered effective medium and homogenization theories for assessing structure-property relations. Metamaterial absorbers are rationally designed composites, resulting from meticulous adjustments in microarchitecture that have recently been accelerated by artificial intelligence (AI)-based algorithms replacing conventional trial-and-error and experimental-based strategies for optimization. These emerging technological routes could also be exploited to add multifunctionality to carbon composite absorbers in actual service environments and to develop the next generation of smart absorbers. This article presents an overview of the achievements, trends, and challenges in these areas from the perspective of composite structures rather than focusing on the individual absorbent phase, a subject that is currently underrepresented in existing literature.}
}
@article{ROTH2025e41364,
title = {A non-randomised open-label exploratory ‘window of opportunity’ study of TG02 treatment in patients with locally advanced primary and recurrent RAS mutant colorectal cancer},
journal = {Heliyon},
volume = {11},
number = {1},
pages = {e41364},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e41364},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024173951},
author = {Sara Roth and Kasmira Claire Wilson and Robert George Ramsay and Catherine Mitchell and Shienny Sampurno and Toan Duc Pham and Joseph Cherng {Huei Kong} and Stephen Q. Wong and Alexander Graham Heriot and Sanjeev Deva and Matthew Burge and Cecilie Sverdrup and Anne-Sophie Moller and Lukasz Kuryk and Jon Amund Eriksen and Magnus Jaderberg and John Raymond Zalcberg and Michael Michael},
keywords = {Colorectal cancer, Cancer vaccine, , TG02, GM-CSF},
abstract = {Background
TG02 is a peptide-based cancer vaccine eliciting immune responses to oncogenic codon 12/13 RAS mutations. This phase 1 clinical trial (NCT02933944) assessed the safety and immunological efficacy of TG02 adjuvanted by GM-CSF in patients with KRAS-mutant colorectal cancer.
Methods
In the interval between completing CRT and pelvic exenteration, patients with resectable KRAS mutation-positive, locally advanced primary or current colorectal cancer, received 5–6 doses of TG02/GM-CSF. Immune response was defined as a positive delayed-type hypersensitivity or positive T cell proliferation assay response. Tumour biopsies were analysed for tumour-infiltrating lymphocytes (TILs) and blood for CEA and ctDNA. TILs and tumouroids were cultured, characterised and tested for their killing efficacy.
Results
Six patients with rectal cancer were recruited to evaluate TG02. Three patients experienced a total of 16 treatment-related adverse events; all grade 1. Four of the 6 patients (66.7 %) had at least one vaccine-induced TG02 immune response. Flow cytometry analysis showed high proportion of PD-1-expressing TILs in 2 of 3 patient specimens’ post-treatment. A partial to near complete pathological response was reported in 4 of 6 patients.
Conclusions
This study demonstrated that TG02/GM-CSF was well tolerated and induced a vaccine specific systemic immune response in the majority of patients. Low numbers limit conclusive clinical outcome reporting. High PD-1 expression on post-treatment TILs encourages the addition of an immune checkpoint inhibitor to TG02 and potentially other studies of peptide vaccines in future studies.}
}
@article{DANE2024100967,
title = {Who’s that lady? — Applying open source intelligence in a history context},
journal = {Endeavour},
volume = {48},
number = {4},
pages = {100967},
year = {2024},
issn = {0160-9327},
doi = {https://doi.org/10.1016/j.endeavour.2024.100967},
url = {https://www.sciencedirect.com/science/article/pii/S0160932724000565},
author = {J. Dane and C. Verhoef},
keywords = {Open source intelligence, Black open access, covert social network analysis, Albert Einstein, Sophie Rotszajn, Catherina Amelia Frankamp, Eva Dina Bruins, Tatyana Alexeyevna Afanasyeva, Mileva Marić, Geertruida Luberta Lorentz, Marie Scanavy-Grigorieff, Henderina Jacoba Schaap-Biegel, Johanna Hermine Biegel, Rebekka Aleida Biegel, Johann Jacob Kern},
abstract = {During a network analysis of the Dutch astronomer and psychologist Rebekka Aleida Biegel (1886–1943), we stumbled upon an often investigated group photo that most likely shows two of her close friends and a third woman posing with Albert Einstein among others in a chemistry laboratory in Zurich while having a tea party. Using data from the Dark Web, face recognition, open source intelligence (OSINT) tools, and artificial intelligence (AI) techniques, we found in total four group portraits of this gathering and were able to determine the true identities of the three women, as well as one of the unknown men in one of the photos, with a very high degree of certainty. Moreover, we determined the exact day and time the photographs were taken: June 30, 1913 around 4:30 PM. After more than a century, the many riddles surrounding these group photos have been solved. By resolving the many questions regarding the (material) historical context of this iconic photograph of Einstein, three years before he published his theory of relativity, new light has been shed on one of the most exciting periods in the history of science. Our innovative research methodology—including AI, Dark Web, and OSINT—enabled us to reconstruct elements of the past of these totally forgotten and heavily marginalized women from many and diverse scattered and unassuming sources and revealed that their place in the history of physics is even more significant than thought. They, too, were part of Einstein’s huge sounding board in the form of his weekly colloquium and had precise astrophysical calculations to add; an indispensible ingredient for proving Einstein’s theory of relativity.}
}
@article{YUAN2025104355,
title = {DeMarking: A defense for network flow watermarking in real-time},
journal = {Computers & Security},
volume = {152},
pages = {104355},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104355},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825000446},
author = {Yali Yuan and Jian Ge and Guang Cheng},
keywords = {Flow watermarking, Defense, Tor, Adversarial attacks, Generative adversarial networks},
abstract = {The network flow watermarking technique associates the two communicating parties by actively modifying certain characteristics of the flow generated by the sender so that it covertly carries some special marking information. Some third-party attackers communicating with the hidden server as a Tor client may attempt de-anonymization attacks to uncover the real identity of the hidden server by using this technique. This compromises the privacy of the anonymized communication system. Therefore, we propose a watermark defense scheme based on deep neural networks. Firstly, we design a training architecture based on generative adversarial networks and adversarial attacks. This architecture can train a converter to convert the original Inter-Packet Delays (IPD) into newly generated “clean” IPDs by the model, causing the adversary’s detector to extract incorrect information and thus unable to perform traffic correlation. Using the trained converter model, we design a watermark defense scheme that can effectively resist time-based watermarking techniques.}
}
@article{BALALLE2025101299,
title = {Reassessing academic integrity in the age of AI: A systematic literature review on AI and academic integrity},
journal = {Social Sciences & Humanities Open},
volume = {11},
pages = {101299},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101299},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125000269},
author = {Himendra Balalle and Sachini Pannilage},
keywords = {Academic integrity, Artificial intelligence, Academic misconduct, AI-Generated writing, Educational ethics},
abstract = {Academic integrity is a key factor in the quality of education that represents honesty, trust, and ethical conduct. In today's rapidly changing educational landscape, artificial intelligence (AI) poses significant challenges to the educational ecosystem's ability to maintain academic integrity. It also affects the qualifications offered by institutes. Although AI supports students in completing academic tasks, it usually risks violating the basic rules of academic integrity. In addition, AI can be used to detect academic misconduct. This study aims to critically examine the impact of AI on academic integrity through a systematic literature review. The research question was developed using the PICO framework, and the articles considered in this study were collected from Scopus, PubMed, DOAJ, and Base. Of 1443 articles, 25 were selected based on the PRISMA framework. We used the Cochrane risk of bias tool (ROBINS-1) to analyse the risk of bias in the selected studies. The discussion section was developed based on PICO frameworks – population of the study, intervention with AI tools, comparison of modern and traditional methods, and outcome of AI use for academic activities – to answer the research question. This research contributes to the ongoing dialogue about AI and academic integrity by emphasising the importance of a balanced approach to using the benefits of AI in education while upholding ethical standards. This study concludes by emphasising the importance of creating a culture of academic integrity to ensure the ethical use of AI for educational purposes.}
}
@article{GERAGHTY2025104054,
title = {Sustainability considerations for end-of-life fibre-reinforced plastic boats},
journal = {Regional Studies in Marine Science},
volume = {83},
pages = {104054},
year = {2025},
issn = {2352-4855},
doi = {https://doi.org/10.1016/j.rsma.2025.104054},
url = {https://www.sciencedirect.com/science/article/pii/S2352485525000453},
author = {Ruadan Geraghty and Jasper Graham-Jones and Richard Pemberton and John Summerscales and Simon Bray},
keywords = {Boats, Disposal, End-of-life, Fibre-reinforced plastic, FRP, GRP, Sustainability},
abstract = {In the 1950s, glass fibre-reinforced polyester resin (GRP, also known as fibreglass or glassfibre) composites replaced wood and metal as the material for small recreational and work boats. The changes resulted from relative ease of manufacture, durability, and low maintenance. New fibres and resins then became available to create a wider range of Fibre-Reinforced Plastics (FRP). Vessels remain serviceable beyond design life: 10 years for inflatables, 20 years for motorboats and 30 years plus for sailboats. Many vessels have now reached end-of-life (EoL) and become Abandoned or Derelict Vessels (ADV). Given that thermosetting resin is not easy to recycle, these boats exist as slowly rotting hulks. There is a growing cohort of stakeholders from various backgrounds becoming concerned about this issue. This review defines sustainability as the balance of Technical, Economic, Environmental Social and Governance (TEESG) and discusses the TEESG considerations for this waste stream.}
}
@article{VELASCO202510,
title = {Emerging trends in the optimization of organic synthesis through high-throughput tools and machine learning},
journal = {Beilstein Journal of Organic Chemistry},
volume = {21},
pages = {10-38},
year = {2025},
issn = {1860-5397},
doi = {https://doi.org/10.3762/bjoc.21.3},
url = {https://www.sciencedirect.com/science/article/pii/S1860539725000088},
author = {Pablo Quijano Velasco and Kedar Hippalgaonkar and Balamurugan Ramalingam},
keywords = {autonomous reactors, data processing, high-throughput experimentation, machine learning, reaction optimization},
abstract = {The discovery of the optimal conditions for chemical reactions is a labor-intensive, time-consuming task that requires exploring a high-dimensional parametric space. Historically, the optimization of chemical reactions has been performed by manual experimentation guided by human intuition and through the design of experiments where reaction variables are modified one at a time to find the optimal conditions for a specific reaction outcome. Recently, a paradigm change in chemical reaction optimization has been enabled by advances in lab automation and the introduction of machine learning algorithms. Therein, multiple reaction variables can be synchronously optimized to obtain the optimal reaction conditions, requiring a shorter experimentation time and minimal human intervention. Herein, we review the currently used state-of-the-art high-throughput automated chemical reaction platforms and machine learning algorithms that drive the optimization of chemical reactions, highlighting the limitations and future opportunities of this new field of research.}
}
@article{LIU2025106016,
title = {Sensing climate justice: A multi-hyper graph approach for classifying urban heat and flood vulnerability through street view imagery},
journal = {Sustainable Cities and Society},
volume = {118},
pages = {106016},
year = {2025},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2024.106016},
url = {https://www.sciencedirect.com/science/article/pii/S2210670724008394},
author = {Pengyuan Liu and Binyu Lei and Weiming Huang and Filip Biljecki and Yuan Wang and Siyu Li and Rudi Stouffs},
keywords = {Spatial modelling, Graph neural network, Multigraph, Hypergraph, Urban resilience},
abstract = {Recognising the increasing complexities posed by climate challenges to urban environments, it is crucial to develop holistic capabilities for urban areas to effectively respond to climate-related risks, forming the backbone of sustainable urban planning strategies and demanding a comprehensive understanding of urban climate justice. It requires a thorough examination of how climate change exacerbates social, economic, and environmental inequalities within urban settings, which requires a series of sophisticated spatial modellings and relies on data collected periodically. This paper introduces a novel dual-GNN approach, Multi-Hyper Graph Neural Network (MHGNN), with street view imagery as input. The proposed model integrates a multigraph and a hypergraph to model intricate spatial patterns for classifying urban climate justice. The multigraph component of the MHGNN captures spatial proximity and pair-wise connections between urban areas to assess climate impacts. Meanwhile, the hypergraph component addresses higher-order dependencies by incorporating hyperedges that connect multiple geographic areas based on their similarities, thus capturing the multi-faceted relationships among areas with comparable geographic characteristics. By harnessing the strengths of both multigraph and hypergraph structures, the MHGNN provides a comprehensive understanding of the spatial dynamics of urban climate justice. It achieves nearly a 24% performance improvement compared to conventional spatial modelling methods, establishing it as a valuable tool for researchers and policymakers in this domain. Codes available at GitHub.11https://github.com/PengyuanLiu1993/SensingUrbanClimate.}
}
@article{TAIWO2025672,
title = {Generative artificial intelligence in construction: A Delphi approach, framework, and case study},
journal = {Alexandria Engineering Journal},
volume = {116},
pages = {672-698},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.12.079},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824016776},
author = {Ridwan Taiwo and Idris Temitope Bello and Sulemana Fatoama Abdulai and Abdul-Mugis Yussif and Babatunde Abiodun Salami and Abdullahi Saka and Mohamed El Amine {Ben Seghier} and Tarek Zayed},
keywords = {Generative artificial intelligence, Generative pre-trained transformer, Large language model, Multimodal AI, Retrieval augmented generation, Construction industry, GenAI, RAG, LLM, GPT, ChatGPT},
abstract = {The construction industry plays a crucial role in the global economy, contributing approximately $10 trillion and employing over 220 million workers worldwide, but encounters numerous productivity challenges with only 1 % annual growth compared to 2.8 % for the global economy. These challenges span various processes, including design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (GenAI), capable of producing new and realistic data or content such as text, images, videos, or code from given inputs or existing knowledge, presents innovative solutions to these challenges. While there is an increasing interest in the applications of GenAI in construction, a detailed analysis of its practical uses, advantages, and areas ripe for development is still evolving. This study contributes to this emerging area by offering an insightful analysis of the current state of generative AI in construction. It has three objectives: (1) to identify and categorize the existing and emerging generative AI opportunities and challenges in the construction industry via a Delphi study; (2) to propose a framework enabling construction firms to build customized GenAI solutions; and (3) to illustrate this framework through a case study that employs GenAI model for querying contract documents. Through systematic review and expert consultation, the study identified 76 potential GenAI applications across construction phases and 18 key challenges distributed across domain-specific, technological, adoption, and ethical categories. The case study's findings show that retrieval augmented generation (RAG) improves the baseline large language model (LLM), GPT-4, by 5.2, 9.4, and 4.8 % in terms of quality, relevance, and reproducibility. The study recommends a structured approach to GenAI implementation, emphasizing the need for domain-specific customization, robust validation protocols, and careful consideration of ethical implications. This study equips academics and construction professionals with a comprehensive analysis and practical framework, facilitating the integration of GenAI techniques to enhance productivity, quality, safety, and sustainability across the construction industry.}
}
@article{MCINTYRE2025102908,
title = {Equitable writing classrooms and programs in the shadow of AI},
journal = {Computers and Composition},
volume = {75},
pages = {102908},
year = {2025},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2024.102908},
url = {https://www.sciencedirect.com/science/article/pii/S8755461524000847},
author = {Megan McIntyre},
keywords = {Writing programs, Equitable writing pedagogy, Writing assessment, Plagiarism, Generative AI},
abstract = {Each year, in TA orientation, in the practicum course, and in professional development sessions, I ask TAs and instructors to consider what is, for me, the key question at the heart of our work as writing teachers: what do we owe our students? And a related and equally important question: what do we owe ourselves? In 2024, just over two years into the public existence of OpenAI's ChatGPT, the contexts for these questions are perhaps more complicated than ever, but I think the answers are mostly the same: we owe our students equitable classrooms, space to try and to fail, compassion and care, and authentic engagement. We owe them the rights our discipline affirmed almost fifty years ago when CCCC adopted Students’ Right to Their Own Language as the official position of the largest organization of writing teachers in the world. This article reviews an approach to the current Generative AI moment that is rooted in these commitments and reflects an approach I call “informed refusal,” which allows us to acknowledge the existence of generative AI without requiring students to use generative AI products. We can continue to teach critical literacies and attend to the things that make first-year writing classrooms unique, especially our attention to individualized feedback on students’ writing and our attention to helping students build self-efficacy via sustainable writing processes and reflective habits of mind. At the same time, I argue against the adoption of detectors and other writing surveillance technologies because of the ways that such tools reinforce overly simplistic notions of plagiarism (Moore-Howard) and can harm our relationships with students.}
}
@article{MAKRIS2025100717,
title = {A comprehensive survey of Federated Intrusion Detection Systems: Techniques, challenges and solutions},
journal = {Computer Science Review},
volume = {56},
pages = {100717},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100717},
url = {https://www.sciencedirect.com/science/article/pii/S157401372400100X},
author = {Ioannis Makris and Aikaterini Karampasi and Panagiotis Radoglou-Grammatikis and Nikolaos Episkopos and Eider Iturbe and Erkuden Rios and Nikos Piperigkos and Aris Lalos and Christos Xenakis and Thomas Lagkas and Vasileios Argyriou and Panagiotis Sarigiannidis},
keywords = {Cybersecurity, Federated Learning, Intrusion detection, Intrusion prevention},
abstract = {Cyberattacks have increased radically over the last years, while the exploitation of Artificial Intelligence (AI) leads to the implementation of even smarter attacks which subsequently require solutions that will efficiently confront them. This need is indulged by incorporating Federated Intrusion Detection Systems (FIDS), which have been widely employed in multiple scenarios involving communication in cyber–physical systems. These include, but are not limited to, the Internet of Things (IoT) devices, Industrial IoT (IIoT), healthcare systems (Internet of Medical Things/IoMT), Internet of Vehicles (IoV), Smart Manufacturing (SM), Supervisory Control and Data Acquisition (SCADA) systems, Multi-access Edge Computing (MEC) devices, among others. Tackling the challenge of cyberthreats in all the aforementioned scenarios is of utmost importance for assuring the safety and continuous functionality of the operations, crucial for maintaining proper procedures in all Critical Infrastructures (CIs). For this purpose, pertinent knowledge of the current status in state-of-the-art (SOTA) federated intrusion detection methods is mandatory, towards encompassing while simultaneously evolving them in order to timely detect and mitigate cyberattack incidents. In this study, we address this challenge and provide the readers with an overview of FL implementations regarding Intrusion Detection in several CIs. Additionally, the distinct communication protocols, attack types and datasets utilized are thoroughly discussed. Finally, the latest Machine Learning (ML) and Deep Learning (DL) frameworks and libraries to implement such methods are also provided.}
}
@article{2025A16,
title = {Instructions for authors},
journal = {Gastrointestinal Endoscopy},
volume = {101},
number = {1},
pages = {A16-A22},
year = {2025},
issn = {0016-5107},
doi = {https://doi.org/10.1016/S0016-5107(24)03708-8},
url = {https://www.sciencedirect.com/science/article/pii/S0016510724037088}
}
@article{DING2025112663,
title = {Large language models for cyber resilience: A comprehensive review, challenges, and future perspectives},
journal = {Applied Soft Computing},
volume = {170},
pages = {112663},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112663},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624014376},
author = {Weiping Ding and Mohamed Abdel-Basset and Ahmed M. Ali and Nour Moustafa},
keywords = {Large Language Model, Cyber Resilience, Cyber Security, Data Privacy and Protection, Network and Endpoint Security},
abstract = {Interconnect cyber system is used by various users and organizations worldwide to perform different activities. These activities are combined with digital information and systems around the organizations to obtain higher accuracy and performance. However, these combinations of activities have faced cyber threats and attacks by single or multiple attackers. So, protecting and saving users' and organizations' sensitive data is a big challenge. So, the cyber resilience concept refers to the ability to prepare, absorb, recover, and adapt against cyberattacks and threats. It is used to mitigate cyberattacks and risks by the ability of the system to recover from threats. Artificial intelligence models enhance cyber resilience using machine learning and deep learning models. One of the most common components of artificial intelligence is large language models (LLM). It is used to understand language from text data and extract features to predict future words or missing in text datasets. LLM can enhance cyber resilience by providing various benefits for users and organizations. We divide the cyber resilience strategies into five parts. We review the LLM in each part, including security posture, data privacy and protection, security awareness, network security, and security automation. The fundamentals of LLMs are introduced as pre-trained models, transformers, encoders, and decoders. Then, we review the challenges of LLM in cyber resilience and cyber defense methods to overcome these challenges. We applied the LLM into three case studies including two for email spam text classifications and one for cyber threat detection. We obtained higher accuracy including 96.67 %, 90.70 %, and 89.94 % from three case studies respectively. Then we compared our LLM with other traditional machine learning models. The results show the LLM has higher accuracy, precision, recall, and f1 score compared with other models. Finally, the future directions of LLM in cyber resilience are provided.}
}
@article{KOTILAINEN2025112333,
title = {Allocating distributed AI/ML applications to cloud–edge continuum based on privacy, regulatory, and ethical constraints},
journal = {Journal of Systems and Software},
volume = {222},
pages = {112333},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112333},
url = {https://www.sciencedirect.com/science/article/pii/S0164121225000019},
author = {Pyry Kotilainen and Niko Mäkitalo and Kari Systä and Ali Mehraj and Muhammad Waseem and Tommi Mikkonen and Juan Manuel Murillo},
keywords = {Internet of Things, IoT, Cloud computing, Model cards, Ethical orchestration, Orchestration, Artificial Intelligence, AI, Ethics, Compliance, Privacy, AI regulation},
abstract = {There is an increasing need for practitioners to address legislative and ethical issues in both the development and deployment of data-driven applications with AI/ML due to growing concerns and regulations, such as GDPR and the EU AI Act. Thus, the field needs a systematic framework for assessing risks and helping to stay compliant with regulations in designing and deploying software systems. Clear and concise descriptions of risks associated with each model and data source are needed to guide the design without acquiring deep knowledge of the regulations. In this paper, we propose a reference architecture for an ethical orchestration system that manages distributed AI/ML applications on the cloud–edge continuum and present a proof-of-concept implementation of the main ideas of the architecture. Our starting point is the methods already in use in the industry, such as model cards, and we extend the idea of model cards to data source cards and software component cards, which provide practitioners and the automated system with relevant information in actionable form. With the metadata card based orchestration system and information about the risk levels of the target infrastructure, the users can create deployments of distributed AI/ML systems that fulfill the regulatory and other requirements.}
}
@incollection{WILHELM2025359,
title = {Chapter 12 - Actions on Objectives},
editor = {Thomas Wilhelm},
booktitle = {Professional Penetration Testing (Third Edition)},
publisher = {Syngress},
edition = {Third Edition},
pages = {359-386},
year = {2025},
isbn = {978-0-443-26478-8},
doi = {https://doi.org/10.1016/B978-0-443-26478-8.00013-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443264788000133},
author = {Thomas Wilhelm},
keywords = {Infrastructure analysis, data exfiltration, high-value targets sensitive data artificial intelligence large language models chatGPT},
abstract = {The actions on objectives stage within the Cyber Kill Chain is where professional penetration testers exfiltrate data with the intent of finding sensitive information that satisfies the objectives of the engagement or allows them to pivot onto additional, more impactful systems. Exfiltration of data includes configuration information on installed programs and services, user information, administrative credentials, infrastructure data, and more. Artificial Intelligence is a new tool being leveraged by professional pentesters to assist in identifying optimum attack vectors. Still in its infancy, Large Language Models (LLMs) are being designed specifically for the pentesting community, using new datasets designed to train the language models. Although their accuracy is comparable to current vulnerability scanners and other similar tools, there is an expectation that in time, LLMs will surpass their capabilities.}
}
@article{HASSAN20243499,
title = {Navigating IoT Security: Insights into Architecture, Key Security Features, Attacks, Current Challenges and AI-Driven Solutions Shaping the Future of Connectivity},
journal = {Computers, Materials and Continua},
volume = {81},
number = {3},
pages = {3499-3559},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.057877},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008609},
author = {Ali Hassan and N. Nizam-Uddin and Asim Quddus and Syed Rizwan Hassan and Ateeq Ur Rehman and Salil Bharany},
keywords = {Internet of Things (IoT), artificial intelligence (AI), IoT architecture, security, attacks in IoT},
abstract = {Enhancing the interconnection of devices and systems, the Internet of Things (IoT) is a paradigm-shifting technology. IoT security concerns are still a substantial concern despite its extraordinary advantages. This paper offers an extensive review of IoT security, emphasizing the technology’s architecture, important security elements, and common attacks. It highlights how important artificial intelligence (AI) is to bolstering IoT security, especially when it comes to addressing risks at different IoT architecture layers. We systematically examined current mitigation strategies and their effectiveness, highlighting contemporary challenges with practical solutions and case studies from a range of industries, such as healthcare, smart homes, and industrial IoT. Our results highlight the importance of AI methods that are lightweight and improve security without compromising the limited resources of devices and computational capability. IoT networks can ensure operational efficiency and resilience by proactively identifying and countering security risks by utilizing machine learning capabilities. This study provides a comprehensive guide for practitioners and researchers aiming to understand the intricate connection between IoT, security challenges, and AI-driven solutions.}
}
@article{MESE2025111960,
title = {Large language models in methodological quality evaluation of radiomics research based on METRICS: ChatGPT vs NotebookLM vs radiologist},
journal = {European Journal of Radiology},
volume = {184},
pages = {111960},
year = {2025},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2025.111960},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X25000464},
author = {Ismail Mese and Burak Kocak},
keywords = {Artificial intelligence, Large language models, Machine learning, Radiomics, Texture analysis},
abstract = {Objectives
This study aimed to evaluate the effectiveness of large language models (LLM) in assessing the methodological quality of radiomics research, using METhodological RadiomICs Score (METRICS) tool.
Methods
This study included open access radiomic research articles published in 2024 across various journals and a preprint repository, all under the Creative Commons Attribution License. Each study was independently evaluated using METRICS by two LLMs, ChatGPT-4 and NotebookLM, and a consensus assessment performed by two radiologists with expertise in radiomics research.
Results
A total of 48 open access articles were included in this study. ChatGPT-4, NotebookLM, and human readers achieved median scores of 79.5 %, 61.6 %, and 69.0 %, respectively, with a statistically significant difference across these evaluations (p < 0.05). Pairwise comparisons indicated no statistically significant difference for NotebookLM vs human experts (p > 0.05), in contrast to other pairs (p < 0.05). Intraclass correlation coefficient (ICC) for ChatGPT-4 and human experts was 0.563 (95 % CI: 0.050–––0.795), corresponding to poor to good agreement. The ICC for ChatGPT-4 and NotebookLM and for human experts and NotebookLM were 0.391 (95 % CI: −0.031–––0.665) and 0.555 (95 % CI: 0.326–––0.723), respectively, indicating poor to moderate agreement. LLMs completed the tasks in a significantly shorter time (p < 0.05). In item-wise reliability analysis, ChatGPT-4 generally demonstrated higher consistency than NotebookLM.
Conclusion
LLMs hold promise for automatically evaluating the quality of radiomics research using METRICS, a new tool that is relatively more complex yet comprehensive compared to its counterparts. However, substantial improvements are needed for full alignment with human experts.}
}
@article{PARVEEN20251689,
title = {Digitalisation of catalytic processes for sustainable production of biobased chemicals and exploration of wider chemical space},
journal = {Catalysis Science & Technology},
volume = {15},
number = {6},
pages = {1689-1701},
year = {2025},
issn = {2044-4753},
doi = {https://doi.org/10.1039/d4cy01525h},
url = {https://www.sciencedirect.com/science/article/pii/S2044475325000759},
author = {Firdaus Parveen and Anna G. Slater},
abstract = {Global warming and the depletion of petroleum resources require immediate and focused attention, and there is a pressing need to accelerate progress. Digital approaches can be leveraged in these efforts, for example in exploring effective replacements for petrochemicals or effectively identifying molecules with better performance. One such potential replacement is lignocellulosic biomass: a sustainable feedstock for producing chemicals and fuels that does not compete with essential food supply. However, the inherent complexity of lignocellulosic biomass and the technical challenges in its transformation pose significant obstacles that require data-driven approaches to solve. Here, we use the catalytic transformation of lignocellulose to value added chemicals as a case study highlighting the critical role of digital technologies, including improved data integration, process optimization, and system-level decision-making in catalyst design, synthesis, and characterization. Data-driven approaches work hand-in-hand with technology: the integration of machine learning (ML) and artificial intelligence (AI) allows for efficient molecule design and optimization; coupling ML/AI with the use of flow chemistry and high-throughput synthesis techniques enhances scalability and sustainability. Together, these innovations can facilitate a more resilient and sustainable chemical industry, reducing dependency on fossil fuels and mitigating environmental impact.}
}
@incollection{KELLER20251,
title = {Chapter 1 - Taxonomy and anatomy},
editor = {Markus Keller},
booktitle = {The Science of Grapevines (Fourth Edition)},
publisher = {Academic Press},
edition = {Fourth Edition},
pages = {1-69},
year = {2025},
isbn = {978-0-443-33006-3},
doi = {https://doi.org/10.1016/B978-0-443-33006-3.00011-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443330063000115},
author = {Markus Keller},
keywords = {Bud, clone, cultivar, grape berry, leaf, root, rootstock, shoot, tendril, },
abstract = {The diverse species of grapevine belong to the botanical family Vitaceae, which includes mostly shrubs and woody lianas that climb using leaf-opposed tendrils. The vast majority of the thousands of grape cultivars belong to the species Vitis vinifera. Some of the other species are used as pest-tolerant rootstocks to which cultivars with desirable fruit properties are grafted. Cultivars are propagated asexually as cuttings so that each individual is a clone of its mother plant. Grapevines comprise vegetative organs (roots, trunk, cordon, shoots, leaves, and tendrils) and reproductive organs (clusters with flowers or berry fruit). All organs are interconnected through the vascular system, comprising the xylem for water and nutrient transport and the phloem for assimilate transport. The roots form the plant–soil interface, while the trunk, cordons, and shoots of a vine form its stem. The shoots carry the leaves, buds, tendrils, and clusters. Leaves are arranged in spiral phyllotaxy in juvenile vines and in alternate phyllotaxy in mature vines. Buds are young, compressed shoots embedded in leaf scales. Tendrils and clusters are modified shoots. After fertilization, the flower pistil develops into the berry fruit. The berry houses up to four seeds surrounded by the endocarp, the mesocarp, or flesh, and the exocarp, or skin.}
}
@article{MAXELOKOCHE20252508,
title = {AI adoption in crowdsourcing},
journal = {Procedia Computer Science},
volume = {253},
pages = {2508-2521},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.311},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925003199},
author = {John Michael {Maxel Okoche} and Marcia Mkansi and Godfrey Mugurusi and Wellington Chakuzira},
keywords = {Artificial Intelligence, Crowdsourcing, Crowdsourcing platforms, Systematic literature reviews},
abstract = {Despite significant technology advances especially in artificial intelligence (AI), crowdsourcing platforms still struggle with issues such as data overload and data quality problems, which hinder their full potential. This study addresses a critical gap in the literature how the integration of AI technologies in crowdsourcing could help overcome some these challenges. Using a systematic literature review of 77 journal papers, we identify the key limitations of current crowdsourcing platforms that included issues of quality control, scalability, bias, and privacy. Our research highlights how different forms of AI including from machine learning (ML), deep learning (DL), natural language processing (NLP), automatic speech recognition (ASR), and natural language generation techniques (NLG) can address the challenges most crowdsourcing platforms face. This paper offers knowledge to support the integration of AI first by identifying types of crowdsourcing applications, their challenges and the solutions AI offers for improvement of crowdsourcing.}
}
@article{YANG2025104482,
title = {Prediction of OCT contours of short-term response to anti-VEGF treatment for diabetic macular edema using generative adversarial networks},
journal = {Photodiagnosis and Photodynamic Therapy},
volume = {52},
pages = {104482},
year = {2025},
issn = {1572-1000},
doi = {https://doi.org/10.1016/j.pdpdt.2025.104482},
url = {https://www.sciencedirect.com/science/article/pii/S1572100025000122},
author = {Xueying Yang and Fabao Xu and Han Yu and Zhongwen Li and Xuechen Yu and Zhiwen Li and Li Zhang and Jie Liu and Shaopeng Wang and Shaopeng Liu and Jiaming Hong and Jianqiao Li},
keywords = {Diabetic macular edema, Generative adversarial networks, Deep neural networks, Anti-vascular endothelial growth factor, Optical coherence tomography},
abstract = {Diabetic macular edema (DME) stands as a leading cause for vision loss among the working-age population. Anti-vascular endothelial growth factor (VEGF) agents are currently recognized as the first-line treatment. However, a significant portion of patients remain insensitive to anti-VEGF, resulting in sustained visual impairment. Therefore, it's imperative to predict prognosis and formulate personalized therapeutic regimens. Generative adversarial networks (GANs) have demonstrated remarkably in forecasting prognosis of diseases, yet their performance is still constrained by the limited availability of real-world data and suboptimal image quality, which subsequently impacts the model's outputs. We endeavor to employ preoperative images along with postoperative OCT contours annotated and extracted via LabelMe and OpenCV to train the model in generating postoperative contours of critical OCT structures instead of previous whole retinal morphology, considerably alleviating the difficulty of output phase and diminishing the requisite quantity of training datasets. Our study reveals that the GAN could serve as an auxiliary instrument for ophthalmologists in determining the prognosis of individuals and screening patients with poor responses to anti-VEGF therapy.}
}
@article{CORDELLA2024101982,
title = {Regulating generative AI: The limits of technology-neutral regulatory frameworks. Insights from Italy's intervention on ChatGPT},
journal = {Government Information Quarterly},
volume = {41},
number = {4},
pages = {101982},
year = {2024},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2024.101982},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X24000741},
author = {Antonio Cordella and Francesco Gualdi},
keywords = {Artificial intelligence, Generative AI, Regulation, Technology-neutral regulatory framework, ChatGPT},
abstract = {Existing literature has predominantly concentrated on the legal, ethical, governance, political, and socioeconomic aspects of AI regulation, often relegating the technological dimension to the periphery, reflecting the design, use, and development of AI regulatory frameworks that are technology-neutral. The emergence and widespread use of generative AI models present new challenges for public regulators aiming at implementing effective regulatory interventions. Generative AI operates on distinctive technological properties that require a comprehensive understanding prior to the deployment of pertinent regulation. This paper focuses on the recent case of the suspension of ChatGPT in Italy to explore the impact the specific technological fabric of generative AI has on the effectiveness of technology-neutral regulation. By drawing on the findings of an exploratory case study, this paper contributes to the understanding of the tensions between the specific technological features of generative AI and the effectiveness of a technology-neutral regulatory framework. The paper offers relevant implications to practice arguing that until this tension is effectively addressed, public regulatory interventions are likely to underachieve their intended objectives.}
}
@article{CIPOLOTTI202587,
title = {Cognitive control & the anterior cingulate cortex: Necessity & coherence},
journal = {Cortex},
volume = {182},
pages = {87-99},
year = {2025},
note = {Outside the box: A celebration of Sergio Della Sala’s contribution to neuropsychology and science dissemination.},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2024.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S001094522400323X},
author = {Lisa Cipolotti and Joe Mole and James K. Ruffle and Amy Nelson and Robert Gray and Parashkev Nachev},
keywords = {Cognitive control, Conflict detection, Stroop, Executive functions, Network lesion-deficit mapping, Conceptual analysis, Anterior cingulate cortex},
abstract = {Influential theories of complex behaviour invoke the notion of cognitive control modulated by conflict between counterfactual actions. Medial frontal cortex, notably the anterior cingulate cortex, has been variously posited as critical to such conflict detection, resolution, or monitoring, largely based on correlative data from functional imaging. Examining performance on the most widely used “conflict” task—Stroop—in a large cohort of patients with focal brain injury (N = 176), we compare anatomical patterns of lesion-inferred neural substrate dependence to those derived from functional imaging, meta-analytically summarised. Our results show that whereas performance is sensitive to the integrity of left lateral frontal regions implicated by functional imaging, it does not depend on medial frontal cortex, despite sampling adequate to reveal robust medial effects in the context of phonemic fluency. We suggest that medial frontal cortex is not critically invoked by Stroop and proceed to review the conceptual grounds for rejecting the core notion of conflict-driven cognitive control.}
}
@article{OZEN2025100679,
title = {Extracting chemical food safety hazards from the scientific literature automatically using large language models},
journal = {Applied Food Research},
volume = {5},
number = {1},
pages = {100679},
year = {2025},
issn = {2772-5022},
doi = {https://doi.org/10.1016/j.afres.2024.100679},
url = {https://www.sciencedirect.com/science/article/pii/S2772502224002890},
author = {Neris Özen and Wenjuan Mu and Esther D. {van Asselt} and Leonieke M. {van den Bulk}},
keywords = {Chemical contamination, Food safety, Information extraction, Prompt engineering, Natural language processing, Artificial intelligence},
abstract = {The number of scientific articles published in the domain of food safety has consistently been increasing over the last few decades. It has therefore become unfeasible for food safety experts to read all relevant literature related to food safety and the occurrence of hazards in the food chain. However, it is important that food safety experts are aware of the newest findings and can access this information in an easy and concise way. In this study, an approach is presented to automate the extraction of chemical hazards from the scientific literature through large language models. The large language model was used out-of-the-box and applied on scientific abstracts; no extra training of the models or a large computing cluster was required. Three different styles of prompting the model were tested to assess which was the most optimal for the task at hand. The prompts were optimized with two validation foods (leafy greens and shellfish) and the final performance of the best prompt was evaluated using three test foods (dairy, maize and salmon). The specific wording of the prompt was found to have a considerable effect on the results. A prompt breaking the task down into smaller steps performed best overall. This prompt reached an average accuracy of 93 % and contained many chemical contaminants already included in food monitoring programs, validating the successful retrieval of relevant hazards for the food safety domain. The results showcase how valuable large language models can be for the task of automatic information extraction from the scientific literature.}
}
@article{ALI2025102922,
title = {AI-driven fusion with cybersecurity: Exploring current trends, advanced techniques, future directions, and policy implications for evolving paradigms– A comprehensive review},
journal = {Information Fusion},
volume = {118},
pages = {102922},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102922},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524007000},
author = {Sijjad Ali and Jia Wang and Victor Chung Ming Leung},
keywords = {AI-driven threat detection, Machine learning in cybersecurity, Automated threat response, Behavioral analysis, Adversarial attacks},
abstract = {The fusion of Artificial Intelligence (AI) into cybersecurity has brought transformative advancements in protecting digital infrastructures from evolving cyber threats. This comprehensive review explores current AI-driven cybersecurity methodologies, emphasizing the capabilities of AI technologies — such as machine learning, deep learning, and natural language processing (NLP) — to enhance threat detection, behavioral analysis, automated response systems, and threat intelligence. The paper discusses AI’s ability to identify advanced persistent threats, zero-day vulnerabilities, and phishing attacks with improved accuracy and adaptability. Additionally, we examine emerging trends such as AI’s fusion with blockchain, the application of quantum computing, and the increasing role of self-healing systems in enhancing cybersecurity resilience. Challenges such as adversarial attacks, ethical concerns, and data privacy issues are critically analyzed, along with AI’s future potential in real-time threat management and its implications for policy and organizational frameworks. By summarizing recent advancements and identifying gaps in existing solutions, this review sets the stage for future AI-enhanced cybersecurity developments, offering insights into how AI can lead to more proactive and adaptive security strategies.}
}
@article{KO2025259,
title = {Security Strategy of Digital Medical Contents Based on Blockchain in Generative AI Model},
journal = {Computers, Materials and Continua},
volume = {82},
number = {1},
pages = {259-278},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.057257},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825000426},
author = {Hoon Ko and Marek R. Ogiela},
keywords = {Digital medical content, medical diagnostic visualization, security analysis, generative AI, blockchain, vulnerability, pattern recognition},
abstract = {This study presents an innovative approach to enhancing the security of visual medical data in the generative AI environment through the integration of blockchain technology. By combining the strengths of blockchain and generative AI, the research team aimed to address the timely challenge of safeguarding visual medical content. The participating researchers conducted a comprehensive analysis, examining the vulnerabilities of medical AI services, personal information protection issues, and overall security weaknesses. This multifaceted exploration led to an in-depth evaluation of the model’s performance and security. Notably, the correlation between accuracy, detection rate, and error rate was scrutinized. This analysis revealed insights into the model’s strengths and limitations, while the consideration of standard deviation shed light on the model’s stability and performance variability. The study proposed practical improvements, emphasizing the reduction of false negatives to enhance detection rate and leveraging blockchain technology to ensure visual data integrity in medical applications. Applying blockchain to generative AI-created medical content addresses key personal information protection issues. By utilizing the distributed ledger system of blockchain, the research team aimed to protect the privacy and integrity of medical data especially medical images. This approach not only enhances security but also enables transparent and tamper-proof record-keeping. Additionally, the use of generative AI models ensures the creation of novel medical content without compromising personal information, further safeguarding patient privacy. In conclusion, this study showcases the potential of blockchain-based solutions in the medical field, particularly in securing sensitive medical data and protecting patient privacy. The proposed approach, combining blockchain and generative AI, offers a promising direction toward more robust and secure medical content management. Further research and advancements in this area will undoubtedly contribute to the development of robust and privacy-preserving healthcare systems, and visual diagnostic systems.}
}
@article{LI2025101616,
title = {Leveraging patent classification based on deep learning: The case study on smart cities and industrial Internet of Things},
journal = {Journal of Informetrics},
volume = {19},
number = {1},
pages = {101616},
year = {2025},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2024.101616},
url = {https://www.sciencedirect.com/science/article/pii/S1751157724001287},
author = {Munan Li and Liang Wang},
keywords = {Technology-field resolution, Patent classification, Deep learning, Semantic analysis, Loss function, Smart cities, Industrial Internet of Things, GPT-4},
abstract = {With the trends of technology convergence and technology interdisciplinarity, technology-field (TF) resolution and classification of patents have gradually been challenged. Whether for patent applicants or for patent examiners, more precisely labeling the TF for a certain patent is important for technological searches. However, determining the TF of a patent may be difficult and may even involve the strategic behavior of patenting, which can cause noise in patent classification systems (PCSs). In addition, some specific patents could contain more TFs than claimed or be assigned questionable IPC codes; subsequently, in a regular search for technology/patents, information could be missed. Considering the advantages of deep learning compared with traditional machine learning algorithms in areas such as natural language processing (NLP), text classification and text sentiment analysis, this paper investigates several popular deep learning models and proposes a large-scale multilabel regression (MLR) model to handle specific patent analyses under situations of small sample learning. To verify the proposed MLR model for patent classification, the case study on smart cities and industrial Internet of Things (IIoT) is conducted. The MLR experiments on the TF resolution of smart cities and IIoT have yielded moderate results compared with those of the latest patent classification studies, which also rely on deep learning and the large language models (LLMs), which include RCNN, Bi-LSTM, BERT and GPT-4 etc. Therefore, the proposed MLR model with a customized loss function could be moderately effective for patent classification within a specific technology theme, could have implications for patent classification and the TF resolution of patents, and could further enrich methodologies for patent mining and informetrics based on artificial intelligence (AI).}
}
@article{SEGHIER2025103173,
title = {From BIM to computational BIM: A systematic review of visual programming application in building research},
journal = {Ain Shams Engineering Journal},
volume = {16},
number = {1},
pages = {103173},
year = {2025},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2024.103173},
url = {https://www.sciencedirect.com/science/article/pii/S2090447924005549},
author = {Taki Eddine Seghier and Chavanont Khosakitchalert and Ziwen Liu and Chukwuka Christian Ohueri and Yaik-Wah Lim and Ahmad Fahmi {Bin Zainazlan}},
keywords = {Computational design, Building information modelling, Algorithm, Generative design, Automation, Parametric},
abstract = {The architecture, engineering, construction, and operation (AECO) industry has embraced the combination of building information modelling (BIM) and computational algorithms to advance digital transformations. Computational BIM has enabled the development and testing of various BIM-based solutions for the building industry. This study conducted a systematic review of the application of computational BIM through visual programming (VP) in building research. The study employed a hybrid approach of bibliometrics and content analyses using seventy-nine publications filtered from Scopus and Web of Science databases. The bibliometric analysis identified publication frequency trends, journal distributions, country distributions, eminent authors, co-authorship networks, and keyword co-occurrence networks. The content analysis identified six key research themes and four major methodological roles of computational BIM in building research. A research framework is proposed to summarize and articulate the state of the art of computational BIM application in building research, research gaps, and future research directions.}
}
@article{KUANG2025126666,
title = {NtNDet: Hardware Trojan detection based on pre-trained language models},
journal = {Expert Systems with Applications},
volume = {271},
pages = {126666},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126666},
url = {https://www.sciencedirect.com/science/article/pii/S095741742500288X},
author = {Shijie Kuang and Zhe Quan and Guoqi Xie and Xiaomin Cai and Xiaoqian Chen and Keqin Li},
keywords = {Gate-level netlists, Hardware Trojan detection, Large language model, Netlist-to-natural-language, Transfer learning},
abstract = {Hardware Trojans (HTs) are malicious modifications embedded in Integrated Circuits (ICs) that pose a significant threat to security. The concealment of HTs and the complexity of IC manufacturing make them difficult to detect. An effective solution is identifying HTs at the gate level through machine learning techniques. However, current methods primarily depend on end-to-end training, which fails to fully utilize the advantages of large-scale pre-trained models and transfer learning. Additionally, they do not take advantage of the extensive background knowledge available in massive datasets. This study proposes an HT detection approach based on large-scale pre-trained NLP models. We propose a novel approach named NtNDet, which includes a method called Netlist-to-Natural-Language (NtN) for converting gate-level netlists into a natural language format suitable for Natural Language Processing (NLP) models. We apply the self-attention mechanism of Transformer to model complex dependencies within the netlist. This is the first application of large-scale pre-trained models for gate-level netlists HT detection, promoting the use of pre-trained models in the security field. Experiments on the Trust-Hub, TRIT-TC, and TRIT-TS benchmarks demonstrate that our approach outperforms existing HT detection methods. The precision increased by at least 5.27%, The True Positive Rate (TPR) by 3.06%, the True Negative Rate (TNR) by 0.01%, and the F1 score increased by about 3.17%, setting a new state-of-the-art in HT detection.}
}
@article{WANG2025104057,
title = {A contracted container-based code component collaboration model with reusable but invisible right management},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104057},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104057},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004163},
author = {Wei Wang and Zhenping Xie},
keywords = {Code collaboration, Right management, Blockchain, Smart contract, Containerization, Privacy protection},
abstract = {Existing methodologies for the code collaboration are facing challenges such as leakage of privacy data and insecure centralized management. To alleviate these challenges, we propose a contracted container-based code component collaboration model. Firstly, a specific blockchain is developed as a computational network that supports the storage and verification of sensitive data, which integrates an off-chain file system and encryption algorithms to realize the traceability and version control of privacy data. Secondly, the containerization and smart contract technology are introduced to enhance access control and the secure management and collaboration of code components. Thirdly, a protocol is designed to ensure the high reliability of cross-node information communication. Finally, the results of the prototype experiment demonstrate that the model effectively resists common attacks, meets critical security criteria, and maintains stable performance in the collaboration process. Moreover, compared to the state-of-the-art research, our model implements more valuable functionality characteristics, design goals and security attributes in the code component collaboration practice.}
}
@article{KO2025123949,
title = {Prominence of corporate science in quantum computing research},
journal = {Technological Forecasting and Social Change},
volume = {212},
pages = {123949},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123949},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524007479},
author = {Hyunmin Ko and Seokbeom Kwon},
keywords = {Corporate research, Complementary asset, Quantum computing, Research impact, Resource-based view},
abstract = {In this study, we empirically examined the growing prominence of corporate science and its influence on quantum computing research. An analysis of approximately 30,000 research papers on quantum computing revealed that firms are increasingly publishing scientifically impactful research compared to noncorporate entities in this field. Additional analyses of text data from research article abstracts using topic modeling indicated that corporate research is concentrated on prominent topics such as quantum computing for Machine Learning/Artificial Intelligence and quantum algorithms, attracting increasing scholarly attention. In contrast, non-corporate research has been relatively dispersed across various topics. Drawing on the Resource-Based View and insights from an interview with a field expert, we theorize that with secured access to unique and rare resources for quantum computing research, corporate researchers are better positioned to experiment and iterate on novel ideas than their noncorporate counterparts. The publication of these research outcomes provides strategic advantages without compromising their appropriability. Our findings have implications for science policymakers and corporate innovation strategists, contributing to the literature on the role of corporate research in scientific progress.}
}
@article{MONFORTLANZAS20251440,
title = {Modeling omics dose-response at the pathway level with DoseRider},
journal = {Computational and Structural Biotechnology Journal},
volume = {27},
pages = {1440-1448},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2025.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2001037025001266},
author = {Pablo Monfort-Lanzas and Johanna M. Gostner and Hubert Hackl},
keywords = {Benchmark dose, Dose-response modeling, Mixed models, Multi-omics, System biology, Trend change dose, Toxicology},
abstract = {The generation of omics data sets has become an important approach in modern pharmacological and toxicological research as it can provide mechanistic and quantitative information on a large scale. Analyses of these data frequently revealed a non-linear dose-response relationship underscoring the importance of the modeling process to infer biological exposure limits. A number of tools have been developed for dose-response modeling and various thresholds have been defined as a quantitative representation of the effect of a substance, such as effective concentrations or benchmark doses (BMD). Here we present DoseRider an easy-to-use web application and a companion R package for linear and non-linear dose-response modeling and assessment of BMD at the level of biological pathways or signatures using generalized mixed effect models. This approach allows to analyze custom or provided multi-omics data such as RNA sequencing or metabolomics data and its annotation of a collection of pathways and gene sets from various species. Moreover, we introduce the concept of the trend change doses (TCDs) as a numerical descriptor of effects derived from complex dose-response curves. The usability of DoseRider was demonstrated by analyses of RNA sequencing data of bisphenol AF (BPAF) treatment of a human breast cancer cell line (MCF-7) at 8 different concentrations using gene sets for chemical and genetic perturbations (MSigDB). The BMD for BPAF and a set of genes upregulated by estrogen in breast cancer was 0.2 µM (95 %-CI 0.1–0.5 µM) and the lowest TCD (TCD1) was 0.003 µM (95 %-CI 0.0006–0.01 µM). The comprehensive presentation of the results underlines the suitability of the system for pharmacogenomics, toxicogenomics, and applications beyond.}
}
@article{ZHANG2025112965,
title = {A survey on Deep Learning in Edge–Cloud Collaboration: Model partitioning, privacy preservation, and prospects},
journal = {Knowledge-Based Systems},
volume = {310},
pages = {112965},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.112965},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125000139},
author = {Xichen Zhang and Roozbeh Razavi-Far and Haruna Isah and Amir David and Griffin Higgins and Michael Zhang},
keywords = {Deep Learning, Edge–Cloud Collaboration, Model partitioning, Privacy preservation},
abstract = {Recently, the rapid advancements of AI technologies and mobile computing have led to the growing prevalence of smart devices and rising demands for on-device Deep Learning applications. Given this context, the Edge–Cloud Collaboration System has attracted considerable attention. This survey article focuses on a typical architecture in such a system, called Partitioned Deep Neural Network. Concretely, a complex Deep Learning model is partitioned into two segments. The shallow part, which serves as the feature extractor, is deployed on the edge device, while the remaining layers are processed on the cloud server for result inferences. We provide a comprehensive overview of Partitioning Deep Neural Networks for the Edge–Cloud Collaboration System, including model split, experimental settings, threat models, and assessment metrics. Then, we conduct a systematic summary of state-of-the-art privacy-preserving technologies, providing detailed comparisons of their advantages and limitations in practice. Finally, we highlight the main open challenges and propose intriguing research problems as future directions from various aspects, including attacking settings, novel application scenarios, evaluation measurements, and the applications and potential influences of Large Language Models in related domains.}
}
@article{DIRO2025103960,
title = {Workplace security and privacy implications in the GenAI age: A survey},
journal = {Journal of Information Security and Applications},
volume = {89},
pages = {103960},
year = {2025},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2024.103960},
url = {https://www.sciencedirect.com/science/article/pii/S221421262400262X},
author = {Abebe Diro and Shahriar Kaisar and Akanksha Saini and Samar Fatima and Pham Cong Hiep and Fikadu Erba},
keywords = {Cybersecurity, Generative AI, ChatGPT, Bard, Privacy, Security, Ethics},
abstract = {Generative Artificial Intelligence (GenAI) is transforming the workplace, but its adoption introduces significant risks to data security and privacy. Recent incidents underscore the urgency of addressing these issues. This comprehensive survey investigates the implications of GenAI integration in workplaces, focusing on its impact on organizational operations and security. We analyze vulnerabilities within GenAI systems, threats they face, and repercussions of AI-driven workplace monitoring. By examining diverse attack vectors like model attacks and automated cyberattacks, we expose their potential to undermine data integrity and privacy. Unlike previous works, this survey specifically focuses on the security and privacy implications of GenAI within workplace settings, addressing issues like employee monitoring, deepfakes, and regulatory compliance. We delve into emerging threats during model training and usage phases, proposing countermeasures such as differential privacy for training data and robust authentication for access control. Additionally, we provide a comprehensive analysis of evolving regulatory frameworks governing AI tools globally. Based on our comprehensive analysis, we propose targeted recommendations for future research and policy-making to promote responsible and secure adoption of GenAI in the workplace, such as incentivizing the development of explainable AI (XAI) and establishing clear guidelines for ethical data usage. This survey equips stakeholders with a comprehensive understanding of GenAI’s complex workplace landscape, empowering them to harness its benefits responsibly while mitigating risks.}
}
@article{DISSANAYAKE2025104098,
title = {The state-of-the-art of crowdsourcing systems: A computational literature review and future research agenda using a text analytics approach},
journal = {Information & Management},
volume = {62},
number = {2},
pages = {104098},
year = {2025},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2025.104098},
url = {https://www.sciencedirect.com/science/article/pii/S0378720625000011},
author = {Indika Dissanayake and Sridhar P. Nerur and Roman Lukyanenko and Minoo Modaresnezhad},
keywords = {Crowdsourcing, Crowdwork, Literature review, Topic modeling, Text analytics, LDA, BERT},
abstract = {Crowdsourcing effectively harnesses diverse skills and perspectives of crowds beyond organizational, geographical, and cultural boundaries. Organizations are gaining invaluable insights through crowdsourcing across diverse domains. This study reviews the growing academic literature on crowdsourcing using advanced topic modeling, an approach to unraveling key themes latent in the literature. Following a systems approach, we adopted inter- and intra-systems perspectives to identify distinct crowdsourcing models and their interrelated components based on a text analysis of the crowdsourcing literature. The paper elucidates the intellectual foundations of crowdsourcing as represented in the literature and offers suggestions for pursuing research that will extend its conceptual boundaries.}
}
@article{2025100044,
title = {17th Clinical Trials on Alzheimer's Disease (CTAD) Madrid, Spain, October 29 - November 1, 2024: Conference proceedings},
journal = {The Journal of Prevention of Alzheimer's Disease},
volume = {12},
number = {1, Supplement },
pages = {100044},
year = {2025},
note = {17th Clinical Trials on Alzheimer’s Disease},
issn = {2274-5807},
doi = {https://doi.org/10.1016/j.tjpad.2024.100044},
url = {https://www.sciencedirect.com/science/article/pii/S2274580724006368}
}
@article{YU2025129238,
title = {CrossCode2Vec: A unified representation across source and binary functions for code similarity detection},
journal = {Neurocomputing},
volume = {620},
pages = {129238},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129238},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224020095},
author = {Gaoqing Yu and Jing An and Jiuyang Lyu and Wei Huang and Wenqing Fan and Yixuan Cheng and Aina Sui},
keywords = {Code Similarity Detection, Representation learning, Cross-modal code matching},
abstract = {Code similarity detection identifies code by analyzing similarities in syntax, semantics, and structure, which includes types of tasks: source-to-source, binary-to-binary, and source-to-binary. Due to encoding and representation disparities between source and binary code, existing methods have mainly focused on individual tasks, without providing a universal solution. Additionally, current source-to-binary tasks only achieve one-to-one matching between source code and binary functions, neglecting the one-to-many relationship inherent between source code and its cross-compiled binaries. In this paper, we propose CrossCode2Vec, a unified framework for representing code in both source and binary functions, which aims to bridge the gap in original coding features and provide a standardized similarity measurement across three code similarity detection tasks. For source code and its corresponding compiled binary, we first design an enhanced Abstract Path Context data preprocessing method, construct an abstract syntax tree (AST) from both source code functions and decompiled binary functions, and implement the function embedding followed by the pre-trained Word2vec model. Then we propose a task-specific data sampling strategy. We establish a one-to-one correspondence between source and binary functions through symbol tables and create a one-to-many relationship between source functions and their cross-compiled binaries based on sampling rules. Finally, we employ a hierarchical LSTM-attention network to facilitate the representation and similarity measurement of functions. We conduct both extrinsic and intrinsic evaluations to confirm the effectiveness of CrossCode2Vec in code representation and code similarity tasks, validating its superiority in model architecture and data processing methods. CrossCode2Vec demonstrates stable and exceptional performance across multiple experiments, reinforcing its ability to bridge the gap between source and binary code representations while effectively measuring their similarities.}
}
@article{LIM2025106095,
title = {Biometric data landscape in Southeast Asia: Challenges and opportunities for effective regulation},
journal = {Computer Law & Security Review},
volume = {56},
pages = {106095},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106095},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924001602},
author = {Abigail Chiu Mei Lim and Lynnette Hui Xian Ng and Araz Taeihagh},
keywords = {Biometrics, Biometrics regulation, ASEAN, Data protection, Southeast Asia},
abstract = {Technology evolves at a breakneck pace. As a result, legislatures are often unable to enact laws that can keep pace with technological changes. The dissonance between the state of the law and the state of technology intensifies with respect to biometric data because the purposes of biometric data use evolve, the types of biometric data expand, and its collection, processing and use have shifted from conventional biometric systems to online platforms. This dissonance is exemplified in the Association of Southeast Asian Nations, where no regional legal instrument regulates biometric data even though governmental agencies, private entities and social media platforms actively employ biometric data and artificial intelligence systems. At national level, only five countries, Malaysia, Singapore, Indonesia, Thailand, and the Philippines, have enacted omnibus data protection legislations that afford some protection to biometric data and govern its use. This article analyses these data protection legislations and assesses their suitability in protecting and governing biometric data in the contemporary era. It identifies common trends amongst the five countries and concludes that more needs to be done to protect biometric data and rights of data subjects. Thereafter, it makes recommendations for changes to improve the state of biometric regulation in Southeast Asia.}
}
@article{ALLALCHERIF2025123906,
title = {Stepping out of the innovation race to embrace outnovation: Fostering well-being and responsible consumption through sustainability, simplicity, authenticity, and nostalgia},
journal = {Technological Forecasting and Social Change},
volume = {210},
pages = {123906},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123906},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524007042},
author = {Oihab Allal-Chérif and José Fernando Gallego-Nicholls and Agustin Carrilero-Castillo and Francisco Javier {Sendra Garcia}},
keywords = {Outnovation, Innovation, Sustainability, Simplicity, Authenticity, Nostalgia, Excellence},
abstract = {This article theorizes and characterizes the concept of “outnovation” as an alternative or a complement to innovation within the framework of grounded theory. Outnovation consists of stepping out of the unrelenting innovation race and removing all unnecessary innovations from a product, focusing instead on sustainability, simplicity, authenticity, and nostalgia. After presenting the dangers and limits of innovative strategies and disasters resulting from poorly mastered innovations, the research studies four different cases, which examples demonstrate that not innovating or suppressing innovations is not synonymous with bankruptcy. At a time when customers are looking for more sustainable products and when many economists advocate degrowth and less unbridled consumption, companies are looking for new forms of differentiation and value creation. Outnovating is a way of getting out of the vicious circle of endless innovation and meeting United Nations' Sustainable Development Goals.}
}
@article{FERNANDO2025104896,
title = {Food-derived bioactive peptides: The gateway to reach the full potential of food proteins for human health},
journal = {Trends in Food Science & Technology},
volume = {157},
pages = {104896},
year = {2025},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2025.104896},
url = {https://www.sciencedirect.com/science/article/pii/S0924224425000329},
author = {Ilekuttige Priyan Shanura Fernando and Jianping Wu},
keywords = {Bioactive peptides, Functional foods, Enzymatic hydrolysis, Peptide stability, Food byproducts},
abstract = {Background
The growing emphasis on preventative healthcare has driven interest in food-derived bioactive peptides (BAPs) for the treatment of a wide range of diseases. These molecules can potentially promote human health through dietary interventions; however, their full capabilities have not yet been fully realized. Recent innovations and ongoing research are continuously expanding the use of BAPs.
Scope and approach
This review explores the preventive potential of BAPs against a wide range of diseases, and their sources, extraction methods, and applications. It critically analyzes preclinical and clinical trial outcomes while acknowledging existing challenges associated with BAPs research and commercialization.
Key findings and conclusions
Food-derived BAPs show promise as functional food ingredients, nutraceuticals, and therapeutic agents with diverse bioactivities including antihypertensive, antioxidant, antimicrobial, and immunomodulatory effects. Challenges in BAP research include sourcing, yield, stability, bioaccessibility, bioavailability, and regulatory hurdles. Advancements in peptide discovery through peptidomics, metagenomics, and genome mining, with preparation methods and AI-powered prediction tools offer potential solutions. Emerging technologies such as quantitative systems, pharmacology models, virtual patients, and digital twins may improve the efficiency of predicting drug efficacy and safety potentially facilitating the translation of BAPs from laboratory research to clinical applications. These advances can pave the way for developing personalized nutrition and precision medicine, offering tailored therapeutic strategies to promote human health.}
}
@article{SAINI2025127440,
title = {A Comprehensive review on technological breakthroughs in precision agriculture: IoT and emerging data analytics},
journal = {European Journal of Agronomy},
volume = {163},
pages = {127440},
year = {2025},
issn = {1161-0301},
doi = {https://doi.org/10.1016/j.eja.2024.127440},
url = {https://www.sciencedirect.com/science/article/pii/S1161030124003617},
author = {Anil Kumar Saini and Anshul Kumar Yadav and  Dhiraj},
keywords = {Precision Agriculture, Internet of Things, Artificial Intelligence, Communication technology},
abstract = {Rapid population expansion has led to a corresponding rise in the demand for sustenance. Researchers have found that traditional agricultural practices are insufficient to meet the demands of commodities, and their inefficiency poses the most pressing obstacle to addressing the growing global food demand. Precision agriculture (PA) is an advanced hierarchy farming system supported by multidisciplinary technologies such as specialized sensors, communication protocols, algorithms, and management tools, helping mitigate the problems of conventional farming by ensuring maximum production and minimum wastage. Given the rapid evolution of the aforementioned multidisciplinary technologies, this review paper analyzed 24337 research documents from 1938 to April 2024 using bibliographical software from the Scopus dataset. Internet of Things (IoT), Agriculture Robots (AR), and Artificial Intelligence (AI) are currently driving ongoing research, with frequency occurrences of 12.245, 8.259, and 7.791, highlighting the trend towards interconnected farming systems and data-driven automated systems. Bibliographical evidence indicates the current utilization of AI, AR, and IoT for accurate assessments like crop yield prediction, disease and weed detection, and soil analysis. Additionally, China is the most productive country in terms of publication, while the United States leads in terms of patents. This review paper also explores emerging trends that could guide future research, including blockchain technology, big data analysis, computing paradigms, and drone technology. Subsequently, a PA framework has been suggested to facilitate innovation in this field, followed by the open issues, highlighting the ongoing concerns related to insufficient infrastructure, integration, cost, and security measures, with the aim to engage all stakeholders.}
}
@article{MOHAMMED2025104215,
title = {Investigation on datasets toward intelligent intrusion detection systems for Intra and inter-UAVs communication systems},
journal = {Computers & Security},
volume = {150},
pages = {104215},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104215},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005212},
author = {Ahmad Burhan Mohammed and Lamia Chaari Fourati},
keywords = {Unmanned aerial vehicles, Intrusion detection systems, Artificial intelligence, Datasets, UAV security, Intra/inter UAV communication system},
abstract = {UAVs, commonly known as drones, are increasingly utilized in various fields such as military operations, surveillance, agriculture, and delivery services. Given their expanding roles, several critical reasons underline the necessity for security and trustworthiness including sensitive operations, and Public Safety and Privacy. Effective and secure communication channels are fundamental to the reliability and efficiency of UAVs. This involves protecting data transmission from various threats and ensuring that the communication protocols are robust against attacks. The security of communication systems directly impacts the overall trustworthiness of UAV operations, making it a critical area of focus for researchers and developers in this field. Intrusion Detection Systems (IDS) are vital components of a resilient architecture, essential for ensuring the security of communication systems. These systems function as diligent sentinels, continuously monitoring network traffic for signs of malicious behavior or rule violations. Additionally, IDS plays a crucial role in protecting communication networks by promptly identifying and responding to potential threats. Integrating IDS with other security measures enables organizations to significantly enhance their overall security posture. Accordingly, several recent studies have proposed advanced approaches using artificial intelligence to enhance the security and trustworthiness of UAV, as well as to mitigate cyberattacks in real time. However, the efficiency of the proposed AI-based approaches relies on the learning phase, which is strongly correlated with the quality of the used datasets. In this context, the objective of this article is to offer a thorough investigation concerning this topic. Certainly, the development and testing of effective AI-based IDS for UAVs require access to a diverse range of datasets that precisely capture the various security challenges and potential attack scenarios. In light of this necessity, the current study undertakes a comprehensive examination of UAV communication systems and their associated networking architectures, both centralized and decentralized. This study specifically focused on Intra and Inter UAV communication systems and utilized relevant IDS datasets. Additionally, it delves into the realm of open datasets that are pertinent to intelligent intrusion detection systems. More specifically, this paper introduces a novel taxonomy designed to categorize datasets relevant to IDS within the UAV context. Furthermore, it furnishes a guide along with recommendations for the selection of appropriate datasets based on predetermined scenarios.}
}
@article{JIANG2025103977,
title = {Reversible source-aware natural language watermarking via customized lexical substitution},
journal = {Information Processing & Management},
volume = {62},
number = {2},
pages = {103977},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103977},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324003364},
author = {Ziyu Jiang and Hongxia Wang and Zhenhao Shi and Run Jiao},
keywords = {Natural language watermarking, Reversible text watermarking, Lexical substitution, Prompt learning},
abstract = {Current natural language watermarking (NLW) methods generate suitable watermark words based on local context using pre-trained models (PLMs), minimizing semantic loss in watermarked text. However, these methods still exhibit some limitations. Specifically, there is room for improvement on substitutes quality and watermark imperceptibility since they integrate off-the-shelf lexical substitution (LS) models, which are not specifically tailored for watermarking algorithms. They make strict synchronization constraints to generate identical substitutes list from the original and the watermarked text, and therefore precludes consideration of some high-quality substitutes, which curtails the watermark capacity. Additionally, the local context changes via watermarking embedding, and these methods cannot losslessly recover the original text, limiting the application of NLW to high-precision scenarios such as government documents, military, and medical applications. To address these issues, we propose a reversible source-aware NLW approach, which performs proactive mining to identify potential reversible watermark positions by virtue of a PLM and subsequently embeds the watermark into the text via source-aware LS. Also, we have designed a novel LS algorithm tailored for NLW to enhance the imperceptibility and textual fidelity of watermarked content. Experiments validate the efficiency of our LS method in generating the most suitable substitutes and verifies that our NLW approach achieves complete reversibility while enhancing watermark capacity and textual fidelity compared to prior arts.}
}
@article{2025101636,
title = {Full Issue PDF},
journal = {JACC: Advances},
volume = {4},
number = {2},
pages = {101636},
year = {2025},
issn = {2772-963X},
doi = {https://doi.org/10.1016/S2772-963X(25)00053-5},
url = {https://www.sciencedirect.com/science/article/pii/S2772963X25000535}
}
@article{YAN2025184,
title = {Digital Twin Enabling Technologies for Advancing Road Engineering and Lifecycle Applications},
journal = {Engineering},
volume = {44},
pages = {184-206},
year = {2025},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2024.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S2095809924007343},
author = {Yu Yan and Lei Ni and Lijun Sun and Ying Wang and Jianing Zhou},
keywords = {Digital twin, Road infrastructure, Enabling technology, Life cycle},
abstract = {Road infrastructure is facing significant digitalization challenges within the context of new infrastructure construction in China and worldwide. Among the advanced digital technologies, digital twin (DT) has gained prominence across various engineering sectors, including the manufacturing and construction industries. Specifically, road engineering has demonstrated a growing interest in DT and has achieved promising results in DT-related applications over the past several years. This paper systematically introduces the development of DT and examines its current state in road engineering by reviewing research articles on DT-enabling technologies, such as model creation, condition sensing, data processing, and interaction, as well as its applications throughout the lifecycle of road infrastructure. The findings indicate that research has primarily focused on data perception and virtual model creation, while real-time data processing and interaction between physical and virtual models remain underexplored. DT in road engineering has been predominantly applied during the operation and maintenance phases, with limited attention given to the construction and demolition phases. Future efforts should focus on establishing uniform standards, developing innovative perception and data interaction techniques, optimizing development costs, and expanding the scope of lifecycle applications to facilitate the digital transformation of road engineering. This review provides a comprehensive overview of state-of-the-art advancements in this field and paves the way for leveraging DT in road infrastructure lifecycle management.}
}
@article{XIAO202560,
title = {Network for knowledge Organization (NEKO): An AI knowledge mining workflow for synthetic biology research},
journal = {Metabolic Engineering},
volume = {87},
pages = {60-67},
year = {2025},
issn = {1096-7176},
doi = {https://doi.org/10.1016/j.ymben.2024.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1096717624001484},
author = {Zhengyang Xiao and Himadri B. Pakrasi and Yixin Chen and Yinjie J. Tang},
keywords = {Foundation model, Large language model, Qwen, Retrieval augmented generation, Knowledge graph},
abstract = {Large language models (LLMs) can complete general scientific question-and-answer, yet they are constrained by their pretraining cut-off dates and lack the ability to provide specific, cited scientific knowledge. Here, we introduce Network for Knowledge Organization (NEKO), a workflow that uses LLM Qwen to extract knowledge through scientific literature text mining. When user inputs a keyword of interest, NEKO can generate knowledge graphs to link bioinformation entities and produce comprehensive summaries from PubMed search. NEKO significantly enhance LLM ability and has immediate applications in daily academic tasks such as education of young scientists, literature review, paper writing, experiment planning/troubleshooting, and new ideas/hypothesis generation. We exemplified this workflow's applicability through several case studies on yeast fermentation and cyanobacterial biorefinery. NEKO's output is more informative, specific, and actionable than GPT-4's zero-shot Q&A. NEKO offers flexible, lightweight local deployment options. NEKO democratizes artificial intelligence (AI) tools, making scientific foundation model more accessible to researchers without excessive computational power.}
}
@article{NORTHCOTT2025100535,
title = {The arrhythmia of bodily urgency: Using rhythmanalysis to understand the organisation of care people living with dementia experience within acute hospital wards},
journal = {SSM - Qualitative Research in Health},
volume = {7},
pages = {100535},
year = {2025},
issn = {2667-3215},
doi = {https://doi.org/10.1016/j.ssmqr.2025.100535},
url = {https://www.sciencedirect.com/science/article/pii/S2667321525000137},
author = {Andy Northcott and Paula Boddington and Katie Featherstone},
abstract = {This article posits Henri Lefebvre's concept of Rhythmanalysis as a novel methodology for observing and understanding the everyday life of the hospital ward and its consequences. To do so we draw on observational data taken across three multi-site studies of acute NHS hospital wards in England and Wales (22 wards across 12 hospitals) between 2015 and 2023. Our analysis of the rhythms of the ward, and of the arrhythmias patients can produce, allow us to develop a detailed and embodied perspective of how the ward is experienced by the many different actors within it. In this paper, we focus on one particular group, people living with dementia, considering how they fit both within and outside the rhythms of the ward, and the dressage used by staff to maintain those rhythms. We conclude by discussing rhythmanalysis as a means to observe and record otherwise underseen aspects of hospital care which can provide a means for researchers to better understand relationships of power, personhood and dignity, and their consequences, within clinical environments.}
}
@article{QU2025117589,
title = {Female versus viral: Understanding the UK gender health inequalities during the Covid-19 pandemic using e-archives},
journal = {Social Science & Medicine},
volume = {366},
pages = {117589},
year = {2025},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2024.117589},
url = {https://www.sciencedirect.com/science/article/pii/S0277953624010438},
author = {Chen Qu},
keywords = {The UK, Web archives, Gender health inequalities, Pandemic, Covid-19, Digital health infrastructure, Novel digital methods},
abstract = {Despite the development of digital health infrastructure, female health inequalities have worsened during the pandemic. This transdisciplinary study, through health, feminist, and infrastructural geographical lens, examines how gender health inequalities may have emerged or worsened during Covid-19 in the UK. This study leverages a novel web archive collection, Python coding-powered data-handling text analysis (of over 0.2 billion words), and thematic analysis to examine three themes: vaccines, social minority groups, and women’s self-care. The findings suggest that the pandemic has impacted health inequalities among British women and girls and more, in a ‘more-than-gender’ way in terms of health (care) outcomes and access. In addition to reflecting on the use of e-archives in this study including suggesting the potential of combining e-archiving, coding, natural language processing (NLP) and generative AI/Large Language Models (LLMs) in producing and analysing trans-temporal (big) datasets, I argue that a geographical crisis perspective that balances the needs of everyday life and possible crises can be considered when preparing for public health emergencies. I adopt the e-archiving of this study to rethink ‘digital health infrastructure’ as ‘actors’, ‘facilitators’, and ‘voicers’, revealing how human-computer interaction and people in the virtual realm can be infrastructure.}
}
@article{RAMOSVIDAL2025112285,
title = {SPL-DB-Sync: Seamless database transformation during feature-driven changes},
journal = {Journal of Systems and Software},
volume = {222},
pages = {112285},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112285},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003297},
author = {Delfina Ramos-Vidal and Wesley K.G. Assunção and Alejandro Cortiñas and Miguel R. Luaces and Oscar Pedreira and Ángeles {Saavedra Places}},
keywords = {Software evolution, Variability management, Database management, Data synchronization},
abstract = {Software Product Line (SPL) Engineering is a reuse-oriented approach to developing a suite of software products that share common components but vary in specific features. The advantages of SPLs (e.g., reducing development costs and time while improving quality) have already been proven in practice. However, despite the success in deriving new products from an SPL, challenges arise in evolving existing products. Altering the feature selection (e.g., adding or removing a feature) for an already existing product poses a challenge regarding the application data stored and managed by derived products, particularly when the features impact an already populated database. In many cases, these modifications imply loss of data or constraint violations. However, in both the state of the art and practice, there are no approaches to support feature and data evolution simultaneously for SPL products. This paper reports a novel evolution approach, SPL-DB-Sync, with actions required for database adjustments when adding or removing features for existing SPL products. Actions delineate modifications necessary within the database. These modifications are associated with the SPL features and linked to the components of the data model they influence. SPL-DB-Sync facilitates the automatic readjustment of the database while preserving clear traceability between features and elements of the data model. The applicability of our evolution model is detailed in four practical scenarios of in-production products of an SPL for Digital Libraries. The contributions of this work are: present a novel evolution approach for SPLs with databases; define an SPL Evolution Model considering data transformation/migration; advance the state of practice between software reuse and data management; and provide insights for practitioners that face the same challenges of evolving both business logic and its data in software products.}
}
@article{2025A11,
title = {Guide for Authors},
journal = {Journal of the American Society of Echocardiography},
volume = {38},
number = {1},
pages = {A11-A20},
year = {2025},
issn = {0894-7317},
doi = {https://doi.org/10.1016/S0894-7317(24)00623-0},
url = {https://www.sciencedirect.com/science/article/pii/S0894731724006230}
}
@incollection{GADIYA2025,
title = {Artificial intelligence-driven patent analysis in drug discovery},
booktitle = {Reference Module in Chemistry, Molecular Sciences and Chemical Engineering},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-12-409547-2},
doi = {https://doi.org/10.1016/B978-0-443-29808-0.00017-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443298080000170},
author = {Yojana Gadiya and Reagon Karki and Philip Gribbon and Andrea Zaliani},
keywords = {Artificial intelligence, Chemoinformatics, Databases, Data mining, Drug discovery, Gene target prioritzation, IP intelligence search, Patent analysis, Patent landscaping},
abstract = {The integration of machine learning (ML) based approaches into patent analysis is revolutionizing the field of drug discovery by providing powerful tools to mine, interpret, and leverage vast amounts of patent data. These ML-driven technologies enable the automated extraction of valuable insights from patent documents, including chemical structures, biological targets, and therapeutic indications, which are crucial for accelerating drug development. This review explores the current state of patent analysis in drug discovery, highlighting successful applications and existing gaps, and discusses future directions, including the development of integrated platforms for comprehensive patent landscaping and the economic valuation of patent impacts.}
}
@article{ALMAFIE2025104002,
title = {Effects of electrospinning parameters on polycaprolactone membrane diameter: An investigation utilizing central composite design and characterization},
journal = {Results in Engineering},
volume = {25},
pages = {104002},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.104002},
url = {https://www.sciencedirect.com/science/article/pii/S2590123025000908},
author = {Muhammad Rama Almafie and Ahmad Fudholi and Rahma Dani and Meutia Kamilatun Nuha AP Idjan and Idha Royani and Ida Sriyanti},
keywords = {Contour, Equation, Membrane, Morphological, Optimization},
abstract = {The synergistic effect of electrospinning parameters, specifically concentration Polycaprolactone solution, voltage, and distance needle to collector, is a critical factor in the production of high-quality micro-nano fibres membranes. This study aimed to investigate the effects of these three parameters on the micro-nano fibres diameter and determine the optimal conditions for achieving the desired characteristics. Electrospinning was used to produce micro-nano fibres with diameters controlled by varying the process parameters. A Central Composite Design was used as an optimization method to evaluate the effects of these parameters on micro-nano fibres diameter. The results showed that all parameters had a significant effect, with the quadratic polynomial model providing the coefficient of determination (R² = 0.9854). The CCD model with desirability function successfully optimized the electrospinning parameters, resulting in a PCL concentration of 11.85 wt%, a voltage of 13.05 kV, and a distance of 82.17 cm. These conditions produced micro-nano fibres with diameters ranging from 1185.17 to 1338.69 nm and the highest desirability of 1.000 for further research. SEM analysis showed that the micro-nano fibres morphology was influenced by the solution concentration, where bead morphology micro-nano fibres were formed at a concentration of 8.30 wt%, while concentrations above 12.50 wt% produced bead-free micro-nano fibres. FTIR analysis revealed the presence of alkane, ester, and ether functional groups in the micro-nano fibres structure, which are important for the molecular integrity. In addition, XRD analysis showed the crystallite size of the micro-nano fibres ranged from 9.77 nm to 85.20 nm, with percentage crystallinity ranging from 19.23 % to 34.74 %. These findings suggest that the optimized PCL micro-nano fibres membranes possess desirable characteristics for potential applications in medical scaffolding, providing a structurally suitable environment for cell growth and tissue regeneration.}
}
@article{YENISENYAVUZ2025107681,
title = {Why and how do organizations create user-led open source consortia? A systematic literature review},
journal = {Information and Software Technology},
volume = {181},
pages = {107681},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107681},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000205},
author = {Elçin {Yenişen Yavuz} and Dirk Riehle},
keywords = {Open source foundations, User-led open source consortia, Collaborative software development, Open-source software projects, User-driven open-source software development, Community-source software development, Coopetition, SLR, Systematic literature review},
abstract = {Context
User-led open source (OS) consortia (foundations) consist of organizations from industries beyond the software industry collaborating to create open-source software solutions for their internal processes. Initially pioneered by higher education organizations in the 2000s, this concept has gained traction in recent years across various industries.
Objective
This study has two research objectives. The first objective is to provide an overview of the current state of the art in this field by identifying previously studied topics and gathering examples from different industries. The second objective is to understand the structure of user-led OS consortia and the motivations of organizations for participating in such consortia.
Method
To gain a comprehensive understanding of this phenomenon, we conducted a systematic literature review, covering the years 2000 to 2023. Furthermore, we performed thematic analysis on 43 selected studies to identify and examine the key characteristics, ecosystems, and the benefits organizations gain from involvement in user-led OS consortia.
Results
We identified 43 unique papers on user-led OS consortia and provided details on 14 sample user-led OS consortia projects. We defined 19 characteristics of user-led OS consortia and 16 benefits for organizations’ involvement. Additionally, we outlined the key actors and their roles in user-led OS consortia.
Conclusion
We provided an overview of the current state of the art in this field. We identified the structure of user-led OS consortia and the organizations’ motivations for participating in such consortia.}
}
@article{HERBOSCH2025106110,
title = {To err is human: Managing the risks of contracting AI systems},
journal = {Computer Law & Security Review},
volume = {56},
pages = {106110},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106110},
url = {https://www.sciencedirect.com/science/article/pii/S0267364925000056},
author = {Maarten Herbosch},
keywords = {Artificial Intelligence, Unilateral mistake, Automated contracts, Intent, Contract validity, law and technology, Law and artificial intelligence, Contract law and AI},
abstract = {Artificial intelligence (AI) increasingly influences contract law. Applications like virtual home assistants can form contracts on behalf of users, while other AI tools can assist parties in deciding whether to contract. The advent of Generative AI has further accelerated and broadened the proliferation of such applications. However, AI systems are inherently imperfect, sometimes leading to unexpected or undesirable contracts, raising concerns about the legal protection of AI deployers. Some authors have suggested that autonomous AI deployment cannot lead to a legally binding contract in the absence of a human “intent”. Others have argued that the system deployer is completely unprotected in cases of undesirable AI output. They argue that that deployment implies that the deployer should bear the risk of any mistake. This article challenges these views by leveraging existing contract formation and mistake frameworks. Traditional analysis demonstrates that AI deployment can produce valid contracts. It also suggests that deployers may invoke the unilateral mistake doctrine, drawing parallels to clerical errors in human contracts. While AI outputs are probabilistic and unpredictable, similar characteristics apply to human decision-making. The potential benefits of AI development justify affording AI deployers protections analogous to those provided in traditional scenarios. To enhance protection, deployers should use high-performing systems with safeguards such as oversight mechanisms and registration tools. As industry standards evolve, these safeguards will become more defined. The analysis concludes that current contract law frameworks are flexible enough to accommodate AI systems, negating the need for a complete overhaul.}
}
@article{AROMAA2025217,
title = {Company perspectives of generative artificial intelligence in industrial work},
journal = {Procedia Computer Science},
volume = {253},
pages = {217-226},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.085},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925000936},
author = {Susanna Aromaa and Päivi Heikkilä and Marko Jurvansuu and Selen Pehlivan and Teijo Väärä and Marko Jurmu},
keywords = {Generative artificial intelligence, industry work, manufacturing, human factors, ergonomics, company perspective},
abstract = {The use of artificial intelligence (AI) technologies in the manufacturing industry is rapidly increasing. During this transformation, it can be difficult to understand how AI will change the way work is done. This study explores how generative AI could change manufacturing work. Data collection was conducted using interviews and a questionnaire with seven representatives from three industrial companies. They identified several application areas for GenAI in the industrial work context, such as design, planning, training, problem solving, coding and data management. They also expressed positive attitudes but raised concerns about trust, safety, acceptability and interoperability. Changes in work were identified as being more related to cognitive aspects such as changing the way of thinking and altering the interaction with people and machines. Therefore, human-AI design efforts should focus especially on cognitive ergonomics. Findings from this study can be used in the manufacturing industry when adopting AI, as well as in identifying research topics in the human-AI research community.}
}
@article{FAUNDEZZANUY2025126214,
title = {Comprehensive analysis of least significant bit and difference expansion watermarking algorithms for online signature signals},
journal = {Expert Systems with Applications},
volume = {267},
pages = {126214},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126214},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424030811},
author = {Marcos Faundez-Zanuy},
keywords = {Watermarking, Biometrics, Online signature, Dynamic time warping, Artificial Intelligence generated signal detection},
abstract = {This paper investigates the efficacy of non-reversible and reversible watermarking techniques applied to online signature signals through an analysis of Least Significant Bit and difference expansion algorithms. The impact of watermark insertion is assessed using dynamic time warping recognition accuracies on a large database. Our experimental section comprises identification rates and verification errors for both random and skilled forgeries, involving 330 users and 25 skilled forgeries per user. Notably, our findings reveal that online signature signals can withstand up to 7 bits per sample insertion in the least significant bit algorithm, with minimal reduction in recognition accuracies. Similar robustness is observed with the difference expansion algorithm. This resilience is noted when employing z-score normalization and an expanded feature set, encompassing delta and delta-delta parameters. The presented results underscore the robustness of the proposed watermarking methods in preserving the integrity of online signature recognition capabilities. Watermarking is currently proposed to differentiate between human-produced signals and those generated by artificial intelligence, with applications including replay attack detection. This study explores the potential application of this technique to online handwriting signals without compromising biometric recognition accuracy. The proposed watermarking techniques not only preserve biometric recognition integrity but also provide a promising solution to distinguish human-generated signals from AI-generated counterparts, a growing challenge in the age of synthetic media.}
}
@article{XIE2025107480,
title = {Intelligent algorithms powered smart devices for atrial fibrillation discrimination},
journal = {Biomedical Signal Processing and Control},
volume = {103},
pages = {107480},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.107480},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424015386},
author = {Liping Xie and Lindong Wang and Dongze Mo and Zelin Zhang and Ming Liang},
keywords = {Wearable technology, Algorithms, Atrial fibrillation diagnosis, Smart devices},
abstract = {Atrial fibrillation (AF) is one of the frequent and potentially dangerous arrhythmias that can participate in cardioembolic stroke and heart failure. Early AF identification is possible by the combination of algorithms with wearable technology, which makes it easier to transform from hospital-based to at-home care for AF detection. This review presents an overview of the combination of intelligent algorithms with smart devices for AF discrimination. The smart devices are summarized in detail. Then, an extensive discussion of AF detection algorithms in three key aspects including database, feature extraction, and classification algorithms, is elaborated. Furthermore, the integration of intelligent algorithms with wearable technology for effective AF monitoring is systematically interpreted. Lastly, the challenges and outlook of smart devices enabled by AF screening algorithms are also discussed. This review aims to provide a comprehensive understanding of AF screening utilizing wearable devices enabled by algorithms.}
}
@article{GU2025148,
title = {Progressive self-supervised learning: A pre-training method for crowd counting},
journal = {Pattern Recognition Letters},
volume = {188},
pages = {148-154},
year = {2025},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2024.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167865524003623},
author = {Yao Gu and Zhe Zheng and Yingna Wu and Guangping Xie and Na Ni},
keywords = {Crowd counting, Self-supervised learning, Dataset construction},
abstract = {Crowd counting technologies possess substantial social significance, and deep learning methods are increasingly seen as potent tools for advancing this field. Traditionally, many approaches have sought to enhance model performance by transferring knowledge from ImageNet, utilizing its classification weights to initialize models. However, the application of these pre-training weights is suboptimal for crowd counting, which involves dense prediction significantly different from image classification. To address these limitations, we introduce a progressive self-supervised learning approach, designed to generate more suitable pre-training weights from a large collection of density-related images. We gathered 173k images using custom-designed prompts and implemented a two-stage learning process to refine the feature representations of image patches with similar densities. In the first stage, mutual information between overlapping patches within the same image is maximized. Subsequently, a combination of global and local losses is evaluated to enhance feature similarity, with the latter assessing patches from different images of comparable densities. Our innovative pre-training approach demonstrated substantial improvements, reducing the Mean Absolute Error (MAE) by 7.5%, 17.6%, and 28.7% on the ShanghaiTech Part A & Part B and UCF_QNRF datasets respectively. Furthermore, when these pre-training weights were used to initialize existing models, such as CSRNet for density map regression and DM-Count for point supervision, a significant enhancement in performance was observed.}
}
@incollection{AKRAM2025385,
title = {Chapter 18 - Fundamentals of privacy-preserving and secure machine learning},
editor = {Marco Lorenzi and Maria A. Zuluaga},
booktitle = {Trustworthy AI in Medical Imaging},
publisher = {Academic Press},
pages = {385-409},
year = {2025},
series = {The MICCAI Society book Series},
isbn = {978-0-443-23761-4},
doi = {https://doi.org/10.1016/B978-0-44-323761-4.00031-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443237614000316},
author = {Aftab Akram and Pascal Zimmer and Clémentine Gritti and Ghassan Karame and Melek Önen},
keywords = {Inference-related attacks, Cryptography, Differential privacy, Trusted execution environment, Evasion attacks and defenses, Poisoning attacks and defenses},
abstract = {This chapter discusses common threats against the privacy and security of Machine Learning (ML), such as inferring sensitive information from ML models and poisoning deployed models. It also discusses multiple countermeasures to overcome those attacks by focusing in particular on defenses that can be applied at various stages, e.g., during the inference and training phases, or capturing different inputs, e.g., model and data.}
}
@article{SIKSTROM2025113078,
title = {Personality in just a few words: Assessment using natural language processing},
journal = {Personality and Individual Differences},
volume = {238},
pages = {113078},
year = {2025},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2025.113078},
url = {https://www.sciencedirect.com/science/article/pii/S0191886925000406},
author = {Sverker Sikström and Ieva Valavičiūtė and Petri Kajonius},
keywords = {Big Five, Personality, Natural language processing, GPT-4, BERT},
abstract = {Assessment of psychological constructs, such as the Big Five personality traits, has predominantly relied on standardized rating scales. While these scales have advantages, we propose that descriptive word-based responses analyzed with natural language processing (NLP) offer a promising alternative for assessing personality traits. We asked participants (N = 663) to describe either their own personality or a person high in one of the Big Five traits using five words. These responses were then analyzed using large language models, namely BERT and GPT-4, which are known for their high-performance NLP capabilities. The primary aim was to assess the validity of word-based responses analyzed by NLP in comparison to the IPIP-NEO-30 rating scale, a commonly used tool for measuring the Big Five traits. Results showed that descriptive word responses had an average prediction accuracy of up to 10 % higher than the rating scale in categorizing the Big Five traits. Additionally, semantic measures showed higher inter-rater reliability, and observer convergence was greater in assessments of others than in self-reports. These findings suggest that descriptive word-based responses may capture more observable and broad aspects of personality compared to traditional rating scales.}
}
@article{ALVAREZFIDALGO2025104005,
title = {CLAVE: A deep learning model for source code authorship verification with contrastive learning and transformer encoders},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104005},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104005},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324003649},
author = {David Álvarez-Fidalgo and Francisco Ortin},
keywords = {Source code authorship verification, Contrastive learning, Transformer encoder, Deep learning, Stylometric representations, Large language models, Python},
abstract = {Source code authorship verification involves determining whether two code fragments are written by the same programmer. It has many uses, including malware authorship analysis, copyright dispute resolution and plagiarism detection. Source code authorship verification is challenging because it must generalize to code written by programmers not included in its training data. In this paper, we present CLAVE (Contrastive Learning for Authorship Verification with Encoder representations), a novel deep learning model for source code authorship verification that leverages contrastive learning and a Transformer Encoder-based architecture. We initially pre-train CLAVE on a dataset of 270,602 Python source code files extracted from GitHub. Subsequently, we fine-tune CLAVE for authorship verification using contrastive learning on Python submissions from 61,956 distinct programmers in Google Code Jam and Kick Start competitions. This approach allows the model to learn stylometric representations of source code, enabling comparison via vector distance for authorship verification. CLAVE achieves an AUC of 0.9782, reduces the error of the state-of-the-art source code authorship verification systems by at least 23.4% and improves the AUC of cutting-edge source code LLMs by 21.9% to 40%. We also evaluate the main components of CLAVE on its AUC performance improvement: pre-training (1.8%), loss function (0.2%–2.8%), input length (0.1%–0.7%), model size (0.2%), and tokenizer (0.1%–0.7%).}
}
@incollection{SINGH2025259,
title = {Preclinical: Drug Target Identification and Validation in Humans},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {259-280},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00145-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027001457},
author = {Harpreet Singh and Rupinder P. Kaur},
keywords = {Drug discovery, Guidelines on target assessment for innovative therapeutics (GOT-IT), Target identification, Target validation},
abstract = {The process of discovering a new drug begins by understanding the biological origin of a disease and recognizing key target molecules involved in its onset and/or progression. In order to understand the cellular processes taking place in response to a drug, it is critical to elucidate its molecular targets. This has tremendous implications for disease prevention and treatment. Correct identification and validation of drug targets is important for the success of clinical trials and drug development initiatives. A number of approaches have been evolving for target identification as well validation ranging from experimental to computational including the recent addition of Artificial Intelligence tools. In addition, drug, drug target and drug interaction databases have also been playing an important role in this early drug discovery phase. This chapter provides an overview of these techniques along with their applications as well as limitations. The recent implementation of Guidelines On Target Assessment for Innovative Therapeutics (GOT-IT) have been also highlighted.}
}
@article{YE2024,
title = {Prospects for synthetic biology in 21st Century agriculture},
journal = {Journal of Genetics and Genomics},
year = {2024},
issn = {1673-8527},
doi = {https://doi.org/10.1016/j.jgg.2024.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S1673852724003692},
author = {Xingyan Ye and Kezhen Qin and Alisdair R. Fernie and Youjun Zhang},
keywords = {Plant synthetic biology, Photosynthesis, Nitrogen fixation, AI integration, Genetic circuits, Precision agriculture},
abstract = {Plant synthetic biology has emerged as a transformative field in agriculture, offering innovative solutions to enhance food security, provide resilience to climate change, and transition to sustainable farming practices. By integrating advanced genetic tools, computational modeling, and systems biology, researchers can precisely modify plant genomes to enhance traits such as yield, stress tolerance, and nutrient use efficiency. The ability to design plants with specific characteristics tailored to diverse environmental conditions and agricultural needs holds great potential to address global food security challenges. Here, we highlight recent advancements and applications of plant synthetic biology in agriculture, focusing on key areas such as photosynthetic efficiency, nitrogen fixation, drought tolerance, pathogen resistance, nutrient use efficiency, biofortification, climate resilience, microbiology engineering, synthetic plant genomes, and the integration of artificial intelligence (AI) with synthetic biology. These innovations aim to maximize resource use efficiency, reduce reliance on external inputs, and mitigate environmental impacts associated with conventional agricultural practices. Despite challenges related to regulatory approval and public acceptance, the integration of synthetic biology in agriculture holds immense promise for creating more resilient and sustainable agricultural systems, contributing to global food security and environmental sustainability. Rigorous multi-field testing of these approaches will undoubtedly be required to ensure reproducibility.}
}
@article{PRASAD2025110011,
title = {Survey on medical image encryption: From classical to deep learning-based approaches},
journal = {Computers and Electrical Engineering},
volume = {123},
pages = {110011},
year = {2025},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2024.110011},
url = {https://www.sciencedirect.com/science/article/pii/S0045790624009364},
author = {Shiv Prasad and Amit Kumar Singh},
keywords = {Medical images, Encryption, Artificial intelligence, Machine/deep learning, Security, Attacks},
abstract = {Nowadays, the significant importance of digital data, particularly in the form of images, in the healthcare domain is receiving more and more attention. Doctors and professionals are increasingly sharing medical images to extract value and enhance the quality of care. However, the security of sensitive medical images has attracted increasingly serious concerns, thus the protection of these images is particularly important. Currently, encryption in the artificial intelligence (AI) domain can be used to protect image information and enhance secure e-healthcare services. This survey attempts to systematically discuss, summarise, and organise the recent trends in medical image encryption from classical to deep learning-based methods. We also explore the promising background knowledge of classical and AI-based image encryption techniques, recent applications, common attacks, and evaluation metrics. Following this, the contributions of various approaches are outlined and compared in terms of different technical aspects. By highlighting recent challenges and opportunities, the authors hope to empower researchers and practitioners to build a more secure system for healthcare as well as for other equally important applications.}
}
@article{LI2025100374,
title = {Machine learning-assisted development of gas separation membranes: A review},
journal = {Carbon Capture Science & Technology},
volume = {14},
pages = {100374},
year = {2025},
issn = {2772-6568},
doi = {https://doi.org/10.1016/j.ccst.2025.100374},
url = {https://www.sciencedirect.com/science/article/pii/S2772656825000144},
author = {An Li and Jianchun Chu and Shaoxuan Huang and Yongqi Liu and Maogang He and Xiangyang Liu},
keywords = {Membrane, Gas separation, Machine learning, Metal organic framework},
abstract = {Gas separation membranes have been a hot topic of research in recent decades due to their low costs, high energy efficiency and wide range of applications. Machine learning provide a fast way to design gas separation membranes with required performance. This review systematically describes the process of machine learning-assisted gas separation membrane development. In addition, the experimental data on CO2/CH4, CO2/N2 and O2/N2 separation performance were summarized to provide basis for future work on machine learning-assisted design of gas separation membrane for carbon dioxide capture, and natural gas purification as well as oxygen or nitrogen enrichment. Moreover, we discuss the classical materials that make up gas separation membranes, including MOFs, polymers and COFs, and analyze the strengths and weaknesses of the different materials. Finally, we discuss the challenges in the development of machine learning method for next-generation gas separation membranes.}
}
@article{VETTERS2025103873,
title = {Getting stakeholders aboard for offshore wind decommissioning: A qualitative study on end-of-life challenges in Belgium},
journal = {Energy Research & Social Science},
volume = {120},
pages = {103873},
year = {2025},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2024.103873},
url = {https://www.sciencedirect.com/science/article/pii/S221462962400464X},
author = {J. Vetters and G. Thomassen and S. {Van Passel}},
keywords = {Renewable energy, Dismantling, Interviews, Waste management, Recycling, Supply chain participation},
abstract = {Decommissioning offshore wind farms presents significant challenges as the sector approaches the final phase of its operational lifecycle. This research examines end-of-life challenges through the perspectives of a diverse range of stakeholders, including industry, government, research, and civil society. While the study focuses on Belgian stakeholders, the challenges and solutions are expected to be relevant to similar cases. Semi-structured interviews identified 67 challenges across five end-of-life phases: planning, dismantling, transport and logistics, waste management, and monitoring site recovery. These challenges span technical, economic, environmental, social, and policy dimensions. Among them, 27 newly recognized challenges were identified. Key issues, such as composite recycling, removal legislation, port suitability, artificial reef effects, and uncertainty surrounding dismantling approaches, emerged as central concerns. These concerns were highlighted by nearly all stakeholder groups. This study addresses gaps in existing knowledge by providing comprehensive stakeholder mapping for the end-of-life phase of offshore wind farms. It incorporates stakeholder perspectives into the identification and evaluation of challenges. To validate findings, the study includes a qualitative analysis that separately examines expert stakeholders. The findings offer a detailed understanding of major concerns in offshore wind decommissioning. Recommendations include ensuring transparent grid connections, developing improved removal strategies, and adopting a more coordinated approach to transport and logistics. Waste management recommendations focus on improving blade design and addressing policy and economic issues for existing blades. The study underscores the importance of stakeholder engagement. It highlights the need for systematic involvement in end-of-life research, offering valuable insights for sustainable decommissioning practices.}
}
@article{XIONG2025129388,
title = {Adaptively forget with crossmodal and textual distillation for class-incremental video captioning},
journal = {Neurocomputing},
volume = {624},
pages = {129388},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129388},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225000608},
author = {Huiyu Xiong and Lanxiao Wang and Heqian Qiu and Taijin Zhao and Benliu Qiu and Hongliang Li},
keywords = {Class-incremental learning, Video captioning, Encoder–Decoder, Multimodal attention, Knowledge distillation},
abstract = {With the increasing volume of video data, it is critical that intelligent systems have the ability to continuously analyze the increasing number of videos while avoiding the usage of data from early tasks. These video data are difficult to store, and training models from scratch face privacy and high energy consumption pitfalls. However, existing research has primarily focused on incremental learning for image and video classification, with limited attention given to analyzing multimodal inputs and leveraging the spatio-temporal contextual information embedded in videos to generate meaningful captions. In this paper, we first present the Class-Incremental Video Captioning (CI-VC) task. There are two difficulties in this task, which are the stability–plasticity dilemma faced in processing textual information with spatio-temporal states and correlating video-text inter-modal information, respectively. To fill this unexplored area, we primitively propose a method to Adaptively Forget with crossmodal and textual Distillation for class-incremental video captioning (AFD). Considering the unique encoder–decoder networks of captioning and the privacy of input data, we refer to the three stages without playback of old samples: visual encoding, textual decoding and network supervision. In the phase of learning new knowledge, we use Fine-grained Sensitivity Selection (FgSS) to refine the selection of stored knowledge from the old model, forgetting some irrelevant information to make room for the memory of new classes. In order to memorize the text information during decoding, we design Growing Word Embedding (G) with elastic capacity to store the embedding tokens of text that have already been seen as textual meta-signal pairs to be retrieved the lexical information during further caption generation. Meanwhile, the Dual-level Knowledge Distillation (DlKD) is performed at both cross-modal semantic information and the textual final output to constrain from the perspective of the extracted level specific features, integrating the newly learned knowledge and consolidating the inter-model and intra-model knowledge of the old classes. To illustrate the ability of our model to resist forgetting, we designed a metric CIDER˜t to detect the stage forgetting rate. Experiments on the public datasets MSR-VTT, MSVD and VATEX show that the proposed method significantly resists the forgetting of previous tasks without replaying old samples, and performs well on the new task.}
}
@article{ZHAO2025463,
title = {Opportunities and challenges in transformer neural networks for battery state estimation: Charge, health, lifetime, and safety},
journal = {Journal of Energy Chemistry},
volume = {102},
pages = {463-496},
year = {2025},
issn = {2095-4956},
doi = {https://doi.org/10.1016/j.jechem.2024.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S209549562400771X},
author = {Jingyuan Zhao and Xuebing Han and Yuyan Wu and Zhenghong Wang and Andrew F. Burke},
keywords = {Transformer, Battery, Health, Lifetime, Safety, SOC, SOH, RUL, Deep learning, Artificial general intelligence},
abstract = {Battery technology plays a crucial role across various sectors, powering devices from smartphones to electric vehicles and supporting grid-scale energy storage. To ensure their safety and efficiency, batteries must be evaluated under diverse operating conditions. Traditional modeling techniques, which often rely on first principles and atomic-level calculations, struggle with practical applications due to incomplete or noisy data. Furthermore, the complexity of battery dynamics, shaped by physical, chemical, and electrochemical interactions, presents substantial challenges for precise and efficient modeling. The Transformer model, originally designed for natural language processing, has proven effective in time-series analysis and forecasting. It adeptly handles the extensive, complex datasets produced during battery cycles, efficiently filtering out noise and identifying critical features without extensive preprocessing. This capability positions Transformers as potent tools for tackling the intricacies of battery data. This review explores the application of customized Transformers in battery state estimation, emphasizing crucial aspects such as charging, health assessment, lifetime prediction, and safety monitoring. It highlights the distinct advantages of Transformer-based models and addresses ongoing challenges and future opportunities in the field. By combining data-driven AI techniques with empirical insights from battery analysis, these pre-trained models can deliver precise diagnostics and comprehensive monitoring, enhancing performance metrics like health monitoring, anomaly detection, and early-warning systems. This integrated approach promises significant improvements in battery technology management and application.}
}
@incollection{NISBET20253,
title = {1 - The background and history of predictive analytics},
editor = {Robert Nisbet and Keith McCormick and Gary Miner},
booktitle = {Handbook of Statistical Analysis (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Boston/Waltham (MA)},
pages = {3-22},
year = {2025},
isbn = {978-0-443-15845-2},
doi = {https://doi.org/10.1016/B978-0-443-15845-2.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443158452000013},
author = {Robert Nisbet and Keith McCormick and Gary Miner},
abstract = {You must be interested in learning how to practice data analytics; otherwise, you would not be reading this book. We know that there are many books available that will give a good introduction to the process of data analytics. Most books on data analytics focus on the features and functions of various data analytics tools or algorithms. Some books do focus on the challenges of performing data analytics tasks. This book is designed to give you an introduction to the practice of data analytics in the real world of business.}
}
@article{HU202512,
title = {Extracellular vesicle therapeutics for cardiac repair},
journal = {Journal of Molecular and Cellular Cardiology},
volume = {199},
pages = {12-32},
year = {2025},
issn = {0022-2828},
doi = {https://doi.org/10.1016/j.yjmcc.2024.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0022282824001950},
author = {Yilan Hu and Weihang Zhang and Shah Rukh Ali and Koji Takeda and Torsten Peter Vahl and Donghui Zhu and Yi Hong and Ke Cheng},
keywords = {Extracellular vesicles, Therapeutics, Cardiac repair, Bioengineering},
abstract = {Extracellular vesicles (EVs) are cell-secreted heterogeneous vesicles that play crucial roles in intercellular communication and disease pathogenesis. Due to their non-tumorigenicity, low immunogenicity, and therapeutic potential, EVs are increasingly used in cardiac repair as cell-free therapy. There exist multiple steps for the design of EV therapies, and each step offers many choices to tune EV properties. Factors such as EV source, cargo, loading methods, routes of administration, surface modification, and biomaterials are comprehensively considered to achieve specific goals. PubMed and Google Scholar were searched in this review, 89 articles related to EV-based cardiac therapy over the past five years (2019 Jan - 2023 Dec) were included, and their key steps in designing EV therapies were counted and analyzed. We aim to provide a comprehensive overview that can serve as a reference guide for researchers to design EV-based cardiac therapies.}
}
@article{CHEN2024101098,
title = {Integration of large language models and federated learning},
journal = {Patterns},
volume = {5},
number = {12},
pages = {101098},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101098},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002708},
author = {Chaochao Chen and Xiaohua Feng and Yuyuan Li and Lingjuan Lyu and Jun Zhou and Xiaolin Zheng and Jianwei Yin},
keywords = {large language models, federated learning, data scarcity, privacy and security},
abstract = {Summary
As the parameter size of large language models (LLMs) continues to expand, there is an urgent need to address the scarcity of high-quality data. In response, existing research has attempted to make a breakthrough by incorporating federated learning (FL) into LLMs. Conversely, considering the outstanding performance of LLMs in task generalization, researchers have also tried applying LLMs within FL to tackle challenges in relevant domains. The complementarity between LLMs and FL has already ignited widespread research interest. In this review, we aim to deeply explore the integration of LLMs and FL. We propose a research framework dividing the fusion of LLMs and FL into three parts: the combination of LLM sub-technologies with FL, the integration of FL sub-technologies with LLMs, and the overall merger of LLMs and FL. We first provide a comprehensive review of the current state of research in the domain of LLMs combined with FL, including their typical applications, integration advantages, challenges faced, and future directions for resolution. Subsequently, we discuss the practical applications of the combination of LLMs and FL in critical scenarios such as healthcare, finance, and education and provide new perspectives and insights into future research directions for LLMs and FL.}
}
@article{LIN2025216436,
title = {Deep learning-assisted methods for accelerating the intelligent screening of novel 2D materials: New perspectives focusing on data collection and description},
journal = {Coordination Chemistry Reviews},
volume = {529},
pages = {216436},
year = {2025},
issn = {0010-8545},
doi = {https://doi.org/10.1016/j.ccr.2025.216436},
url = {https://www.sciencedirect.com/science/article/pii/S0010854525000062},
author = {Yuandong Lin and Ji Ma and Yong-Guang Jia and Chongchong Yu and Jun-Hu Cheng},
keywords = {2D materials, Deep learning, Data collections, Data descriptions, Material screenings},
abstract = {Since the isolation of graphene, the interest in two-dimensional (2D) materials has been steadily growing thanks to their unique chemical and physical properties, as well as their potential for various applications. Deep learning (DL), currently one of the most sophisticated machine learning (ML) models, is emerging as a highly effective tool for intelligently investigating and screening 2D materials. The utilization of abundant data sources, appropriate descriptors, and neural networks enables the prediction of the structural and physicochemical properties of undiscovered 2D materials based on DL. Specifically, high-quality and well-described data plays a crucial role in effective model training, accurate predictions, and the discovery of new 2D materials. It also promotes reproducibility, collaboration, and continuous improvement within this field. This tutorial review is dedicated to an examination of the characterization, prediction, and discovery of 2D materials facilitated by various DL techniques. It focuses on the perspective of data collection and description, aiming to provide a clearer understanding of underlying principles and predicting outcomes. In addition, it also offers insights into future research prospects. The growing acceptance of DL is set to accelerate and transform the study of 2D materials.}
}
@article{ANDERSEN2025102813,
title = {Generative Artificial Intelligence (GenAI) in the research process – A survey of researchers’ practices and perceptions},
journal = {Technology in Society},
volume = {81},
pages = {102813},
year = {2025},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2025.102813},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X2500003X},
author = {Jens Peter Andersen and Lise Degn and Rachel Fishberg and Ebbe K. Graversen and Serge P.J.M. Horbach and Evanthia Kalpazidou Schmidt and Jesper W. Schneider and Mads P. Sørensen},
keywords = {Generative Artificial Intelligence (GenAI), Research process, Research practice, use cases, Research integrity},
abstract = {This study explores the use of generative AI (GenAI) and research integrity assessments of use cases by researchers, including PhD students, at Danish universities. Conducted through a survey sent to all Danish researchers from January to February 2024, the study received 2534 responses and evaluated 32 GenAI use cases across five research phases: idea generation, research design, data collection, data analysis, and writing/reporting. Respondents reported on their own and colleagues' GenAI usage. They also assessed whether the practices in the use cases were considered good research practice. Through an explorative factor analysis, we identified three clusters of perception: "GenAI as a work horse", "GenAI as a language assistant only", and "GenAI as a research accelerator". The findings further show varied opinions on GenAI's research integrity implications. Language editing and data analysis were generally viewed positively, whereas experiment design and peer review tasks faced more criticism. Controversial areas included image creation/modification and synthetic data, with comments highlighting the need for critical and reflexive use of GenAI. Usage differed by main research area, with technical and quantitative sciences reporting slightly higher usage and more positive assessments. Junior researchers used GenAI more than senior colleagues, while no significant gender differences were observed. The study underscores the need for adaptable, discipline-specific guidelines for GenAI use in research, developed collaboratively with experts to align with diverse research practices and minimize ethical and practical misalignment.}
}
@article{COLONNA2025106105,
title = {The end of open source? Regulating open source under the cyber resilience act and the new product liability directive},
journal = {Computer Law & Security Review},
volume = {56},
pages = {106105},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106105},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924001705},
author = {Liane Colonna},
keywords = {Open source software, Information security, Model card, SBOM},
abstract = {Rooted in idealism, the open-source model leverages collaborative intelligence to drive innovation, leading to major benefits for both industry and society. As open-source software (OSS) plays an increasingly central role in driving the digitalization of society, policymakers are examining the interactions between upstream open-source communities and downstream manufacturers. They aim to leverage the benefits of OSS, such as performance enhancements and adaptability across diverse domains, while ensuring software security and accountability. The regulatory landscape is on the brink of a major transformation with the recent adoption of both the Cyber Resilience Act (CRA) as well as the Product Liability Directive (PLD), raising concerns that these laws could threaten the future of OSS. This paper investigates how the CRA and the PDL regulate OSS, specifically exploring the scope of exemptions found in the laws. It further explores how OSS practices might adapt to the evolving regulatory landscape, focusing on the importance of documentation practices to support compliance obligations, thereby ensuring OSS's continued relevance and viability. It concludes that due diligence requirements mandate a thorough assessment of OSS components to ensure their safety for integration into commercial products and services. Documentation practices like security attestations, Software Bill of Materials (SBOMs), data cards and model cards will play an increasingly important role in the software supply chain to ensure that downstream entities can meet their obligations under these new legal frameworks.}
}
@article{ZHANG2025126438,
title = {Self-enhancing defense for protecting against model stealing attacks on deep learning systems},
journal = {Expert Systems with Applications},
volume = {269},
pages = {126438},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126438},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425000600},
author = {Chenlong Zhang and Senlin Luo and Jiawei Li and Limin Pan and Chuan Lu},
keywords = {Model stealing defense, Self-enhancing method, Model stealing attack, Security and privacy, Deep hash},
abstract = {Defending against model stealing (MS) is crucial for safeguarding intellectual property and the security of deep learning applications. Current countermeasures, however, have notable shortcomings. First, defense strategies reliant on distribution classification often fail to accurately identify attack samples with semantic and visual similarities, thereby reducing their effectiveness. Second, the method of leveraging query samples from unknown origins to bolster defense capability in application scenarios remains an unresolved yet critical issue. This paper presents SED (Self-Enhancing Model Stealing Defense Method), an innovative defense method against model stealing. SED incorporates a deep hashing model and introduces a novel Penalty-Weighted Hamming (PWH) distance for sample segmentation, which effectively overcomes the drawbacks of traditional distribution-based classification. Subsequently, SED employs dynamic temperature scaling and label flipping to realize defense. Moreover, SED maintains an archive of historical query samples and utilizes a greedy algorithm to construct a database of malicious samples, thereby improving defense tactics for future queries similar to those catalogued. Experimental results confirm that SED substantially diminishes the accuracy of the attackers’ substitute models and effectively utilizes historical data for self-enhancement.}
}
@article{KIDMOSE2025100871,
title = {A review of smart vehicles in smart cities: Dangers, impacts, and the threat landscape},
journal = {Vehicular Communications},
volume = {51},
pages = {100871},
year = {2025},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2024.100871},
url = {https://www.sciencedirect.com/science/article/pii/S2214209624001463},
author = {Brooke Kidmose},
keywords = {Automotive, Connected and autonomous vehicle, Smart transportation system, Smart city, Cybersecurity, Threat landscape},
abstract = {The humble, mechanical automobile has gradually evolved into our modern connected and autonomous vehicles (CAVs)—also known as “smart vehicles.” Similarly, our cities are gradually developing into “smart cities,” where municipal services from transportation networks to utilities to recycling to law enforcement are integrated. The idea, with both smart vehicles and smart cities, is that more data leads to better, more informed decisions. Smart vehicles and smart cities would acquire data from their own equipment (e.g., cameras, sensors) and from their connections—e.g., connections to fellow smart vehicles, to road-side infrastructure, to smart transportation systems (STSs), etc. Unfortunately, the paradigm of smart vehicles in smart cities is rife with danger and ripe for misuse. One vulnerable system or service could become an attacker's entry point, facilitating access to every connected vehicle, device, etc. Worse, smart vehicles and smart cities are inherently cyber-physical; a cyberattack can have physical consequences, including destruction of infrastructure and loss of life. Lastly, to leverage all the benefits of smart vehicles in smart cities, we would need to accept exorbitant levels of data collection and surveillance, which, in the absence of ironclad privacy protections, could lead to total lack of privacy. In this work, we define the automotive context—i.e., smart vehicles—within the larger context of smart cities as our threat landscape. Then, we enumerate and describe all of the (1) threats, (2) attack surfaces & targets, (3) areas of concern (indirect vulnerabilities & threats), and (4) impacts of smart vehicles in smart cities. Our objective is to demonstrate that the dangers are real and imminent—in the hope that they will be addressed before an attack on the “smart vehicles in smart cities” paradigm results in loss of life.}
}
@article{GATTIGLIA2025225,
title = {Managing Artificial Intelligence in Archeology. An overview},
journal = {Journal of Cultural Heritage},
volume = {71},
pages = {225-233},
year = {2025},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2024.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S1296207424002516},
author = {Gabriele Gattiglia},
keywords = {Archaeology, Artificial intelligence, Big Data, Theory, Ethics},
abstract = {The integration of AI in archaeology poses several risks due to the oversimplification of complex archaeological data for computational ease. This reductionist approach fosters a deterministic view, treating provisional classifications as definitive truths and influencing subsequent interpretations. The reliance on legacy data and Big Data for AI training risks perpetuating outdated ideas and frameworks. As AI expands from automating tasks to interpreting and creating reconstructions, archaeologists must adopt a critical approach to avoid biased and harmful outputs. The deterministic view of AI hinders informed debate. Archaeologists should engage in discussions that address the classificatory, and ethical aspects as well as the materiality of AI. The accumulation of data in AI mimics storytelling but lacks the interpretative depth needed to understand historical human perspectives. Developing theories and narrative practices is essential to making archaeological data meaningful. The shift from a representational to a co-creative view of data is necessary to understand its re-use and the power dynamics involved. Finally, to normalise AI in archaeology, a critical and sceptical approach is needed to integrate AI into the real world and understand its implications and ethical considerations.}
}
@article{IHNAINI2024102263,
title = {Semantic similarity on multimodal data: A comprehensive survey with applications},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {10},
pages = {102263},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102263},
url = {https://www.sciencedirect.com/science/article/pii/S1319157824003525},
author = {Baha Ihnaini and Belal Abuhaija and Ebenezer Atta Mills and Massudi Mahmuddin},
keywords = {Semantic Similarity, Similarity Measures, Multimodal Semantic Similarity, Semantic Similarity Applications, Machine Learning, And Deep Learning},
abstract = {Recently, the revival of the semantic similarity concept has been featured by the rapidly growing artificial intelligence research fueled by advanced deep learning architectures enabling machine intelligence using multimodal data. Thus, semantic similarity in multimodal data has gained substantial attention among researchers. However, the existing surveys on semantic similarity measures are restricted to a single modality, mainly text, which significantly limits the capability to understand the intelligence of real-world application scenarios. This study critically reviews semantic similarity approaches by shortlisting 223 vital articles from the leading databases and digital libraries to offer a comprehensive and systematic literature survey. The notable contribution is to illuminate the evolving landscape of semantic similarity and its crucial role in understanding, interpreting, and extracting meaningful information from multimodal data. Primarily, it highlights the challenges and opportunities inherent in different modalities, emphasizing the significance of advancements in cross-modal and multimodal semantic similarity approaches with potential application scenarios. Finally, the survey concludes by summarizing valuable future research directions. The insights provided in this survey improve the understanding and pave the way for further innovation by guiding researchers in leveraging the strength of semantic similarity for an extensive range of real-world applications.}
}
@article{JIANG2025100566,
title = {AI drug discovery tools and analysis technology: New methods aid in studying the compatibility of Traditional Chinese Medicine},
journal = {Pharmacological Research - Modern Chinese Medicine},
volume = {14},
pages = {100566},
year = {2025},
issn = {2667-1425},
doi = {https://doi.org/10.1016/j.prmcm.2024.100566},
url = {https://www.sciencedirect.com/science/article/pii/S2667142524002082},
author = {Qiwu Jiang and Suhan Yang and Shan He and Fei Li},
keywords = {Traditional Chinese Medicine compatibility, AI drug discovery tool, Combination prediction, Compatibility mechanisms, Compatibility ratio optimization},
abstract = {Introduction
The compatibility of Traditional Chinese Medicine (TCM) holds the potential for reducing toxicity and enhancing efficacy, serving as a crucial guide for the clinical application of TCM. In recent years, the development of artificial intelligence (AI) drug discovery tools has introduced novel approaches for analyzing the multichemical components of TCM, thereby saving time and efforts in experiments.
Methods
The keywords "Traditional Chinese Medicine" and "Artificial Intelligence", "Traditional Chinese Medicine" and "drug compatibility" were searched across various literature databases, including Web of Science, Google Scholar, PubMed, and Elsevier. Over 100 articles were reviewed, and after narrowing the selection to those focused on compatibility, the chosen studies were carefully analyzed to summarize the latest developments for this review.
Results
The review introduce AI drug discovery tools, including virtual screening, target prediction, ADMET prediction, and data mining, along with their roles in studying TCM compatibility. The results further provide insights of AI's application in TCM combination prediction, TCM compatibility mechanisms, and optimization of TCM compatibility ratio within the TCM compatibility research field.
Discussion
Traditional Chinese Medicine uses holistic formulas involving multiple components, targets, and pathways for disease treatment, but scientific explanations of these formulas are limited. AI aids TCM research by predicting combinations, mechanisms, and optimizing ratios, which improves efficiency and reducing costs. However, AI predictions may not be definitely accurate, and traditional expertise is still essential for validation. Future applications of AI in TCM require improved tools and collaboration between AI and TCM researchers.}
}
@article{LUAN2025107347,
title = {Banking prudentials, leverage, and innovation partnership choice in China},
journal = {Journal of Banking & Finance},
volume = {171},
pages = {107347},
year = {2025},
issn = {0378-4266},
doi = {https://doi.org/10.1016/j.jbankfin.2024.107347},
url = {https://www.sciencedirect.com/science/article/pii/S0378426624002619},
author = {Fushu Luan and Yang Chen and Lin Lang and King Yoong Lim},
keywords = {Bank regulation, China, Corporate leverage, Innovation and patenting, State-owned enterprises},
abstract = {In a theoretical context where innovators borrow loans or settle for state-owned enterprise (SOE) sponsorship for their projects, we examine the effects of banking prudential regulations and their interaction with corporate leverage on the patenting partnership choice in China using a unique matched patent-firm-bank loan dataset for 15,623 observations in the 2013–17 period. We use a unique instrumental variable (IV) strategy to identify idiosyncratic bank prudential reform shocks associated with the post-2012 Basel III regulation and find prudential metrics (corporate leverage) of the financiers (firms) to positively (negatively) influence SOE patenting partnership choice, though prudential regulation mitigates the latter. Prudential reforms therefore come at a cost of further SOE dominance. However, conditional on an innovation project being SOE sponsored, we find positive spillover effect from the SOE’s employment mandate to loan productivity. Our results are robust across different IV strategies, alternative measures, sub-sample and mechanism analyses.}
}
@article{202584,
title = {Guide for Authors},
journal = {Intelligent Medicine},
volume = {5},
number = {1},
pages = {84-90},
year = {2025},
issn = {2667-1026},
doi = {https://doi.org/10.1016/S2667-1026(25)00007-5},
url = {https://www.sciencedirect.com/science/article/pii/S2667102625000075}
}
@article{KAMPHORST2024102779,
title = {Digital Recording and the Hazards of Unbounded Moralized Judgment},
journal = {Technology in Society},
pages = {102779},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102779},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24003270},
author = {B.A. Kamphorst and E. O’Neill},
keywords = {digital recording, moralized judgment, norms, privacy, technology ethics},
abstract = {In today’s techno-social environment, it is easy to make, store, and share digital recordings, such as photographs, audio fragments, and video streams, at an unprecedented scale. While there are often obvious immediate benefits to making and sharing digital recordings, serious hazards associated with these practices have thus far gone under-appreciated. We contend that today’s digital recording practices threaten to radically alter how we perceive and evaluate ourselves and others, producing an ongoing, socially and morally disruptive shift towards unbounded moralized judgment. The shift toward unbounded moralized judgment in turn poses several hazards, including widespread, difficult-to-restore reputation damage, negatively altered self-perceptions, and the stifling of morally right behavior. Our central claim is that in the current techno-social environment, every individual has a pro tanto reason to avoid being recorded and to avoid recording others. On the occasions where the reasons for recording outweigh those against, more must be done to counteract the hazards introduced by recording. We conclude the paper by outlining possible avenues for technical, regulatory, and societal approaches to mitigating the hazards of unbounded moralized judgment.}
}
@article{KARAPATAKIS2025100118,
title = {Metaverse crimes in virtual (Un)reality: Fraud and sexual offences under English law},
journal = {Journal of Economic Criminology},
volume = {7},
pages = {100118},
year = {2025},
issn = {2949-7914},
doi = {https://doi.org/10.1016/j.jeconc.2024.100118},
url = {https://www.sciencedirect.com/science/article/pii/S2949791424000708},
author = {Andreas Karapatakis},
keywords = {Metaverse, Crypto assets, Artificial Intelligence (AI), Fraud, Financial crime},
abstract = {The technological evolution has not only opened new frontiers but has also become an indispensable part of our daily lives. However, the technology that enhances our lives presents a dual reality—it offers opportunities for criminals while creating challenges for law enforcement. Fraud, particularly, has become a pervasive issue. In response, virtual asset service providers must take measures to tackle cryptocurrency-related fraud. Nevertheless, this becomes challenging if the perpetrator exists solely within the virtual world. In 1992, Neal Stephenson used the term ‘Metaverse’ to describe a virtual world where people interact with each other using avatars. Over time, the Metaverse has transformed into a complex concept akin to 'cyberspace'. The Metaverse is a virtual environment that uses technologies to mimic the real world. As this virtual space became intertwined with financial transactions, especially through cryptocurrencies, the Metaverse evolved into a medium for perpetrating scams. Within this context, the article addresses the challenges associated with criminal activity in the Metaverse. Considering the potential applications of AI, cryptocurrencies and Non-Fungible Tokens, three main challenges can be identified: 1) decentralisation, 2) anonymity of the user, and 3) lack of regulation. This article examines the applicability of existing legislation to regulate criminal activity in the Metaverse through doctrinal research. Using a comparative approach, it analyses the challenges of addressing virtual crimes by contrasting fraud (Fraud Act 2006) with sexual assault (Sexual Offences Act 2003), highlighting the complexity of addressing crimes involving physical contact in virtual spaces compared to financial crimes.}
}
@article{BARA2025106063,
title = {AI contextual information shapes moral and aesthetic judgments of AI-generated visual art},
journal = {Cognition},
volume = {257},
pages = {106063},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2025.106063},
url = {https://www.sciencedirect.com/science/article/pii/S0010027725000034},
author = {Ionela Bara and Richard Ramsey and Emily S. Cross},
keywords = {AI-generated art, Moral judgments, Aesthetic judgments, Contextual information, Implicit moral associations},
abstract = {Throughout history, art creation has been regarded as a uniquely human means to express original ideas, emotions, and experiences. However, as Generative Artificial Intelligence reshapes visual, aesthetic, legal, and economic culture, critical questions arise about the moral and aesthetic implications of AI-generated art. Despite the growing use of AI tools in art, the moral impact of AI involvement in the art creation process remains underexplored. Understanding moral judgments of AI-generated art is essential for assessing AI's impact on art and its alignment with ethical norms. Across three pre-registered experiments combining explicit and implicit paradigms with Bayesian modelling, we examined how information about AI systems influences moral and aesthetic judgments and whether human art is implicitly associated with positive attributes compared to AI-generated art. Experiment 1 revealed that factual information about AI backend processes reduced moral acceptability and aesthetic appeal in certain contexts, such as gaining financial incentives and art status. Experiment 2 showed that additional information about AI art's success had no clear impact on moral judgments. Experiment 3 demonstrated that an implicit association task did not reliably link human art with positive attributes and AI art with negative ones. These findings show that factual information about AI systems shapes judgments, while different information doses about AI art's success have limited moral impact. Additionally, implicit associations between human-made and AI-generated art are similar. This work enhances understanding of moral and aesthetic perceptions of AI-generated art, emphasizing the importance of examining human—AI interactions in an arts context, and their current and evolving societal implications.}
}
@article{SUN2025469,
title = {Reimbursement and Regulatory Landscape for Artificial Intelligence in Medical Technology},
journal = {Gastrointestinal Endoscopy Clinics of North America},
volume = {35},
number = {2},
pages = {469-484},
year = {2025},
note = {Artificial Intelligence in Endoscopy},
issn = {1052-5157},
doi = {https://doi.org/10.1016/j.giec.2024.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1052515724001119},
author = {Edward Sun and Glenn Littenberg},
keywords = {AI medical technology, AI medical devices, Reimbursement, Regulation, CPT, TPT, NTAP, TCET}
}
@article{BONGELLI2025108662,
title = {Exploring online patient-doctor interactions. An epistemic and pragmatic analysis of Q&A patterns in an Italian “Ask to the doctor” medical forum},
journal = {Patient Education and Counseling},
volume = {134},
pages = {108662},
year = {2025},
issn = {0738-3991},
doi = {https://doi.org/10.1016/j.pec.2025.108662},
url = {https://www.sciencedirect.com/science/article/pii/S0738399125000291},
author = {Ramona Bongelli and Alessia Bertolazzi and Marina Paolanti and Ilaria Riccioni},
keywords = {Medical forum, Q&A services, Anxiety, Hypothyroidism, Questions, Answers, Epistemic positions},
abstract = {The main objective of this research is to investigate the epistemic and pragmatic management of patient-doctor interactions in Italian online health communities. To achieve this goal, an advanced web scraping methodology was used to extract from an Italian Q&A service (within the healthcare platforms, Il Mio Dottore) 200 pairs of questions and answers concerning two pathological conditions: anxiety and hypothyroidism. We first tagged the two sub-corpora and analyzed them both quantitatively and qualitatively to establish (i) what types of questions were used by patients, and what epistemic attitude and pragmatic function they convey; (ii) whether doctors’ replies were aligned or not; (iii) whether there were differences between the two sub-corpora. The results revealed many similarities between the two sub-corpora, but also some differences, mainly concerning doctors’ response patterns, with a tendency towards misalignment more pronounced in the anxiety sub-corpus. The practical implications of this and similar research may be numerous. First, they can improve understanding of the epistemic and pragmatic dynamics at play in Q&A services. Secondly, such knowledge can be used to formulate practical recommendations to foster better alignment with patients, thereby improving their engagement. Finally, this knowledge can guide the development of chatbot design guidelines.}
}
@article{DEER2025212,
title = {Toward open science in marketing research},
journal = {International Journal of Research in Marketing},
volume = {42},
number = {1},
pages = {212-233},
year = {2025},
issn = {0167-8116},
doi = {https://doi.org/10.1016/j.ijresmar.2024.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167811624001150},
author = {Lachlan Deer and Susanne J. Adler and Hannes Datta and Natalie Mizik and Marko Sarstedt},
keywords = {Open access, Open data, Open science, Preregistration, Transparency},
abstract = {The open science paradigm has gained prominence in marketing as researchers seek to enhance the validity, reliability, and transparency of research methods and findings. Journals and institutions increasingly encourage or require open science practices, and many authors have started to adapt to and meet these new research and publishing expectations. We provide guidance for effectively implementing open science practices in empirical marketing research. Our recommendations, are tailored to the unique methodological approaches and challenges of each subdiscipline and their specific research contexts. Successful integration of these practices into academic marketing research will require concerted and collaborative efforts among authors, journals, institutions, and funding agencies. We argue that the gradual, thoughtful adoption of these principles and practices will improve the quality and efficiency of scientific discovery.}
}
@article{FU2025105347,
title = {Generative AI in the context of assistive technologies: Trends, limitations and future directions},
journal = {Image and Vision Computing},
volume = {154},
pages = {105347},
year = {2025},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2024.105347},
url = {https://www.sciencedirect.com/science/article/pii/S0262885624004529},
author = {Biying Fu and Abdenour Hadid and Naser Damer},
keywords = {Assistive AI, Generative AI, Generative models, Assistive systems, Assistive technologies and services},
abstract = {With the tremendous successes of Large Language Models (LLMs) like ChatGPT for text generation and Dall-E for high-quality image generation, generative Artificial Intelligence (AI) models have shown a hype in our society. Generative AI seamlessly delved into different aspects of society ranging from economy, education, legislation, computer science, finance, and even healthcare. This article provides a comprehensive survey on the increased and promising use of generative AI in assistive technologies benefiting different parties, ranging from the assistive system developers, medical practitioners, care workforce, to the people who need the care and the comfort. Ethical concerns, biases, lack of transparency, insufficient explainability, and limited trustworthiness are major challenges when using generative AI in assistive technologies, particularly in systems that impact people directly. Key future research directions to address these issues include creating standardized rules, establishing commonly accepted evaluation metrics and benchmarks for explainability and reasoning processes, and making further advancements in understanding and reducing bias and its potential harms. Beyond showing the current trends of applying generative AI in the scope of assistive technologies in four identified key domains, which include care sectors, medical sectors, helping people in need, and co-working, the survey also discusses the current limitations and provides promising future research directions to foster better integration of generative AI in assistive technologies.}
}
@article{HABLER2025104090,
title = {Adversarial machine learning threat analysis and remediation in Open Radio Access Network (O-RAN)},
journal = {Journal of Network and Computer Applications},
volume = {236},
pages = {104090},
year = {2025},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2024.104090},
url = {https://www.sciencedirect.com/science/article/pii/S1084804524002674},
author = {Edan Habler and Ron Bitton and Dan Avraham and Eitan Klevansky and Dudu Mimran and Oleg Brodt and Heiko Lehmann and Yuval Elovici and Asaf Shabtai},
keywords = {Open Radio Access Networks, Adversarial machine learning, Security and privacy, Threat analysis},
abstract = {O-RAN is a new, open, adaptive, and intelligent RAN architecture. Motivated by the success of artificial intelligence in other domains, O-RAN strives to leverage machine learning (ML) to automatically and efficiently manage network resources in diverse use cases such as traffic steering, quality of experience prediction, and anomaly detection. Unfortunately, it has been shown that ML-based systems are vulnerable to an attack technique referred to as adversarial machine learning (AML). This special kind of attack has already been demonstrated in recent studies and in multiple domains. In this paper, we present a systematic AML threat analysis for O-RAN. We start by reviewing relevant ML use cases and analyzing the different ML workflow deployment scenarios in O-RAN. Then, we define the threat model, identifying potential adversaries, enumerating their adversarial capabilities, and analyzing their main goals. Next, we explore the various AML threats associated with O-RAN and review a large number of attacks that can be performed to realize these threats and demonstrate an AML attack on a traffic steering model. In addition, we analyze and propose various AML countermeasures for mitigating the identified threats. Finally, based on the identified AML threats and countermeasures, we present a methodology and a tool for performing risk assessment for AML attacks for a specific ML use case in O-RAN.}
}
@article{MAO2025107191,
title = {SyniEMG: An open-source platform for synthesizing intramuscular electromyography signals from kinematic inputs},
journal = {Biomedical Signal Processing and Control},
volume = {102},
pages = {107191},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.107191},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424012497},
author = {Juzheng Mao and Honghan Li and Jinyang Yu and Haijun Wu and Miguel Bordallo López and Yongkun Zhao},
keywords = {Intramuscular electromyography, Neuromechanics, Reverse synthesis, Machine learning, Generative model},
abstract = {Intramuscular Electromyography (iEMG) is a critical tool for neuromuscular diagnostics but is limited by its invasive nature, which causes patient discomfort and incurs significant costs. To address these challenges, we propose SyniEMG, an innovative, open-source platform that synthesizes iEMG signals from kinematic data using a hybrid generative model, providing a non-invasive and cost-effective alternative. SyniEMG integrates Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, attention mechanisms, and Generative Adversarial Networks (GANs) to effectively capture the spatial and temporal patterns necessary for accurate iEMG signal synthesis. The platform tackles the largely unexplored challenge of reverse neuromechanical modeling by converting kinematic data into continuous muscle activity signals, marking a significant advancement over traditional forward modeling techniques. We validated the platform using datasets, demonstrating its capability to accurately capture muscle activity dynamics. Moreover, SyniEMG requires only a brief training session to collect both iEMG and kinematic signals, enabling efficient synthesis of extended iEMG sequences. This approach reduces the risks associated with prolonged electrode use and provides a practical solution for continuous clinical monitoring and research. Quantitative evaluations in both time and frequency domains confirm the effectiveness of the platform, particularly in exoskeleton applications for motion enhancement and rehabilitation. By enabling indirect monitoring of muscle activity without the need for EMG sensors, SyniEMG has the potential to minimize injury risks, such as overstretching, during movement.}
}
@article{ZHANG2025100301,
title = {On large language models safety, security, and privacy: A survey},
journal = {Journal of Electronic Science and Technology},
volume = {23},
number = {1},
pages = {100301},
year = {2025},
issn = {1674-862X},
doi = {https://doi.org/10.1016/j.jnlest.2025.100301},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X25000023},
author = {Ran Zhang and Hong-Wei Li and Xin-Yuan Qian and Wen-Bo Jiang and Han-Xiao Chen},
keywords = {Large language models, Privacy issues, Safety issues, Security issues},
abstract = {The integration of artificial intelligence (AI) technology, particularly large language models (LLMs), has become essential across various sectors due to their advanced language comprehension and generation capabilities. Despite their transformative impact in fields such as machine translation and intelligent dialogue systems, LLMs face significant challenges. These challenges include safety, security, and privacy concerns that undermine their trustworthiness and effectiveness, such as hallucinations, backdoor attacks, and privacy leakage. Previous works often conflated safety issues with security concerns. In contrast, our study provides clearer and more reasonable definitions for safety, security, and privacy within the context of LLMs. Building on these definitions, we provide a comprehensive overview of the vulnerabilities and defense mechanisms related to safety, security, and privacy in LLMs. Additionally, we explore the unique research challenges posed by LLMs and suggest potential avenues for future research, aiming to enhance the robustness and reliability of LLMs in the face of emerging threats.}
}
@article{PEREZVELASCO2025350,
title = {In-hospital linagliptin for management simplification and hypoglycemia reduction in very old patients with type 2 diabetes},
journal = {Medicina Clínica},
volume = {164},
number = {7},
pages = {350-357},
year = {2025},
issn = {0025-7753},
doi = {https://doi.org/10.1016/j.medcli.2024.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0025775324006997},
author = {Miguel A. Pérez-Velasco and Julio Osuna-Sánchez and Mercedes Millán-Gómez and Michele Ricci and Almudena López-Sampalo and María-Rosa Bernal-López and Ricardo Gómez-Huelgas and Luis M. Pérez-Belmonte},
keywords = {Age≥80, Type 2 diabetes, Linagliptin, Insulin, Hospitalization, Edad ≥80, Diabetes mellitus tipo 2, Linagliptina, Insulina, Hospitalización},
abstract = {Introduction and objectives
The role of in-hospital dipeptidyl peptidase-4 inhibitors in very old patients has not been widely described. This work analyzes the simplification of in-hospital antihyperglycemic management (less insulin use) and reductions in hypoglycemia events using linagliptin in patients aged≥80 years with type 2 diabetes.
Patients and methods
This real-world observational study included hospitalized patients≥80 years with type 2 diabetes treated with an antihyperglycemic protocol of either basal-bolus insulin or linagliptin between January 2016 and December 2023. A 1:1 propensity score matching analysis was performed.
Results
Post-matching, 944 patients were included in each group. The total and basal insulin doses and number of daily injections were significantly lower in the linagliptin group than the basal-bolus insulin group with no differences in glycemic efficacy. Regarding safety, patients on the basal-bolus insulin regimen had more hypoglycemic events. The use of basal-bolus insulin regimen (odds ratio: 4.22; 95% confidence interval: 2.14–6.28; p<0.001), a higher total insulin dose (odds ratio: 3.55; 95% confidence interval: 2.02–5.36; p<0.001) and the number of insulin injections (odds ratio: 2.86; 95% confidence interval: 1.50–4.12; p=0.002) were associated with a greater risk of hypoglycemia. Other hypoglycemia risk factors were older age, moderate–severe functional dependence, moderate–severe dementia, polypharmacy, and complex health status.
Conclusions
The linagliptin regimen simplified in-hospital antihyperglycemic management and reduced hypoglycemia events compared to basal-bolus insulin regimen in patients with type 2 diabetes aged≥80 years. Basal-bolus insulin use and clinical factors were associated with hypoglycemia. The linagliptin regimen could be considered as standard of care for older adult type 2 diabetes patients in the hospital setting.
Resumen
Antecedentes y objetivo
El papel de los inhibidores de la dipeptidil-peptidasa-4 intrahospitalario en pacientes de edad avanzada no ha sido ampliamente descrito. Este estudio analiza la simplificación del manejo antihiperglucémico intrahospitalario (uso de menos insulina) y la reducción de hipoglucemia usando linagliptina en pacientes de ≥80 años con diabetes mellitus tipo 2.
Materiales y métodos
Estudio observacional en pacientes hospitalizados con ≥80 años con diabetes tipo 2 tratados con el protocolo antihiperglucémico que incluye el regimen insulina en bolo-basal o linagliptina, entre enero 2016-diciembre 2023. Se realizó un análisis de puntuaciones de propensión 1:1.
Resultados
Tras la propensión, 944 pacientes fueron incluidos por grupo. Las dosis de insulina total y basal y el número de inyecciones fueron significativamente menores en el grupo linagliptina sin diferencias en la eficacia glucémica. Respecto a la seguridad, los pacientes con insulina bolo-basal tuvieron más hipoglucemias. El uso de insulina bolo-basal (Odds Ratio: 4.22; Intervalo de confianza 95%: 2.14-6.28; p<0.001), mayor dosis de insulina total (Odds Ratio: 3.55; Intervalo de confianza 95%: 2.02-5.36; p<0.001) y número de inyecciones (Odds Ratio: 2.86; Intervalo de confianza 95%: 1.50-4.12; p=0.002) fueron asociados con mayor riesgo de hipoglucemia. Otros factores fueron la edad avanzada, dependencia funcional moderada-severa, demencia moderada-severa, polifarmacia y estado de salud complejo.
Conclusiones
Linagliptina simplificó el manejo antihiperglucémico y redujo hipoglucemias respecto al regimen de insulina en bolo-basal en pacientes de ≥80 años con diabetes tipo 2. El uso del régimen bolo-basal y factores clínicos fueron asociados con la hipoglucemia. Linagliptina intrahospitalaria podría ser considerada como el tratamiento estándar para pacientes de edad avanzada con diabetes tipo 2.}
}