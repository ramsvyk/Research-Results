@article{GIBERT2022117957,
title = {Fusing feature engineering and deep learning: A case study for malware classification},
journal = {Expert Systems with Applications},
volume = {207},
pages = {117957},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117957},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422011927},
author = {Daniel Gibert and Jordi Planes and Carles Mateu and Quan Le},
keywords = {Malware classification, Machine learning, Deep learning, Feature extraction, Feature fusion},
abstract = {Machine learning has become an appealing signature-less approach to detect and classify malware because of its ability to generalize to never-before-seen samples and to handle large volumes of data. While traditional feature-based approaches rely on the manual design of hand-crafted features based on experts’ knowledge of the domain, deep learning approaches replace the manual feature engineering process by an underlying system, typically consisting of a neural network with multiple layers, that perform both feature learning and classification altogether. However, the combination of both approaches could substantially enhance detection systems. In this paper we present an hybrid approach to address the task of malware classification by fusing multiple types of features defined by experts and features learned through deep learning from raw data. In particular, our approach relies on deep learning to extract N-gram like features from the assembly language instructions and the bytes of malware, and texture patterns and shapelet-based features from malware’s grayscale image representation and structural entropy, respectively. These deep features are later passed as input to a gradient boosting model that combines the deep features and the hand-crafted features using an early-fusion mechanism. The suitability of our approach has been evaluated on the Microsoft Malware Classification Challenge benchmark and results show that the proposed solution achieves state-of-the-art performance and outperforms gradient boosting and deep learning methods in the literature.}
}
@article{FINDER2022108266,
title = {Time-interval temporal patterns can beat and explain the malware},
journal = {Knowledge-Based Systems},
volume = {241},
pages = {108266},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108266},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122000843},
author = {Ido Finder and Eitam Sheetrit and Nir Nissim},
keywords = {Malware, Detection, Time series, Time interval, Machine learning},
abstract = {Malware-based cyber-attacks are mainly aimed at obtaining sensitive data, intellectual property theft, denying critical services and data, and financial gain. Malware has continuously evolved, becoming more sophisticated and evasive, and thus it remains a major cyber-security threat. To keep pace with malware’s evolution, there is a critical need to develop new, advanced malware detection methods. Widely-used solutions, such as antivirus software and other static host-based intrusion detection systems, have limitations, particularly in detecting new, unknown, and evasive malware. Many of the limitations of static analysis can be overcome when dynamic malware analysis is leveraged by machine learning (ML) algorithms by executing the malware in an isolated environment (e.g., sandbox), which enables the acquisition of rich behavioral and time-oriented information associated with malware behavior. Prior studies have proposed various detection methods based on dynamically extracted API calls for malware detection, but other than simple order-based approaches, the use of more advanced time-based methods has not been explored. In this paper, we propose a more comprehensive detection framework which, by analyzing the raw multivariate time-series data associated with malware execution, can accurately capture malware behavior and provide clear explainability regarding malware behavior and detection model decisions. We are the first to mine and automatically discover meaningful and explainable time-interval temporal API call patterns associated with malware behavior and leverage them, using a variety of ML algorithms, for malware detection and categorization. To evaluate our proposed solution, we established a comprehensive dynamic-analysis environment using Cuckoo Sandbox and analyzed more than 17,000 portable executables executed in Windows 10, the most widely-used operating system today. We conducted extensive experiments on malware detection and categorization and compared the performance of our solution to state-of-the-art methods, including non-time-oriented (classic ML algorithms) and order-based methods (LSTM networks). The results show that our proposed solution outperforms the other methods, obtaining 99.6% detection accuracy for unknown malware and 97.65% categorization accuracy. In a more complex scenario of detecting an unknown malware type with unseen modus operandi, our method obtained almost 90% detection accuracy, outperforming the state-of-the-art methods. To demonstrate our ability to provide human explainability, we present some temporal patterns of different malware families that we discovered which shed light on malware behavior that can be used by cyber-security experts to better understand malware, better defend against future attacks, and even attribute malware campaigns to the cyber-attackers launching them.}
}
@article{JAIN2023104672,
title = {Social movements and institutional entrepreneurship as facilitators of technology transition: The case of free/open-source software},
journal = {Research Policy},
volume = {52},
number = {2},
pages = {104672},
year = {2023},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2022.104672},
url = {https://www.sciencedirect.com/science/article/pii/S0048733322001937},
author = {Sanjay Jain and Habib A. Islam and Martin C. Goossen and Anil Nair},
keywords = {Technological regimes, Multi-level perspective, Strategic niche management, micro-histories, Social movements, Institutional entrepreneurship},
abstract = {We integrate insights from the literature on social movements and institutional entrepreneurship into the strategic niche management (SNM) and multilevel perspective (MLP) frameworks to understand the emergence of Linux, a free/open-source operating system, in a regime dominated by proprietary operating systems such as Unix and Windows NT. Employing a “microhistories” methodology, we document how actors in the free/open-source movement took steps that enabled an alternate technological niche to form, gain momentum and eventually infiltrate the extant regime. Our account delineates the key role that actors play in shaping the identity of a niche, amplifying its presence, and finally mainstreaming it. We observe a heterogenous response by incumbents to the emergent niche and highlight the sustained coexistence of a niche and regime as a distinct form of technological transition. Finally, we demonstrate the significant impact that a niche can have, spanning beyond the targeted regime, and becoming part of the landscape. Our insights highlight how tracing the processes involved in the emergence and development of a niche can provide a prospective and generative understanding of technological transition, thereby contributing to and complementing the extant SNM and MLP literatures.}
}
@article{ZHAO20233325,
title = {A review on the application of molecular descriptors and machine learning in polymer design},
journal = {Polymer Chemistry},
volume = {14},
number = {29},
pages = {3325-3346},
year = {2023},
issn = {1759-9954},
doi = {https://doi.org/10.1039/d3py00395g},
url = {https://www.sciencedirect.com/science/article/pii/S1759995423001808},
author = {Yuankai Zhao and Roger J. Mulder and Shadi Houshyar and Tu C. Le},
abstract = {Polymers are an important class of materials with vast arrays of physical and chemical properties and have been widely used in many applications and industrial products. Although there have been many successful polymer design studies, the pace of materials discovery research can be accelerated to meet the high demand for new, functional materials. With the advanced development of artificial intelligence, the use of machine learning has shown great potential in data-driven design and the discovery of polymers to date. Several polymer datasets have been compiled, allowing robust machine learning models to be trained and provide accurate predictions of various polymer properties. Such models are useful for screening promising candidate polymers with high-performing properties prior to lab synthesis. In this review, we focus on the most critical components of polymer design using molecular descriptors and machine learning algorithms. A summary of existing polymer databases is provided, and the different categories of polymer descriptors are discussed in detail. The application of these descriptors in machine learning studies of polymer design is critically reviewed, leading to a discussion of the challenges, opportunities, and future perspectives for polymer research using these advanced computational tools.}
}
@article{GUARINO2022117942,
title = {PaTRIZ: A framework for mining TRIZ contradictions in patents},
journal = {Expert Systems with Applications},
volume = {207},
pages = {117942},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117942},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422011800},
author = {Guillaume Guarino and Ahmed Samet and Denis Cavallucci},
keywords = {Patent, Deep learning, NLP, Contradiction, TRIZ},
abstract = {Patents are a significant source of information about inventions. However, understanding the content of a patent with the aim of using it for an automatic solution search is still an unsolved challenge. To achieve this purpose, a model based on the TRIZ theory (Altshuller, 1984) has been developed. This theory introduces the notion of contradiction, which is a reliable and domain-independent technique to formulate the problem solved by each patent through an opposition between parameters of a system. Each patent is considered a solution concept to a contradiction. Mining contradictions, therefore, means characterizing solution concepts. In this paper, we propose a new approach called PaTRIZ, a complete framework for patent analysis based on a combination of sentences and word-level deep neural networks. The word-level network, called ParaBERT, comprises a novel Conditional Random Field structure, developed to integrate syntactic information. The idea is to mine the patent’s motivating problem (aka contradiction), which is fundamental to understanding the invention and identifying for which purpose it could be used. The models are evaluated on built-in real-world datasets.}
}
@article{2022100189,
title = {Full Issue PDF},
journal = {JACC: Advances},
volume = {1},
number = {5},
pages = {100189},
year = {2022},
issn = {2772-963X},
doi = {https://doi.org/10.1016/S2772-963X(22)00287-3},
url = {https://www.sciencedirect.com/science/article/pii/S2772963X22002873}
}
@article{OGINK2023102621,
title = {Mechanisms in open innovation: A review and synthesis of the literature},
journal = {Technovation},
volume = {119},
pages = {102621},
year = {2023},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2022.102621},
url = {https://www.sciencedirect.com/science/article/pii/S0166497222001687},
author = {Ruben H.A.J. Ogink and Martin C. Goossen and A. Georges L. Romme and Henk Akkermans},
keywords = {Open innovation, Literature review, Generative mechanisms, Research synthesis, Mechanisms, Research agenda},
abstract = {A large body of literature explores the role of context, structure, actors, and outcomes of open innovation (OI), yet pays little attention to the mechanisms underlying these relationships. In this review paper, we synthesize the OI literature using a context-mechanism-outcome approach to identify and classify the various mechanisms observed in empirical OI studies. Our findings demonstrate that the OI literature draws on a wide variety of mechanisms originating from the fields of management, sociology, economics, and psychology. The fifteen mechanisms most frequently observed in the literature fall into four categories: governance and policies; environmental dynamics and interactions; knowledge, skills, and capabilities; and learning by doing. Moreover, by examining the levels of analysis of these mechanisms, we observe substantial differences in how these mechanisms operate at the individual, project, firm, network, and society level. Finally, we identify various avenues for future research arising from our synthesis of the literature.}
}
@article{MADAN2023101774,
title = {AI adoption and diffusion in public administration: A systematic literature review and future research agenda},
journal = {Government Information Quarterly},
volume = {40},
number = {1},
pages = {101774},
year = {2023},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2022.101774},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X22001101},
author = {Rohit Madan and Mona Ashok},
keywords = {Public administration, Public values, AI tensions, Absorptive capacity, Data governance, AI adoption, AI diffusion},
abstract = {Artificial Intelligence (AI) implementation in public administration is gaining momentum heralded by the hope of smart public services that are personalised, lean, and efficient. However, the use of AI in public administration is riddled with ethical tensions of fairness, transparency, privacy, and human rights. We call these AI tensions. The current literature lacks a contextual and processual understanding of AI adoption and diffusion in public administration to be able to explore such tensions. Previous studies have outlined risks, benefits, and challenges with the use of AI in public administration. However, a large gap remains in understanding AI tensions as they relate to public value creation. Through a systematic literature review grounded in public value management and the resource-based view of the firms, we identify technology-organisational-environmental (TOE) contextual variables and absorptive capacity as factors influencing AI adoption as discussed in the literature. To our knowledge, this is the first paper that outlines distinct AI tensions from an AI implementation and diffusion perspective within public administration. We develop a future research agenda for the full AI innovation lifecycle of adoption, implementation, and diffusion.}
}
@article{RANI2022200148,
title = {A multi-modal bone suppression, lung segmentation, and classification approach for accurate COVID-19 detection using chest radiographs},
journal = {Intelligent Systems with Applications},
volume = {16},
pages = {200148},
year = {2022},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2022.200148},
url = {https://www.sciencedirect.com/science/article/pii/S2667305322000850},
author = {Geeta Rani and Ankit Misra and Vijaypal Singh Dhaka and Deepak Buddhi and Ravindra Kumar Sharma and Ester Zumpano and Eugenio Vocaturo},
keywords = {SARS-CoV-2, COVID-19, Deep learning, Biomedical, Medical imaging},
abstract = {The high transmission rate of COVID-19 and the lack of quick, robust, and intelligent systems for its detection have become a point of concern for the public, Government, and health experts worldwide. The study of radiological images is one of the fastest ways to comprehend the infectious spread and diagnose a patient. However, it is difficult to differentiate COVID-19 from other pneumonic infections. The purpose of this research is to provide an automatic, precise, reliable, robust, and intelligent assisting system ‘Covid Scanner’ for mass screening of COVID-19, Non-COVID Viral Pneumonia, and Bacterial Pneumonia from healthy chest radiographs. To train the proposed system, the authors of this research prepared novel a dataset called, “COVID-Pneumonia CXR”. The system is a coherent integration of bone suppression, lung segmentation, and the proposed classifier, ‘EXP-Net’. The system reported an AUC of 96.58% on the validation dataset and 96.48% on the testing dataset comprising chest radiographs. The results from the ablation study prove the efficacy and generalizability of the proposed integrated pipeline of models. To prove the system's reliability, the feature heatmaps visualized in the lung region were validated by radiology experts. Moreover, a comparison with the state-of-the-art models and existing approaches shows that the proposed system finds clearer demarcation between the highly similar chest radiographs of COVID-19 and Non-COVID viral pneumonia. The copyright of “Covid Scanner” is protected with registration number SW-13625/2020. The code for the models used in this research is publicly available at: https://github.com/Ankit-Misra/multi_modal_covid_detection/.}
}
@incollection{APPLING2022585,
title = {Machine Learning for Understanding Inland Water Quantity, Quality, and Ecology},
editor = {Thomas Mehner and Klement Tockner},
booktitle = {Encyclopedia of Inland Waters (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {585-606},
year = {2022},
isbn = {978-0-12-822041-2},
doi = {https://doi.org/10.1016/B978-0-12-819166-8.00121-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191668001213},
author = {Alison P. Appling and Samantha K. Oliver and Jordan S. Read and Jeffrey M. Sadler and Jacob A. Zwart},
keywords = {Artificial intelligence, Classification and regression trees, Clustering, Data mining, Deep learning, Dimensionality reduction, Machine learning, Neural networks},
abstract = {This chapter provides an overview of machine learning models and their applications to the science of inland waters. Such models serve a wide range of purposes for science and management: predicting water quality, quantity, or ecological dynamics across space, time, or hypothetical scenarios; vetting and distilling raw data for further modeling or analysis; generating and exploring hypotheses; estimating physically or biologically meaningful parameters for use in further modeling; and revealing patterns in complex, multidimensional data or model outputs. An important research frontier is the injection of limnological knowledge into machine-learning models, which has shown great promise for increasing such models’ accuracy, trustworthiness, and interpretability. Here we describe a few of the most powerful machine learning tools, describe best practices for employing these tools and injecting knowledge guidance, and give examples of their applications to advance understanding of inland waters.}
}
@article{PATEL202319,
title = {Visual dubbing pipeline with localized lip-sync and two-pass identity transfer},
journal = {Computers & Graphics},
volume = {110},
pages = {19-27},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2022.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0097849322001984},
author = {Dhyey Patel and Houssem Zouaghi and Sudhir Mudur and Eric Paquette and Serge Laforest and Martin Rouillard and Tiberiu Popa},
keywords = {Visual dubbing, Reenactment, Style transfer},
abstract = {Visual dubbing uses visual computing and deep learning to alter the lip and mouth articulations of the actor to sync with the dubbed speech. It has the potential to greatly improve the content generated from the dubbing industry. Quality of the dubbed result is primary for the industry. An important requirement is that visual lip sync changes be localized to the mouth region and not affect the rest of the actor’s face or the rest of the video frame. Current methods can create realistic looking fake faces with expressions. However, many fail to localize lip sync and have quality problems such as identity loss, low-res, blurs, face skin feature or color loss, and temporal jitter. These problems mainly arise because end-to-end training of networks to correctly disentangle these different visual dubbing parameters (pose, skin color, identity, lip movements, etc.) is very difficult to achieve. Our main contribution is a new visual dubbing pipeline, in which, instead of end-to-end training we apply incrementally different disentangling techniques for each parameter. Our pipeline is composed of three main steps: pose alignment, identity transfer and video reassembly. Expert models in each step are fine-tuned for the actor. We propose an identity transfer network with an added style block, which with pre-training is able to decouple face components, specifically identity and expression, and also works with short video clips like TV ads. Our pipeline also includes novel stages related to temporal smoothing of the reenacted face, actor specific super resolution to retain fine facial details, and a second pass through the identity transfer network for preserving actor identity. Localization of lip-sync is achieved by restricting changes in the original video frame to just the actor’s mouth region. The results are convincing, and a user survey also confirms their quality. Relevant quantitative metrics are included.}
}
@article{LIU202211964,
title = {Deep learning in single-molecule imaging and analysis: recent advances and prospects},
journal = {Chemical Science},
volume = {13},
number = {41},
pages = {11964-11980},
year = {2022},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d2sc02443h},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023005850},
author = {Xiaolong Liu and Yifei Jiang and Yutong Cui and Jinghe Yuan and Xiaohong Fang},
abstract = {ABSTRACT
Single-molecule microscopy is advantageous in characterizing heterogeneous dynamics at the molecular level. However, there are several challenges that currently hinder the wide application of single molecule imaging in bio-chemical studies, including how to perform single-molecule measurements efficiently with minimal run-to-run variations, how to analyze weak single-molecule signals efficiently and accurately without the influence of human bias, and how to extract complete information about dynamics of interest from single-molecule data. As a new class of computer algorithms that simulate the human brain to extract data features, deep learning networks excel in task parallelism and model generalization, and are well-suited for handling nonlinear functions and extracting weak features, which provide a promising approach for single-molecule experiment automation and data processing. In this perspective, we will highlight recent advances in the application of deep learning to single-molecule studies, discuss how deep learning has been used to address the challenges in the field as well as the pitfalls of existing applications, and outline the directions for future development.}
}
@incollection{XU2022433,
title = {Functionally Graded Lattice Structures: Fabrication Methods, Mechanical Properties, Failure Mechanisms and Applications},
editor = {M H Ferri Aliabadi and Winston O. Soboyejo},
booktitle = {Comprehensive Structural Integrity (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {433-466},
year = {2022},
isbn = {978-0-323-91945-6},
doi = {https://doi.org/10.1016/B978-0-12-822944-6.00019-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128229446000190},
author = {Zhuo Xu and Seyed Mohammad Javad Razavi and Majid R. Ayatollahi},
keywords = {Additive manufacturing, Architected cellular materials, Cellular structures, Energy absorption, Functionally graded lattice structures, Graded lattice structures, Lattice structures, Lightweight design, Metamaterials, Porous materials},
abstract = {Lattice structures have become a universal three-dimensional design model that can be treated as an excellent candidate for energy absorption and light-weighting purposes. Development of recent fabrication techniques such as Additive Manufacturing (AM) has given further flexibility in design and fabrication of these porous structures. Although the topic of design and optimizing uniform lattice structures (ULS) have attracted considerable attention during the past decade, there is still a knowledge gap in the design of functionally graded lattice structures (FGLS). Due to the unique method of customizing the structural distributions and performances, FGLSs can have a multifunctional nature that requires further studies. This chapter reviews the fabrication methods, mechanical properties, and industrial applications of FGLSs fabricated via various AM technologies, as well as a comparison between the mechanical properties of ULS and FGLS.}
}
@article{WILT2022103761,
title = {Performing color-evasiveness: A DisCrit analysis of educators’ discourse in the U.S.},
journal = {Teaching and Teacher Education},
volume = {117},
pages = {103761},
year = {2022},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2022.103761},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X22001354},
author = {Courtney L. Wilt and Subini A. Annamma and Jennifer M. Wilmot and Sylvia N. Nyegenye and Amanda L. Miller and Elizabeth E. Jackson},
keywords = {DisCrit, Discourse, White educators, Color-blindness, Color-evasive racism},
abstract = {This study explores how an ideology of color-evasive racism (i.e., color evasiveness; Annamma et al., 2017) imbued white educators' discourse surrounding intersectional inequities in schools for Girls of Color in the U.S. Our analysis of interview and focus group data addresses a gap in educational research identifying color-evasive racism in discourse by in-service educators, specifically for white educators making sense of inequities in schools. We draw from Bonilla-Silva's (2018) application of color-blindness to discourse to identify three specific discursive frames that white educators employ, namely 1) centering self, 2) claiming white racial innocence, and 3) employing progressive notions, and the discursive tools within each. This focus on white educators' discourse expands understandings of how color-evasivene racism is employed, (re)producing intersectional inequities in education. Given that each of these educators was nominated because of their strengths working with Girls of Color, we believe this paper's significance captures the complexities of teaching in a system of white supremacy and identifies underlying ideologies animating discourse that can be disrupted through a Disability Critical Race Theory (DisCrit) lens.}
}
@article{RAO2023103047,
title = {Threat modeling framework for mobile communication systems},
journal = {Computers & Security},
volume = {125},
pages = {103047},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.103047},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822004394},
author = {Siddharth Prakash Rao and Hsin-Yi Chen and Tuomas Aura},
keywords = {Threat modeling, Security framework, Mobile communication},
abstract = {This paper presents a domain-specific threat-modeling framework for the cellular mobile networks. We survey known attacks against mobile communication and organize them into attack phases, tactical objectives, and techniques. The Bhadra framework aims to provide a structured way to analyze and communicate threats on a level that abstracts away the technical details but still provides meaningful insights into the adversarial behavior. Our goals are similar to existing threat modeling frameworks for enterprise information systems, but with a focus on mobile operator networks. The framework fills a gap that has existed in tools and methodology for sharing of threat intelligence within and between organizations in the telecommunications industry. The paper includes concrete case studies of applying the framework. It can also be read as a survey of attacks against mobile networks. CCS CONCEPTS Security and privacy → Security requirements; Mobile and wireless security; Networks→ Networks Mobile networks}
}
@article{ZHANG2023103672,
title = {A survey on privacy inference attacks and defenses in cloud-based Deep Neural Network},
journal = {Computer Standards & Interfaces},
volume = {83},
pages = {103672},
year = {2023},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2022.103672},
url = {https://www.sciencedirect.com/science/article/pii/S0920548922000435},
author = {Xiaoyu Zhang and Chao Chen and Yi Xie and Xiaofeng Chen and Jun Zhang and Yang Xiang},
keywords = {Privacy inference attack, Privacy defense, Deep Neural Network, Cloud computing},
abstract = {Deep Neural Network (DNN), one of the most powerful machine learning algorithms, is increasingly leveraged to overcome the bottleneck of effectively exploring and analyzing massive data to boost advanced scientific development. It is not a surprise that cloud computing providers offer the cloud-based DNN as an out-of-the-box service. Though there are some benefits from the cloud-based DNN, the interaction mechanism among two or multiple entities in the cloud inevitably induces new privacy risks. This survey presents the most recent findings of privacy attacks and defenses appeared in cloud-based neural network services. We systematically and thoroughly review privacy attacks and defenses in the pipeline of cloud-based DNN service, i.e., data manipulation, training, and prediction. In particular, a new theory, called cloud-based ML privacy game, is extracted from the recently published literature to provide a deep understanding of state-of-the-art research. Finally, the challenges and future work are presented to help researchers to continue to push forward the competitions between privacy attackers and defenders.}
}
@article{DENTER2022102130,
title = {Measuring generative appropriability: Experiments with US semiconductor patents},
journal = {World Patent Information},
volume = {70},
pages = {102130},
year = {2022},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2022.102130},
url = {https://www.sciencedirect.com/science/article/pii/S0172219022000369},
author = {Nils M. Denter and Mei Yun Lai},
keywords = {Generative appropriability, Semiconductor patents, Knowledge flow, Textual data, USPTO patents},
abstract = {This study presents a novel approach to quantify Generative Appropriability (GA) – a firm's capability to exploit current inventions to generate follow-on inventions (cumulative GA) and preclude rivals from them (preclusive GA). Unlike prior research, our approach relies not only on patent citations but also on texts. In the first step, we preprocess the texts of the focal patents and their citations. In the second step, we measure knowledge flow by similarity measurements. In the third step, we calculate cumulative and preclusive GA by means of constructed formulae. To test our approach, we select particularly active firms in patenting US semiconductor inventions. First, results from knowledge flow confirm our initial concerns on measuring knowledge flow by means of patent citation count. Second, results from GA show different patterns among the semiconductor firms and firms which are well-positioned in both perspectives and firms which lack cumulative GA but show good values in preclusive GA.}
}
@article{IHME2022101010,
title = {Combustion machine learning: Principles, progress and prospects},
journal = {Progress in Energy and Combustion Science},
volume = {91},
pages = {101010},
year = {2022},
issn = {0360-1285},
doi = {https://doi.org/10.1016/j.pecs.2022.101010},
url = {https://www.sciencedirect.com/science/article/pii/S0360128522000193},
author = {Matthias Ihme and Wai Tong Chung and Aashwin Ananda Mishra},
keywords = {Machine learning, Data-driven methods, Combustion},
abstract = {Progress in combustion science and engineering has led to the generation of large amounts of data from large-scale simulations, high-resolution experiments, and sensors. This corpus of data offers enormous opportunities for extracting new knowledge and insights—if harnessed effectively. Machine learning (ML) techniques have demonstrated remarkable success in data analytics, thus offering a new paradigm for data-intense analyses and scientific investigations through combustion machine learning (CombML). While data-driven methods are utilized in various combustion areas, recent advances in algorithmic developments, the accessibility of open-source software libraries, the availability of computational resources, and the abundance of data have together rendered ML techniques ubiquitous in scientific analysis and engineering. This article examines ML techniques for applications in combustion science and engineering. Starting with a review of sources of data, data-driven techniques, and concepts, we examine supervised, unsupervised, and semi-supervised ML methods. Various combustion examples are considered to illustrate and to evaluate these methods. Next, we review past and recent applications of ML approaches to problems in combustion, spanning fundamental combustion investigations, propulsion and energy-conversion systems, and fire and explosion hazards. Challenges unique to CombML are discussed and further opportunities are identified, focusing on interpretability, uncertainty quantification, robustness, consistency, creation and curation of benchmark data, and the augmentation of ML methods with prior combustion-domain knowledge.}
}
@article{WIDDICKS2022102853,
title = {Escaping unsustainable digital interactions: Toward “more meaningful” and “moderate” online experiences},
journal = {International Journal of Human-Computer Studies},
volume = {165},
pages = {102853},
year = {2022},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2022.102853},
url = {https://www.sciencedirect.com/science/article/pii/S1071581922000799},
author = {Kelly Widdicks and Christian Remy and Oliver Bates and Adrian Friday and Mike Hazas},
keywords = {Sustainability, Moderate, Meaningful, Interactions, Digital devices, Online services, Digital wellbeing, Work productivity, Social relationships, Online privacy},
abstract = {Growing and even excessive use of digital technology has unquestionably fuelled demand for digital devices and online services leading to a wide range of societal and environmental impacts. In sustainability terms, ICT as a whole is estimated to produce up to nearly 4% of global greenhouse gas emissions. As presumed responsible innovators, the HCI community should now consider design strategies that will reduce use and demand for digital technology for the good of both its users and the planet—strategies perhaps even seen as retrogressive in an era where digital technology is constantly implicated in innovation and economic growth. Prior work has noted the potential to design “more moderate” interactions for sustainability, simultaneously addressing negative societal impacts on users’ wellbeing, relationships, productivity at work, and privacy. In this paper, we explore how we may design intentionally moderate digital interactions that retain our participants’ “more meaningful” experiences. We report on the outcomes of two design workshops to uncover experiences of meaningful device and service use, to inform practical designs for ‘moderate and meaningful’ interaction. From this, we offer design recommendations that aim to address the multiple negative impacts that digital technology can create, and discuss the possible barriers to these designs.}
}
@article{MIRSKY2023103006,
title = {The Threat of Offensive AI to Organizations},
journal = {Computers & Security},
volume = {124},
pages = {103006},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.103006},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822003984},
author = {Yisroel Mirsky and Ambra Demontis and Jaidip Kotak and Ram Shankar and Deng Gelei and Liu Yang and Xiangyu Zhang and Maura Pintor and Wenke Lee and Yuval Elovici and Battista Biggio},
keywords = {Offensive AI, APT, Cyber security, Organization security, Adversarial machine learning, Deepfake, AI-Capable adversary},
abstract = {AI has provided us with the ability to automate tasks, extract information from vast amounts of data, and synthesize media that is nearly indistinguishable from the real thing. However, positive tools can also be used for negative purposes. In particular, cyber adversaries can use AI to enhance their attacks and expand their campaigns. Although offensive AI has been discussed in the past, there is a need to analyze and understand the threat in the context of organizations. For example, how does an AI-capable adversary impact the cyber kill chain? Does AI benefit the attacker more than the defender? What are the most significant AI threats facing organizations today and what will be their impact on the future? In this study, we explore the threat of offensive AI on organizations. First, we present the background and discuss how AI changes the adversary’s methods, strategies, goals, and overall attack model. Then, through a literature review, we identify 32 offensive AI capabilities which adversaries can use to enhance their attacks. Finally, through a panel survey spanning industry, government and academia, we rank the AI threats and provide insights on the adversaries.}
}
@article{XIE2023101043,
title = {Toward autonomous laboratories: Convergence of artificial intelligence and experimental automation},
journal = {Progress in Materials Science},
volume = {132},
pages = {101043},
year = {2023},
issn = {0079-6425},
doi = {https://doi.org/10.1016/j.pmatsci.2022.101043},
url = {https://www.sciencedirect.com/science/article/pii/S0079642522001244},
author = {Yunchao Xie and Kianoosh Sattari and Chi Zhang and Jian Lin},
keywords = {Artificial intelligence, Autonomous experimentation platform, Machine learning, Materials science},
abstract = {The ever-increasing demand for novel materials with superior properties inspires retrofitting traditional research paradigms in the era of artificial intelligence and automation. An autonomous experimental platform (AEP) has emerged as an exciting research frontier that achieves full autonomy via integrating data-driven algorithms such as machine learning (ML) with experimental automation in the material development loop from synthesis, characterization, and analysis, to decision making. In this review, we started with a primer to describe how to develop data-driven algorithms for solving material problems. Then, we systematically summarized recent progress on automated material synthesis, ML-enabled data analysis, and decision-making. Finally, we discussed the challenges and opportunities in an endeavor to develop the next-generation AEP for ultimately realizing an autonomous or self-driving laboratory. This review will provide insights for researchers aiming to learn the frontier of ML in materials science and deploy AEP in their labs for accelerating material development.}
}
@article{KHANLOU2022105440,
title = {Readers Theatre as an arts-based approach to education: A scoping review on experiences of adult learners and educators},
journal = {Nurse Education Today},
volume = {116},
pages = {105440},
year = {2022},
issn = {0260-6917},
doi = {https://doi.org/10.1016/j.nedt.2022.105440},
url = {https://www.sciencedirect.com/science/article/pii/S0260691722001769},
author = {Nazilla Khanlou and Luz Maria Vazquez and Attia Khan and Brenda Orazietti and Grace Ross},
keywords = {Arts-based pedagogy, Drama performance, Nurse education, Readers Theatre, Scoping review, Student active learning},
abstract = {Background
Arts-based educational methodologies have been implemented in nursing and other health disciplines to promote person-centered approaches to care. Readers Theatre has been applied as a tool to promote compassionate and holistic approaches to care. Readers Theatre is a form of drama that requires participants to read aloud a scripted narrative to the audience.
Objectives
To examine the extant literature on experiences of adult learners and educators in utilizing Readers Theatre, and its potential suitability for nurse education. The review question was: “What are the learning experiences of adult students and the teaching experiences of educators in the uptake of Readers Theatre?”
Design and data sources
Scoping review guidelines proposed by Arksey and O'Malley were adopted. Academic databases searches were carried out in ProQuest, JSTOR, Scholars Portal, EBSCO, Web of Science, PubMed, Expanded Academic ASAP, and Scopus.
Review methods
The search and keyword strategy was developed by two reviewers and approved by the lead author, and a librarian. All titles and abstracts were individually examined by the two reviewers with discrepancies discussed and resolved by both parties. Data were extracted for thematic analysis.
Results
A total of 31 studies were selected for the final sample. Four themes were identified within the scoping review relevant to Readers Theatre teaching-learning experiences: 1) principles and characteristics; 2) awareness, understanding, caring and empathy; 3) cross-disciplinary collaboration, interdisciplinary education, and knowledge dissemination; and 4) promoting students' skills.
Conclusions
Readers Theatre has the potential to be utilized within a nursing curriculum, and particularly in theory and substantive class-based courses, through active group learning, in the application phase of knowledge acquisition.}
}
@article{HASANZADEH2022101665,
title = {Could artificial intelligence revolutionize the development of nanovectors for gene therapy and mRNA vaccines?},
journal = {Nano Today},
volume = {47},
pages = {101665},
year = {2022},
issn = {1748-0132},
doi = {https://doi.org/10.1016/j.nantod.2022.101665},
url = {https://www.sciencedirect.com/science/article/pii/S1748013222002936},
author = {Akbar Hasanzadeh and Michael R. Hamblin and Jafar Kiani and Hamid Noori and Joseph M. Hardie and Mahdi Karimi and Hadi Shafiee},
keywords = {Gene therapy, AI, Gene delivery vehicles, CRISPR/Cas, Nanobots, MRNA vaccine carriers},
abstract = {Gene therapy enables the introduction of nucleic acids like DNA and RNA into host cells, and is expected to revolutionize the treatment of a wide range of diseases. This growth has been further accelerated by the discovery of CRISPR/Cas technology, which allows accurate genomic editing in a broad range of cells and organisms in vitro and in vivo . Despite many advances in gene delivery and the development of various viral and non-viral gene delivery vectors, the lack of highly efficient non-viral systems with low cellular toxicity remains a challenge. The application of cutting-edge technologies such as artificial intelligence (AI) has great potential to find new paradigms to solve this issue. Herein, we review AI and its major subfields including machine learning (ML), neural networks (NNs), expert systems, deep learning (DL), computer vision and robotics. We discuss the potential of AI-based models and algorithms in the design of targeted gene delivery vehicles capable of crossing extracellular and intracellular barriers by viral mimicry strategies. We finally discuss the role of AI in improving the function of CRISPR/Cas systems, developing novel nanobots, and mRNA vaccine carriers.}
}
@article{PREIKSAITIS2023,
title = {Opportunities, Challenges, and Future Directions of Generative Artificial Intelligence in Medical Education: Scoping Review},
journal = {JMIR Medical Education},
volume = {9},
year = {2023},
issn = {2369-3762},
doi = {https://doi.org/10.2196/48785},
url = {https://www.sciencedirect.com/science/article/pii/S2369376223000697},
author = {Carl Preiksaitis and Christian Rose},
keywords = {medical education, artificial intelligence, ChatGPT, Bard, AI, educator, scoping, review, learner, generative},
abstract = {Background
Generative artificial intelligence (AI) technologies are increasingly being utilized across various fields, with considerable interest and concern regarding their potential application in medical education. These technologies, such as Chat GPT and Bard, can generate new content and have a wide range of possible applications.
Objective
This study aimed to synthesize the potential opportunities and limitations of generative AI in medical education. It sought to identify prevalent themes within recent literature regarding potential applications and challenges of generative AI in medical education and use these to guide future areas for exploration.
Methods
We conducted a scoping review, following the framework by Arksey and O'Malley, of English language articles published from 2022 onward that discussed generative AI in the context of medical education. A literature search was performed using PubMed, Web of Science, and Google Scholar databases. We screened articles for inclusion, extracted data from relevant studies, and completed a quantitative and qualitative synthesis of the data.
Results
Thematic analysis revealed diverse potential applications for generative AI in medical education, including self-directed learning, simulation scenarios, and writing assistance. However, the literature also highlighted significant challenges, such as issues with academic integrity, data accuracy, and potential detriments to learning. Based on these themes and the current state of the literature, we propose the following 3 key areas for investigation: developing learners’ skills to evaluate AI critically, rethinking assessment methodology, and studying human-AI interactions.
Conclusions
The integration of generative AI in medical education presents exciting opportunities, alongside considerable challenges. There is a need to develop new skills and competencies related to AI as well as thoughtful, nuanced approaches to examine the growing use of generative AI in medical education.}
}
@incollection{2023861,
title = {Index},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {861-952},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.18001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305180017}
}
@article{BEDNARZ2022105667,
title = {Hidden depths: The effects of extrinsic data collection on consumer insurance contracts},
journal = {Computer Law & Security Review},
volume = {45},
pages = {105667},
year = {2022},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2022.105667},
url = {https://www.sciencedirect.com/science/article/pii/S0267364922000152},
author = {Zofia Bednarz and Kayleen Manwaring},
keywords = {Artificial intelligence, Big data, Consumer insurance, Privacy, Data protection},
abstract = {ABSTRACT
Commentators have predicted that the insurance industry will soon benefit from technological advancements, such as developments in Artificial Intelligence (‘AI’) and Big Data. The application of AI- and Big Data-powered tools promises cost reduction, the creation of innovative products, and the potential to offer more efficient and tailored services to consumers. However, these new opportunities are mirrored by new legal and regulatory challenges. This article discusses challenges facing Australian data protection law, focusing on (potential) collection of consumers' data by insurers from non-traditional sources. In particular, we examine situations in which consumers may not be aware that the data collected could end up being used to price insurance. In our analysis, we discuss two useful examples of such non-traditional data sources: customer loyalty schemes and social media. These may give rise to several concerning data practices, including a significant increase in the collection of consumers' data by insurers. We argue that datafication of insurer processes may fuel excessive data collection in the context of insurance contracts, generating a substantial risk of harm to consumers, especially in terms of discrimination, exclusion, and unaffordability of insurance. We complement our analysis with the discussion of Australian insurance-specific provisions, asking if, and how, the harms examined could be adequately addressed.}
}
@article{PALMER2022930,
title = {Institutional pioneers and articulation work in digital platform infrastructure-building},
journal = {Journal of Business Research},
volume = {142},
pages = {930-945},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.12.067},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321009814},
author = {Mark Palmer and Inci Toral and Yann Truong and Fiona Lowe},
keywords = {Digital platforms, Articulation work, Infrastructure-building, Tension, Mobile payments, Cashless society},
abstract = {Digital platforms are an important organising form in business-to-business markets and have mirrored increasing research in end-user customers' interactions with digital platforms. Much less studied are the digital platform infrastructures underpinning this customer interfacing activity which must be built and maintained for digital platforms to exist and operate. We explore how institutional pioneers attempted to build a new digital platform with a vision of the cashless society beyond the traditional payment methods. Our findings demonstrate the insightful role of institutional pioneers in digital infrastructure-building through energizing the direction, network goals, positioning with other market-actors in the backstage. We show how the tensions produced by the organizing and ordering activities in the digital infrastructure field are resolved through brokering, alignment and workarounds. We unravel the way institutional pioneers use articulation work to define a legitimate course of actions for all actors in their organizing of standards, structure and behavioural focus.}
}
@article{MANDAL20221451,
title = {Digital image steganography: A literature survey},
journal = {Information Sciences},
volume = {609},
pages = {1451-1488},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.07.120},
url = {https://www.sciencedirect.com/science/article/pii/S002002552200809X},
author = {Pratap Chandra Mandal and Imon Mukherjee and Goutam Paul and B.N. Chatterji},
keywords = {Data hiding, Steganography, Spatial domain, Transform domain, Information security, Steganalysis},
abstract = {Steganography is the art of concealing information in a cover media in such a way that the presence of the information is unknown. Digital image steganography accomplishes the potential for protected communication that is crucial in most of the applications nowadays. Steganography has several beneficial applications. It has been driven to the frontrunner of present security systems by the amazing development in computational power, the rise in security consciousness. The main challenge in proposing a steganographic technique is to maintain a suitable balance among higher embedding capacity, imperceptibility, and security that separate it from correlated systems like cryptography and watermarking. This article offers an extensive state-of-the-art review and analysis of some recent steganographic techniques. Furthermore, we have discussed popular steganography tools in detail. Challenges in the recent deep learning based steganographic techniques have been addressed. To explore the domain, the article concludes with mentioning some future research directions.}
}
@article{JIA2022112856,
title = {Household cooking in the context of carbon neutrality: A machine-learning-based review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {168},
pages = {112856},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112856},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122007389},
author = {Jun-Jun Jia and Mengshu Zhu and Chu Wei},
keywords = {Household cooking, Latent dirichlet allocation, Topic modeling, Textual big data, Machine learning, Carbon neutrality},
abstract = {About 6.7% of global greenhouse gas emissions is caused by household cooking activities and thus it is of significance to identify research gaps between current studies and future directions in the context of carbon neutrality. To this end, the Latent Dirichlet Allocation topic model is used to review a total of 1440 household cooking studies from international journals written in English between 1983 and 2021. The textual mining technique helps to identify 20 topics in machine-learning sense, involving 8 research disciplines. In addition to energy field, household cooking is most relevant to disciplines of Multidisciplinary, Clinical Medicine, Chemistry, Economics and Business, and Geosciences. Energy ladder hypothesis and energy poverty are the most prevalent topics and asymmetric dependence relationships are unveiled among the 20 topics. Almost all cooking topics focus on health risk elimination and the transition to cleaner fuels while the target of carbon neutrality has not been adequately considered. The practical cooking fuel transition pathway, health co-benefits, impacts of the shift in cooking methods and practice on cultural diversity and human society driven by carbon neutrality constitute potential research directions. The machine-learning literature review research framework used in the study can be generalized in era of big data.}
}
@article{ARJOMANDIRAD2022103634,
title = {Correlation-based feature extraction from computer-aided design, case study on curtain airbags design},
journal = {Computers in Industry},
volume = {138},
pages = {103634},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103634},
url = {https://www.sciencedirect.com/science/article/pii/S016636152200029X},
author = {Mohammad {Arjomandi Rad} and Kent Salomonsson and Mirza Cenanovic and Henrik Balague and Dag Raudberget and Roland Stolt},
keywords = {Feature extraction, CAD/CAE, Parametric models, Medial Axis, Design Automation, Machine Learning, Regression Analysis, Curtain Airbag},
abstract = {Many high-level technical products are associated with changing requirements, drastic design changes, lack of design information, and uncertainties in input variables which makes their design process iterative and simulation-driven. Regression models have been proven to be useful tools during design, altering the resource-intensive finite element simulation models. However, building regression models from computer-aided design (CAD) parameters is associated with challenges such as dealing with too many parameters and their low or coupled impact on studied outputs which ultimately requires a large training dataset. As a solution, extraction of hidden features from CAD is presented on the application of volume simulation of curtain airbags concerning geometric changes in design loops. After creating a prototype that covers all aspects of a real curtain airbag, its CAD parameters have been analyzed to find out the correlation between design parameters and volume as output. Next, using the design of the experiment latin hypercube sampling method, 100 design samples are generated and the corresponding volume for each design sample was assessed. It was shown that selected CAD parameters are not highly correlated with the volume which consequently lowers the accuracy of prediction models. Various geometric entities, such as the medial axis, are used to extract several hidden features (referred to as sleeping parameters). The correlation of the new features and their performance and precision through two regression analyses are studied. The result shows that choosing sleeping parameters as input reduces dimensionality and the need to use advanced regression algorithms, allowing designers to have more accurate predictions (in this case approximately 95%) with a reasonable number of samples. Furthermore, it was concluded that using sleeping parameters in regression-based tools creates real-time prediction ability in the early development stage of the design process which could contribute to lower development lead time by eliminating design iterations.}
}
@article{STAHL2022101557,
title = {Identifying wetland areas in historical maps using deep convolutional neural networks},
journal = {Ecological Informatics},
volume = {68},
pages = {101557},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101557},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122000061},
author = {Niclas Ståhl and Lisa Weimann},
keywords = {Analysis of historical maps, Convolutional neural networks, Wetland management, Wetland restoration},
abstract = {The local environment and land usages have changed a lot during the past one hundred years. Historical documents and materials are crucial in understanding and following these changes. Historical documents are, therefore, an important piece in the understanding of the impact and consequences of land usage change. This, in turn, is important in the search of restoration projects that can be conducted to turn and reduce harmful and unsustainable effects originating from changes in the land-usage. This work extracts information on the historical location and geographical distribution of wetlands, from hand-drawn maps. This is achieved by using deep learning (DL), and more specifically a convolutional neural network (CNN). The CNN model is trained on a manually pre-labelled dataset on historical wetlands in the area of Jönköping county in Sweden. These are all extracted from the historical map called “Generalstabskartan”. The presented CNN performs well and achieves a F1-score of 0.886 when evaluated using a 10-fold cross validation over the data. The trained models are additionally used to generate a GIS layer of the presumable historical geographical distribution of wetlands for the area that is depicted in the southern collection in Generalstabskartan, which covers the southern half of Sweden. This GIS layer is released as an open resource and can be freely used. To summarise, the presented results show that CNNs can be a useful tool in the extraction and digitalisation of non-textual information in historical documents, such as historical maps. A modern GIS material that can be used to further understand the past land-usage change is produced within this research. Previously, no material of this detail and extent have been available, due to the large effort needed to manually create such. However, with the presented resource better quantifications and estimations of historical wetlands that have been lost can be made.}
}
@article{WAN2022226,
title = {A comprehensive survey on robust image watermarking},
journal = {Neurocomputing},
volume = {488},
pages = {226-247},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.02.083},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222002533},
author = {Wenbo Wan and Jun Wang and Yunming Zhang and Jing Li and Hui Yu and Jiande Sun},
keywords = {Image watermarking, Robustness, Deep learning, HDR image, Model watermarking},
abstract = {With the rapid development and popularity of the Internet, multimedia security has become a general essential concern. Especially, as manipulation of digital images gets much easier, the challenges it brings to authentication certification are increasing. As part of the solution, digital watermarking has made significant contributions to image content security and has attracted increasing attention. In this paper, we present a comprehensive review on digital image watermarking methods that were published in recent years illustrating the conventional schemes in different domains. We provide an overview of geometric invariant techniques and emerging watermarking methods for novel medias, such as depth image based rendering (DIBR), high dynamic range (HDR), screen content images (SCIs), and point cloud model. Particularly, as deep learning has achieved a great success in the field of image processing, and has also successfully been used in the field of digital watermarking, learning-based watermarking methods using various neural networks are summarized according to the utilization of neural networks in the single stage training (SST) and double stage training (DST). Finally, we provide an analysis and summary on those methods, and suggest some future research directions.}
}
@article{ZAHOOR2023101385,
title = {International vertical alliances within the international business field: A systematic literature review and future research agenda},
journal = {Journal of World Business},
volume = {58},
number = {1},
pages = {101385},
year = {2023},
issn = {1090-9516},
doi = {https://doi.org/10.1016/j.jwb.2022.101385},
url = {https://www.sciencedirect.com/science/article/pii/S1090951622000761},
author = {Nadia Zahoor and Zaheer Khan and Oded Shenkar},
keywords = {International vertical alliances, Buyer-supplier alliances, Backward linkages, Forward linkages, International business, Systematic literature review},
abstract = {International vertical alliances (IVAs) have garnered increasing scholarly interest in the strategy and international business (IB) literature. Our review of 111 papers published in major IB journals from 2000 to 2020 sheds light on the antecedents, key mediators, moderators and outcomes of IVAs. To generate insights, we juxtaposed forward and backward alliances and compared IVAs with their domestic vertical and horizontal counterparts. In this paper, we highlight key areas for future IVA research, including—but not limited to—broadening the scope of the investigation in order to integrate new theories and methods suited to examine such alliances in the IB field.}
}
@article{NICOL202311,
title = {Great debates in cardiac computed tomography: OPINION: “Artificial intelligence and the future of cardiovascular CT – Managing expectation and challenging hype”},
journal = {Journal of Cardiovascular Computed Tomography},
volume = {17},
number = {1},
pages = {11-17},
year = {2023},
issn = {1934-5925},
doi = {https://doi.org/10.1016/j.jcct.2022.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1934592522002519},
author = {Edward D. Nicol and Jonathan R. Weir-McCall and Leslee J. Shaw and Eric Williamson},
keywords = {Artificial intelligence, Machine learning, Cardiovascular CT, Hype},
abstract = {This manuscript has been written as a follow-up to the “AI/ML great debate” featured at the 2021 Society of Cardiovascular Computed Tomography (SCCT) Annual Scientific Meeting. In debate style, we highlighti the need for expectation management of AI/ML, debunking the hype around current AI techniques, and countering the argument that in its current day format AI/ML is the “silver bullet” for the interpretation of daily clinical CCTA practice.}
}
@article{SANDEEPA2022100405,
title = {A survey on privacy for B5G/6G: New privacy challenges, and research directions},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100405},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100405},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000723},
author = {Chamara Sandeepa and Bartlomiej Siniarski and Nicolas Kourtellis and Shen Wang and Madhusanka Liyanage},
keywords = {Beyond 5G, 6G, Privacy issues, Privacy solutions, Artificial intelligence, Machine learning, Explainable AI, Survey},
abstract = {Massive developments in mobile wireless telecommunication networks have been made during the last few decades. At present, mobile users are getting familiar with the latest 5G networks, and the discussion for the next generation of Beyond 5G (B5G)/6G networks has already been initiated. It is expected that B5G/6G will push the existing network capabilities to the next level, with higher speeds, enhanced reliability and seamless connectivity. To make these expectations a reality, research is progressing on new technologies, architectures, and intelligence-based decision-making processes related to B5G/6G. Privacy considerations are a crucial aspect that requires further attention in such developments, as billions of people and devices will be transmitting data through the upcoming network. However, the main recognition remains biased towards the network security. A discussion focused on privacy of B5G/6G is lacking at the moment. To address the gap, this paper provides a comprehensive survey on privacy-related aspects of B5G/6G networks. First, it discusses a taxonomy of different privacy perspectives. Based on the taxonomy, the paper then conceptualizes a set of challenges that appear as barriers to reach privacy preservation. Next, this work provides a set of solutions applicable to the proposed architecture of B5G/6G networks to mitigate the challenges. It also provides an overview of standardization initiatives for privacy preservation. Finally, the paper concludes with a roadmap of future directions, which will be an arena for new research towards privacy-enhanced B5G/6G networks. This work provides a basis for privacy aspects that will significantly impact peoples’ daily lives when using these future networks.}
}
@article{CHANDRA2022e00323,
title = {Non-fungible token-enabled entrepreneurship: A conceptual framework},
journal = {Journal of Business Venturing Insights},
volume = {18},
pages = {e00323},
year = {2022},
issn = {2352-6734},
doi = {https://doi.org/10.1016/j.jbvi.2022.e00323},
url = {https://www.sciencedirect.com/science/article/pii/S235267342200021X},
author = {Yanto Chandra},
keywords = {Non-fungible token, NFT, Entrepreneurship, Virtual, Enabler},
abstract = {Non-fungible tokens (NFTs) have taken the world by storm. Initially started as an art/game experiment, the NFT has given rise to a new form of entrepreneurship in the virtual world with massive opportunities and affordances. However, research into the entrepreneurial aspect of NFTs and the role of agency in the process is limited. In this article, I examine the concept of NFT-enabled Entrepreneurship (or NFTE). I first identify the main characteristics of NFTs, then define NFTE and discuss the related assumptions, and finally propose a conceptual framework for NFTE and investigate its enablers. I conclude by proposing NFTE as a novel domain of entrepreneurship theory and practice with extensive new research opportunities, and the plausibility of using NFT as an alternative mode of knowledge production in which scholars become “NFT creators.”}
}
@article{NELSON2022100483,
title = {Deep forecasting of translational impact in medical research},
journal = {Patterns},
volume = {3},
number = {5},
pages = {100483},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100483},
url = {https://www.sciencedirect.com/science/article/pii/S266638992200068X},
author = {Amy P.K. Nelson and Robert J. Gray and James K. Ruffle and Henry C. Watkins and Daniel Herron and Nick Sorros and Danil Mikhailov and M. Jorge Cardoso and Sebastien Ourselin and Nick McNally and Bryan Williams and Geraint E. Rees and Parashkev Nachev},
keywords = {deep learning, representation learning, natural language processing, research impact, translational research},
abstract = {Summary
The value of biomedical research—a $1.7 trillion annual investment—is ultimately determined by its downstream, real-world impact, whose predictability from simple citation metrics remains unquantified. Here we sought to determine the comparative predictability of future real-world translation—as indexed by inclusion in patents, guidelines, or policy documents—from complex models of title/abstract-level content versus citations and metadata alone. We quantify predictive performance out of sample, ahead of time, across major domains, using the entire corpus of biomedical research captured by Microsoft Academic Graph from 1990–2019, encompassing 43.3 million papers. We show that citations are only moderately predictive of translational impact. In contrast, high-dimensional models of titles, abstracts, and metadata exhibit high fidelity (area under the receiver operating curve [AUROC] > 0.9), generalize across time and domain, and transfer to recognizing papers of Nobel laureates. We argue that content-based impact models are superior to conventional, citation-based measures and sustain a stronger evidence-based claim to the objective measurement of translational potential.}
}
@article{LASTILLA2022102875,
title = {Self-supervised learning for medieval handwriting identification: A case study from the Vatican Apostolic Library},
journal = {Information Processing & Management},
volume = {59},
number = {3},
pages = {102875},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.102875},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322000097},
author = {Lorenzo Lastilla and Serena Ammirati and Donatella Firmani and Nikos Komodakis and Paolo Merialdo and Simone Scardapane},
keywords = {Self-supervised learning, Manuscripts, Handwriting identification},
abstract = {In this paper, we consider the task of automatically identifying whether different parts of medieval and modern manuscripts can be traced back to the same copyist/scribe, a problem of significant interest in paleography. Currently, the application of deep learning techniques in the context of scribe recognition has been hindered by the lack of a sufficiently large, labeled dataset, since the labeling process is incredibly complex and time-consuming. Here, we propose the first successful application of the recent framework of self-supervised learning to the field of digital paleography, wherein we pretrain a convolutional neural network by leveraging large amounts of unlabeled manuscripts. To this end, we build a novel dataset consisting of both labeled and unlabeled manuscripts for copyist identification extracted from the Vatican Apostolic Library. We show that fine-tuning this model to the task of interest significantly outperforms other baselines, including the common setup of initializing the network from general-domain features, or training the model from scratch, also in terms of generalization power. Overall, our results reveal the strong potential of self-supervised techniques in the field of digital paleography, where unlabeled data (i.e., digitized manuscripts) is nowadays available, while labeled data is scarcer.}
}
@article{WILSON2023102243,
title = {Learning neuroscience: Investigating influences of notetaking materials and individual differences},
journal = {Learning and Individual Differences},
volume = {101},
pages = {102243},
year = {2023},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2022.102243},
url = {https://www.sciencedirect.com/science/article/pii/S1041608022001303},
author = {Julia T. Wilson and Hilary E. Miller-Goldwater and Blaire M. Porter and Patricia J. Bauer},
keywords = {Classroom learning, Individual differences, Diagrams, Notetaking, Spatial cognition},
abstract = {How can we support classroom learning? Individual differences between students (e.g., cognitive skills and notetaking styles) is one factor that may relate to learning and interact with notetaking materials (e.g., diagram handouts and notetaking medium) to influence learning. However, the interaction between these factors is not well-understood. Accordingly, in this study, we presented short neuroscience lectures to 18–23-year-old undergraduates and investigated the interactions between notetaking materials and individual differences (cognitive skills: spatial/verbal reasoning; and notetaking style: verbatim copying/key terms) on learning. We found minimal overall effects of notetaking materials on learning. However, spatial and verbal reasoning related to learning. Additionally, in a handwritten condition, verbatim copying in notes was associated with lower learning whereas more key terms in notes was associated with higher learning. These results demonstrate that, to best support neuroscience learning in the classroom, we must consider individual differences and how they interact with notetaking materials.}
}
@article{KHOSRAVI2023100151,
title = {Learnersourcing in the age of AI: Student, educator and machine partnerships for content creation},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100151},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100151},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000309},
author = {Hassan Khosravi and Paul Denny and Steven Moore and John Stamper},
keywords = {Learnersourcing, Crowdsourcing in education, Student generating content, Human-AI partnership},
abstract = {Engaging students in creating novel content, also referred to as learnersourcing, is increasingly recognised as an effective approach to promoting higher-order learning, deeply engaging students with course material and developing large repositories of content suitable for personalised learning. Despite these benefits, some common concerns and criticisms are associated with learnersourcing (e.g., the quality of resources created by students, challenges in incentivising engagement and lack of availability of reliable learnersourcing systems), which have limited its adoption. This paper presents a framework that considers the existing learnersourcing literature, the latest insights from the learning sciences and advances in AI to offer promising future directions for developing learnersourcing systems. The framework is designed around important questions and human-AI partnerships relating to four key aspects: (1) creating novel content, (2) evaluating the quality of the created content, (3) utilising learnersourced contributions of students and (4) enabling instructors to support students in the learnersourcing process. We then present two comprehensive case studies that illustrate the application of the proposed framework in relation to two existing popular learnersourcing systems.}
}
@article{WEI2022232125,
title = {Machine learning for battery research},
journal = {Journal of Power Sources},
volume = {549},
pages = {232125},
year = {2022},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2022.232125},
url = {https://www.sciencedirect.com/science/article/pii/S0378775322011028},
author = {Zheng Wei and Qiu He and Yan Zhao},
keywords = {Machine learning, Battery materials, Battery state prediction},
abstract = {Batteries are vital energy storage carriers in industry and in our daily life. There is continued interest in the developments of batteries with excellent service performance and safety. Traditional trial-and-error experimental approaches have the limitations of high-cost and low-efficiency. Atomistic computational simulations are relatively expensive and take long time to screen massive materials. The rapid development of machine learning (ML) has brought innovations in many fields and has also changed the paradigm of the battery research. Numerous ML applications have emerged in the battery community, such as novel materials discovery, property prediction, and characterization. In this review, we introduced the workflow of ML, where the task, data, feature engineering, and evaluation were involved. Several typical ML models used in batteries were highlighted. In addition, we summarized the applications of ML for the discovery of novel materials, and for property and battery state prediction. The challenges for the application of ML in batteries were also discussed.}
}
@incollection{BAO2023399,
title = {Chapter 14 - Machine learning data processing as a bridge between microscopy and the brain},
editor = {Yuebing Zheng and Zilong Wu},
booktitle = {Intelligent Nanotechnology},
publisher = {Elsevier},
pages = {399-420},
year = {2023},
series = {Materials Today},
isbn = {978-0-323-85796-3},
doi = {https://doi.org/10.1016/B978-0-323-85796-3.00014-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323857963000147},
author = {Yijun Bao and Yiyang Gong},
keywords = {Computing methodology, Machine learning, Genetically encoded indicators, Video processing, Fluorescent microscopy},
abstract = {Neuroscientists use genetically encoded indicators and advanced microscopy to optically record the activities of numerous neurons in animal brains at high speed. The extraction of neural activity from individual neurons in imaging movies requires a multistep video processing pipeline, including correcting motion artifacts, segmenting spatial footprints of neurons, extracting temporal traces, and inferring activity spikes. Many neuron segmentation and spike inference algorithms have been developed using various machine learning techniques, including unsupervised learning, supervised learning, and a combination of both. The neuron segmentation algorithms work for different input data types, including 2D summary images, 3D videos as a whole or in blocks, or 3D videos frame-by-frame. These methods can replace tedious human labeling and automate the analysis of neural activity. Fast online processing methods potentially enable real-time closed-loop neuroscience experiments.}
}
@article{GUENDUEZ2023101719,
title = {Strategically constructed narratives on artificial intelligence: What stories are told in governmental artificial intelligence policies?},
journal = {Government Information Quarterly},
volume = {40},
number = {1},
pages = {101719},
year = {2023},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2022.101719},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X22000521},
author = {Ali A. Guenduez and Tobias Mettler},
keywords = {Artificial intelligence (AI), Policy research, Structural topic modeling (STM), Narrative policy framework (NPF), Role of government},
abstract = {What stories are told in national artificial intelligence (AI) policies? Combining the novel technique of structural topic modeling (STM) and qualitative narrative analysis, this paper examines the policy narratives in 33 countries’ AI policies. We uncover six common narratives that are dominating the political agenda concerning AI. Our findings show that the policy narratives' saliences vary across time and countries. We make several contributions. First, our narratives describe well-grounded, supportable conceptions of AI among governments, and show that AI is still a fairly novel, multilayered, and controversial phenomenon. Building on the premise that human sensemaking is best represented and supported by narration, we address the applied rhetoric of governments to either minimize the risks or exalt the opportunities of AI. Second, we uncover the four prominent roles governments seek  to take concerning AI implementation: enabler, leader, regulator, and/or user. Third, we make a methodological contribution toward data-driven, computationally-intensive theory development. Our methodological approach and the identified narratives present key starting points for further research.}
}
@article{WANG202351,
title = {Pre-Trained Language Models and Their Applications},
journal = {Engineering},
volume = {25},
pages = {51-65},
year = {2023},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S2095809922006324},
author = {Haifeng Wang and Jiwei Li and Hua Wu and Eduard Hovy and Yu Sun},
keywords = {Pre-trained models, Natural language processing},
abstract = {Pre-trained language models have achieved striking success in natural language processing (NLP), leading to a paradigm shift from supervised learning to pre-training followed by fine-tuning. The NLP community has witnessed a surge of research interest in improving pre-trained models. This article presents a comprehensive review of representative work and recent progress in the NLP field and introduces the taxonomy of pre-trained models. We first give a brief introduction of pre-trained models, followed by characteristic methods and frameworks. We then introduce and analyze the impact and challenges of pre-trained models and their downstream applications. Finally, we briefly conclude and address future research directions in this field.}
}
@article{LAM20221417,
title = {Parameterized Computational Framework for the Description and Design of Genetic Circuits of Morphogenesis Based on Contact-Dependent Signaling and Changes in Cell–Cell Adhesion},
journal = {ACS Synthetic Biology},
volume = {11},
number = {4},
pages = {1417-1439},
year = {2022},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.0c00369},
url = {https://www.sciencedirect.com/science/article/pii/S2161506322000821},
author = {Calvin Lam and Sajeev Saluja and George Courcoubetis and Dottie Yu and Christian Chung and Josquin Courte and Leonardo Morsut},
keywords = {synthetic biology, self-organization, tissue engineering, computational modeling, cellular Potts, juxtacrine signaling, synNotch, patterning, morphogenesis},
abstract = {Synthetic development is a nascent field of research that uses the tools of synthetic biology to design genetic programs directing cellular patterning and morphogenesis in higher eukaryotic cells, such as mammalian cells. One specific example of such synthetic genetic programs was based on cell–cell contact-dependent signaling using synthetic Notch pathways and was shown to drive the formation of multilayered spheroids by modulating cell–cell adhesion via differential expression of cadherin family proteins in a mouse fibroblast cell line (L929). The design method for these genetic programs relied on trial and error, which limited the number of possible circuits and parameter ranges that could be explored. Here, we build a parameterized computational framework that, given a cell–cell communication network driving changes in cell adhesion and initial conditions as inputs, predicts developmental trajectories. We first built a general computational framework where contact-dependent cell–cell signaling networks and changes in cell–cell adhesion could be designed in a modular fashion. We then used a set of available in vitro results (that we call the “training set” in analogy to similar pipelines in the machine learning field) to parameterize the computational model with values for adhesion and signaling. We then show that this parameterized model can qualitatively predict experimental results from a “testing set” of available in vitro data that varied the genetic network in terms of adhesion combinations, initial number of cells, and even changes to the network architecture. Finally, this parameterized model is used to recommend novel network implementation for the formation of a four-layered structure that has not been reported previously. The framework that we develop here could function as a testing ground to identify the reachable space of morphologies that can be obtained by controlling contact-dependent cell–cell communications and adhesion with these molecular tools and in this cellular system. Additionally, we discuss how the model could be expanded to include other forms of communication or effectors for the computational design of the next generation of synthetic developmental trajectories.
}
}
@incollection{BHOIR2022215,
title = {Chapter 9 - Person-based automation with artificial intelligence Chatbots: A driving force of Industry 4.0},
editor = {Aboul Ella Hassanien and Jyotir Moy Chatterjee and Vishal Jain},
booktitle = {Artificial Intelligence and Industry 4.0},
publisher = {Academic Press},
pages = {215-244},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-88468-6},
doi = {https://doi.org/10.1016/B978-0-323-88468-6.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323884686000036},
author = {Smita Vinit Bhoir and Sunita R. Patil and Ibtisam Yakub Mogul},
keywords = {Chatbots, Industry 4.0, Artificial intelligence (AI), Machine learning (ML), Hybrid Intention Multiclass Classification Model (HI-MCM)},
abstract = {New technologies called chatbots or virtual assistants have emerged to simplify the interaction between human beings and computer systems. For example, chatbots and virtual assistants are some of the newest technologies designed to simplify human–computer interaction in banking. As per Industry 4.0 standards, person-based automation is required, which can be provided with the help of chatbots. In this chapter, we introduce chatbots, examine their role in Industry 4.0, and demonstrate their real-life importance using case studies. We also analyze the role of recent technologies for automation. The chapter presents the background for developing chatbots, request-response model for user intention analysis, classification, and generating appropriate responses. The major challenge in chatbot development is understanding user intentions and sentiments behind the chats. The proposed Hybrid Intention Multiclass Classification Model (HI-MCM) shows how accurately user intentions can be classified for automating and speeding a Web-based search task through chatbots. This chapter will help chatbot researchers to explore the development of person-based automation based on Industry 4.0 standards, using artificial intelligence (AI), machine learning (ML), and natural language processing (NLP) technologies.}
}
@article{WANG20230086,
title = {Drone-Based Harvest Data Prediction Can Reduce On-Farm Food Loss and Improve Farmer Income},
journal = {Plant Phenomics},
volume = {5},
pages = {0086},
year = {2023},
issn = {2643-6515},
doi = {https://doi.org/10.34133/plantphenomics.0086},
url = {https://www.sciencedirect.com/science/article/pii/S2643651524001419},
author = {Haozhou Wang and Tang Li and Erika Nishida and Yoichiro Kato and Yuya Fukano and Wei Guo},
abstract = {On-farm food loss (i.e., grade-out vegetables) is a difficult challenge in sustainable agricultural systems. The simplest method to reduce the number of grade-out vegetables is to monitor and predict the size of all individuals in the vegetable field and determine the optimal harvest date with the smallest grade-out number and highest profit, which is not cost-effective by conventional methods. Here, we developed a full pipeline to accurately estimate and predict every broccoli head size (n > 3,000) automatically and nondestructively using drone remote sensing and image analysis. The individual sizes were fed to the temperature-based growth model and predicted the optimal harvesting date. Two years of field experiments revealed that our pipeline successfully estimated and predicted the head size of all broccolis with high accuracy. We also found that a deviation of only 1 to 2 days from the optimal date can considerably increase grade-out and reduce farmer's profits. This is an unequivocal demonstration of the utility of these approaches to economic crop optimization and minimization of food losses.}
}
@article{TERZIYAN202391,
title = {Encryption and Generation of Images for Privacy-Preserving Machine Learning in Smart Manufacturing},
journal = {Procedia Computer Science},
volume = {217},
pages = {91-101},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.205},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022839},
author = {Vagan Terziyan and Diana Malyk and Mariia Golovianko and Vladyslav Branytskyi},
keywords = {Industry 4.0, data privacy, anonymization, syntetic data generation, image processing, autoencoders},
abstract = {Current advances in machine (deep) learning and the exponential growth of data collected by and shared between smart manufacturing processes give a unique opportunity to get extra value from that data. The use of public machine learning services actualizes the issue of data privacy. Ordinary encryption protects the data but could make it useless for the machine learning objectives. Therefore, “privacy of data vs. value from data” is the major dilemma within the privacy preserving machine learning activity. Special encryption techniques or synthetic data generation are being in focus to address the issue. In this paper, we discuss a complex hybrid protection algorithm, which assumes sequential use of two components: homeomorphic data space transformation and synthetic data generation. Special attention is given to the privacy of image data. Specifics of image representation require special approaches towards encryption and synthetic image generation. We suggest use of (convolutional, variational) autoencoders and pre-trained feature extractors to enable applying privacy protection algorithms on top of the latent feature vectors captured from the images, and we updated the hybrid algorithms composed of homeomorphic transformation-as-encryption plus synthetic image generation accordingly. We show that an encrypted image can be reconstructed (by the pre-trained Decoder component of the convolutional variational autoencoder) into a secured representation from the extracted (by either the Encoder or a feature extractor) and encrypted (homeomorphic transformation of the latent space) feature vector.}
}
@article{SETIADI2023108908,
title = {Digital image steganography survey and investigation (goal, assessment, method, development, and dataset)},
journal = {Signal Processing},
volume = {206},
pages = {108908},
year = {2023},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2022.108908},
url = {https://www.sciencedirect.com/science/article/pii/S0165168422004479},
author = {De Rosal Ignatius Moses Setiadi and Supriadi Rustad and Pulung Nurtantio Andono and Guruh Fajar Shidik},
keywords = {Image Steganography Review, Image Steganalysis Review, Explicit Steganography Review, Critical Steganography Review, Steganography Investigation, Data Hiding Survey},
abstract = {Digital steganography has a long history, starting to be developed in the 90s until now. The main aspects of early steganography are security, imperceptibility, and payload. Security is an essential aspect of steganography, where its development competes with the steganalysis method. The development of steganographic methods is not only based on statistics but also involves adaptive methods, machine learning, artificial intelligence, generative adversarial networks (GAN), convolution neural networks (CNN), coverless techniques, etc. Several new methods add robustness as an important aspect. This makes the development of steganographic methods more varied and generally focuses the goal of the method on one or two aspects. Nowadays, steganography articles, especially surveys, rarely explicitly classify steganography based on its goals. So this survey contributes to classifying steganographic research based on goals and its assessments. This paper also reviews the use of assessment tools in depth because it is closely related to the goal of steganography. Challenges or issues, methods, developments, and popular datasets are also reviewed. Steganalysis literature was also added to improve the quality of the study and analysis. Finally, this survey provides discussion, investigation, critical analysis, and explicit summary so novice researchers can more easily understand image steganography development.}
}
@article{QIAN2022103098,
title = {Understanding public opinions on social media for financial sentiment analysis using AI-based techniques},
journal = {Information Processing & Management},
volume = {59},
number = {6},
pages = {103098},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103098},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322001996},
author = {Cheng Qian and Nitya Mathur and Nor Hidayati Zakaria and Rameshwar Arora and Vedika Gupta and Mazlan Ali},
keywords = {Non-fungible tokens (NFT), Emotion analysis, Sentiment analysis, Financial trends, Twitter, Ethereum},
abstract = {The digital currency has taken the financial markets by storm ever since its inception. Academia and industry are focussing on Artificial intelligence (AI) tools and techniques to study and gain an understanding of how businesses can draw insights from the large-scale data available online. As the market is driven by public opinions, and social media today provides an encouraging platform to share ideas and views; organizations and policy-makers could use the natural language processing (NLP) technology of AI to analyze public sentiments. Recently, a new and moderately unconventional instrument known as non-fungible tokens (NFTs) is emerging as an upcoming business market. Unlike the stock market, no precise quantitative parameters exist for the price determination of NFTs. Instead, NFT markets are driven more by public opinion, expectations, the perception of buyers, and the goodwill of creators. This study evaluates human emotions on the social media platforms Twitter posted by the public relating to NFTs. Additionally, this study conducts secondary market analysis to determine the reasons for the growing acceptance of NFTs through sentiment and emotion analysis. We segregate tweets using Pearson Product-Moment Correlation Coefficient (PPMCC) and study 8-scale emotions (Anger, Anticipation, Disgust, Fear, Joy, Sadness, Surprise, and Trust) along with Positive and Negative sentiments. Tweets majorly contained positive sentiment (∼ 72%), and positive emotions like anticipation and trust were found to be predominant all over the world. This is the first of its kind financial and emotional analysis of tweets pertaining to NFTs to the best of our understanding.}
}
@incollection{KOOLE2023291,
title = {Chapter Five - From intentions to action: An integrative review of action control theory and research},
editor = {Bertram Gawronski},
series = {Advances in Experimental Social Psychology},
publisher = {Academic Press},
volume = {68},
pages = {291-375},
year = {2023},
booktitle = {Advances In Experimental Social Psychology},
issn = {0065-2601},
doi = {https://doi.org/10.1016/bs.aesp.2023.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065260123000151},
author = {Sander L. Koole and Nils B. Jostmann and Nicola Baumann},
keywords = {Action control, Volition, Motivation, Action orientation, State orientation, Intention–behavior gap, Intention–behavior relations, Personality},
abstract = {People differ in how readily they enact their intentions. Some people manage to enact very difficult intentions (e.g., writing a book or starting a business) under demanding circumstances (e.g., extreme stress or oppressive bureaucracy). Other people struggle to enact even mundane intentions (e.g., replying to an email or taking out the trash). These individual differences in intention enactment have been the central focus of action-theoretical research. Section 1 of this chapter traces the historical development of the action-theoretical perspective, from prescientific notions to Action Control Theory (ACT; Kuhl, 1984). Section 2 presents an update of ACT in the form of ACTψ (‘act-psi’). According to ACTψ, efficient action control requires the person to use affect regulation to coordinate the interplay of intention memory and intuitive behavior control. Chronic individual differences in affect regulation presumably underlie the personality disposition of action versus state orientation. Section 3 considers the measurement of action versus state orientation. Section 4 reviews research showing that, as compared with state-oriented people, action-oriented people: (a) Enact demanding and self-directed intentions more efficiently in real life and controlled settings; (b) Form, maintain, and update their intentions more readily; (c) Regulate own affective states more rapidly, are better shielded against stress and mental illness, and display more personal growth. Finally, Section 5 considers how the action-theoretical perspective complements social-psychological approaches to intention–behavior relations.}
}
@incollection{CASINI2022151,
title = {Chapter 3 - Building digital revolution},
editor = {Marco Casini},
booktitle = {Construction 4.0},
publisher = {Woodhead Publishing},
pages = {151-186},
year = {2022},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-12-821797-9},
doi = {https://doi.org/10.1016/B978-0-12-821797-9.00013-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217979000131},
author = {Marco Casini},
keywords = {Construction 4.0, building information modeling, cloud computing, edge computing, Internet of things, 5G network, artificial intelligence, machine learning, Big Data, advanced analytics, nanotechnology, digital twin, digital building life cycle, augmented digital design, data-driven design, connected construction, building automation, automated construction site},
abstract = {The chapter provides and overview of the digital revolution that is changing the Architecture, Engineering, Construction and Operation (AECO) industry and leading to the Construction 4.0 model, based on augmented digital design, connected and automated construction processes, and smart building operations and maintenance. The key technology drivers—building information modeling, cloud and edge computing, Internet of things, 5G networks, artificial intelligence and machine learning, Big Data and advanced analytics, and nanotechnology—are described, highlighting their role in the digital transformation of the construction sector and the new opportunities brought in terms of higher productivity and building quality. A thorough analysis of Construction 4.0 tools and methods is given, describing the applications and advantages in the whole value chain of the new “digital twin building life cycle” achievable with the full integration of all these digital technologies.}
}
@article{BERNABEI2023100172,
title = {Students’ use of large language models in engineering education: A case study on technology acceptance, perceptions, efficacy, and detection chances},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100172},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100172},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000516},
author = {Margherita Bernabei and Silvia Colabianchi and Andrea Falegnami and Francesco Costantino},
keywords = {LLM, ChatGPT, Higher education, Essay generation},
abstract = {The accessibility of advanced Artificial Intelligence-based tools, like ChatGPT, has made Large Language Models (LLMs) readily available to students. These LLMs can generate original written content to assist students in their academic assessments. With the rapid adoption of LLMs, exemplified by the popularity of OpenAI's ChatGPT, there is a growing need to explore their application in education. Few studies examine students' use of LLMs as learning tools. This paper focuses on the application of ChatGPT in engineering higher education through an in-depth case study. It investigates whether engineering students can generate high-quality university essays with LLMs assistance, whether existing LLMs identification systems can detect essays produced with LLMs, and how students perceive the usefulness and acceptance of LLMs in learning. The research adopts a deductive/inductive approach, combining conceptualization and empirical evidence analysis. The study involves mechanical and management engineering students, who compose essays using LLMs. The essay assessment showed good results, but some recommendations emerged for teachers and students. Thirteen LLMs detectors were tested without achieving satisfactory results, suggesting to avoid LLMs ban. In addition, students were administered a questionnaire based on constructs and items that follow the technology acceptance models available in the literature. The results contribute to qualitative evidence by highlighting possible future research and educational practices.}
}
@article{2022I,
title = {Full Issue PDF},
journal = {JACC: CardioOncology},
volume = {4},
number = {4},
pages = {I-CXXXIX},
year = {2022},
note = {Amyloidosis: Mini-Focus Issue},
issn = {2666-0873},
doi = {https://doi.org/10.1016/S2666-0873(22)00437-9},
url = {https://www.sciencedirect.com/science/article/pii/S2666087322004379}
}
@article{MIKOLAJCZYK20232971,
title = {Retrosynthesis from transforms to predictive sustainable chemistry and nanotechnology: a brief tutorial review},
journal = {Green Chemistry},
volume = {25},
number = {8},
pages = {2971-2991},
year = {2023},
issn = {1463-9262},
doi = {https://doi.org/10.1039/d2gc04750k},
url = {https://www.sciencedirect.com/science/article/pii/S1463926223000122},
author = {Alicja Mikolajczyk and Uladzislau Zhdan and Sylvain Antoniotti and Adam Smolinski and Karolina Jagiello and Piotr Skurski and Moussab Harb and Tomasz Puzyn and Jaroslaw Polanski},
abstract = {Retrosynthesis is a tool initially developed to simplify the planning of the synthesis of organic molecules using a symbolic strategy involving disconnections to synthons. It can perform better when the initial strategy is supported by computer-assisted methods both in its strategy and tactic parts. With the progress of chemical knowledge management assisted by computer technologies, retrosynthesis got an opportunity to involve database mining, reaction prediction, machine learning (ML), and other data science tools, which allows for covering inorganic compounds and nanoparticles, for which strategy, e.g., the design of reaction conditions, is a critical issue. Bimetallic catalysts can be a surprising target. Retrosynthesis is also essential for green and sustainable chemistry. On the one hand, synthon representation makes it possible to select green type processes and reactants among many possible options; on the other hand, recent computer technologies involving ML-based methods give a chance to more precise control of the green and sustainable metrics at the early stage of its design (before synthesis). A variety of such metrics are described in the literature. Many of them are intuitive heuristics, especially for sustainability evaluation. Green methods are among natural retrosynthesis goals since chemists searching for simplifications always preferred safer and cleaner methods than hazardous ones. Chemical intuition is more important than rigorous quantification in traditional approaches. With the growing availability of novel retrosynthetic tools controlled by green and sustainable metrics, we can hope to observe the significant development of predictive green and sustainable methods. As predictive greenness and sustainability engage broad chemical areas and contemporary software tends to be a black-box-like architecture, we designed this tutorial to provide an easily understandable background for the chemical and materials science audience involved in drug and material design and discovery.}
}
@article{MICHELSON2022101729,
title = {Pushing the boundaries: Erotic romance and the symbolic boundary nexus},
journal = {Poetics},
volume = {94},
pages = {101729},
year = {2022},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2022.101729},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X22001115},
author = {Anna Michelson},
keywords = {Romance, Genre, Symbolic boundaries, Classification, Legitimation},
abstract = {How do contested emerging subgenres become legitimated and institutionalized? This case illustrates the meso-level negotiation of community sense (Wohl, 2015) as stakeholders of a genre (romance fiction) debate whether genre boundaries include a new subgenre (erotic romance). Erotic romance upended conventions by introducing explicit and sometimes unconventional sex into the traditionally heteronormative romance genre. However, opposition to subgenre inclusion involved more than sexual content. Drawing on interviews (n = 40) and text data from Romantic Times (n = 360) and Romance Writers Report (n = 180), I find that mainstream incorporation of erotic romance involved community negotiation of multiple symbolic boundary debates: (1) What is acceptable sexuality? (2) What is a real book? (3) Who is a professional author? Erotic romance was fully institutionalized after best-selling Fifty Shades of Grey forced the community to confront all three boundary debates at once. Each debate represents a different symbolic boundary around the mainstream romance genre, but the case can only be fully understood by examining how they intersect. I conceptualize that intersection as the symbolic boundary nexus and argue that analyzing genre classifications as a set of intersecting boundaries is a productive approach for understanding how cultural communities negotiate contested classification processes.}
}
@article{RAY2023121,
title = {ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {3},
pages = {121-154},
year = {2023},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2023.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S266734522300024X},
author = {Partha Pratim Ray},
keywords = {ChatGPT, Language model, GPT-3.5, Generative AI, Conversational AI, Context understanding, Natural language processing},
abstract = {In recent years, artificial intelligence (AI) and machine learning have been transforming the landscape of scientific research. Out of which, the chatbot technology has experienced tremendous advancements in recent years, especially with ChatGPT emerging as a notable AI language model. This comprehensive review delves into the background, applications, key challenges, and future directions of ChatGPT. We begin by exploring its origins, development, and underlying technology, before examining its wide-ranging applications across industries such as customer service, healthcare, and education. We also highlight the critical challenges that ChatGPT faces, including ethical concerns, data biases, and safety issues, while discussing potential mitigation strategies. Finally, we envision the future of ChatGPT by exploring areas of further research and development, focusing on its integration with other technologies, improved human-AI interaction, and addressing the digital divide. This review offers valuable insights for researchers, developers, and stakeholders interested in the ever-evolving landscape of AI-driven conversational agents. This study explores the various ways ChatGPT has been revolutionizing scientific research, spanning from data processing and hypothesis generation to collaboration and public outreach. Furthermore, the paper examines the potential challenges and ethical concerns surrounding the use of ChatGPT in research, while highlighting the importance of striking a balance between AI-assisted innovation and human expertise. The paper presents several ethical issues in existing computing domain and how ChatGPT can invoke challenges to such notion. This work also includes some biases and limitations of ChatGPT. It is worth to note that despite of several controversies and ethical concerns, ChatGPT has attracted remarkable attentions from academia, research, and industries in a very short span of time.}
}
@article{BEZIRHAN2023100161,
title = {Automated reading passage generation with OpenAI's large language model},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100161},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100161},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000401},
author = {Ummugul Bezirhan and Matthias {von Davier}},
keywords = {Automated item generation, Natural language processing, Large language models, Reading assessment, AI-Generated reading passages},
abstract = {The widespread usage of computer-based assessments and individualized learning platforms has increased demand for the rapid production of high-quality items. Automated item generation (AIG), the process of using item models to generate new items with the help of computer technology, was proposed to reduce reliance on human subject experts. While AIG has been used in test development, recent advances in machine learning algorithms offer the potential to enhance its efficiency further. This paper presents an innovative approach utilizing OpenAI's latest transformer-based language model, GPT-3, to generate reading passages. Existing reading passages were used in carefully engineered prompts to ensure the AI-generated text has similar content and structure to a fourth-grade reading passage. Multiple passages were generated for each prompt, and the final passage was selected based on Lexile score agreement with the original passage. To ensure accuracy, a human editor conducted a simple revision of the chosen passage, correcting any grammatical and factual errors. To evaluate the effectiveness of the AI-generated passages, human judges assessed their coherence and appropriateness for fourth-grade readers. The results indicated that GPT-3-produced passages closely resembled human-authored passages regarding coherence, appropriateness, and readability for the target audience. By combining GPT-3's capabilities with carefully designed prompts and human editing, this study demonstrates an efficient and effective method for generating reading passages. The findings highlight the potential of incorporating large language models into automated item generation, contributing to improved scalability and quality in educational assessment development.}
}
@incollection{CHEN2023931,
title = {Chapter 38 - Computational methods for scaffold hopping},
editor = {Bin Yu and Ning Li and Caiyun Fu},
booktitle = {Privileged Scaffolds in Drug Discovery},
publisher = {Academic Press},
pages = {931-948},
year = {2023},
isbn = {978-0-443-18611-0},
doi = {https://doi.org/10.1016/B978-0-443-18611-0.00008-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443186110000085},
author = {Xingyu Chen and Runyu Miao and Leihao Zhang and Zhen Yuan and Honglin Li and Shiliang Li},
keywords = {Biological activity, Fragment replacement, Machine learning, Pharmacophore, Scaffold hopping, Similarity},
abstract = {Drug discovery is an expensive and risky process. For a long time, researchers have developed several approaches to reducing failure during that process. Scaffold hopping is often used to find several compounds with different structures but similar biological activities. Thus, scaffold hopping has an important role in the field of drug design. This chapter introduces the principles and shows some successful cases of computational methods for scaffold hopping. We divided these methods into five categories according to principles including pharmacophore, similarity, fragment replacement, machine learning, and biological activity similarity. We also discuss the shortcomings of computational methods for scaffold hopping and look forward to their future development.}
}
@article{MACAS2022109032,
title = {A survey on deep learning for cybersecurity: Progress, challenges, and opportunities},
journal = {Computer Networks},
volume = {212},
pages = {109032},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109032},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622001864},
author = {Mayra Macas and Chunming Wu and Walter Fuertes},
keywords = {Cybersecurity, Artificial intelligence, Machine learning, Deep learning, Cyber-threat, Botnets, Intrusion detection, Spam filtering, Encrypted traffic analysis},
abstract = {As the number of Internet-connected systems rises, cyber analysts find it increasingly difficult to effectively monitor the produced volume of data, its velocity and diversity. Signature-based cybersecurity strategies are unlikely to achieve the required performance for detecting new attack vectors. Moreover, technological advances enable attackers to develop sophisticated attack strategies that can avoid detection by current security systems. As the cyber-threat landscape worsens, we need advanced tools and technologies to detect, investigate, and make quick decisions regarding emerging attacks and threats. Applications of artificial intelligence (AI) have the potential to analyze and automatically classify vast amounts of Internet traffic. AI-based solutions that automate the detection of attacks and tackle complex cybersecurity problems are gaining increasing attention. This paper comprehensively presents the promising applications of deep learning, a subfield of AI based on multiple layers of artificial neural networks, in a wide variety of security tasks. Before critically and comparatively surveying state-of-the-art solutions from the literature, we discuss the key characteristics of representative deep learning architectures employed in cybersecurity applications, we introduce the emerging trends in deep learning, and we provide an overview of necessary resources like a generic framework and suitable datasets. We identify the limitations of the reviewed works, and we bring forth a vision of the current challenges of the area, providing valuable insights and good practices for researchers and developers working on related problems. Finally, we uncover current pain points and outline directions for future research to address them.}
}
@article{XU20221664,
title = {Smart breeding driven by big data, artificial intelligence, and integrated genomic-enviromic prediction},
journal = {Molecular Plant},
volume = {15},
number = {11},
pages = {1664-1695},
year = {2022},
issn = {1674-2052},
doi = {https://doi.org/10.1016/j.molp.2022.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1674205222002957},
author = {Yunbi Xu and Xingping Zhang and Huihui Li and Hongjian Zheng and Jianan Zhang and Michael S. Olsen and Rajeev K. Varshney and Boddupalli M. Prasanna and Qian Qian},
keywords = {smart breeding, genomic selection, integrated genomic-enviromic selection, spatiotemporal omics, crop design, machine and deep learning, big data, artificial intelligence},
abstract = {The first paradigm of plant breeding involves direct selection-based phenotypic observation, followed by predictive breeding using statistical models for quantitative traits constructed based on genetic experimental design and, more recently, by incorporation of molecular marker genotypes. However, plant performance or phenotype (P) is determined by the combined effects of genotype (G), envirotype (E), and genotype by environment interaction (GEI). Phenotypes can be predicted more precisely by training a model using data collected from multiple sources, including spatiotemporal omics (genomics, phenomics, and enviromics across time and space). Integration of 3D information profiles (G-P-E), each with multidimensionality, provides predictive breeding with both tremendous opportunities and great challenges. Here, we first review innovative technologies for predictive breeding. We then evaluate multidimensional information profiles that can be integrated with a predictive breeding strategy, particularly envirotypic data, which have largely been neglected in data collection and are nearly untouched in model construction. We propose a smart breeding scheme, integrated genomic-enviromic prediction (iGEP), as an extension of genomic prediction, using integrated multiomics information, big data technology, and artificial intelligence (mainly focused on machine and deep learning). We discuss how to implement iGEP, including spatiotemporal models, environmental indices, factorial and spatiotemporal structure of plant breeding data, and cross-species prediction. A strategy is then proposed for prediction-based crop redesign at both the macro (individual, population, and species) and micro (gene, metabolism, and network) scales. Finally, we provide perspectives on translating smart breeding into genetic gain through integrative breeding platforms and open-source breeding initiatives. We call for coordinated efforts in smart breeding through iGEP, institutional partnerships, and innovative technological support.}
}
@article{DORIAN20232927,
title = {Machine Learning Based Fault Anticipation for 3D Printing*},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {2927-2932},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.1414},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323018220},
author = {Voydie Dorian and Goupil Louis and Chanthery Elodie and Travé-Massuyès Louise and Delautier Sébastien},
keywords = {Machine Learning, Time Series Modeling, Additive Manufacturing, Fault Prediction, Fault Anticipation, Fault Detection},
abstract = {In recent years, 3D printing has seen a stellar rise despite its inability to deliver constant quality goods. This article presents a machine learning experiment that results in a model performing fault prediction, in the sense of forecasting the fault, on the printed parts so that printer parameters can be corrected before the faults appear. This model is able to predict faults in real-time during printing, even in the case of multiple faults. It relies on multiple sensors gathering time-series data during printing, a pre-processing of these data to extract the most relevant features and several machine learning algorithms, each suited and tuned to predict at best each fault. A benchmark for testing and tuning the different algorithms is presented. The resulting model has been implemented on a plastic delta 3D printer and tested for the prediction of eight different faults. The best performing model is a random forest, but decision trees are almost as good while explaining what causes the fault.}
}
@article{SHAHEER2022100970,
title = {Internationalization of Digital Innovations: A Rapidly Evolving Research Stream},
journal = {Journal of International Management},
volume = {28},
number = {4},
pages = {100970},
year = {2022},
issn = {1075-4253},
doi = {https://doi.org/10.1016/j.intman.2022.100970},
url = {https://www.sciencedirect.com/science/article/pii/S107542532200045X},
author = {Noman Shaheer and Kijong Kim and Sali Li},
keywords = {Digital innovation, Digital internationalization, Digital entrepreneurship, Demand-side perspective, Institutional transitions},
abstract = {This perspective paper synthesizes the burgeoning literature on internationalization of digital innovations to identify promising areas for future research. We first integrate multiple perspectives on digital innovations to offer a coherent and unified definition of digital innovations. Next, we synthesize digital innovation research into a theoretical framework that discusses the interrelationship between digital innovations and international business environment and its implications for international penetration of digital innovations. Our synthesis of literature highlights the increased research focus on demand-side perspective as an appropriate theoretical framework to explain the unique dynamics of digital internationalization. Finally, we take account of recent trends in digital economy as well as important gaps in current literature to propose promising avenues for future research. We particularly emphasize the need to integrate current institutional transitions of a digital era as well as emerging theoretical perspectives such as demand-side perspective and opportunity logic in digital internationalization research. We hope our perspective paper will contribute toward a more systematic and theoretically grounded advancement of digital internationalization research.}
}
@article{STEINMANN2022940,
title = {Autonomous high-throughput computations in catalysis},
journal = {Chem Catalysis},
volume = {2},
number = {5},
pages = {940-956},
year = {2022},
issn = {2667-1093},
doi = {https://doi.org/10.1016/j.checat.2022.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S266710932200104X},
author = {Stephan N. Steinmann and Angga Hermawan and Mohammed {Bin Jassar} and Zhi Wei Seh},
keywords = {heterogeneous catalysis, autonomous computations, autonomous laboratories, high-throughput screening, interface, electrocatalysis, machine learning},
abstract = {Summary
Autonomous atomistic computations are excellent tools to accelerate the development of heterogeneous (electro-)catalysts. In this perspective, we critically review the achieved progress to accelerate high-throughput screening aimed at identifying promising catalyst materials via databases, workflow managers, and machine-learning techniques. Outstanding challenges are also discussed extensively: the modification and stability of catalyst surfaces under realistic reaction conditions is key for meaningful predictions. Furthermore, adequately accounting for solvent effects remains a topic of active research particularly relevant for biomass transformations and electrocatalysis. Finally, efficient, autonomous workflows for investigating active sites of amorphous catalysts remain underdeveloped. The computations can also be supplemented with autonomous laboratories, which allow the performance of sophisticated experiments driven by artificial intelligence-augmented design of experiments, reducing human-time investment for optimizing synthesis and reaction conditions as well as catalyst characterizations. The combination of autonomous computations and laboratories promise to power the dearly needed transition to a sustainable chemical industry.}
}
@article{TOMASSINI2022107191,
title = {Brain-on-Cloud for automatic diagnosis of Alzheimer’s disease from 3D structural magnetic resonance whole-brain scans},
journal = {Computer Methods and Programs in Biomedicine},
volume = {227},
pages = {107191},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.107191},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722005727},
author = {Selene Tomassini and Agnese Sbrollini and Giacomo Covella and Paolo Sernani and Nicola Falcionelli and Henning Müller and Micaela Morettini and Laura Burattini and Aldo Franco Dragoni},
keywords = {Alzheimer’s disease, Cloud computing, Computer-aided diagnosis, Convolutional LSTM, 3D structural magnetic resonance, Supervised deep learning},
abstract = {Background and objective
Alzheimer’s disease accounts for approximately 70% of all dementia cases. Cortical and hippocampal atrophy caused by Alzheimer’s disease can be appreciated easily from a T1-weighted structural magnetic resonance scan. Since a timely therapeutic intervention during the initial stages of the syndrome has a positive impact on both disease progression and quality of life of affected subjects, Alzheimer’s disease diagnosis is crucial. Thus, this study relies on the development of a robust yet lightweight 3D framework, Brain-on-Cloud, dedicated to efficient learning of Alzheimer’s disease-related features from 3D structural magnetic resonance whole-brain scans by improving our recent convolutional long short-term memory-based framework with the integration of a set of data handling techniques in addition to the tuning of the model hyper-parameters and the evaluation of its diagnostic performance on independent test data.
Methods
For this objective, four serial experiments were conducted on a scalable GPU cloud service. They were compared and the hyper-parameters of the best experiment were tuned until reaching the best-performing configuration. In parallel, two branches were designed. In the first branch of Brain-on-Cloud, training, validation and testing were performed on OASIS-3. In the second branch, unenhanced data from ADNI-2 were employed as independent test set, and the diagnostic performance of Brain-on-Cloud was evaluated to prove its robustness and generalization capability. The prediction scores were computed for each subject and stratified according to age, sex and mini mental state examination.
Results
In its best guise, Brain-on-Cloud is able to discriminate Alzheimer’s disease with an accuracy of 92% and 76%, sensitivity of 94% and 82%, and area under the curve of 96% and 92% on OASIS-3 and independent ADNI-2 test data, respectively.
Conclusions
Brain-on-Cloud shows to be a reliable, lightweight and easily-reproducible framework for automatic diagnosis of Alzheimer’s disease from 3D structural magnetic resonance whole-brain scans, performing well without segmenting the brain into its portions. Preserving the brain anatomy, its application and diagnostic ability can be extended to other cognitive disorders. Due to its cloud nature, computational lightness and fast execution, it can also be applied in real-time diagnostic scenarios providing prompt clinical decision support.}
}
@article{CASOLA2022117627,
title = {Summarization, simplification, and generation: The case of patents},
journal = {Expert Systems with Applications},
volume = {205},
pages = {117627},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117627},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422009356},
author = {Silvia Casola and Alberto Lavelli},
keywords = {Natural Language Processing, Patent mining, Summarization, Simplification, Natural Language Generation, Survey},
abstract = {We survey Natural Language Processing (NLP) approaches to summarizing, simplifying, and generating patents’ text. While solving these tasks has important practical applications – given patents’ centrality in the R&D process – patents’ idiosyncrasies open peculiar challenges to the current NLP state of the art. This survey aims at (a) describing patents’ characteristics and the questions they raise to the current NLP systems, (b) critically presenting previous work and its evolution, and (c) drawing attention to directions of research in which further work is needed. To the best of our knowledge, this is the first survey of generative approaches in the patent domain.}
}
@article{AMINUDDIN20225822,
title = {AuSR1: Authentication and self-recovery using a new image inpainting technique with LSB shifting in fragile image watermarking},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part B},
pages = {5822-5840},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822000428},
author = {Afrig Aminuddin and Ferda Ernawan},
keywords = {Blind fragile watermarking, Self-embedding, Image authentication, Self-recovery, Image inpainting},
abstract = {With the rapid development of multimedia technology, editing and manipulating digital images have become more accessible than ever. This paper proposed color image authentication based on blind fragile image watermarking for tamper detection and self-recovery named AuSR1. The AuSR1 divides each channel of the cover image into non-overlapping blocks with the size of 2 × 2 pixels. The authentication data is embedded into the original block location, while the recovery data is embedded into the distant location from the original location based on the block mapping algorithm. The watermark data is then embedded into the 2 LSB to achieve high quality of the recovered image under tampering attacks. In addition, the permutation algorithm is applied to ensure the security of the watermark data. The AuSR1 utilizes a three-layer authentication algorithm to achieve a high detection rate.The experimental results show that the scheme produced a PSNR value of 45.57 dB and an SSIM value of 0.9972 of the watermarked images. Furthermore, the AuSR1 detected the tampered area of the images with a high precision value of 0.9943. In addition, the recovered image achieved a PSNR value of 27.64 dB and an SSIM value of 0.9339 on a 50% tampering rate.}
}
@article{SHARMA2022107217,
title = {Technological revolutions in smart farming: Current trends, challenges & future directions},
journal = {Computers and Electronics in Agriculture},
volume = {201},
pages = {107217},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107217},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922005324},
author = {Vivek Sharma and Ashish Kumar Tripathi and Himanshu Mittal},
keywords = {Smart farming, Current Trends in smart farming, Precision agriculture, Agriculture 4.0, Machine Learning},
abstract = {With increasing population, the demand for agricultural productivity is rising to meet the goal of “Zero Hunger”. Consequently, farmers have optimized the agricultural activities in a sustainable way with the modern technologies. This integration has boosted the agriculture production due to high potentiality in assisting the farmers. The impulse towards the technological advancement has revived the traditional agriculture methods and resulted in eco-friendly, sustainable, and efficient farming. This has revolutionized the era of smart farming which primarily alliance with modern technologies like, big data, machine learning, deep learning, swarm intelligence, internet-of-things, block chain, robotics and autonomous system, cloud-fog-edge computing, cyber physical systems, and generative adversarial networks (GAN). To cater the same, a detailed survey on ten hot-spots of smart farming is presented in this paper. The survey covers the technology-wise state-of-the-art methods along with their application domains. Moreover, the publicly available data sets with existing research challenges are investigated. Lastly, the paper concludes with suggestions to the identified problems and possible future research directions.}
}
@article{2022e1,
title = {PDF of the Full Issue},
journal = {Annals of Emergency Medicine},
volume = {80},
number = {4, Supplement },
pages = {e1-e193},
year = {2022},
issn = {0196-0644},
doi = {https://doi.org/10.1016/S0196-0644(22)01098-8},
url = {https://www.sciencedirect.com/science/article/pii/S0196064422010988}
}
@article{REEDY2023100313,
title = {Interpol review of digital evidence for 2019–2022},
journal = {Forensic Science International: Synergy},
volume = {6},
pages = {100313},
year = {2023},
issn = {2589-871X},
doi = {https://doi.org/10.1016/j.fsisyn.2022.100313},
url = {https://www.sciencedirect.com/science/article/pii/S2589871X22000985},
author = {Paul Reedy}
}
@article{2023100310,
title = {Pathology Visions 2022 Overview},
journal = {Journal of Pathology Informatics},
volume = {14},
pages = {100310},
year = {2023},
issn = {2153-3539},
doi = {https://doi.org/10.1016/j.jpi.2023.100310},
url = {https://www.sciencedirect.com/science/article/pii/S2153353923001244}
}
@article{WANG2023119054,
title = {Open world long-tailed data classification through active distribution optimization},
journal = {Expert Systems with Applications},
volume = {213},
pages = {119054},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.119054},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422020723},
author = {Min Wang and Lei Zhou and Qian Li and An-an Zhang},
keywords = {Active learning, Cost-sensitive, Long-tailed distribution, Open set classification},
abstract = {Real-world data exhibits a long-tailed label distribution, which leads to classification bias. Popular re-sampling or re-weighting methods usually require known category information. However, learning from long-tailed data with open categories is a challenging issue. In this paper, we propose an active distribution optimization algorithm (DALC) to handle the interesting issue. Through clustering, querying and classification iterations, we explore new categories and balance label distribution. For clustering, we present an exploration technique that adaptively obtains optimal data distribution with minimal total distance/cost. For each query, we design a critical instance selection strategy with the cluster information. For classification, we establish an ensemble model to continuously balance the label distribution. We conducted experiments on synthetic, benchmark and domain datasets. The results of the significance test verified the effectiveness of DALC and its superiority over state-of-the-art long-tailed data classification and open set classification algorithms.}
}
@article{LIU2022e384,
title = {The medical algorithmic audit},
journal = {The Lancet Digital Health},
volume = {4},
number = {5},
pages = {e384-e397},
year = {2022},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(22)00003-6},
url = {https://www.sciencedirect.com/science/article/pii/S2589750022000036},
author = {Xiaoxuan Liu and Ben Glocker and Melissa M McCradden and Marzyeh Ghassemi and Alastair K Denniston and Lauren Oakden-Rayner},
abstract = {Summary
Artificial intelligence systems for health care, like any other medical device, have the potential to fail. However, specific qualities of artificial intelligence systems, such as the tendency to learn spurious correlates in training data, poor generalisability to new deployment settings, and a paucity of reliable explainability mechanisms, mean they can yield unpredictable errors that might be entirely missed without proactive investigation. We propose a medical algorithmic audit framework that guides the auditor through a process of considering potential algorithmic errors in the context of a clinical task, mapping the components that might contribute to the occurrence of errors, and anticipating their potential consequences. We suggest several approaches for testing algorithmic errors, including exploratory error analysis, subgroup testing, and adversarial testing, and provide examples from our own work and previous studies. The medical algorithmic audit is a tool that can be used to better understand the weaknesses of an artificial intelligence system and put in place mechanisms to mitigate their impact. We propose that safety monitoring and medical algorithmic auditing should be a joint responsibility between users and developers, and encourage the use of feedback mechanisms between these groups to promote learning and maintain safe deployment of artificial intelligence systems.}
}
@article{MURALIDHARAN2022108711,
title = {The infinite race between steganography and steganalysis in images},
journal = {Signal Processing},
volume = {201},
pages = {108711},
year = {2022},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2022.108711},
url = {https://www.sciencedirect.com/science/article/pii/S016516842200250X},
author = {Trivikram Muralidharan and Aviad Cohen and Assaf Cohen and Nir Nissim},
keywords = {Steganography, Steganalysis, Images, Machine learning, Deep learning},
abstract = {Steganography is the primary method by which individuals can communicate covertly; cryptography, on the other hand, fails at this, as it is possible to detect (the presence of) encrypted-communication. Steganalysis has been used to detect the presence of steganography and acts as a countermeasure to it. The ongoing race between image-steganography and steganalysis methods has resulted in the need for this paper which surveys and compares developments in these two intertwined-fields. This work covers over 150 papers that demonstrate the significant improvements made in steganography and steganalysis over the last three-decades. We mention the novelty of the method proposed in each paper, as well as the evaluation results and the paper's contribution to the field. We provide several taxonomies for steganography and steganalysis methods, based on the approach and techniques underlying the methods, which allows us to perform the first comprehensive comparison of steganography and steganalysis methods. This comparison sheds light on the existing-gaps between the two connected domains and can be used to identify and prioritize the steganography methods that require immediate remediation using steganalysis methods. Lastly, we follow the chronological-evolution of steganography and steganalysis methods over the years, an overview which highlights the infinite-nature of this race.}
}
@article{KOLADE2022101960,
title = {Can university-industry-government collaborations drive a 3D printing revolution in Africa? A triple helix model of technological leapfrogging in additive manufacturing},
journal = {Technology in Society},
volume = {69},
pages = {101960},
year = {2022},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2022.101960},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X22001014},
author = {Oluwaseun Kolade and Abiodun Adegbile and David Sarpong},
abstract = {The protracted disruption of Covid-19 pandemic on global supply chains has renewed calls for a new model of manufacturing that removes the need for centralised high-volume production and large inventory stocking. Drawing ideas from the Triple Helix model of university-industry-government innovation, this paper analyses the prospects for a 3D manufacturing revolution in Africa, a continent which was was disproportionately affected in the rounds of international border restrictions imposed in response to the Omicron variant of the virus. Taking a conceptual approach supported with case illustrations, the paper reviews the evolution of 3D printing technologies, the disruptive impact they have had on the traditional supply chain and the global expansion of the 3D printing market. Highlighting the favourable conditions for technological leapfrogging within the African context, the paper proposes a new integrative framework that explains how the emergence of new hybrid organisations from the Triple Helix can drive a promising manufacturing future for the continent -with small and medium enterprises playing a key role.}
}
@article{CHAUNCEY2023100182,
title = {A framework and exemplars for ethical and responsible use of AI Chatbot technology to support teaching and learning},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100182},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100182},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000619},
author = {Sarah A. Chauncey and H. Patricia McKenna},
keywords = {AI ethics, AI responsibility, AI-Rich learning environments, Cognitive flexibility, Critical thinking, Self-regulation},
abstract = {The aim of this paper is to investigate the ethical and responsible use of AI chatbots in education in support of critical thinking, cognitive flexibility and self-regulation in terms of their potential to enhance and motivate teaching and learning in contemporary education environments. AI chatbots such as ChatGPT by OpenAI appear to be improving in conversational and other capabilities and this paper explores such advances using version 4. Based on a review of the research literature, a conceptual framework is formulated for responsible use of AI chatbots in education supporting cognitive flexibility in AI-rich learning environments. The framework is then operationalized for use in this paper through the development of exemplars for math, english language arts (ELA), and studying with ChatGPT to close learning gaps in an effort to foster more ethical and responsible approaches to the design and development of AI chatbots for application and use in teaching and learning environments. This paper extends earlier foundational work on cognitive flexibility and AI chatbots as well as work on cognitive flexibility in support of creativity and innovation with AI chatbots in urban civic spaces.}
}
@article{EGGER2022106874,
title = {Medical deep learning—A systematic meta-review},
journal = {Computer Methods and Programs in Biomedicine},
volume = {221},
pages = {106874},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.106874},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722002565},
author = {Jan Egger and Christina Gsaxner and Antonio Pepe and Kelsey L. Pomykala and Frederic Jonske and Manuel Kurz and Jianning Li and Jens Kleesiek},
keywords = {Deep learning, Artificial neural networks, Machine learning, Data analysis, Image analysis, Medical image analysis, Medical image processing, Medical imaging, Patient data, Pathology, Detection, Segmentation, Registration, Generative adversarial networks, PubMed, Systematic, Review, Survey, Meta-review, Meta-survey},
abstract = {Deep learning has remarkably impacted several different scientific disciplines over the last few years. For example, in image processing and analysis, deep learning algorithms were able to outperform other cutting-edge methods. Additionally, deep learning has delivered state-of-the-art results in tasks like autonomous driving, outclassing previous attempts. There are even instances where deep learning outperformed humans, for example with object recognition and gaming. Deep learning is also showing vast potential in the medical domain. With the collection of large quantities of patient records and data, and a trend towards personalized treatments, there is a great need for automated and reliable processing and analysis of health information. Patient data is not only collected in clinical centers, like hospitals and private practices, but also by mobile healthcare apps or online websites. The abundance of collected patient data and the recent growth in the deep learning field has resulted in a large increase in research efforts. In Q2/2020, the search engine PubMed returned already over 11,000 results for the search term ‘deep learning’, and around 90% of these publications are from the last three years. However, even though PubMed represents the largest search engine in the medical field, it does not cover all medical-related publications. Hence, a complete overview of the field of ‘medical deep learning’ is almost impossible to obtain and acquiring a full overview of medical sub-fields is becoming increasingly more difficult. Nevertheless, several review and survey articles about medical deep learning have been published within the last few years. They focus, in general, on specific medical scenarios, like the analysis of medical images containing specific pathologies. With these surveys as a foundation, the aim of this article is to provide the first high-level, systematic meta-review of medical deep learning surveys.}
}
@article{HUYNHTHE2023105581,
title = {Artificial intelligence for the metaverse: A survey},
journal = {Engineering Applications of Artificial Intelligence},
volume = {117},
pages = {105581},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105581},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622005711},
author = {Thien Huynh-The and Quoc-Viet Pham and Xuan-Qui Pham and Thanh Thi Nguyen and Zhu Han and Dong-Seong Kim},
keywords = {Artificial intelligence, Blockchain, Deep learning, Immersive experience, Machine learning, Machine vision, Metaverse, Metaverse applications, Networking, Virtual worlds},
abstract = {Along with the massive growth of the Internet from the 1990s until now, various innovative technologies have been created to bring users breathtaking experiences with more virtual interactions in cyberspace. Many virtual environments have been developed with immersive experience and digital transformation, but most are incoherent instead of being integrated into a platform. In this context, metaverse has been introduced as a shared virtual world that is fueled by many emerging technologies. Among such technologies, artificial intelligence (AI) has shown the great importance of enhancing immersive experience and enabling human-like intelligence of virtual agents. In this survey, we make a beneficial effort to explore the role of AI, including machine learning algorithms and deep learning architectures, in the foundation and development of the metaverse. As the main contributions, we convey a comprehensive investigation of AI-based methods concerning several technical aspects (e.g., natural language processing, machine vision, blockchain, networking, digital twin, and neural interface) that have potentials to build virtual worlds in the metaverse. Furthermore, several primary AI-aided applications, including healthcare, manufacturing, smart cities, and gaming, are studied to be promisingly deployed in the virtual worlds. Finally, we conclude the key contribution and open some future research directions of AI for the metaverse. Serving as a foundational survey, this work will help researchers, including experts and non-experts in related fields, in applying, developing, and optimizing AI techniques to polish the appearance of virtual worlds and improve the quality of applications built in the metaverse.}
}
@incollection{BORKAR2023159,
title = {Chapter 5 - Pharmacophore modeling},
editor = {Rupesh Kumar Gautam and Mohammad Amjad Kamal and Pooja Mittal},
booktitle = {Computational Approaches in Drug Discovery, Development and Systems Pharmacology},
publisher = {Academic Press},
pages = {159-182},
year = {2023},
isbn = {978-0-323-99137-7},
doi = {https://doi.org/10.1016/B978-0-323-99137-7.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323991377000046},
author = {Maheshkumar Borkar and Arati Prabhu and Abhishek Kanugo and Rupesh Kumar Gautam},
keywords = {Structure and ligand-based pharmacophore, HipHop, hydrogen, Ligand profiling, Pharmacophore fingerprints, Conformational analysis, Molecular superimposition},
abstract = {Pharmacophore modeling is an important part of “computer-aided drug design (CADD)” and has led to numerous successful research outcomes. It contributed significantly in the rational drug design approach. The pharmacophore model abstracts crucial structural attributes of a molecule that are crucial for pharmacological activity, along with their relative positions in three-dimensional space. There are various ligand- and structure-based methods that have been developed for improved pharmacophore modeling and fruitfully applied in de novo design, lead optimization, virtual screening, off-target and target identification, side effect, and ADME-tox modeling. Current chapter gives a comprehensive impression of pharmacophore modeling, focus on various types of pharmacophores, methodology development, and its vast spectrum of applications.}
}
@article{DUSHNITSKY2022104463,
title = {Why do incumbents fund startups? A study of the antecedents of corporate venture capital in China},
journal = {Research Policy},
volume = {51},
number = {3},
pages = {104463},
year = {2022},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2021.104463},
url = {https://www.sciencedirect.com/science/article/pii/S0048733321002559},
author = {Gary Dushnitsky and Lei Yu},
keywords = {Corporate Venture Capital, Venture Capital, Startups, Innovation, China},
abstract = {Established firms are instrumental in funding entrepreneurial ventures, a practice known as corporate venture capital (CVC). Yet, our knowledge of the reasons firms engage in CVC is calibrated mainly on data from the United States and Europe. Such a restricted focus limits our understanding of CVC practices and objectives. Accordingly, we adopt an abductive approach to study the antecedents of CVC in China. The country is a vibrant entrepreneurial setting, second only to the USA in total startup numbers and funding amounts. We construct a comprehensive data of Chinese CVCs during the late 2010s by integrate Chinese and international databases. Cross-industry analyses of CVC patterns underscore a novel objective; one that is predominantly associated with harnessing growth through market expansion rather than the prevailing view of CVC as a window on technology. The findings mirror the features of the Chinese setting, where entrepreneurs profit from the dramatic expansion in economic activity and serve as a vehicle to leverage the global innovation frontier.}
}
@article{VOEVODSKY2023107283,
title = {C-system of a module over a Jf-relative monad},
journal = {Journal of Pure and Applied Algebra},
volume = {227},
number = {6},
pages = {107283},
year = {2023},
issn = {0022-4049},
doi = {https://doi.org/10.1016/j.jpaa.2022.107283},
url = {https://www.sciencedirect.com/science/article/pii/S002240492200281X},
author = {Vladimir Voevodsky},
abstract = {Let F be the category with the set of objects N and morphisms given by the functions between the standard finite sets of the corresponding cardinalities. Let Jf:F→Sets(U) be the obvious functor from this category to the category of sets in a given Grothendieck universe U. In this paper we construct, for any Jf-relative monad RR and any left RR-module LM, a C-system C(RR,LM) and explicitly compute the action of the four B-system operations on its B-sets. In the introduction we explain in detail the relevance of this result to the construction of the term C-systems of type theories.}
}
@article{FERRARI2022306,
title = {Measuring 3D face deformations from RGB images of expression rehabilitation exercises},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {4},
pages = {306-323},
year = {2022},
note = {Virtual-reality and intelligent hardware in digital twins A)},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000456},
author = {Claudio Ferrari and Stefano Berretti and Pietro Pala and Alberto Del Bimbo},
keywords = {3D morphable face model, Sparse and locally coherent 3DMM components, Local and asymmetric, Face deformations, Face rehabilitation, Face deformation measure},
abstract = {Background
The accurate (quantitative) analysis of 3D face deformation is a problem of increasing interest in many applications. In particular, defining a 3D model of the face deformation into a 2D target image to capture local and asymmetric deformations remains a challenge in existing literature. A measure of such local deformations may be a relevant index for monitoring the rehabilitation exercises of patients suffering from Parkinson’s or Alzheimer’s disease or those recovering from a stroke.
Methods
In this paper, a complete framework that allows the construction of a 3D morphable shape model (3DMM) of the face is presented for fitting to a target RGB image. The model has the specific characteristic of being based on localized components of deformation. The fitting transformation is performed from 3D to 2D and guided by the correspondence between landmarks detected in the target image and those manually annotated on the average 3DMM. The fitting also has the distinction of being performed in two steps to disentangle face deformations related to the identity of the target subject from those induced by facial actions.
Results
The method was experimentally validated using the MICC-3D dataset, which includes 11 subjects. Each subject was imaged in one neutral pose and while performing 18 facial actions that deform the face in localized and asymmetric ways. For each acquisition, 3DMM was fit to an RGB frame whereby, from the apex facial action and the neutral frame, the extent of the deformation was computed. The results indicate that the proposed approach can accurately capture face deformation, even localized and asymmetric deformations.
Conclusion
The proposed framework demonstrated that it is possible to measure deformations of a reconstructed 3D face model to monitor facial actions performed in response to a set of targets. Interestingly, these results were obtained using only RGB targets, without the need for 3D scans captured with costly devices. This paves the way for the use of the proposed tool in remote medical rehabilitation monitoring.}
}
@article{MEKOUAR20221,
title = {A survey on blockchain-based Recommender Systems: Integration architecture and taxonomy},
journal = {Computer Communications},
volume = {187},
pages = {1-19},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422000342},
author = {Loubna Mekouar and Youssef Iraqi and Issam Damaj and Tarek Naous},
keywords = {Recommender Systems, Blockchain, Smart contracts, Oracles, Distributed ledgers, Security},
abstract = {A Recommender System (RS) is an integral part of present-day leading web services, such as YouTube, Amazon, Netflix, and many others. Modern RSs are challenged to go beyond their traditional role of predicting user preferences to efficiently provide reliable, carefully personalized, and highly accurate recommendations. This paper thoroughly explores and analyzes state-of-the-art literature surveys on RS to extract important challenges and open issues. Our goal in this paper is to survey the literature to extract essential features of RSs and Blockchain (BC), focusing on their integration. Because of the lack of an existing foundation of BC-based RSs, the intrinsic BC aspects in RSs are identified and described. Integrating BC technology within RSs can achieve many benefits such as transparency, decentralization, and security. To that end, a thorough study of the papers on current BC-based RSs is presented along with a synthesized comprehensive taxonomy. Furthermore, a modular RS architecture, encompassing on-chain and off-chain storage and computation processes, is designed. This paper also includes a thorough discussion on the validity of the proposed architecture, BC limitations concerning RSs, and the derivation of a rich set of pointers to future research directions.}
}
@article{LV2022214486,
title = {Nanochemistry advancing photon conversion in rare-earth nanostructures for theranostics},
journal = {Coordination Chemistry Reviews},
volume = {460},
pages = {214486},
year = {2022},
issn = {0010-8545},
doi = {https://doi.org/10.1016/j.ccr.2022.214486},
url = {https://www.sciencedirect.com/science/article/pii/S0010854522000819},
author = {Ruichan Lv and Micah Raab and Yanxing Wang and Jie Tian and Jun Lin and Paras N. Prasad},
keywords = {Nanochemistry, Photo conversion, Rare earth, Theronostics},
abstract = {Rare-earth (RE) doped nanoparticles show unique features of photon conversion from an incident wavelength to a more suitable wavelength at an intended biological site, thus enhancing the scope of theranostics. A number of reviews have already addressed biomedical applications of photon upconversion luminescence (UCL) from infrared (IR) to a shorter wavelength. However, there has been a great deal of recent interest in using photon downshifting luminescence (DSL) in RE ions to produce wavelengths in the near infrared (NIR) optical transparency windows such as NIR II and NIR III to enable deep tissue penetration with significantly less scattering for 3D deep tissue imaging. This review is unique in scope and distinct from past reviews as we present nanochemistry approaches assisted by the new area of materials informatics utilizing artificial intelligence (AI) and machine learning to produce optimized multishell nanostructures containing RE ions. It introduces approaches for photosentitization utilizing new mechanisms of energy transfer for photon harvesting by strongly absorbing dye antennas to produce highly efficient both photon UCL and DSL (in some cases concurrently). This includes dye conjugation for sensitization, luminescence modulation by metal and other elemental co-doping, core-multishell structure for controlling excitation dynamics with minimal heating, and hierarchical composite nanostructures for multimodal MRI, CT, photoacoustic, cerenkov, UCL, and NIR II imaging. It presents AI machine learning assisted material informatics including discrete dipole approximation (DDA) simulation, heuristic algorithms (HAs), logistic regression (LR), and support vector machine (SVM) as providing valuable insight for nanochemistry by searching optimized element, concentration, and key influence element, which can improve the efficiency compared with the conventional “trial and error” method or intuitive experiments. We describe surface modification of these photonic nanoprobes for in vitro/ vivo deep tissue bioimaging, and for multimodal imaging. Also, the probes can be used for sensing, accurate NIR nanothermometry, theranostics, and imaging guided synergistic photodynamic therapy (PDT), photothermal therapy (PTT), photoactive therapy, and controlled drug release. Selected examples of theranostics such as the brain theranostics with neurophotonics, preclinical surgery navigation with the developed NIR II imaging are provided. We hope that this timely account of our current understanding and status of preclinically used RE luminescence probes will hopefully entice an abroad range of scientists in different disciplineses.}
}
@article{SONG2022121963,
title = {Investigating new design concepts based on customer value and patent data: The case of a future mobility door},
journal = {Technological Forecasting and Social Change},
volume = {184},
pages = {121963},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.121963},
url = {https://www.sciencedirect.com/science/article/pii/S004016252200484X},
author = {Kisik Song and Siyeong Yun and Leehee Kim and Sungjoo Lee},
keywords = {Design concept, Technology opportunity, Patent analysis, Text mining, User value, Future mobility},
abstract = {Design-based strategies are becoming as important in corporate competitive approaches as technology-based strategies, and accordingly, design patents have emerged as a mechanism for capturing technology opportunities. The text data of the design patent are not only simpler than the complex text structure of the utility patent but also clearly explain the application and design characteristics of the invention. Therefore, design patents have high value as an innovation database that can be complemented with utility data. Despite the potential of design patents as a source of technology intelligence, however, most studies on capturing technology opportunities have focused on utility patents. Therefore, this study proposed an approach to investigate new design concepts using design patents, and it employed the approach in the case of a future mobility door. More specifically, we first identified valuable design concepts within the target field (i.e., vehicle doors) and reference field (i.e., oven doors) to be applied to the target object (i.e., future mobility door), and we prioritized the ideas by technology- and user value-related criteria. Then, the highly-prioritized ideas were provided with the experts in the automobile industry to verify the effectiveness of the proposed approach. The research outputs are expected to contribute to the development of product design concepts in a company by helping to discover technology opportunities with reference to the designs in other industries as well as within the target industry.}
}
@article{LIU2023102458,
title = {Digital innovation and performance of manufacturing firms: An affordance perspective},
journal = {Technovation},
volume = {119},
pages = {102458},
year = {2023},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2022.102458},
url = {https://www.sciencedirect.com/science/article/pii/S0166497222000050},
author = {Yang Liu and Jiuyu Dong and Liang Mei and Rui Shen},
keywords = {Digital innovation, Profit from innovation, Intellectual property right, Chinese manufacturing firm},
abstract = {We explore the underlying mechanisms and institutional conditions of profiting from digital innovation in the context of manufacturing firms. Building on affordance theory, we propose that digital innovations positively affect manufacturing firms’ performance via innovation speed and operational efficiency due to the affordance of digital technology. Moreover, the interactions between digital and institutional affordances suggest that the intellectual property rights (IPR) protection system (i.e. IPR regime and its enforcement) negatively moderates the relationship between digital innovation adoption and innovation speed, as well as operational efficiency. Results from a longitudinal sample of Chinese listed firms support our hypotheses. Our findings contribute to the emerging literature on profiting from digital innovation and provide managerial implications for manufacturing firms in emerging markets.}
}
@article{HARFOUCHE2023154,
title = {A primer on artificial intelligence in plant digital phenomics: embarking on the data to insights journey},
journal = {Trends in Plant Science},
volume = {28},
number = {2},
pages = {154-184},
year = {2023},
issn = {1360-1385},
doi = {https://doi.org/10.1016/j.tplants.2022.08.021},
url = {https://www.sciencedirect.com/science/article/pii/S1360138522002278},
author = {Antoine L. Harfouche and Farid Nakhle and Antoine H. Harfouche and Orlando G. Sardella and Eli Dart and Daniel Jacobson},
keywords = {AI system architecture, black box models, data analytics, digital phenomics, explainable artificial intelligence, interpretable by design models},
abstract = {Artificial intelligence (AI) has emerged as a fundamental component of global agricultural research that is poised to impact on many aspects of plant science. In digital phenomics, AI is capable of learning intricate structure and patterns in large datasets. We provide a perspective and primer on AI applications to phenome research. We propose a novel human-centric explainable AI (X-AI) system architecture consisting of data architecture, technology infrastructure, and AI architecture design. We clarify the difference between post hoc models and 'interpretable by design' models. We include guidance for effectively using an interpretable by design model in phenomic analysis. We also provide directions to sources of tools and resources for making data analytics increasingly accessible. This primer is accompanied by an interactive online tutorial.}
}
@article{FANG2022104900,
title = {Machine learning accelerates the materials discovery},
journal = {Materials Today Communications},
volume = {33},
pages = {104900},
year = {2022},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2022.104900},
url = {https://www.sciencedirect.com/science/article/pii/S235249282201741X},
author = {Jiheng Fang and Ming Xie and Xingqun He and Jiming Zhang and Jieqiong Hu and Yongtai Chen and Youcai Yang and Qinglin Jin},
keywords = {Machine learning, Big data, Data mining, Materials design},
abstract = {As the big data generated by the development of modern experiments and computing technology becomes more and more accessible, the material design method based on machine learning (ML) has opened a new paradigm for materials science research. With its ability to automatically solve complex tasks, machine learning is being used as a new method to help discover the relevance of materials, understand materials' properties, and accelerate the discovery of materials. This paper first introduces the general process of machine learning in materials science. Secondly, the applications of machine learning in material properties prediction, classification and identification, auxiliary micro-scale characterization, phase transformation research and phase diagram construction, process optimization, service behavior evaluation, accelerating the development of computational simulation technology, multi-objective optimization and inverse design of materials are reviewed. Finally, we discuss the main challenges and possible solutions in machine learning, and predict the potential research directions.}
}
@article{BORUTA20234021,
title = {Computation-aided studies related to the induction of specialized metabolite biosynthesis in microbial co-cultures: An introductory overview},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {4021-4029},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S2001037023002866},
author = {Tomasz Boruta},
keywords = {Co-culture, Specialized metabolites, Data analysis, Molecular networks, Genome mining, Optimization},
abstract = {Co-cultivation is an effective method of inducing the production of specialized metabolites (SMs) in microbial strains. By mimicking the ecological interactions that take place in natural environment, this approach enables to trigger the biosynthesis of molecules which are not formed under monoculture conditions. Importantly, microbial co-cultivation may lead to the discovery of novel chemical entities of pharmaceutical interest. The experimental efforts aimed at the induction of SMs are greatly facilitated by computational techniques. The aim of this overview is to highlight the relevance of computational methods for the investigation of SM induction via microbial co-cultivation. The concepts related to the induction of SMs in microbial co-cultures are briefly introduced by addressing four areas associated with the SM induction workflows, namely the detection of SMs formed exclusively under co-culture conditions, the annotation of induced SMs, the identification of SM producer strains, and the optimization of fermentation conditions. The computational infrastructure associated with these areas, including the tools of multivariate data analysis, molecular networking, genome mining and mathematical optimization, is discussed in relation to the experimental results described in recent literature. The perspective on the future developments in the field, mainly in relation to the microbiome-related research, is also provided.}
}
@article{MENG2022106207,
title = {Intelligent disassembly of electric-vehicle batteries: a forward-looking overview},
journal = {Resources, Conservation and Recycling},
volume = {182},
pages = {106207},
year = {2022},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2022.106207},
url = {https://www.sciencedirect.com/science/article/pii/S0921344922000556},
author = {Kai Meng and Guiyin Xu and Xianghui Peng and Kamal Youcef-Toumi and Ju Li},
keywords = {Electric vehicle battery, disassembly, recycling, artificial intelligence, machine learning, sustainability},
abstract = {Retired electric-vehicle lithium-ion battery (EV-LIB) packs pose severe environmental hazards. Efficient recovery of these spent batteries is a significant way to achieve closed-loop lifecycle management and a green circular economy. It is crucial for carbon neutralization, and for coping with the environmental and resource challenges associated with the energy transition. EV-LIB disassembly is recognized as a critical bottleneck for mass-scale recycling. Automated disassembly of EV-LIBs is extremely challenging due to the large variety and uncertainty of retired EV-LIBs. Recent advances in artificial intelligence (AI) machine learning (ML) provide new ways for addressing these problems. This study aims to provide a systematic review and forward-looking perspective on how AI/ML methodology can significantly boost EV-LIB intelligent disassembly for achieving sustainable recovery. This work examines the key advances and research opportunities of emerging intelligent technologies for EV-LIB disassembly, and recycling and reuse of industrial products in general. We show that AI could benefit the whole disassembly process, particularly addressing the uncertainty and safety issues. Currently, EV-LIB state prognostics, disassembly decision-making as well as target detection are indicated as promising areas to realize intelligence. The challenges still exist for extensive autonomy due to present AI's inherent limitations, mechanical and chemical complexities, and sustainable benefits concerns. This paper provides the practical map to direct how to implement EV-LIB intelligent disassembly as well as forward-looking perspectives for addressing these challenges.}
}
@article{LI2023,
title = {Improving an Electronic Health Record–Based Clinical Prediction Model Under Label Deficiency: Network-Based Generative Adversarial Semisupervised Approach},
journal = {JMIR Medical Informatics},
volume = {11},
year = {2023},
issn = {2291-9694},
doi = {https://doi.org/10.2196/47862},
url = {https://www.sciencedirect.com/science/article/pii/S2291969423000273},
author = {Runze Li and Yu Tian and Zhuyi Shen and Jin Li and Jun Li and Kefeng Ding and Jingsong Li},
keywords = {semisupervised learning, generative adversarial network, network analysis, label deficiency, clinical prediction, electronic health record, EHR, clinical prediction, adversarial network, data set},
abstract = {Background
Observational biomedical studies facilitate a new strategy for large-scale electronic health record (EHR) utilization to support precision medicine. However, data label inaccessibility is an increasingly important issue in clinical prediction, despite the use of synthetic and semisupervised learning from data. Little research has aimed to uncover the underlying graphical structure of EHRs.
Objective
A network-based generative adversarial semisupervised method is proposed. The objective is to train clinical prediction models on label-deficient EHRs to achieve comparable learning performance to supervised methods.
Methods
Three public data sets and one colorectal cancer data set gathered from the Second Affiliated Hospital of Zhejiang University were selected as benchmarks. The proposed models were trained on 5% to 25% labeled data and evaluated on classification metrics against conventional semisupervised and supervised methods. The data quality, model security, and memory scalability were also evaluated.
Results
The proposed method for semisupervised classification outperforms related semisupervised methods under the same setup, with the average area under the receiver operating characteristics curve (AUC) reaching 0.945, 0.673, 0.611, and 0.588 for the four data sets, respectively, followed by graph-based semisupervised learning (0.450, 0.454, 0.425, and 0.5676, respectively) and label propagation (0.475,0.344, 0.440, and 0.477, respectively). The average classification AUCs with 10% labeled data were 0.929, 0.719, 0.652, and 0.650, respectively, comparable to that of the supervised learning methods logistic regression (0.601, 0.670, 0.731, and 0.710, respectively), support vector machines (0.733, 0.720, 0.720, and 0.721, respectively), and random forests (0.982, 0.750, 0.758, and 0.740, respectively). The concerns regarding the secondary use of data and data security are alleviated by realistic data synthesis and robust privacy preservation.
Conclusions
Training clinical prediction models on label-deficient EHRs is indispensable in data-driven research. The proposed method has great potential to exploit the intrinsic structure of EHRs and achieve comparable learning performance to supervised methods.}
}
@article{LU2023158849,
title = {Automatic control and optimal operation for greenhouse gas mitigation in sustainable wastewater treatment plants: A review},
journal = {Science of The Total Environment},
volume = {855},
pages = {158849},
year = {2023},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.158849},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722059484},
author = {Hao Lu and Huazhe Wang and Qinglian Wu and Haichao Luo and Qi Zhao and Banghai Liu and Qishi Si and Shanshan Zheng and Wanqian Guo and Nanqi Ren},
keywords = {WWTPs, GHG emissions, Plant-wide models, Control methods, Multi-objective optimization},
abstract = {In order to promote low-carbon sustainable operational management of the wastewater treatment plants (WWTPs), automatic control and optimal operation technologies, which devote to improving effluent quality, operational costs and greenhouse gas (GHG) emissions, have flourished in recent years. There is no consensus on the design procedure for optimal control/operation of sustainable WWTPs. In this review, we summarize recent researches on developing control and optimization strategies for GHG mitigation in WWTPs. Faced with the fact that direct carbon dioxide (CO2) emissions (considered biological origin) are generally not included in the carbon footprint of WWTPs, direct emissions (nitrous oxide (N2O), methane (CH4)) and indirect emissions are paid much attention. Firstly, the plant-wide models with GHG dynamic simulation, which are employed to design and evaluate the automatic control schemes as well as representative studies on identifying key factors affecting GHG emissions or comprehensive performance are outlined. Then, both traditional and advanced control methods commonly used in GHG mitigation are reviewed in detail, followed by the multi-objective optimization practices of control/operational parameters. Based on the mentioned control and (or) optimization strategies, a novel design framework for the optimal control/operation of sustainable WWTPs is proposed. The findings and design framework proposed in the paper will provide guidance for GHG mitigation and sustainable operation in WWTPs. It is foreseeable that more accurate and appropriate plant-wide models together with flexible control methods and intelligent optimization strategies will be developed to satisfy the upgrading requirements of WWTPs in the future.}
}
@article{GHASEMZADEH2022102487,
title = {Collaborating with users to innovate: A systematic literature review},
journal = {Technovation},
volume = {116},
pages = {102487},
year = {2022},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2022.102487},
url = {https://www.sciencedirect.com/science/article/pii/S0166497222000347},
author = {Khatereh Ghasemzadeh and Guido Bortoluzzi and Zornitsa Yordanova},
keywords = {Firm-user collaboration, User innovation, Innovation strategy, Innovation management, Review},
abstract = {The purpose of this study is to systematize and consolidate a scattered literature on the theme of firm-user collaboration by focusing on the strategic, organizational, and managerial dynamics of firms. To achieve this aim, a systematic review of 152 articles was carried out. Papers were first organized into six clusters of firm-user collaboration: (1) Identifying and Selecting Users and Ideas, (2) Organizing Collaboration with Users, (3) Networking with Users, (4) Engaging Users in the Innovation Process, (5) Developing Resources and Capabilities to support Collaboration with Users, and (6) Strategizing for Users’ Involvement. The main topics within each area were then organized sequentially, following a typical innovation-management process to facilitate the identification of further research opportunities and under-addressed topics that could be relevant to tackle. The paper contributes to the innovation literature by providing a firm-centered perspective on the strategic, organizational, and managerial preconditions and dynamics needed to enable and enhance collaboration with users.}
}
@incollection{HAZRATIFARD2023237,
title = {Chapter 12 - Review of using machine learning in secure IoT healthcare},
editor = {Patricia Ordóñez {de Pablos} and Xi Zhang},
booktitle = {Accelerating Strategic Changes for Digital Transformation in the Healthcare Industry},
publisher = {Academic Press},
pages = {237-269},
year = {2023},
volume = {2},
series = {Information Technologies in Healthcare Industry},
isbn = {978-0-443-15299-3},
doi = {https://doi.org/10.1016/B978-0-443-15299-3.00007-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152993000075},
author = {Mehdi Hazratifard and Vibhav Agrawal and Fayez Gebali and Haytham Elmiligi and Mohammad Mamun},
keywords = {Machine learning, Continuous authentication, Anomaly detection, Telehealth, Smart healthcare, Insider attack detection},
abstract = {The healthcare industry is experiencing a digital transformation through telehealth. As a result, users’ information is at risk of being compromised by intruders. Machine learning can provide the sector with reliable protection against potential threats to address security and privacy concerns. In this chapter, we explore possible machine-learning solutions to two security challenges in the telehealth system: continuous authentication and detecting insider attacks. Authentication is the process of confirming the identity of a device or a person before connecting to the system. One of the most effective authentication techniques in telehealth is to verify IoT devices constantly to enhance cybersecurity protection on an ongoing basis. Continuous authentication uses machine learning algorithms to monitor all telehealth network activities, from users and devices, and utilizes classification techniques to detect unauthorized activities. This method relies on the verification of the identity of registered users to avoid unauthorized access as a proactive approach to mitigate the security risks. The main advantage of using machine learning over traditional authentication methods is that it does not need the user's attention while being authenticated continuously over time. Another advantage is higher reliability and accuracy. The second security challenge is insider attacks. Insider attacks occur when a user is authenticated legitimately to the system and aims to perform malicious activities such as stealing confidential information. Machine learning-based outlier detection methods are often used to identify users’ abnormal behaviors and prevent further access to confidential information.}
}
@article{ZHENG2022103628,
title = {Leveraging blockchain technology to control contextualized business risks: Evidence from China},
journal = {Information & Management},
volume = {59},
number = {7},
pages = {103628},
year = {2022},
note = {Blockchain Innovations: Business Opportunities and Management Challenges},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103628},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000404},
author = {Shuning Zheng and Yueqiu Hu and Alain Yee Loong Chong and Chee-Wee Tan},
keywords = {Blockchain, Risk management, Opportunistic risk, Trust-evoking technology},
abstract = {The complexity and diversity of socio-economic environments call for a more nuanced consideration of contextualized risks confronting enterprises operating in these environments. By dissecting six cases that successfully adopted blockchain technology in China, we present findings of a grounded theory study into the deployment of blockchain for managing contextualized risks and opportunistic risks. Findings reveal that applying blockchain can augment risk management by controlling opportunistic risk, the latter of which denotes the variability arising from opportunistic practices of internal agents or external parties afforded by their immediate socio-economic environment. Particularly, we not only identify credibility, predatory, and compliance risk as three distinct types of opportunistic risk stemming from the unique socio-economic environment of China, but we also illustrate how blockchain could be leveraged to deal with such opportunistic risks through trust-evoking mechanisms. We discovered that blockchain could evoke trust between individuals-to-organization, organization-to-organization, and organization-to-individuals by bolstering competency, fostering benevolence, and gauging integrity in individual, inter-organizational and intra-organizational contexts, respectively.}
}
@article{ROZANEC202311094,
title = {Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {11094-11099},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.817},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323011941},
author = {Jože M. Rožanec and Patrik Zajec and Spyros Theodoropoulos and Erik Koehorst and Blaž Fortuna and Dunja Mladenić},
keywords = {Manufacturing plant control, Intelligent manufacturing systems, Advanced manufacturing, Industry 4.0, Smart Manufacturing, Visual Inspection, Quality Inspection, Data Augmentation},
abstract = {Quality control is a crucial activity manufacturing companies perform to ensure their products conform to the requirements and specifications. The introduction of artificial intelligence models enables to automate the visual quality inspection, speeding up the inspection process and ensuring all products are evaluated under the same criteria. In this research, we compare supervised and unsupervised defect detection techniques and explore data augmentation techniques to mitigate the data imbalance in the context of automated visual inspection. Furthermore, we use Generative Adversarial Networks for data augmentation to enhance the classifiers’ discriminative performance. Our results show that state-of-the-art unsupervised defect detection does not match the performance of supervised models but can reduce the labeling workload if tolerating some labeling errors. Furthermore, the best classification performance was achieved considering GAN-based data generation with AUC ROC scores equal to or higher than 0,9898. We performed the research with real-world data provided by Philips Consumer Lifestyle BV.}
}
@article{VANDERSLOOT2022105716,
title = {Deepfakes: regulatory challenges for the synthetic society},
journal = {Computer Law & Security Review},
volume = {46},
pages = {105716},
year = {2022},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2022.105716},
url = {https://www.sciencedirect.com/science/article/pii/S0267364922000632},
author = {Bart {van der Sloot} and Yvette Wagensveld},
keywords = {Deepfake, synethetic media, post-truth era, Privacy, freedom of expression, rule of law, democracy, social equality, fake news, non-consensual fake porn},
abstract = {With the rise of deepfakes and synthetic media, the question as to what is real and what is not will become increasingly important and politized. Deepfakes can be used to spread fake news, influence elections, introduce highly realistic fake evidence in courts and make fake porno movies. Each of these applications potentially has a big impact on society, social relationships, democracy and the rule of law. The question this article shall assess is whether the current regulatory regime suffices to address these potential harms and if not, which additional rules and principles should be adopted. It will discuss several potential amendments to the privacy and data protection regime, limitations to the freedom of expression and ex ante rules on the distribution of use of deepfake-technologies.}
}
@article{DIAL2023158,
title = {Baseline structural imaging correlates of treatment outcomes in semantic variant primary progressive aphasia},
journal = {Cortex},
volume = {158},
pages = {158-175},
year = {2023},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2022.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0010945222002829},
author = {Heather R. Dial and Eduardo Europa and Stephanie M. Grasso and Maria Luisa Mandelli and Kristin M. Schaffer and H. Isabel Hubbard and Lisa D. Wauters and Lindsey Wineholt and Stephen M. Wilson and Maria Luisa Gorno-Tempini and Maya L. Henry},
keywords = {Semantic variant primary progressive aphasia, Lexical retrieval treatment, Magnetic resonance imaging, Anomia, Treatment outcomes},
abstract = {Semantic variant primary progressive aphasia (svPPA) is a neurodegenerative disorder characterized by a loss of semantic knowledge in the context of anterior temporal lobe atrophy (left > right). Core features of svPPA include anomia and single-word comprehension impairment. Despite growing evidence supporting treatment for anomia in svPPA, there is a paucity of research investigating neural mechanisms supporting treatment-induced gains and generalization to untrained items. In the current study, we examined the relation between the structural integrity of brain parenchyma (tissue inclusive of gray and white matter) at pre-treatment and treatment outcomes for trained and untrained items in a group of 19 individuals with svPPA who completed lexical retrieval treatment. Two structural neuroimaging approaches were used: an exploratory, whole-brain, voxel-wise approach and an a priori region of interest (ROI) approach. Based on previous research, bilateral temporal (inferior, middle, and superior temporal gyri), parietal (supramarginal and angular gyri), frontal (inferior and middle frontal gyri) and medial temporal (hippocampus and parahippocampal gyri) ROIs were selected from the Automated Anatomical Labeling (AAL) atlas. Analyses revealed improved naming of trained items and generalization to untrained items following treatment, providing converging evidence that individuals with svPPA can benefit from treatment for anomia. Better post-treatment naming accuracy was associated with the structural integrity of inferior parietal cortex and the hippocampus. Specifically, improved naming of trained items was related to the left supramarginal (phonological processing) and angular gyri (phonological and semantic processing), and improved naming of trained and untrained items was related to the left hippocampus (episodic, context-based memory). Future research should examine treatment outcomes in relation to pre-treatment functional and structural connectivity as well as changes in network dynamics following speech-language intervention to further elucidate the neural mechanisms underlying treatment response in svPPA and related disorders.}
}
@article{ANSARI2022214523,
title = {Recent progress of fluorescent materials for fingermarks detection in forensic science and anti-counterfeiting},
journal = {Coordination Chemistry Reviews},
volume = {462},
pages = {214523},
year = {2022},
issn = {0010-8545},
doi = {https://doi.org/10.1016/j.ccr.2022.214523},
url = {https://www.sciencedirect.com/science/article/pii/S0010854522001187},
author = {Anees A. Ansari and Khalid M. Aldajani and Abdulaziz N. AlHazaa and Hamad A. Albrithen},
keywords = {Fingermarks, Forensic, Anticounterfeiting, Fluorescent, Lanthanides, Upconversion},
abstract = {This review summarized the applied techniques and applied nanomaterials (NMs) for the progress of latent fingerprints (LFPs) images on several surfaces. Used numerous types of NMs and their benefits along with their quality of the LFPs images on the porous and non-porous substrates. Several conventional techniques used for the examining of FPs pictures such as physical (powder dusting), chemical (cyanoacrylate, ninhydrin, AgNO3, fluorescent dye, etc.), and instrumental (gas chromatography, Raman scattering, Fourier transform infrared, etc.) have been discussed and gradually compromised their disadvantage in the current forensic science demand such as high contrast, good visualization, high sensitivity & selectivity, minimized auto-fluorescent, and low toxicity. The benefits and experimental results conducted by the researchers on various kinds of metal, metal oxides, plasmonic NPs, fluorescent NPs (conjugated polymer NPs, quantum dots(QDs), nonmetallic NPs, mesoporous silica NPs, and lanthanide (Ln3+) NPs, etc.) for the expansion of LFPs images on dissimilar surfaces. Despite the use of NPs in forensic sciences for the detection of LFPs pictures, the main emphasis is on luminescent Ln3+-NPs/ upconversion (UC) NPs. These luminescent Ln3+NPs/ UCNPs can produce more contrast, high visible, highly sensitive, selective, long-life decay imaging pictures of LFPs on different substrates (porous and non-porous) with minimized toxicity, which is lacking in most of the traditional fluorescent NMs. Therefore, this review provides a comprehensive a systematic overview of current trends on LFPs imaging development in forensic sciences. Currently, more studies are required to develop the most efficient, high-performance, surface-functionalized, highly biocompatible, and nontoxic Ln3+ NPs/ UCNPs for the recognition of LFPs images on different surfaces.}
}
@article{HERNANDEZ202228,
title = {Synthetic data generation for tabular health records: A systematic review},
journal = {Neurocomputing},
volume = {493},
pages = {28-45},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.04.053},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222004349},
author = {Mikel Hernandez and Gorka Epelde and Ane Alberdi and Rodrigo Cilla and Debbie Rankin},
keywords = {Synthetic data generation, Generative adversarial networks, Privacy preserving data, Data sharing, Healthcare, Artificial intelligence},
abstract = {Synthetic data generation (SDG) research has been ongoing for some time with promising results in different application domains, including healthcare, biometrics and energy consumption. The need for a robust SDG solution to capitalise on advances in Big Data and AI technology has never been greater to enable access to useful data while ensuring reasonable privacy protections. This paper presents a systematic review from the last 5 years (2016–2021) to analyse and report on recent approaches in synthetic tabular data generation (STDG) with a focus on the healthcare application context to preserve patient privacy, paying special attention to the contribution of Generative Adversarial Networks (GAN). In total 34 publications have been retrieved and analysed. A classification of approaches has been proposed and the performance of GAN-based approaches has been extensively analysed. From the systematic review it has been concluded that there is no universal method or metric to evaluate and benchmark the performance of various approaches and that further research is needed to improve the generalisability of GANs to find a model that works optimally across tabular healthcare data.}
}