@article{ELLIS2022100068,
title = {Twelve key challenges in medical machine learning and solutions},
journal = {Intelligence-Based Medicine},
volume = {6},
pages = {100068},
year = {2022},
issn = {2666-5212},
doi = {https://doi.org/10.1016/j.ibmed.2022.100068},
url = {https://www.sciencedirect.com/science/article/pii/S2666521222000217},
author = {Randall J. Ellis and Ryan M. Sander and Alfonso Limon},
keywords = {Machine learning, Baseline models, Performance metrics, Imbalanced datasets, Model and label uncertainty, Reproducibility},
abstract = {The utility of machine learning in biomedicine is being investigated in various contexts, including for diagnostic and interpretive purposes for imaging modalities, quantifying disease risk, and processing text from physician and patient reports. To best facilitate the potential of machine learning, clinicians and computational scientists must inform one another about the nature of their clinical challenges and available methods for solving them, respectively. To this end, clinicians need to critically evaluate machine learning studies conducted to solve relevant problems in medicine. This article serves as a checklist for clinicians to understand and appraise machine learning studies and help facilitate productive conversations between the clinical and data science communities to improve human health.}
}
@article{BORTOLUSSI2021223,
title = {Adversarial Learning of Robust and Safe Controllers for Cyber-Physical Systems},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {5},
pages = {223-228},
year = {2021},
note = {7th IFAC Conference on Analysis and Design of Hybrid Systems ADHS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.502},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321012775},
author = {Luca Bortolussi and Francesca Cairoli and Ginevra Carbone and Francesco Franchina and Enrico Regolin},
keywords = {Robust control, Signal Temporal Logic, Adversarial Learning, Data-based Control, Test generation, Safe control},
abstract = {We introduce a novel learning-based approach to synthesize safe and robust controllers for autonomous Cyber-Physical Systems and, at the same time, to generate challenging tests. This procedure combines formal methods for model verification with Generative Adversarial Networks. The method learns two Neural Networks: the first one aims at generating troubling scenarios for the controller, while the second one aims at enforcing the safety constraints. We test the proposed method on a variety of case studies.}
}
@article{SCHOLLER202183,
title = {Trajectory Prediction for Marine Vessels using Historical AIS Heatmaps and Long Short-Term Memory Networks⁎⁎This research is sponsored by the Danish Innovation Fund, The Danish Maritime Fund, Orients Fund and the Lauritzen Foundation through the Autonomy part of the ShippingLab project, Grant number 8090-00063B.},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {16},
pages = {83-89},
year = {2021},
note = {13th IFAC Conference on Control Applications in Marine Systems, Robotics, and Vehicles CAMS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.077},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321014798},
author = {Frederik E.T. Schöller and Thomas T. Enevoldsen and Jonathan B. Becktor and Peter N. Hansen},
keywords = {Trajectory Prediction, Autonomous Marine Vessels, Machine Learning, Autonomous Navigation, AIS Data},
abstract = {Estimating the trajectory of other vessels is essential when navigating a marine vessel, both as a human navigator and as a machine. By estimating the trajectories of other vessels, sub-systems such as collision avoidance algorithms can plan ahead accordingly in order to avoid conflicts. To estimate the trajectories of other vessels, the use of Automatic Identification System (AIS) is a good candidate data-point, as this is becoming increasingly more common, and in some cases even mandated, on-board vessels. This paper presents a data-driven approach that uses the historical AIS data within a selected area in the Danish waters. The historical data is transformed into a probabilistic heat map using Kernel Density Estimation (KDE), and is further encoded using a Convolutional Autoencoder (CAE) before entered into the estimation scheme. The estimation scheme consists of a Long Short-term Memory (LSTM) model, in a Generative Adversarial Network (GAN) configuration, which is sampled multiple times, yielding a single trajectory prediction with uncertainty. The performance of the estimation scheme is demonstrated and compared against two other commonly used methods, showing that the probabilistic heat map provides valuable information, compared to the baseline methods.}
}
@article{JIANG2021102953,
title = {Function-level obfuscation detection method based on Graph Convolutional Networks},
journal = {Journal of Information Security and Applications},
volume = {61},
pages = {102953},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102953},
url = {https://www.sciencedirect.com/science/article/pii/S2214212621001654},
author = {Shuai Jiang and Yao Hong and Cai Fu and Yekui Qian and Lansheng Han},
keywords = {Obfuscation detection, Graph Convolutional Network, Control Flow Graph},
abstract = {The obfuscation detection technology is an important auxiliary means of malware detection. Also, for security practitioners, it can carry out automatic obfuscation detection before manual reverse analysis, which helps reverse engineers to perform reverse analysis more specifically. Existing obfuscation detection methods are mainly for Android applications and based on traditional machine learning, whose detection granularity is coarse, generality is poor, and the performance is not good enough. To address these issues, in this paper, we propose a function-level obfuscation detection method based on Graph Convolutional Networks for X86 assembly code and Android applications. Firstly, our method is function-level obfuscation detection, and we extract the Control Flow Graph (CFG) of each function as its feature, including the adjacency matrix and the basic block feature matrix. Secondly, we build a hybrid neural network model GCN-LSTM as our obfuscation detection model, which combines the Graph Convolutional Network (GCN) and the Long Short-Term Memory (LSTM). Finally, we conduct experiments using real-world open-source programs and compare results with baseline methods. For function-level detection, the accuracy of our method is 94.7575% for X86 assembly code and 98.9457% for Android applications, both of which are better than baseline methods. For APK-level detection, our method can almost completely detect the obfuscated APKs. Experimental results show that our method performs well for both X86 assembly code and Android applications and is superior to the baseline methods in both function-level detection and APK-level detection. Our research showcases a successful application of the Graph Convolutional Network and the Control Flow Graph on code obfuscation detection problems.}
}
@article{READ2022104496,
title = {On the working memory of humans and great apes: Strikingly similar or remarkably different?},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {134},
pages = {104496},
year = {2022},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2021.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S0149763421005674},
author = {Dwight W. Read and Héctor M. Manrique and Michael J. Walker},
keywords = {Working memory, Human evolution, Cognitive evolution, Comparative psychology, Chimpanzee, Hominin evolution, Theory of mind, Planning},
abstract = {In this article we review publications relevant to addressing widely reported claims in both the academic and popular press that chimpanzees working memory (WM) is comparable to, if not exceeding, that of humans. WM is a complex multidimensional construct with strong parallels in humans to prefrontal cortex and cognitive development. These parallels occur in chimpanzees, but to a lesser degree. We review empirical evidence and conclude that the size of WM in chimpanzees is 2 ± 1 versus Miller’s famous 7 ± 2 in humans. Comparable differences occur in experiments on chimpanzees relating to strategic and attentional WM subsystems. Regardless of the domain, chimpanzee WM performance is comparable to that of humans around the age of 4 or 5. Next, we review evidence showing parallels among the evolution of WM capacity in hominins ancestral to Homo sapiens, the phylogenetic evolution of hominins leading to Homo sapiens, and evolution in the complexity of stone tool technology over this time period.}
}
@incollection{PATRA2021271,
title = {Chapter 13 - Application of Docking for Lead Optimization},
editor = {Mohane S. Coumar},
booktitle = {Molecular Docking for Computer-Aided Drug Design},
publisher = {Academic Press},
pages = {271-294},
year = {2021},
isbn = {978-0-12-822312-3},
doi = {https://doi.org/10.1016/B978-0-12-822312-3.00012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128223123000126},
author = {Jeevan Patra and Deepanmol Singh and Sapna Jain and Neeraj Mahindroo},
keywords = {Lead optimization, Ligand Efficiency, Molecular docking, Relative free binding energy},
abstract = {Lead optimization is a process for improving a hit or a lead compound to design drug candidates with improved efficacy and drug-like properties. Advancements in molecular structure determination and computational techniques have significantly boosted lead optimization process. Molecular docking has emerged as a reliable and cost-effective technique for lead identification and optimization using computational methods. Computational tools with advanced scoring functions utilizing artificial intelligence improve predictions. However, despite the advances in computational techniques, the success is limited and further improvement in these tools is required to simulate the complex biological environments. In this chapter, we will discuss applications of molecular docking for lead optimization in drug discovery. Applications based on molecular mechanics, thermodynamics and kinetics profiling, relative free binding energies, and toxicology predictions for optimization of lead molecules are highlighted.}
}
@article{MELOCCHI2021119901,
title = {Quality considerations on the pharmaceutical applications of fused deposition modeling 3D printing},
journal = {International Journal of Pharmaceutics},
volume = {592},
pages = {119901},
year = {2021},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2020.119901},
url = {https://www.sciencedirect.com/science/article/pii/S0378517320308863},
author = {Alice Melocchi and Francesco Briatico-Vangosa and Marco Uboldi and Federico Parietti and Maximilian Turchi and Didier {von Zeppelin} and Alessandra Maroni and Lucia Zema and Andrea Gazzaniga and Ahmed Zidan},
keywords = {3D printing, Fused deposition modeling, Drug product fabrication, Quality, Safety},
abstract = {3D printing, and particularly fused deposition modeling (FDM), has rapidly brought the possibility of personalizing drug therapies to the forefront of pharmaceutical research and media attention. Applications for this technology, described in published articles, are expected to grow significantly in 2020. Where are we on this path, and what needs to be done to develop a FDM 2.0 process and make personalized medicines available to patients? Based on literature analysis, this manuscript aims to answer these questions and highlight the critical technical aspects of FDM as an emerging technology for manufacturing safe, high-quality personalized oral drug products. In this collaborative paper, experts from different fields contribute strategies for ensuring the quality of starting materials and discuss the design phase, printer hardware and software, the process, the environment and the resulting products, from the perspectives of both patients and operators.}
}
@incollection{RAJASEKAR2022333,
title = {Chapter 19 - Security analytics},
editor = {Prashant Johri and Adarsh Anand and Jüri Vain and Jagvinder Singh and Mohammad Quasim},
booktitle = {System Assurances},
publisher = {Academic Press},
pages = {333-354},
year = {2022},
series = {Emerging Methodologies and Applications in Modelling},
isbn = {978-0-323-90240-3},
doi = {https://doi.org/10.1016/B978-0-323-90240-3.00019-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323902403000199},
author = {Vani Rajasekar and J. Premalatha and Rajesh Kumar Dhanaraj},
keywords = {Security tools, Intrusion detection, Security threats, Anomaly detection, Cyber security},
abstract = {Security analytics is a cyber security strategy that focuses on analyzing data to create robust cyber security interventions. It implies the usage of security analytic tools to improve the identification of proactive attacks and providing countermeasures. By gathering, normalizing, and analyzing network traffic for threat actions, security analytics tools identify behaviors that suggest malicious activity. The domain of security analytics is full of potentials and provides organizations looking to remain on top of vulnerabilities and one step ahead of cybercriminals with a comprehensive solution. Security analytics along with big data capabilities and threat intelligence helps to identify, analyze, and mitigate internal threats, cyber threats, and targeted attacks. Deep learning techniques and big data analytics are rapidly growing traction in the era of the security sector today. The NoSQL graph model is a leveraging security analytics and visualization technique. This approach gathers information from varied host and distributed network sources, connect them to a graph database, capturing complex relationship in the cyber security domain. In that respect, security analytics can also assist in formulating efficient ways of responding to attacks. The major applications of security analytics are network monitoring, cloud traffic, remote user behavior data, business applications, cyber security management, IoT security management, network security analytics, and big data security analytics. This chapter focuses on the introduction to security analytics, its need, challenges, applications, and its future research directions.}
}
@article{VYHMEISTER2022377,
title = {Lessons learn on responsible AI implementation: the ASSISTANT use case},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {377-382},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.422},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322017086},
author = {Eduardo Vyhmeister and Gabriel G. Castane and Johan Buchholz and Per-Olov Östberg},
keywords = {AI, standardisation, responsible AI, AI ethics, Human centered automation, design methodology for HMS, AI en manufacturing},
abstract = {Currently, pioneer companies are working hard to construct applied ethical frameworks in different sectors for using AI components that generate trust in their clients and workforce. However, independent of these few companies, there is still a considerable gap between understanding the impact of using responsible AI components, the implications of the lack of use, and what is currently applied in the industrial sector. Given that industry has shown an increased commitment to incorporating AI components, works focus on broadening the understanding of manufacturing sector stakeholders of what approaches could be considered within AI life-cycle, reducing the gap between principles and actionable requirements, and defining fundamental considerations based on risk management for incorporating, and managing, AI-based on responsible AI are required. In this work, we present a summary of the most suitable approaches that can be used for implementation and the lessons learned from a European Funded project (ASSISTANT).}
}
@article{OSTERN2022103552,
title = {Organizations’ approaches to blockchain: A critical realist perspective},
journal = {Information & Management},
volume = {59},
number = {7},
pages = {103552},
year = {2022},
note = {Blockchain Innovations: Business Opportunities and Management Challenges},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2021.103552},
url = {https://www.sciencedirect.com/science/article/pii/S0378720621001269},
author = {Nadine Kathrin Ostern and Friedrich Holotiuk and Jürgen Moormann},
keywords = {Blockchain, Distributed ledger, Critical realism, IT adoption, Sensemaking, Financial organizations},
abstract = {Organizations face manifold implementation barriers in blockchain adoption. Of particular interest is the pre-adoption phase, where knowledge and attitudes guide organizations’ approaches toward a new technology. This paper examines organizations’ approaches to blockchain through a sensemaking lens to identify how blockchain prototype development is guided by perceived business value of and sentiments toward the technology. Taking a critical realist perspective, we examine divergences between organizations’ approaches toward blockchain adoption, i.e., what they do, and why and how they approach blockchain. We differentiate between four types of approaches and provide recommendations how the pre-adoption phase can be considered in academic analyses.}
}
@article{FANG2021107505,
title = {A3CMal: Generating adversarial samples to force targeted misclassification by reinforcement learning},
journal = {Applied Soft Computing},
volume = {109},
pages = {107505},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107505},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621004282},
author = {Zhiyang Fang and Junfeng Wang and Jiaxuan Geng and Yingjie Zhou and Xuan Kan},
keywords = {Malware classification, Adversarial samples, Deep reinforcement learning, A3C},
abstract = {Machine learning algorithms have been proved to be vulnerable to adversarial attacks. The potential adversary is able to force the model to produce deliberate errors by elaborately modifying the training samples. For malware analysis, most of the existing research on evasion attacks focuses on a detection scenario, while less attention is paid to the classification scenario which is vital to decide a suitable system response in time. To fulfill this gap, this paper tries to address the misclassification problem in malware analysis. A reinforcement learning model named A3CMal is proposed. This adversarial model aims to generate adversarial samples which can fool the target classifier. As a core component of A3CMal, the self-learning agent constantly takes optimal actions to confuse the classification by slightly modifying samples on the basis of the observed states. Extensive experiments are performed to test the validity of A3CMal. The results show that the proposed A3CMal can force the target classifier to make wrong predictions while preserving the malicious functionality of the malware. Remarkably, not only can it cause the system to indicate an incorrect classification, but also can mislead the target model to classify malware into a specific category. Furthermore, our experiments demonstrate that the PE-based classifier is vulnerable to the adversarial samples generated by A3CMal.}
}
@article{HANNOUSSE2021104347,
title = {Towards benchmark datasets for machine learning based website phishing detection: An experimental study},
journal = {Engineering Applications of Artificial Intelligence},
volume = {104},
pages = {104347},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104347},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621001950},
author = {Abdelhakim Hannousse and Salima Yahiouche},
keywords = {Website phishing attacks, Machine learning, Dataset benchmarking, Information security},
abstract = {The increasing popularity of the Internet led to a substantial growth of e-commerce. However, such activities have main security challenges primary caused by cyberfraud and identity theft. Therefore, checking the legitimacy of visited web pages is a crucial task to secure costumers’ identities and prevent phishing attacks. The use of machine learning is widely recognized as a promising solution. The literature is rich with studies that use machine learning techniques for website phishing detection. However, their findings are dataset dependent and are far away from generalization. Two main reasons for this unfortunate state are the impracticable replication and absence of appropriate benchmark datasets for fair evaluation of systems. Moreover, phishing tactics are continuously evolving and proposed systems are not following those rapid changes. In this paper, we present a general scheme for building reproducible and extensible datasets for website phishing detection. The aim is to (1) enable comparison of systems adopting different features, (2) overtake the short-lived nature of phishing websites, and (3) keep track of the evolution of phishing tactics. For experimenting the proposed scheme, we start by adopting a refined categorization of website phishing features and we systematically select a total of 87 commonly recognized ones, we categorize them, and we made them subjects for relevance and runtime analysis. We use the collected set of features to build a dataset in light of the proposed scheme. Thereafter, we use a conceptual replication approach to check the genericity of former findings for the built dataset. Specifically, we evaluate the performance of classifiers on individual and combined categories of features, we investigate different combinations of models, and we explore the effects of filter and wrapper methods on the selection of discriminative features. The results show that Random Forest is the most predictive classifier. Features gathered from external services are the most discriminative where features extracted from web page contents are less distinguishing. Besides external service based features, some web page content features are found not suitable for runtime detection. The use of hybrid features provided the best accuracy score of 96.61%. By investigating different feature selection methods, filter-based ranking with incremental removal of less important features improved the performance up to 96.83% better than wrapper methods.}
}
@article{HURMELINNALAUKKANEN2022104417,
title = {Distinguishing between appropriability and appropriation: A systematic review and a renewed conceptual framing},
journal = {Research Policy},
volume = {51},
number = {1},
pages = {104417},
year = {2022},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2021.104417},
url = {https://www.sciencedirect.com/science/article/pii/S0048733321002122},
author = {Pia Hurmelinna-Laukkanen and Jialei Yang},
keywords = {Appropriability, Appropriation, Innovation, Systematic literature review},
abstract = {This study systematically reviews 200 articles published over the past three decades to reveal how appropriability and appropriation have been explained and how those perspectives resonate with developments in the innovation environment. Our results show that despite the extensive stream of literature, little effort has been made to systematically advance theory on appropriability and appropriation. Based on and extending prior literature, we propose a conceptual framing that distinguishes appropriability and appropriation, and that explains how innovating organizations build their readiness to benefit from innovation and how they realize that potential. We outline appropriability as the potential to benefit from an innovation, which accrues through instruments of appropriability: isolating appropriability mechanisms and complementary assets; and appropriation as the realization of that potential, which manifests in private and social benefits when the instruments are employed in processes for exclusion, leverage, or disclosure. We highlight the strategic importance of aligning these elements and appropriability conditions in realizing appropriation outcomes. The paper closes with a discussion on the framework's applications and relevant future research avenues.}
}
@article{SHIBATA2021262,
title = {Non-local musical statistics as guides for audio-to-score piano transcription},
journal = {Information Sciences},
volume = {566},
pages = {262-280},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521002516},
author = {Kentaro Shibata and Eita Nakamura and Kazuyoshi Yoshii},
keywords = {Music transcription, Multipitch detection, Rhythm quantization, Deep neural network, Statistical modelling},
abstract = {We present an automatic piano transcription system that converts polyphonic audio recordings into musical scores. This has been a long-standing problem of music information processing, and recent studies have made remarkable progress in the two main component techniques: multipitch detection and rhythm quantization. Given this situation, we study a method integrating deep-neural-network-based multipitch detection and statistical-model-based rhythm quantization. In the first part, we conducted systematic evaluations and found that while the present method achieved high transcription accuracies at the note level, some global characteristics of music, such as tempo scale, metre (time signature), and bar line positions, were often incorrectly estimated. In the second part, we formulated non-local statistics of pitch and rhythmic contents that are derived from musical knowledge and studied their effects in inferring those global characteristics. We found that these statistics are markedly effective for improving the transcription results and that their optimal combination includes statistics obtained from separated hand parts. The integrated method had an overall transcription error rate of 7.1% and a downbeat F-measure of 85.6% on a dataset of popular piano music, and the generated transcriptions can be partially used for music performance and assisting human transcribers, thus demonstrating the potential for practical applications.}
}
@incollection{TYRCHAN2022111,
title = {Chapter 4 - Approaches using AI in medicinal chemistry},
editor = {Takashiro Akitsu},
booktitle = {Computational and Data-Driven Chemistry Using Artificial Intelligence},
publisher = {Elsevier},
pages = {111-159},
year = {2022},
isbn = {978-0-12-822249-2},
doi = {https://doi.org/10.1016/B978-0-12-822249-2.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222492000025},
author = {Christian Tyrchan and Eva Nittinger and Dea Gogishvili and Atanas Patronov and Thierry Kogej},
keywords = {QSAR, Machine learning, De novo design, Synthesis prediction, AI, Active learning, Inverse QSAR, Nonadditivity},
abstract = {The challenge of pharmacological effect prediction and its relation to analog design consists of the decision of which molecule to make next on the basis of the available data, medicinal chemistry knowledge, experience, and intuition. In the second half of the 1900s century, attempts were made to relate narcotics pharmacology to their physicochemical properties by specifically using distribution and partition coefficients. This was shortly followed by Paul Ehrlich's observation to attribute the pharmacological effect of a compound to a specific functional group. However, it was only in 1971 that this observation was called pharmacophore. About 30years after Ehrlich, Hammett related the effect of changes in structure on reaction mechanisms, specifically based on resonance interaction of an aromatic ring. This model was later extended by Taft by separating the inductive effects from the steric properties of substituents. This concept was formalized with the work of Hansch, Free, and Wilson which reasoned that the biological activity for a set of analogs could be described by the contributions that substituents or structural elements make to the activity of a parent structure. This led to the analytical description of general quantitative-structure–activity relationship studies (QSAR). If the question “What to synthesize next?” is answered then “How to synthesize it?” follows up. The prediction of chemical reactions starting from educts or the product and educing input reactants and reaction conditions is a fundamental scientific problem. As QSAR computer-aided synthesis planning (CASP) has a long history starting in the 1960s with LHASA a rule-based approach to retrosynthesis planning.}
}
@article{AHMAD2022100452,
title = {Developing future human-centered smart cities: Critical analysis of smart city security, Data management, and Ethical challenges},
journal = {Computer Science Review},
volume = {43},
pages = {100452},
year = {2022},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100452},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000885},
author = {Kashif Ahmad and Majdi Maabreh and Mohamed Ghaly and Khalil Khan and Junaid Qadir and Ala Al-Fuqaha},
keywords = {Smart cities, Machine learning, AI ethics, Adversarial attacks, Explainability, Interpretability, Privacy, Security, Data management, Data auditing, Data ownership, Data bias, Trojan attacks, Evasion attacks},
abstract = {As the globally increasing population drives rapid urbanization in various parts of the world, there is a great need to deliberate on the future of the cities worth living. In particular, as modern smart cities embrace more and more data-driven artificial intelligence services, it is worth remembering that (1) technology can facilitate prosperity, wellbeing, urban livability, or social justice, but only when it has the right analog complements (such as well-thought out policies, mature institutions, responsible governance); and (2) the ultimate objective of these smart cities is to facilitate and enhance human welfare and social flourishing. Researchers have shown that various technological business models and features can in fact contribute to social problems such as extremism, polarization, misinformation, and Internet addiction. In the light of these observations, addressing the philosophical and ethical questions involved in ensuring the security, safety, and interpretability of such AI algorithms that will form the technological bedrock of future cities assumes paramount importance. Globally there are calls for technology to be made more humane and human-centered. In this paper, we analyze and explore key challenges including security, robustness, interpretability, and ethical (data and algorithmic) challenges to a successful deployment of AI in human-centric applications, with a particular emphasis on the convergence of these concepts/challenges. We provide a detailed review of existing literature on these key challenges and analyze how one of these challenges may lead to others or help in solving other challenges. The paper also advises on the current limitations, pitfalls, and future directions of research in these domains, and how it can fill the current gaps and lead to better solutions. We believe such rigorous analysis will provide a baseline for future research in the domain.}
}
@incollection{ARUL202187,
title = {5 - Deep learning methods for data classification},
editor = {D. Binu and B.R. Rajakumar},
booktitle = {Artificial Intelligence in Data Mining},
publisher = {Academic Press},
pages = {87-108},
year = {2021},
isbn = {978-0-12-820601-0},
doi = {https://doi.org/10.1016/B978-0-12-820601-0.00001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012820601000001X},
author = {V.H. Arul},
keywords = {Data classification, deep learning, data mining, clustering, database, training samples, data warehouse, information processing, data science, activation function},
abstract = {Deep learning is the key aspect of machine learning and artificial intelligence. In the past decades the methods introduced from the research of deep learning concepts impact an extensive range of information and signal processing task. The hierarchical models in the deep learning have the facility to learn various levels of data representation corresponding to different abstraction levels that enable the concept of representation in a dense way. Hence, the deep learning methods are extensively used in the last decades in various automatic classification processes. Various deep learning methods developed to perform the data classification process in the data mining activity are discussed in this chapter. Data classification is a data mining technique, where the training samples or database tuples are effectively analyzed to generate a generalized data. However, the classification scheme is used to sort out the future data samples and to provide superior understanding with the contents in the database.}
}
@article{DIZON2021105635,
title = {Laws of encryption: An emerging legal framework},
journal = {Computer Law & Security Review},
volume = {43},
pages = {105635},
year = {2021},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2021.105635},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921001084},
author = {Michael Anthony C. Dizon and Peter John Upson},
keywords = {Encryption laws, Cybercrime, Export control, Criminal procedure, Human rights, Cybersecurity},
abstract = {This article examines the emerging legal framework of encryption. It reviews the different categories of law that make up this legal framework, namely: export control laws, substantive cybercrime laws, criminal procedure laws, human rights laws, and cybersecurity laws. These laws are analysed according to which of the three regulatory subjects or targets they specifically address: the technology of encryption, the parties to encryption, or encrypted data and communications. For each category of law, illustrative examples of international and national laws are discussed. This article argues that understanding the legal framework of encryption is essential to determining how this technology is currently regulated and how these regulations can be improved. It concludes that the legal framework is the key to discerning the present state and future direction of encryption laws and policies.}
}
@article{CHIARELLO2021103447,
title = {Data science for engineering design: State of the art and future directions},
journal = {Computers in Industry},
volume = {129},
pages = {103447},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103447},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521000543},
author = {Filippo Chiarello and Paola Belingheri and Gualtiero Fantoni},
keywords = {Engineering design, Data science, Literature review, Scoping review, State of the art},
abstract = {Engineering design (ED) is the process of solving technical problems within requirements and constraints to create new artifacts. Data science (DS) is the inter-disciplinary field that uses computational systems to extract knowledge from structured and unstructured data. The synergies between these two fields have a long story and throughout the past decades, ED has increasingly benefited from an integration with DS. We present a literature review at the intersection between ED and DS, identifying the tools, algorithms and data sources that show the most potential in contributing to ED, and identifying a set of challenges that future data scientists and designers should tackle, to maximize the potential of DS in supporting effective and efficient designs. A rigorous scoping review approach has been supported by Natural Language Processing techniques, in order to offer a review of research across two fuzzy-confining disciplines. The paper identifies challenges related to the two fields of research and to their interfaces. The main gaps in the literature revolve around the adaptation of computational techniques to be applied in the peculiar context of design, the identification of data sources to boost design research and a proper featurization of this data. The challenges have been classified considering their impacts on ED phases and applicability of DS methods, giving a map for future research across the fields. The scoping review shows that to fully take advantage of DS tools there must be an increase in the collaboration between design practitioners and researchers in order to open new data driven opportunities.}
}
@article{YANG2021206,
title = {Learning multi-granularity features from multi-granularity regions for person re-identification},
journal = {Neurocomputing},
volume = {432},
pages = {206-215},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S092523122031897X},
author = {Kaiwen Yang and Jiwei Yang and Xinmei Tian},
keywords = {Person re-ID, Multi-granularity feature fusion, Human region localization},
abstract = {Part-based methods for person re-identification have been widely studied. In existing part-based methods, although multiple parts are explored, only coarse-grained features of these parts are utilized. Thus, too much fine-grained information is discarded, which limits their ability to extract detailed discriminative features. To tackle this problem, we propose a novel person re-identification network to learn discriminative features across multiple granularities from body regions which are also multi-grained. Specifically, we detect multi-granularity body regions at different stages of a backbone network, and multi-granularity features are learned from body regions with corresponding granularities. To overcome the severe mismatching problem of fine-grained regions and to learn discriminative features, the detection of multi-granularity body regions and the learning of multi-granularity features are jointly optimized. This joint optimization pushes the learned features concentrating on body regions. Moreover, with the body regions well located, the multi-granularity features can be well aligned. Extensive experiments on four popular datasets show that our method is the state-of-the-art in recent years.}
}
@article{MEYER2020106034,
title = {The commons: A model for understanding collective action and entrepreneurship in communities},
journal = {Journal of Business Venturing},
volume = {35},
number = {5},
pages = {106034},
year = {2020},
issn = {0883-9026},
doi = {https://doi.org/10.1016/j.jbusvent.2020.106034},
url = {https://www.sciencedirect.com/science/article/pii/S0883902619301429},
author = {Camille Meyer},
keywords = {Commons, Decommodification, Community enterprise, Institutional Analysis and Development framework, Microfinance, Brazil},
abstract = {The creation of commons—resources that are shared, accessible, and collectively owned and managed by communities—is increasingly being adopted by social entrepreneurs as a way of contributing to community development and putting value into economic activities. Yet, little research is evident related to the entrepreneurial processes involved in the creation and commercialization of these shared resources. Drawing on the Institutional Analysis and Development framework developed by Ostrom (2005), I explain how commons are entrepreneurially created. Based on a comparative study of five community banks in Brazil, I derive two ideological principles of collective entrepreneurship that help sustain commercialization of commons without commodification, namely ‘self-organization’ and ‘right to access’. I elucidate how these principles are enacted across venture levels through downward and upward mechanisms of social control facilitated by entrepreneurs who enhance collective action. This article contributes to the entrepreneurship theory of commons by explaining how commons are entrepreneurially created and by adding the collective entrepreneurship principles and mechanisms that commons of different types need in order to achieve and sustain wealth-creation options without incurring the downsides of commodification.}
}
@article{GESNER2021256,
title = {Robust Data-Driven Error Compensation for a Battery Model},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {7},
pages = {256-261},
year = {2021},
note = {19th IFAC Symposium on System Identification SYSID 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.368},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321011423},
author = {Philipp Gesner and Frank Kirschbaum and Richard Jakobi and Ivo Horstkötter and Bernard Bäker},
keywords = {Nonlinear models, Neural Networks, System Identification, Automobile industry},
abstract = {Models of traction batteries are an essential tool throughout the development of automotive drivetrains. Surprisingly, today’s massively collected battery data is not yet used for more accurate and reliable simulations. Primarily, the non-uniform excitation during regular battery operations prevent a consequent utilization of such measurements. Hence, there is a need for methods which enable robust models based on large datasets. For that reason, a data-driven error model is introduced enhancing an existing physically motivated model. A neural network compensates the existing dynamic error and is further limited based on a description of the underlying data. This paper tries to verify the effectiveness and robustness of the general setup and additionally evaluates a one-class support vector machine as the proposed model for the training data distribution. Based on five datasets it is shown, that gradually limiting the data-driven error compensation outside the boundary leads to a similar improvement and an increased overall robustness.}
}
@incollection{COLLINS2021571,
title = {Appendix 2 - Artificial Intelligence (AI) and Big Data},
editor = {Albert Lester},
booktitle = {Project Management, Planning and Control (Eighth Edition)},
publisher = {Butterworth-Heinemann},
edition = {Eighth Edition},
pages = {571-589},
year = {2021},
isbn = {978-0-12-824339-8},
doi = {https://doi.org/10.1016/B978-0-12-824339-8.15002-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243398150023},
author = {Graham Collins}
}
@article{TEUBNER2020101642,
title = {Literature review: Understanding information systems strategy in the digital age},
journal = {The Journal of Strategic Information Systems},
volume = {29},
number = {4},
pages = {101642},
year = {2020},
note = {2020 Review Issue},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2020.101642},
url = {https://www.sciencedirect.com/science/article/pii/S0963868720300500},
author = {R. Alexander Teubner and Jan Stockhinger},
keywords = {IT/IS strategy, Strategic information systems planning, Digital strategy, Digital business strategy, Digital infrastructure, Digital Transformation},
abstract = {IT/IS strategy is of central importance to practice and many well-developed lines of research have contributed to our understanding of IT/IS strategy. However, throughout the last decade, digitalization has fundamentally transformed the business world and put into question traditional strategy wisdom. As information technologies are the driver of this digital transformation, we can expect an even more fundamental change in IT/IS strategy thinking. To verify this expectation, we undertook an in-depth, extensive review of the academic literature on this topic. Our review, which is time-framed to the years 2008–2018, distils five different directions in the development of IT/IS strategy research. It also identifies a shift in how IT/IS strategy is defined and investigated over this period. Moreover, we present an emerging debate on how digitalization challenges traditional IT/IS strategy wisdom. As this debate is still in its infancy, we take it further by entering into the larger discussion on digitalization, including digital innovation, digital ecosystems, and digital transformation. Building on this, we derive at deeper insights on how IT/IS strategy could, should, or should better not be understood in the digital age.}
}
@article{ZHAO20201624,
title = {Advancing computer-aided drug discovery (CADD) by big data and data-driven machine learning modeling},
journal = {Drug Discovery Today},
volume = {25},
number = {9},
pages = {1624-1638},
year = {2020},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2020.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1359644620302646},
author = {Linlin Zhao and Heather L. Ciallella and Lauren M. Aleksunes and Hao Zhu},
abstract = {Advancing a new drug to market requires substantial investments in time as well as financial resources. Crucial bioactivities for drug candidates, including their efficacy, pharmacokinetics (PK), and adverse effects, need to be investigated during drug development. With advancements in chemical synthesis and biological screening technologies over the past decade, a large amount of biological data points for millions of small molecules have been generated and are stored in various databases. These accumulated data, combined with new machine learning (ML) approaches, such as deep learning, have shown great potential to provide insights into relevant chemical structures to predict in vitro, in vivo, and clinical outcomes, thereby advancing drug discovery and development in the big data era.}
}
@article{2021I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {14},
number = {1},
pages = {I-CCCXXIII},
year = {2021},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(20)31073-1},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X20310731}
}
@article{YANG20204411,
title = {Application of Negative Design To Design a More Desirable Virtual Screening Library},
journal = {Journal of Medicinal Chemistry},
volume = {63},
number = {9},
pages = {4411-4429},
year = {2020},
issn = {1520-4804},
doi = {https://doi.org/10.1021/acs.jmedchem.9b01476},
url = {https://www.sciencedirect.com/science/article/pii/S1520480420005074},
author = {Zi-Yi Yang and Jun-Hong He and Ai-Ping Lu and Ting-Jun Hou and Dong-Sheng Cao},
abstract = {Negative design is a group of virtual screening methods that aims at weeding out compounds with undesired properties during the early stages of drug development. These methods are mainly designed to predict three important types of pharmacological properties: drug-likeness, frequent hitters, and toxicity. In order to achieve high screening efficiency, most negative design methods are physicochemical property-based and/or substructure-based rules or filters. Such methods have advantages of simplicity and good interpretability, but they also suffer from some defects such as inflexibility, discontinuity, and hard decision-making. In this review, the advances in negative design for the evaluations of drug-likeness, frequent hitters, and toxicity are outlined. In addition, the related Web servers and software packages developed recently for negative design are summarized. Finally, future research directions in this field are discussed.
}
}
@article{NEARYZAJICZEK20216403,
title = {Stain-free identification of tissue pathology using a generative adversarial network to infer nanomechanical signatures††Electronic supplementary information (ESI) available. See DOI: 10.1039/d1na00527h},
journal = {Nanoscale Advances},
volume = {3},
number = {22},
pages = {6403-6414},
year = {2021},
issn = {2516-0230},
doi = {https://doi.org/10.1039/d1na00527h},
url = {https://www.sciencedirect.com/science/article/pii/S2516023023007311},
author = {Lydia Neary-Zajiczek and Clara Essmann and Anita Rau and Sophia Bano and Neil Clancy and Marnix Jansen and Lauren Heptinstall and Elena Miranda and Amir Gander and Vijay Pawar and Delmiro Fernandez-Reyes and Michael Shaw and Brian Davidson and Danail Stoyanov},
abstract = {ABSTRACT
Intraoperative frozen section analysis can be used to improve the accuracy of tumour margin estimation during cancer resection surgery through rapid processing and pathological assessment of excised tissue. Its applicability is limited in some cases due to the additional risks associated with prolonged surgery, largely from the time-consuming staining procedure. Our work uses a measurable property of bulk tissue to bypass the staining process: as tumour cells proliferate, they influence the surrounding extra-cellular matrix, and the resulting change in elastic modulus provides a signature of the underlying pathology. In this work we accurately localise atomic force microscopy measurements of human liver tissue samples and train a generative adversarial network to infer elastic modulus from low-resolution images of unstained tissue sections. Pathology is predicted through unsupervised clustering of parameters characterizing the distributions of inferred values, achieving 89% accuracy for all samples based on the nominal assessment (n = 28), and 95% for samples that have been validated by two independent pathologists through post hoc staining (n = 20). Our results demonstrate that this technique could increase the feasibility of intraoperative frozen section analysis for use during resection surgery and improve patient outcomes.}
}
@article{STOCK2021193,
title = {Make bloom and let wither: Biopolitics of precision agriculture at the dawn of surveillance capitalism},
journal = {Geoforum},
volume = {122},
pages = {193-203},
year = {2021},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2021.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0016718521001135},
author = {Ryan Stock and Maaz Gardezi},
keywords = {Precision agriculture, Biopolitics, Surveillance capitalism, Data grabbing, Governmentality, Agrarian question of labor},
abstract = {Precision agriculture is an assemblage of data-driven agricultural technologies, discursively articulated as a clever gambit against climate-induced food insecurity. Agritech companies engage in data grabbing as an accumulation strategy and to influence farmers’ behaviors, opening new agrarian frontiers for surveillance capitalism. Wielding big data to manage and optimize species within food production systems becomes a biopolitical calculation of “make bloom and let wither,” with implications for the agrarian question of labor. Drawing on mixed methods fieldwork from South Dakota and Vermont, we document how different actors perform and contest agri-algorithmic subjectivities, producing novel terrains of food politics and neoliberal state-citizen relations.}
}
@article{ZHENG2021113666,
title = {Exploiting machine learning for bestowing intelligence to microfluidics},
journal = {Biosensors and Bioelectronics},
volume = {194},
pages = {113666},
year = {2021},
issn = {0956-5663},
doi = {https://doi.org/10.1016/j.bios.2021.113666},
url = {https://www.sciencedirect.com/science/article/pii/S095656632100703X},
author = {Jiahao Zheng and Tim Cole and Yuxin Zhang and Jeeson Kim and Shi-Yang Tang},
keywords = {Machine learning, Microfluidics, Intelligent systems, Deep learning},
abstract = {Intelligent microfluidics is an emerging cross-discipline research area formed by combining microfluidics with machine learning. It uses the advantages of microfluidics, such as high throughput and controllability, and the powerful data processing capabilities of machine learning, resulting in improved systems in biotechnology and chemistry. Compared to traditional microfluidics using manual analysis methods, intelligent microfluidics needs less human intervention, and results in a more user-friendly experience with faster processing. There is a paucity of literature reviewing this burgeoning and highly promising cross-discipline. Therefore, we herein comprehensively and systematically summarize several aspects of microfluidic applications enabled by machine learning. We list the types of microfluidics used in intelligent microfluidic applications over the last five years, as well as the machine learning algorithms and the hardware used for training. We also present the most recent advances in key technologies, developments, challenges, and the emerging opportunities created by intelligent microfluidics.}
}
@article{WANG2021107763,
title = {The algorithmic composition for music copyright protection under deep learning and blockchain},
journal = {Applied Soft Computing},
volume = {112},
pages = {107763},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107763},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621006840},
author = {Nana Wang and Hui Xu and Feng Xu and Lei Cheng},
keywords = {Algorithmic composition, Blockchain, Copyright protection, Deep learning, Neural network},
abstract = {To strengthen music copyright protection effectively, a new deep learning neural network music composition neural network (MCNN) is proposed. The probability distribution of LSTM generation is adjusted by constructing a reasonable reward function. Music theory rules are used to constrain the generated music style to realize the intelligent generation of specific music style. Then, the digital music copyright protection system based on blockchain is constructed from three perspectives of confirming right, using right, and protecting right. The validity of the model is further verified by relevant data. The results show that the composition algorithm based on deep learning can realize music creation, and the qualified rate reaches 95.11%. Compared with the composition algorithm in the latest study, the model achieves 62.4 percent satisfaction with subjective samples and a recognition rate of 75.6 percent for musical sentiment classification. It is proved that the music copyright protection model based on block chain can ensure that the copyright owners of works obtain corresponding economic benefits from various distribution channels, which is helpful to build a harmonious music market environment. In short, the innovation of this study is reflected in that it fills in the gap of detailed comparative study of the differences in the application of different models, realizes the framework of music copyright protection system, and provides convenient conditions for composers.}
}
@article{AVERSANO2021100389,
title = {A systematic review on Deep Learning approaches for IoT security},
journal = {Computer Science Review},
volume = {40},
pages = {100389},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100389},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000290},
author = {Lerina Aversano and Mario Luca Bernardi and Marta Cimitile and Riccardo Pecori},
keywords = {Internet of Things, Security, Systematic review, Deep Learning},
abstract = {The constant spread of smart devices in many aspects of our daily life goes hand in hand with the ever-increasing demand for appropriate mechanisms to ensure they are resistant against various types of threats and attacks in the Internet of Things (IoT) environment. In this context, Deep Learning (DL) is emerging as one of the most successful and suitable techniques to be applied to different IoT security aspects. This work aims at systematically reviewing and analyzing the research landscape about DL approaches applied to different IoT security scenarios. The contributions we reviewed are classified according to different points of view into a coherent and structured taxonomy in order to identify the gap in this pivotal research area. The research focused on articles related to the keywords ’deep learning’, ’security’ and ’Internet of Things’ or ’IoT’ in four major databases, namely IEEEXplore, ScienceDirect, SpringerLink, and the ACM Digital Library. We selected and reviewed 69 articles in the end. We have characterized these studies according to three main research questions, namely, the involved security aspects, the used DL network architectures, and the engaged datasets. A final discussion highlights the research gaps still to be investigated as well as the drawbacks and vulnerabilities of the DL approaches in the IoT security scenario.}
}
@article{ZIMMERER2020103,
title = {Automated profiling of spontaneous speech in primary progressive aphasia and behavioral-variant frontotemporal dementia: An approach based on usage-frequency},
journal = {Cortex},
volume = {133},
pages = {103-119},
year = {2020},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2020.08.027},
url = {https://www.sciencedirect.com/science/article/pii/S0010945220303336},
author = {Vitor C. Zimmerer and Chris J.D. Hardy and James Eastman and Sonali Dutta and Leo Varnet and Rebecca L. Bond and Lucy Russell and Jonathan D. Rohrer and Jason D. Warren and Rosemary A. Varley},
keywords = {Language profiles, Dementia, Primary progressive aphasia, Frontotemporal dementia, Usage-frequency},
abstract = {Language production provides important markers of neurological health. One feature of impairments of language and cognition, such as those that occur in stroke aphasia or Alzheimer's disease, is an overuse of high frequency, “familiar” expressions. We used computerized analysis to profile narrative speech samples from speakers with variants of frontotemporal dementia (FTD), including subtypes of primary progressive aphasia (PPA). Analysis was performed on language samples from 29 speakers with semantic variant PPA (svPPA), 25 speakers with logopenic variant PPA (lvPPA), 34 speakers with non-fluent variant PPA (nfvPPA), 14 speakers with behavioral variant FTD (bvFTD) and 20 older normal controls (NCs). We used frequency and collocation strength measures to determine use of familiar words and word combinations. We also computed word counts, content word ratio and a combination ratio, a measure of the degree to which the individual produces connected language. All dementia subtypes differed significantly from NCs. The most discriminating variables were word count, combination ratio, and content word ratio, each of which distinguished at least one dementia group from NCs. All participants with PPA, but not participants with bvFTD, produced significantly more frequent forms at the level of content words, word combinations, or both. Each dementia group differed from the others on at least one variable, and language production variables correlated with established behavioral measures of disease progression. A machine learning classifier, using narrative speech variables, achieved 90% accuracy when classifying samples as NC or dementia, and 59.4% accuracy when matching samples to their diagnostic group. Automated quantification of spontaneous speech in both language-led and non-language led dementias, is feasible. It allows extraction of syndromic profiles that complement those derived from standardized tests, warranting further evaluation as candidate biomarkers. Inclusion of frequency-based language variables benefits profiling and classification.}
}
@article{POLVORA2020120091,
title = {Blockchain for industrial transformations: A forward-looking approach with multi-stakeholder engagement for policy advice},
journal = {Technological Forecasting and Social Change},
volume = {157},
pages = {120091},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120091},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520309173},
author = {Alexandre Pólvora and Susana Nascimento and Joana S. Lourenço and Fabiana Scapolo},
keywords = {Blockchain, Industry, Innovation policy, Technology uptake, Foresight, Stakeholder engagement},
abstract = {Beyond more recognized financial applications of Blockchain, its potential for other sectors has increasingly come to the foreground. Yet, its development still faces questions over impact, added value, or concrete paths for widespread deployment. In this paper we argue for a transdisciplinary forward-looking approach to address such uncertainties, based on the processes and findings of the research project #Blockchain4EU: Blockchain for Industrial Transformations, which was developed inside the European Commission with a focus on multi-stakeholder engagement and co-creation. We invested in a mix of desk research with qualitative methods, including interviews, surveys, or ethnographic explorations, together with participatory workshops for collective vision building and speculative prototyping for policy. Our main findings underline key sociotechnical challenges and opportunities for the development and uptake of Blockchain in specific European industrial and business contexts, taking into account the complexity of policy, economic, social, technical, legal and environmental elements. But, aiming to push the frontiers of what's common practice in advice for policy when looking into early-stage technologies as Blockchain, we also strive to emphasize how our approach can benefit decision-makers through robust methodological and conceptual processes that are simultaneously evidence-based and experimental in their delivery and impact.}
}
@article{WANG2020101538,
title = {Machine learning in additive manufacturing: State-of-the-art and perspectives},
journal = {Additive Manufacturing},
volume = {36},
pages = {101538},
year = {2020},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2020.101538},
url = {https://www.sciencedirect.com/science/article/pii/S2214860420309106},
author = {C. Wang and X.P. Tan and S.B. Tor and C.S. Lim},
keywords = {Additive manufacturing, Process, Machine learning, Production, Design},
abstract = {Additive manufacturing (AM) has emerged as a disruptive digital manufacturing technology. However, its broad adoption in industry is still hindered by high entry barriers of design for additive manufacturing (DfAM), limited materials library, various processing defects, and inconsistent product quality. In recent years, machine learning (ML) has gained increasing attention in AM due to its unprecedented performance in data tasks such as classification, regression and clustering. This article provides a comprehensive review on the state-of-the-art of ML applications in a variety of AM domains. In the DfAM, ML can be leveraged to output new high-performance metamaterials and optimized topological designs. In AM processing, contemporary ML algorithms can help to optimize process parameters, and conduct examination of powder spreading and in-process defect monitoring. On the production of AM, ML is able to assist practitioners in pre-manufacturing planning, and product quality assessment and control. Moreover, there has been an increasing concern about data security in AM as data breaches could occur with the aid of ML techniques. Lastly, it concludes with a section summarizing the main findings from the literature and providing perspectives on some selected interesting applications of ML in research and development of AM.}
}
@article{2021I,
title = {Full Issue PDF},
journal = {JACC: Case Reports},
volume = {3},
number = {10},
pages = {I-XCVII},
year = {2021},
issn = {2666-0849},
doi = {https://doi.org/10.1016/S2666-0849(21)00692-6},
url = {https://www.sciencedirect.com/science/article/pii/S2666084921006926}
}
@article{ALINAGHIAN2021488,
title = {How do social enterprises manage business relationships? A review of the literature and directions for future research},
journal = {Journal of Business Research},
volume = {136},
pages = {488-498},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321005580},
author = {Leila Alinaghian and Kamran Razmdoost},
keywords = {Business relationships, Social enterprise, Systematic review, Business networks},
abstract = {Social enterprise–business relationships are an emerging unique form of business relationships. Whilst scholars have recently shown a growing interest in investigating the practices that social enterprises adopt to manage their relationships with businesses, the present literature lacks a synthesis of major findings and a reflection on current developments. The purpose of this paper is to critically and systematically review and assess the current status of research on practices through which social enterprise manage business relationships and to provide an organising framework for future scholarship. Adopting a systematic literature review approach, a total of 51 articles were reviewed. The results of our thematic analysis revealed that social enterprises engage in four key practices of initiation, persuasion, conflict resolution, and value creation to manage their relationships with businesses. Our review of literature also sheds light on the determinants and outcomes of these practices and offers avenues for future research.}
}
@incollection{DEVEREUX2021129,
title = {Chapter 7 - Data-driven materials discovery for solar photovoltaics},
editor = {Jennifer Dunn and Prasanna Balaprakash},
booktitle = {Data Science Applied to Sustainability Analysis},
publisher = {Elsevier},
pages = {129-164},
year = {2021},
isbn = {978-0-12-817976-5},
doi = {https://doi.org/10.1016/B978-0-12-817976-5.00008-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128179765000085},
author = {Leon R. Devereux and Jacqueline M. Cole},
keywords = {Photovoltaics, Data science, Materials discovery, Quantum chemistry, High-throughput, Computational screening, Natural language processing, Text-mining, Materials database, Electronic structure, Absorption properties},
abstract = {The increase in global energy demand, coupled with the urgent necessity to transition to a fully sustainable energy infrastructure, means inexpensive, versatile and efficient solar energy technology will be in very high demand over the coming decades. Solar photovoltaic (PV) devices are reliant on highly specialized materials in order to harvest light as efficiently as possible. Many materials engineers are now using data science techniques to accelerate their search for new materials with optimized properties for solar devices. This chapter discusses such techniques, sorted into three overarching categories: machine learning for property prediction, high-throughput computational screening, and automated database generation. We then focus in on case studies of recent projects that have used the above techniques to design novel materials for next-generation PV technologies which present an alternative to conventional silicon crystal cells: dye-sensitized solar cells, organic polymer PV and organic/inorganic hybrid perovskite PV devices. These devices share the benefit of using relatively cheap starting materials and they have unique applications, such as smart windows and wearable devices. However, each technology has also experienced limitations in material design which have acted as obstacles to commercialization. This chapter will discuss these issues and how the different techniques of data science for materials discovery are being used to overcome such bottlenecks. Following this, we look at a vision for the future of PV materials engineering in the form of “inverse design”, assisted by autoencoder machine learning methods.}
}
@article{KO2020103242,
title = {Patent-trademark linking framework for business competition analysis},
journal = {Computers in Industry},
volume = {122},
pages = {103242},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103242},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519311169},
author = {Namuk Ko and Byeongki Jeong and Janghyeok Yoon and Changho Son},
keywords = {Technology-based firms, Business intelligence, Competitive information, Goods and services, Patent, Trademark},
abstract = {A major concern of technology-based firms (TBFs) is gaining a competitive edge, for which TBFs develop technologies to embed in their products and services to differentiate themselves from their competitors. TBFs that share similar products or services must compete against each other, which means that business competition among TBFs should be conducted at the product and service levels. However, faced with challenges in obtaining information of the products and services for which TBFs apply their own technologies, previous studies have not thoroughly analyzed the business competition from the perspectives of products and services. Therefore, this study proposes a framework to better understand the business competition among TBFs by linking their business areas and technologies, which refer to products and services in their trademarks, and the technologies described in their patents, respectively. This framework provides the identification of technology-based business portfolios and an understanding of the business competition from both an overall business perspective and within specific areas, considering the technological capabilities and business willingness of the TBFs. This study academically contributes as the first study to conduct a business competition analysis at the product and service levels and industrially contributes by suggesting a systematic process that will help in understanding the rapidly evolving business competition environment.}
}
@article{SODA2021104343,
title = {Brokerage evolution in innovation contexts: Formal structure, network neighborhoods and knowledge},
journal = {Research Policy},
volume = {50},
number = {10},
pages = {104343},
year = {2021},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2021.104343},
url = {https://www.sciencedirect.com/science/article/pii/S0048733321001414},
author = {Giuseppe Soda and Akbar Zaheer and Xiaoming Sun and Wentian Cui},
keywords = {Co-patenting networks, Network evolution, Innovation management, Closure, Disintermediation},
abstract = {Research in a number of fields has shown that brokerage is typically fragile while creating consequential outcomes. However, little work has examined the conditions under which brokerage ends, and furthermore, whether and when it terminates with closure in a closed triad that includes the broker, or in a dyad that connects the previously-disconnected alters but disintermediates the broker. We employ a comprehensive theoretical framework drawing on constrained agency to study these questions in a context of organizational innovation. Specifically, we investigate the role of hierarchy, inventors’ network neighborhoods and knowledge differences in shaping the evolution of brokerage. We test our ideas in the a setting of co-patenting in 41 large Chinese research-intensive organizations over the period 1996-2008, with a dataset of 36,338 patents applied for by these organizations. We first show that the type of brokerage ending matters for innovation outcomes by demonstrating that disintermediation creates more subsequent innovativeness than closure. Thereafter, we use a two-step model to first model the termination of brokerage and in the second step to predict the type of closing: disintermediation or closure. Our results show that the broker's and alters’ hierarchical rank similarity promotes disintermediation, as does alters’ connectedness in network neighborhoods, while knowledge differences among the broker and alters encourage the evolution of brokerage toward closure. We spell out the implications of our findings for organizational innovation and the management of R&D.}
}
@article{LIAO2021103244,
title = {GIFMarking: The robust watermarking for animated GIF based deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103244},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103244},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001590},
author = {Xin Liao and Jing Peng and Yun Cao},
keywords = {Animated GIF images, Robust watermarking, 3D convolutional neural networks, Adversarial network},
abstract = {Animated GIF has become a key communication tool in contemporary social platforms thanks to highly compatible with affective performance, and it is gradually adopted in commercial applications. Therefore, the copyright protection of the animated GIF requires more attention. Digital watermarking is an effective method to embed invisible data into a digital medium that can identify the creator or authorized users. However, few works have been devoted to robust watermarking for the animated GIF. One of the main challenges is that the animated image also contains time frame dimension information compare with still images. This paper proposes a robust blind watermarking framework based 3D convolutional neural networks for the animated GIF image, which achieves watermark image embedding and extraction for the animated GIF. Also, noise simulation is developed in frame-level to ensure robustness for the attack of the temporal dimension in this framework. Furthermore, the invisibility of the watermarked animated image is optimized by adversarial learning. Experimental results provide the effectiveness of the proposed framework and show advantages over existing works.}
}
@article{MIMURA2021100404,
title = {Static detection of malicious PowerShell based on word embeddings},
journal = {Internet of Things},
volume = {15},
pages = {100404},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100404},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000482},
author = {Mamoru Mimura and Yui Tajiri},
keywords = {PowerShell, Latent Semantic Indexing, Doc2vec, XGBoost},
abstract = {While traditional malware relies on executables to function, fileless malware resides in memory to evade traditional detection methods. PowerShell which is a legitimate management tool used by system administrators provides an ideal cover for attackers. Many studies attempted to detect unknown malware with machine learning techniques. However, there are a few studies for detecting malicious PowerShell. Previous studies proposed methods of detecting malicious PowerShell with deep neural networks. Previous methods require decoding obfuscated samples for dynamic code evaluation. Decoding obfuscated samples is a troublesome task and is often time consuming. Security devices such as intrusion detection system (IDS) or sandbox are located at a point that can monitor all inbound traffic. In general, this traffic contains too massive samples to analyze by dynamic analysis. Therefore, a light-weight static method is desirable. In addition, some studies use their private dataset to evaluate their methods. In this paper, we propose a static method of detecting malicious PowerShell based on word embeddings. In our method, PowerShell scripts are separated into words, and these words are used as features for machine learning techniques. We improved the feature extraction method by selecting frequent words. To provide reproducibility, we obtained thousands of samples from multiple websites which are publicly available. The best F1 score achieves 0.995 in practical environment, and achieves 0.985 in 5-fold cross-validation. Furthermore, we identified their malware families, and confirmed our method is effective to new ones.}
}
@article{ERENPREISA2022119,
title = {Paradoxes of cancer: Survival at the brink},
journal = {Seminars in Cancer Biology},
volume = {81},
pages = {119-131},
year = {2022},
note = {A Special International Conference on Polyploidy, Senescence, Evolution and Cancer},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2020.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X20302698},
author = {Jekaterina Erenpreisa and Kristine Salmina and Olga Anatskaya and Mark S. Cragg},
keywords = {Cancer atavism, Reversible polyploidy, Self-organisation, Cancer “life-cycle”, Aneuploidy, Bivalency},
abstract = {The fundamental understanding of how Cancer initiates, persists and then progresses is evolving. High-resolution technologies, including single-cell mutation and gene expression measurements, are now attainable, providing an ever-increasing insight into the molecular details. However, this higher resolution has shown that somatic mutation theory itself cannot explain the extraordinary resistance of cancer to extinction. There is a need for a more Systems-based framework of understanding cancer complexity, which in particular explains the regulation of gene expression during cell-fate decisions. Cancer displays a series of paradoxes. Here we attempt to approach them from the view-point of adaptive exploration of gene regulatory networks at the edge of order and chaos, where cell-fate is changed by oscillations between alternative regulators of cellular senescence and reprogramming operating through self-organisation. On this background, the role of polyploidy in accessing the phylogenetically pre-programmed “oncofetal attractor” state, related to unicellularity, and the de-selection of unsuitable variants at the brink of cell survival is highlighted. The concepts of the embryological and atavistic theory of cancer, cancer cell “life-cycle”, and cancer aneuploidy paradox are dissected under this lense. Finally, we challenge researchers to consider that cancer “defects” are mostly the adaptation tools of survival programs that have arisen during evolution and are intrinsic of cancer. Recognition of these features should help in the development of more successful anti-cancer treatments.}
}
@article{YOON2020102164,
title = {A systematic approach to prioritizing R&D projects based on customer-perceived value using opinion mining},
journal = {Technovation},
volume = {98},
pages = {102164},
year = {2020},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2020.102164},
url = {https://www.sciencedirect.com/science/article/pii/S0166497218306874},
author = {Byungun Yoon and Yujin Jeong and Keeeun Lee and Sungjoo Lee},
keywords = {Technology assessment, Customer-perceived value, Structural equation model, Opinion mining, Technology strategy},
abstract = {As product development has recently emphasized user innovation, it should necessarily reflect customer-perceived value, as well as technological value itself. However, while previous studies for technology planning have focused on analyzing the potential of technology, they have not considered the customer-perceived value that technology can create in a new product. Therefore, this research suggests a new approach to assessing the level of technology and evaluating R&D projects, by investigating customer-perceived value on technology through the use of the structural equation model and opinion mining. For this, the assessment framework is developed in terms of technology, product quality, and customer satisfaction, respectively, by investigating a variety of databases on each factor. The relationship between technology level and customer satisfaction is analyzed, using structural equation model and opinion mining. Based on the results, a strategy for technology development is established through gap analysis and simulation, after selecting and evaluating technologies that need to be developed. The proposed approach is applied to the real case of a moving system, in particular an automobile door, and we obtained that an R&D project for hinge-related technology would be promising, enhancing the customer satisfaction. It can suggest a future direction for new technology development. This paper contributes to enhancing the efficiency of technology planning based on the customer-perceived value, enabling the launch of new R&D projects.}
}
@article{MITTAL2021102163,
title = {A survey on hardware security of DNN models and accelerators},
journal = {Journal of Systems Architecture},
volume = {117},
pages = {102163},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102163},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121001168},
author = {Sparsh Mittal and Himanshi Gupta and Srishti Srivastava},
keywords = {Hardware security, Trojan, Fault-injection attack, Side-channel attack, Encryption, Deep neural network},
abstract = {As “deep neural networks” (DNNs) achieve increasing accuracy, they are getting employed in increasingly diverse applications, including security-critical applications such as medical and defense. This immense use of DNNs has motivated the researchers to scrutinizingly study their security vulnerability and propose countermeasures, especially in the context of hardware security. In this paper, we present a survey of techniques for the hardware security of DNNs. For the research works, we highlight the threat-model, key idea for launching attack and defense strategies. We organize the works on salient categories to highlight their strengths and limitations. This paper aims to equip researchers with the knowledge of recent advances in DNN security and motivate them to think of security as the first principle.}
}
@article{BALAHA2021102156,
title = {Hybrid COVID-19 segmentation and recognition framework (HMB-HCF) using deep learning and genetic algorithms},
journal = {Artificial Intelligence in Medicine},
volume = {119},
pages = {102156},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102156},
url = {https://www.sciencedirect.com/science/article/pii/S0933365721001494},
author = {Hossam Magdy Balaha and Magdy Hassan Balaha and Hesham Arafat Ali},
keywords = {Classification, Convolutional neural network (CNN), COVID-19, Data augmentation (DA), Deep learning (DL), Genetic algorithms (GA), Optimization, Transfer learning (TL)},
abstract = {COVID-19 (Coronavirus) went through a rapid escalation until it became a pandemic disease. The normal and manual medical infection discovery may take few days and therefore computer science engineers can share in the development of the automatic diagnosis for fast detection of that disease. The study suggests a hybrid COVID-19 framework (named HMB-HCF) based on deep learning (DL), genetic algorithm (GA), weighted sum (WS), and majority voting principles in nine phases. Its segmentation phase suggests a lung segmentation algorithm using X-Ray images (named HMB-LSAXI) for extracting lungs. Its classification phase is built from a hybrid convolutional neural network (CNN) architecture using an abstractly-designed CNN (named HMB1-COVID19) and transfer learning (TL) pre-trained models (VGG16, VGG19, ResNet50, ResNet101, Xception, DenseNet121, DenseNet169, MobileNet, and MobileNetV2). The hybrid CNN architecture is used for learning, classification, and parameters optimization while GA is used to optimize the hyperparameters. This hybrid working mechanism is combined in an overall algorithm named HMB-DLGA. The study experiments implemented the WS approach to evaluate the models' performance using the loss, accuracy, F1-score, precision, recall, and area under curve (AUC) metrics with different pre-defined ratios. A collected, combined, and unified X-Ray dataset from 8 different public datasets was used alongside the regularization, dropout, and data augmentation techniques to limit the overall overfitting. The applied experiments reported state-of-the-art metrics. VGG16 reported 100% WS metric (i.e., 0.0097, 99.78%, 0.9984, 99.89%, 99.78%, and 0.9996 for the loss, accuracy, F1, precision, recall, and AUC respectively) concerning the highest WS. It also reported a 99.92% WS metric (i.e., 0.0099, 99.84%, 0.9984, 99.84%, 99.84%, and 0.9996 for the loss, accuracy, F1, precision, recall, and AUC respectively) concerning the last reported WS result. HMB-HCF was validated on 13 different public datasets to verify its generalization. The best-achieved metrics were compared with 13 related studies. These extensive experiments' target was the applicability verification and generalization.}
}
@article{ADESEMOWO2021102131,
title = {Towards a conceptual definition for IT assets through interrogating their nature and epistemic uncertainty},
journal = {Computers & Security},
volume = {105},
pages = {102131},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.102131},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820304041},
author = {A. Kayode Adesemowo},
keywords = {Risk, Epistemic uncertainty, Tangibility, Critical realism, Digital transformation, Information assets, Conceptual definition, IT assets},
abstract = {Security breaches and consequentially reputational risk are upon us on an almost daily basis. They are part of the risk that beset organizations as they innovate and derive value from their information technology (IT) investments. IT assets that must be identified and safeguarded, and values extracted from them are contributors to the risk. Literature is awash with models of threats to assets and use of mid-range theories. There is a growing literature on digital technologies within digital transformation. However, intrinsic nature and epistemic uncertainty of IT assets have not received attention. Therefore, how can a conceptual definition for IT assets flow from understanding their nature, given their inherent epistemic uncertainty? Drawing from critical realism principles, this paper investigates existing definitions for- and re-interrogates the tangible nature (tangibility) of IT assets. It was found that despite their ubiquity and due to their evolving nature, IT assets lack universal understanding and definition. Executives (and professionals) views about IT assets are informed by their industry sector and role (present and/or previous). Consequently, this paper recommends a conceptual definition of IT assets that would assist; with coherence in the asset identification stage of risk assessment, (possibly IT audits and IT valuation programs), with understanding of the intrinsic nature of IT assets in themselves, and aid organizations in holistically engaging with IT assets. This paper's IT assets definition contributes to the call for theorizing the “IT” in IT. It poses new probing questions about ‘old’, ‘established’ beliefs as IT assets evolve in the era of digital transformation, fourth industrial revolution, knowledge economy and beyond.}
}
@article{SALINGAROS2020455,
title = {Connecting to the World: Christopher Alexander’s Tool for Human-Centered Design},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {6},
number = {4},
pages = {455-481},
year = {2020},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2020.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405872620300666},
author = {Nikos A. Salingaros},
keywords = {Design, Beauty, Architecture, Human-centered design, Connecting, Eye tracking},
abstract = {Beauty connects us viscerally to the material universe. Life forms evolved to experience biological connectedness as an absolute necessity for survival. Starting one century ago, however, dominant culture deliberately reversed the mechanism responsible for visceral connection. The resulting disconnection from the material world will continue to have long-lasting negative consequences for human well-being. Christopher Alexander describes how to revive the visceral connecting process, creating conditions for human-centered design in our times. Biological connectedness arises from an organic projection of the designer’s “self” onto the material reality of the object being designed, and to its physical context. Exploring multiple scenarios using informational feedback avoids letting the designer’s ego or imposed images exert a controlling influence. Implementing Alexander’s connecting method could revolutionize design, with the potential to produce a new, nourishing art and architecture. Recent developments in biophilia and neuro-design help to better understand Alexander’s ideas, using results not available at the time he was developing his theory.}
}
@article{AZURI2021878,
title = {The role of convolutional neural networks in scanning probe microscopy: a review},
journal = {Beilstein Journal of Nanotechnology},
volume = {12},
pages = {878-901},
year = {2021},
issn = {2190-4286},
doi = {https://doi.org/10.3762/bjnano.12.66},
url = {https://www.sciencedirect.com/science/article/pii/S2190428621000897},
author = {Ido Azuri and Irit Rosenhek-Goldian and Neta Regev-Rudzki and Georg Fantner and Sidney R Cohen},
keywords = {atomic force microscopy (AFM), deep learning, machine learning, neural networks, scanning probe microscopy (SPM)},
abstract = {Progress in computing capabilities has enhanced science in many ways. In recent years, various branches of machine learning have been the key facilitators in forging new paths, ranging from categorizing big data to instrumental control, from materials design through image analysis. Deep learning has the ability to identify abstract characteristics embedded within a data set, subsequently using that association to categorize, identify, and isolate subsets of the data. Scanning probe microscopy measures multimodal surface properties, combining morphology with electronic, mechanical, and other characteristics. In this review, we focus on a subset of deep learning algorithms, that is, convolutional neural networks, and how it is transforming the acquisition and analysis of scanning probe data.}
}
@article{FOXE2020223,
title = {Visuospatial short-term and working memory disturbance in the primary progressive aphasias: Neuroanatomical and clinical implications},
journal = {Cortex},
volume = {132},
pages = {223-237},
year = {2020},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2020.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S001094522030321X},
author = {David Foxe and Muireann Irish and Daniel Roquet and Angela Scharfenberg and Nathan Bradshaw and John R. Hodges and James R. Burrell and Olivier Piguet},
keywords = {Visuospatial, Working memory, Spatial span, Primary progressive aphasia, Alzheimer's disease},
abstract = {Introduction
Primary progressive aphasia (PPA) comprises three main variants: logopenic (lv-PPA), non-fluent (nfv-PPA) and semantic variant (sv-PPA). Differentiating the language profiles of the PPA variants remains challenging, especially for lv-PPA and nfv-PPA. As such, diagnostic tools that do not rely on speech and language may offer some utility. Here, we investigated the short-term and working memory profiles of the PPA variants and typical Alzheimer's disease (AD), with a particular interest in the visuospatial system. We hypothesised visuospatial short-term and working memory would be more compromised in lv-PPA and AD than in the other PPA variants, and that this would relate to degeneration of posterior temporoparietal brain regions.
Method
Thirty-three lv-PPA, 26 nfv-PPA, 31 sv-PPA and 58 AD patients, and 45 matched healthy controls were recruited. All participants completed the WMS-III Spatial and Digit Span tasks and underwent a structural brain MRI for voxel-based morphometry analyses.
Results
Relative to Controls, Spatial Span Forward (SSF) performance was impaired in lv-PPA and AD but not in nfv-PPA or sv-PPA. In contrast, Digit Span Forward (DSF) performance was impaired in lv-PPA and nfv-PPA (to a similar level), and AD, but was relatively intact in sv-PPA. As expected, most backward span scores across both modalities were lower than forward span scores. Neuroimaging analyses revealed that SSF and SSB performance in all patients combined correlated with grey matter intensity decrease in several clusters located in temporo-parieto-occipital brain regions. Post-hoc group comparisons of these regions showed that grey matter loss was more extensive in the lv-PPA and AD groups than in the nfv-PPA and sv-PPA groups.
Conclusions
The findings suggest that the visuospatial short-term and working memory profiles of the PPA variants are separable and likely reflect their distinct patterns of temporo-parieto-occipital brain atrophy.}
}
@article{RUSSO2020104869,
title = {Active learning for anomaly detection in environmental data},
journal = {Environmental Modelling & Software},
volume = {134},
pages = {104869},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104869},
url = {https://www.sciencedirect.com/science/article/pii/S1364815220309269},
author = {Stefania Russo and Moritz Lürig and Wenjin Hao and Blake Matthews and Kris Villez},
keywords = {Active learning, Anomaly detection, Machine learning, Environmental monitoring},
abstract = {Due to the growing amount of data from in-situ sensors in environmental monitoring, it becomes necessary to automatically detect anomalous data points. Nowadays, this is mainly performed using supervised machine learning models, which need a fully labelled data set for their training process. However, the process of labelling data is typically cumbersome and, as a result, a hindrance to the adoption of machine learning methods for automated anomaly detection. In this work, we propose to address this challenge by means of active learning. This method consists of querying the domain expert for the labels of only a selected subset of the full data set. We show that this reduces the time and costs associated to labelling while delivering the same or similar anomaly detection performances. Finally, we also show that machine learning models providing a nonlinear classification boundary are to be recommended for anomaly detection in complex environmental data sets.}
}
@article{WANG2022103945,
title = {Visual detection and tracking algorithms for minimally invasive surgical instruments: A comprehensive review of the state-of-the-art},
journal = {Robotics and Autonomous Systems},
volume = {149},
pages = {103945},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103945},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021002232},
author = {Yan Wang and Qiyuan Sun and Zhenzhong Liu and Lin Gu},
keywords = {Surgical robots, Machine vision, Detection and tracking of surgical instruments, Deep learning, Feature extraction},
abstract = {Minimally invasive surgical instrument visual detection and tracking is one of the core algorithms of minimally invasive surgical robots. With the development of machine vision and robotics, related technologies such as virtual reality, three-dimensional reconstruction, path planning, and human–machine collaboration can be applied to surgical operations to assist clinicians or use surgical robots to complete clinical operations. The minimally invasive surgical instrument vision detection and tracking algorithm analyzes the image transmitted by the surgical robot endoscope, extracting the position of the surgical instrument tip in the image, so as to provide the surgical navigation. This technology can greatly improve the accuracy and success rate of surgical operations. The purpose of this paper is to further study the visual detection and tracking technology of minimally invasive surgical instruments, summarize the existing research results, and apply it to the surgical robot project. By reading the literature, the author summarized the theoretical basis and related algorithms of this technology in recent years. Finally, the author compares the accuracy, speed and application scenario of each algorithm, and analyzes the advantages and disadvantages of each algorithm. The papers included in the review were selected through Web of Science, Google Scholar, PubMed and CNKI searches using the keywords: “object detection”, “object tracking”, “surgical tool detection”, “surgical tool tracking”, “surgical instrument detection” and “surgical instrument tracking” limiting results to the year range 1985–2021. Our study shows that this technology will have a great development prospect in the aspects of accuracy and real-time improvement in the future.}
}
@article{SHIYAMSUNDAR20214,
title = {Potentials and caveats of AI in hybrid imaging},
journal = {Methods},
volume = {188},
pages = {4-19},
year = {2021},
note = {Artificial Intelligence Approaches for Imaging Biomarkers},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2020.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1046202320302188},
author = {Lalith Kumar {Shiyam Sundar} and Otto Muzik and Irène Buvat and Luc Bidaut and Thomas Beyer},
keywords = {Hybrid imaging, Artificial intelligence, Deep learning, Machine learning, Radiomics},
abstract = {State-of-the-art patient management frequently mandates the investigation of both anatomy and physiology of the patients. Hybrid imaging modalities such as the PET/MRI, PET/CT and SPECT/CT have the ability to provide both structural and functional information of the investigated tissues in a single examination. With the introduction of such advanced hardware fusion, new problems arise such as the exceedingly large amount of multi-modality data that requires novel approaches of how to extract a maximum of clinical information from large sets of multi-dimensional imaging data. Artificial intelligence (AI) has emerged as one of the leading technologies that has shown promise in facilitating highly integrative analysis of multi-parametric data. Specifically, the usefulness of AI algorithms in the medical imaging field has been heavily investigated in the realms of (1) image acquisition and reconstruction, (2) post-processing and (3) data mining and modelling. Here, we aim to provide an overview of the challenges encountered in hybrid imaging and discuss how AI algorithms can facilitate potential solutions. In addition, we highlight the pitfalls and challenges in using advanced AI algorithms in the context of hybrid imaging and provide suggestions for building robust AI solutions that enable reproducible and transparent research.}
}
@incollection{COHEN202177,
title = {Chapter 5 - Dealing with data: strategies of preprocessing data},
editor = {Stanley Cohen},
booktitle = {Artificial Intelligence and Deep Learning in Pathology},
publisher = {Elsevier},
pages = {77-92},
year = {2021},
isbn = {978-0-323-67538-3},
doi = {https://doi.org/10.1016/B978-0-323-67538-3.00005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323675383000051},
author = {Stanley Cohen},
keywords = {Bias, Data preprocessing, Dimensional reduction, Eigenvalues, Eigenvectors, Feature engineering, Feature extraction, Manifolds, Principal component analysis},
abstract = {A machine learning model is only as good as the data it is given to work on. Data may have redundancies, missing values, artifactual outliers, irrelevant features (noise), and so on. In addition, the feature vectors that characterize data are usually of high dimension (each sample has a very large number of features or attributes). For a variety of technical reasons, this may also compromise performance. Direct observation of the data, in addition to cleaning up the data, may lead to removal of some of the unimportant or compromised features, thus reducing dimensionality as well. In addition, there are a number of mathematical techniques by which machine learning algorithms can reduce dimensionality in an automatic and unsupervised manner. These considerations are an important part of the armamentarium of every data scientist and machine learning specialist and must be understood by the pathologists partnering with them to ensure the clinical applicability and validity of the final result. Of equal importance is the need to avoid bias from contamination or limitations of the dataset. In addition to ethical and moral issues, such bias can lead to incorrect conclusions from that data.}
}
@article{BOTTA2021228,
title = {NeuNAC: A novel fragile watermarking algorithm for integrity protection of neural networks},
journal = {Information Sciences},
volume = {576},
pages = {228-241},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.06.073},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521006642},
author = {Marco Botta and Davide Cavagnino and Roberto Esposito},
keywords = {Deep neural network, Fragile watermarking, Integrity protection, Linear transformation},
abstract = {The last decade has witnessed a massive deployment of Machine Learning tools in everyday life automated tasks. Neural Networks are nowadays in use in a growing number of application areas because of their excellent performances. Unfortunately, it has been shown by many researchers that they can be attacked and fooled in several different ways, and this can dangerously impair their ability to correctly perform their tasks. In this paper we describe a watermarking algorithm that can protect and verify the integrity of (Deep) Neural Networks when deployed in safety critical systems, such as autonomous driving systems or monitoring and surveillance systems.}
}
@incollection{ZOHURI2022837,
title = {Chapter 27 - Artificial intelligence, machine learning, and deep learning driving big data},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {837-888},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00027-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000271},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Artificial intelligence, Deep learning, Internet of things, Machine learning},
abstract = {Artificial Intelligence (AI) is one of those technologies that seem to be expanding outward in every direction, and this expansion is driven by sheer volume of data that we are encountering in our daily routine life in these days, where technology is deviating from its tradition format to the more modern world of electronic gadget. In today's modern technological world, we are accumulating so much data in real time with speed of electron through the world of Internet of Things (IoT), and it comes to us in Omni-Direction space. Dealing with these data in the form of structured and unstructured, we have no choice except to turn to AI for assistant, in order to stay on top of all the information that is collected from these data for us to have a knowledge that allows us to consequently have power of right and accurate decision-making in real time in our daily routine as stakeholder. However, comes with AI needs its two other integrated components, namely Machine Learning (ML) and Deep Learning (DL) to be more effective to us as human being, where we are relying on Artificial Intelligence to run our day-to-day operation in very resilience form.}
}
@article{FAVORSKAYA20211504,
title = {Semantic segmentation of multispectral satellite images for land use analysis based on embedded information},
journal = {Procedia Computer Science},
volume = {192},
pages = {1504-1513},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.154},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921016483},
author = {Margarita N. Favorskaya and Alexandr G. Zotin},
keywords = {Semantic segmentation, multispectral satellite image, texture recognition, watermarking schemes, inpainting},
abstract = {Semantic segmentation of satellite and aerial imageries has many applications including automated map making, land use analysis, urban planning and so on. This paper presents a special type of semantic segmentation. First, the boundaries of natural objects such as agricultural fields are detected and approximated by polygons. Second, texture recognition based on Digital Wavelet Transform (DWT) and Local Binary Patterns (LBPs) is implemented using a limited textural dictionary. The obtained information is embedded using DWT and Arnold’s transform. Such watermarked images can be publicly available, but semantic information after extraction and inpainting is provided only to the authorized users. Such semantic labelling is very useful in land use analysis of rural territories.}
}
@article{SUN2021103548,
title = {Generativity and the paradox of stability and flexibility in a platform architecture: A case of the Oracle Cloud Platform},
journal = {Information & Management},
volume = {58},
number = {8},
pages = {103548},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2021.103548},
url = {https://www.sciencedirect.com/science/article/pii/S0378720621001221},
author = {Ruonan Sun and Shirley Gregor and Erwin Fielt},
keywords = {Platform, Generativity, Architecture, Paradox, Stability, Flexibility, Oracle Cloud},
abstract = {Generativity is a technology’s capability of producing new outputs without input from the originator. Platforms are important technologies that embrace generativity. While the literature generally assumes that generativity arises from platform governance and high-level platform design, we propose that generativity also arises from a platform’s three architectural components: the base, the interface, and the add-ons. Drawing on a case study of the Oracle Cloud Platform, we reveal how generativity emerges through the paradox of stability and flexibility in a platform’s architectural components. Further, standardization navigates this paradox by coordinating the dependencies between stability and flexibility across heterogeneous stakeholders.}
}
@article{ALMAHMOUD2020113598,
title = {A modified bond energy algorithm with fuzzy merging and its application to Arabic text document clustering},
journal = {Expert Systems with Applications},
volume = {159},
pages = {113598},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113598},
url = {https://www.sciencedirect.com/science/article/pii/S095741742030422X},
author = {Rana Husni AlMahmoud and Bassam Hammo and Hossam Faris},
keywords = {Bond energy algorithm, Arabic text document clustering, Fuzzy Merging},
abstract = {Conventional textual documents clustering algorithms suffer from several shortcomings, such as the slow convergence of the immense high-dimensional data, the sensitivity to the initial value, and the understandability of the description of the resulted clusters. Although many clustering algorithms have been developed for English and other languages, very few have tackled the problem of clustering the under-resourced Arabic language. In this work, we propose a modified version of the Bond Energy Algorithm (BEA) combined with a fuzzy merging technique to solve the problem of Arabic text document clustering. The proposed algorithm, Clustering Arabic Documents based on Bond Energy, hereafter named CADBE, attempts to identify and display natural variable clusters within huge sized data. CADBE has three steps to cluster Arabic documents: the first step instantiates a cluster affinity matrix using the BEA, the second step uses a new and novel method to partition the cluster matrix automatically into small coherent clusters, and the last step uses a fuzzy merging technique to merge similar clusters based on the associations and interrelations between the resulted clusters. Experimental results showed that the proposed algorithm effectively outperformed the conventional clustering algorithms such as Expectation–Maximization (EM), Single Linkage, and UPGMA in terms of clustering purity and entropy. It also outperformed k-means, k-means++, spherical k-means, and CoclusMod in most test cases. However, there are several merits of CADBE. First, unlike the traditional clustering algorithms, it does not require to specify the number of clusters. In addition, it produces clusters with distinct boundaries, which makes its results more objective, and finally it is deterministic, such that it is insensitive to the order in which documents are presented to the algorithm.}
}
@article{YUAN2021102221,
title = {Deep learning for insider threat detection: Review, challenges and opportunities},
journal = {Computers & Security},
volume = {104},
pages = {102221},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102221},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821000456},
author = {Shuhan Yuan and Xintao Wu},
keywords = {Deep learning, Insider threats, Insiders, Cybersecurity},
abstract = {Insider threats, as one type of the most challenging threats in cyberspace, usually cause significant loss to organizations. While the problem of insider threat detection has been studied for a long time in both security and data mining communities, the traditional machine learning based detection approaches, which heavily rely on feature engineering, are hard to accurately capture the behavior difference between insiders and normal users due to various challenges related to the characteristics of underlying data, such as high-dimensionality, complexity, heterogeneity, sparsity, lack of labeled insider threats, and the subtle and adaptive nature of insider threats. Advanced deep learning techniques provide a new paradigm to learn end-to-end models from complex data. In this brief survey, we first introduce commonly-used datasets for insider threat detection and review the recent literature about deep learning for such research. The existing studies show that compared with traditional machine learning algorithms, deep learning models can improve the performance of insider threat detection. However, applying deep learning to further advance the insider threat detection task still faces several limitations, such as lack of labeled data, adaptive attacks. We discuss such challenges and suggest future research directions that have the potential to address challenges and further boost the performance of deep learning for insider threat detection.}
}
@article{SHARMA2021102316,
title = {Fifty years of information management research: A conceptual structure analysis using structural topic modeling},
journal = {International Journal of Information Management},
volume = {58},
pages = {102316},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102316},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221000098},
author = {Anuj Sharma and Nripendra P. Rana and Robin Nunkoo},
keywords = {Information management, Structural topic models, Topic modeling, Generative models, Text analytics},
abstract = {Information management is the management of organizational processes, technologies, and people which collectively create, acquire, integrate, organize, process, store, disseminate, access, and dispose of the information. Information management is a vast, multi-disciplinary domain that syndicates various subdomains and perfectly intermingles with other domains. This study aims to provide a comprehensive overview of the information management domain from 1970 to 2019. Drawing upon the methodology from statistical text analysis research, this study summarizes the evolution of knowledge in this domain by examining the publication trends as per authors, institutions, countries, etc. Further, this study proposes a probabilistic generative model based on structural topic modeling to understand and extract the latent themes from the research articles related to information management. Furthermore, this study graphically visualizes the variations in the topic prevalences over the period of 1970 to 2019. The results highlight that the most common themes are data management, knowledge management, environmental management, project management, service management, and mobile and web management. The findings also identify themes such as knowledge management, environmental management, project management, and social communication as academic hotspots for future research.}
}
@article{SEGUNDOMARCOS2020100663,
title = {Promoting children’s creative thinking through reading and writing in a cooperative learning classroom},
journal = {Thinking Skills and Creativity},
volume = {36},
pages = {100663},
year = {2020},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2020.100663},
url = {https://www.sciencedirect.com/science/article/pii/S187118711930166X},
author = {Rafael Ibán {Segundo Marcos} and Verónica {López Fernández} and María Teresa {Daza González} and Jessica Phillips-Silver},
keywords = {Creative thinking, Divergent thinking, Cooperative learning, Academic achievement, Literacy},
abstract = {The objectives of the present study were to investigate whether students' creative thinking can be enhanced through a structured program of reading and writing activities in the context of a cooperative learning classroom, and to test for a possible correlation between improvements in creative thinking and improvements in academic performance. Sixty fifth-grade students from a primary school in the south of Spain participated over two months: half received reading and writing activities in a cooperative learning classroom (experimental group, n=30), and half received the standard fifth-grade reading and writing program (control group, n=30). Creative thinking was assessed through a divergent thinking task (CREA Test; Corbalán et al., 2003), and Grade Point Average (GPA) was used as index of academic achievement. The results revealed a significant increase in creativity scores in the experimental group as compared with the control, and a moderate positive correlation between creative thinking and academic achievement. The present findings are consistent with the idea that creative thinking (divergent thinking) can be enhanced with reading and writing activities implemented through cooperative learning in school-age children.}
}
@article{QIN2022101427,
title = {3D CAD model retrieval based on sketch and unsupervised variational autoencoder},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101427},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101427},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001798},
author = {Feiwei Qin and Shi Qiu and Shuming Gao and Jing Bai},
keywords = {Model retrieval, Structural semantics, Sketch, Unsupervised learning, Deep learning},
abstract = {How to quickly, accurately retrieve and effectively reuse 3D CAD models that conform to user’s design intention has become an urgent problem in product design. However, there are several problems with the existing retrieval methods, like not being fast, or accurate, or hard to use. Hence it is difficult to meet the actual needs of the industry. In this paper, we propose a 3D CAD model retrieval approach that considers the speed, accuracy and ease of use at the same time, based on sketches and unsupervised learning. Firstly, the loop is used as the fundamental element of sketch/view, and automatic structural semantics capture algorithms are proposed to extract and construct attributed loop relation tree; Secondly, the recursive neural network based deep variational autoencoders is constructed and optimized to transform arbitrary shapes and sizes of loop relation tree into fixed length descriptor; Finally, based on the fixed length vector descriptor, the sketches and views of 3D CAD models are embedded into the same target feature space, and k-nearest neighbors algorithm is adopted to conduct fast CAD model matching on the feature space. In this manner, a prototype 3D CAD model retrieval system is developed. Experiments on the dataset containing about two thousand 3D CAD models validate the feasibility and effectiveness of the proposed approach.}
}
@article{DELGOSHA2022102236,
title = {Discovering IoT implications in business and management: A computational thematic analysis},
journal = {Technovation},
volume = {118},
pages = {102236},
year = {2022},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2021.102236},
url = {https://www.sciencedirect.com/science/article/pii/S0166497221000171},
author = {Mohammad Soltani Delgosha and Nastaran Hajiheydari and Mojtaba Talafidaryani},
keywords = {Internet of things, Topic modelling, Business model, Thematic analysis, Business and management, Future research},
abstract = {IoT as a disruptive technology is contributing toward ground-breaking experiences in contemporary enterprises and in our daily life. Rapidly changing business environment and phenomenally evolving technology enhancement necessitate a robust understanding of IoT implications from business and management perspective. The current study benefits from an explanatory sequential mixed-method approach to represent and interpret the inductive topical framework of IoT literature in business and management with emphasis on business model. Bayesian statistical topic model called latent Dirichlet allocation is employed to conduct a comprehensive analysis of 347 related scholarly articles to reveal the topical composition of related research. Further, we followed a thematic analysis for interpreting the extracted topics and gaining in-depth qualitative insights. Theoretical implications with emphasizing on research agenda for future study avenues and managerial implications based on influential themes are provided.}
}
@article{RUSSO2021117695,
title = {The value of human data annotation for machine learning based anomaly detection in environmental systems},
journal = {Water Research},
volume = {206},
pages = {117695},
year = {2021},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2021.117695},
url = {https://www.sciencedirect.com/science/article/pii/S0043135421008897},
author = {Stefania Russo and Michael D. Besmer and Frank Blumensaat and Damien Bouffard and Andy Disch and Frederik Hammes and Angelika Hess and Moritz Lürig and Blake Matthews and Camille Minaudo and Eberhard Morgenroth and Viet Tran-Khac and Kris Villez},
keywords = {Machine learning, Anomaly detection, Environmental systems, Labels},
abstract = {Anomaly detection is the process of identifying unexpected data samples in datasets. Automated anomaly detection is either performed using supervised machine learning models, which require a labelled dataset for their calibration, or unsupervised models, which do not require labels. While academic research has produced a vast array of tools and machine learning models for automated anomaly detection, the research community focused on environmental systems still lacks a comparative analysis that is simultaneously comprehensive, objective, and systematic. This knowledge gap is addressed for the first time in this study, where 15 different supervised and unsupervised anomaly detection models are evaluated on 5 different environmental datasets from engineered and natural aquatic systems. To this end, anomaly detection performance, labelling efforts, as well as the impact of model and algorithm tuning are taken into account. As a result, our analysis reveals the relative strengths and weaknesses of the different approaches in an objective manner without bias for any particular paradigm in machine learning. Most importantly, our results show that expert-based data annotation is extremely valuable for anomaly detection based on machine learning.}
}
@article{BOVEIRI2020106767,
title = {Medical image registration using deep neural networks: A comprehensive review},
journal = {Computers & Electrical Engineering},
volume = {87},
pages = {106767},
year = {2020},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106767},
url = {https://www.sciencedirect.com/science/article/pii/S0045790620306224},
author = {Hamid Reza Boveiri and Raouf Khayami and Reza Javidan and Alireza Mehdizadeh},
keywords = {Convolutional neural network (CNN), Deep learning, Deep reinforcement learning, Deformable registration, Generative adversarial network (GAN), Image-guided intervention, Medical image registration, One-shot registration, Precision medicine, Stacked auto-encoders (SAEs)},
abstract = {Image-guided interventions are saving the lives of a large number of patients where the image registration should indeed be considered as the most complex and complicated issue to be tackled. On the other hand, a huge progress in the field of machine learning has recently made by the possibility of implementing deep neural networks on the contemporary many-core GPUs. It has opened up a promising window to challenge with many medical applications in more efficient and effective ways, where the registration is not an exception. In this paper, a comprehensive review on the state-of-the-art literature known as medical image registration using deep neural networks is presented. The review is systematic and encompasses all the related works previously published in the field. Key concepts, statistical analysis from different points of view, confining challenges, novelties and main contributions, key-enabling techniques, future directions, and prospective trends all are discussed and surveyed in details in this comprehensive review. This review allows a deep understanding and insight for the readers active in the field who are investigating the state-of-the-art and seeking to contribute the future literature.}
}
@article{LI2021171,
title = {A survey of Deep Neural Network watermarking techniques},
journal = {Neurocomputing},
volume = {461},
pages = {171-193},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.07.051},
url = {https://www.sciencedirect.com/science/article/pii/S092523122101095X},
author = {Yue Li and Hongxia Wang and Mauro Barni},
keywords = {Intellectual property protection, Deep Neural Networks, Watermarking, White box vs black box watermarking, Watermarking and DNN backdoors},
abstract = {Protecting the Intellectual Property Rights (IPR) associated to Deep Neural Networks (DNNs) is a pressing need pushed by the high costs required to train such networks and by the importance that DNNs are gaining in our society. Following its use for Multimedia (MM) IPR protection, digital watermarking has recently been considered as a mean to protect the IPR of DNNs. While DNN watermarking inherits some basic concepts and methods from MM watermarking, there are significant differences between the two application areas, thus calling for the adaptation of media watermarking techniques to the DNN scenario and the development of completely new methods. In this paper, we overview the most recent advances in DNN watermarking, by paying attention to cast them into the bulk of watermarking theory developed during the last two decades, while at the same time highlighting the new challenges and opportunities characterising DNN watermarking. Rather than trying to present a comprehensive description of all the methods proposed so far, we introduce a new taxonomy of DNN watermarking and present a few exemplary methods belonging to each class. We hope that this paper will inspire new research in this exciting area and will help researchers to focus on the most innovative and challenging problems in the field.}
}
@article{LYKOUSAS2021114808,
title = {Large-scale analysis of grooming in modern social networks},
journal = {Expert Systems with Applications},
volume = {176},
pages = {114808},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114808},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002499},
author = {Nikolaos Lykousas and Constantinos Patsakis},
keywords = {Online grooming, Social networks, LDA, Text analysis, Emoji},
abstract = {Social networks are evolving to engage their users more by providing them with more functionalities. One of the most attracting ones is streaming. Users may broadcast part of their daily lives to thousands of others world-wide and interact with them in real-time. Unfortunately, this feature is reportedly exploited for grooming. In this work, we provide the first in-depth analysis of this problem for social live streaming services. More precisely, using a dataset that we collected, we identify predatory behaviours and grooming on chats that bypassed the moderation mechanisms of the LiveMe, the service under investigation. Beyond the traditional text approaches, we also investigate the relevance of emojis in this context, as well as the user interactions through the gift mechanisms of LiveMe. Finally, our analysis indicates the possibility of grooming towards minors, showing the extent of the problem in such platforms.}
}
@article{TRAPPEY2021120511,
title = {An intelligent patent recommender adopting machine learning approach for natural language processing: A case study for smart machinery technology mining},
journal = {Technological Forecasting and Social Change},
volume = {164},
pages = {120511},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120511},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520313378},
author = {Amy Trappey and Charles V. Trappey and Alex Hsieh},
keywords = {Natural language processing, Patent recommendation, Word embedding, Technology mining and trend analysis},
abstract = {Recommendation systems are widely applied in many fields, such as online customized product searches and customer-centric advertisements. This research develops the methodology for a patent recommender to discover semantically relevant patents for further technology mining and trend analysis. The proposed recommender adopts machine learning (ML) algorithms for natural language processing (NLP) to represent patent documents in vector space and to enable semantic analyses of the patent documents. The ML approach of neural network (NN) language models, trained by domain patent documents (text) as a training set, convert patent documents into vectors and, thus, can identify semantically similar patents using document similarity measures. In particular, the proposed recommender is deployed to in-depth case studies for advanced patent recommendations. The case domain of smart machinery is used to better enable smart manufacturing by incorporating innovative technologies, such as intelligent sensors, intelligent controllers, and intelligent decision making. The research uses six sub-domains in smart machinery technologies as the case studies to verify the superior accuracy and efficacy of the recommender system and methodologies.}
}
@article{2020I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {13},
number = {9},
pages = {I-CCXI},
year = {2020},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(20)30692-6},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X20306926}
}
@article{PUTEAUX2021103085,
title = {A survey of reversible data hiding in encrypted images – The first 12 years},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103085},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103085},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100050X},
author = {Pauline Puteaux and SimYing Ong and KokSheik Wong and William Puech},
keywords = {Multimedia security, Image encryption, Data hiding, Signal processing in the encrypted domain},
abstract = {In the last few years, with the increasing popularity of cloud computing and the availability of mobile smart devices as well as ubiquitous network connections, more and more users are uploading their personal data to remote servers. However, this can lead to significant security breaches, where confidentiality, integrity and authentication are constantly threatened. To overcome these multiple problems, multimedia data must be secured, for example by means of encryption before transmission and storage. In this survey, we look into the issues involved in handling encrypted multimedia data, and more specifically we focus on reversible data hiding in encrypted images (RDHEI). The aim of this survey is to present the birth and evolution of RDHEI methods over the last 12 years. We first highlight different classes and characteristics of RDHEI, then describe representative RDHEI methods. A comparison table is presented to summarize the key features and achievements of each representative RDHEI method considered in this survey. Finally, we share the future outlook of emerging applications and open research topics relevant to RDHEI for the next 12 years and beyond.}
}
@article{LE202010378,
title = {Neuraldecipher – reverse-engineering extended-connectivity fingerprints (ECFPs) to their molecular structures††Electronic supplementary information (ESI) available: Detailed information regarding the model architectures and computation time, degeneracy analysis for the ECFPs as well as the information loss due to hash collision. See DOI: 10.1039/d0sc03115a},
journal = {Chemical Science},
volume = {11},
number = {38},
pages = {10378-10389},
year = {2020},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d0sc03115a},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023028882},
author = {Tuan Le and Robin Winter and Frank Noé and Djork-Arné Clevert},
abstract = {ABSTRACT
Protecting molecular structures from disclosure against external parties is of great relevance for industrial and private associations, such as pharmaceutical companies. Within the framework of external collaborations, it is common to exchange datasets by encoding the molecular structures into descriptors. Molecular fingerprints such as the extended-connectivity fingerprints (ECFPs) are frequently used for such an exchange, because they typically perform well on quantitative structure–activity relationship tasks. ECFPs are often considered to be non-invertible due to the way they are computed. In this paper, we present a fast reverse-engineering method to deduce the molecular structure given revealed ECFPs. Our method includes the Neuraldecipher, a neural network model that predicts a compact vector representation of compounds, given ECFPs. We then utilize another pre-trained model to retrieve the molecular structure as SMILES representation. We demonstrate that our method is able to reconstruct molecular structures to some extent, and improves, when ECFPs with larger fingerprint sizes are revealed. For example, given ECFP count vectors of length 4096, we are able to correctly deduce up to 69% of molecular structures on a validation set (112 K unique samples) with our method.}
}
@article{SENGUPTA20202017,
title = {Proposed Requirements for Cardiovascular Imaging-Related Machine Learning Evaluation (PRIME): A Checklist: Reviewed by the American College of Cardiology Healthcare Innovation Council},
journal = {JACC: Cardiovascular Imaging},
volume = {13},
number = {9},
pages = {2017-2035},
year = {2020},
issn = {1936-878X},
doi = {https://doi.org/10.1016/j.jcmg.2020.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X20306367},
author = {Partho P. Sengupta and Sirish Shrestha and Béatrice Berthon and Emmanuel Messas and Erwan Donal and Geoffrey H. Tison and James K. Min and Jan D’hooge and Jens-Uwe Voigt and Joel Dudley and Johan W. Verjans and Khader Shameer and Kipp Johnson and Lasse Lovstakken and Mahdi Tabassian and Marco Piccirilli and Mathieu Pernot and Naveena Yanamala and Nicolas Duchateau and Nobuyuki Kagiyama and Olivier Bernard and Piotr Slomka and Rahul Deo and Rima Arnaout},
keywords = {artificial intelligence, cardiovascular imaging, checklist, digital health, machine learning, reporting guidelines, reproducible research},
abstract = {Machine learning (ML) has been increasingly used within cardiology, particularly in the domain of cardiovascular imaging. Due to the inherent complexity and flexibility of ML algorithms, inconsistencies in the model performance and interpretation may occur. Several review articles have been recently published that introduce the fundamental principles and clinical application of ML for cardiologists. This paper builds on these introductory principles and outlines a more comprehensive list of crucial responsibilities that need to be completed when developing ML models. This paper aims to serve as a scientific foundation to aid investigators, data scientists, authors, editors, and reviewers involved in machine learning research with the intent of uniform reporting of ML investigations. An independent multidisciplinary panel of ML experts, clinicians, and statisticians worked together to review the theoretical rationale underlying 7 sets of requirements that may reduce algorithmic errors and biases. Finally, the paper summarizes a list of reporting items as an itemized checklist that highlights steps for ensuring correct application of ML models and the consistent reporting of model specifications and results. It is expected that the rapid pace of research and development and the increased availability of real-world evidence may require periodic updates to the checklist.}
}
@article{NYAMEKYE2021101040,
title = {Prospects for laser based powder bed fusion in the manufacturing of metal electrodes: A review},
journal = {Applied Materials Today},
volume = {23},
pages = {101040},
year = {2021},
issn = {2352-9407},
doi = {https://doi.org/10.1016/j.apmt.2021.101040},
url = {https://www.sciencedirect.com/science/article/pii/S2352940721001050},
author = {Patricia Nyamekye and Pinja Nieminen and Mohammad Reza Bilesan and Eveliina Repo and Heidi Piili and Antti Salminen},
keywords = {Electrochemical process, Electrode, Flow optimization, Gold recovery, Laser powder bed fusion},
abstract = {Additive manufacturing, (AM), includes seven subcategories that can directly manufacture components structures from a computer-designed model layer by layer. Laser-based powder bed fusion (L-PBF) is one of type of the subcategories of AM. L-PBF is a fast and cost-efficient production method that offers the advantages of being implementable with a diverse range of raw materials, possessing a high level of freedom in customization, and producing less waste. L-PBF can potentially enable the production of hierarchically complex shaped electrochemical separation units. This study examines the use of L-PBF for the fabrication of metal electrodes for electrochemical processes. The aim is to address a literature gap by presenting a state-of-the-art review of L-PBF electrodes used in electrochemical cells. The study investigates existing research on electrochemistry and identifies potential benefits from use of L-PBF metal electrodes. Electrochemical reactors in industry require electrodes with a large electrode/electrolyte interface that can hold electrolytes efficiently and reduce the diffusion path of electrons and ions on the active surface of electrode. Meeting these demands require electrodes with specific characteristics such as high surface area and improved mass transport. This review shows that L-PBF can manufacture optimized electrodes satisfying the requirements for electrochemical cells in industrial applications}
}
@article{GRIFFITHS2020401,
title = {How Can Hearing Loss Cause Dementia?},
journal = {Neuron},
volume = {108},
number = {3},
pages = {401-412},
year = {2020},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2020.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0896627320306103},
author = {Timothy D. Griffiths and Meher Lad and Sukhbinder Kumar and Emma Holmes and Bob McMurray and Eleanor A. Maguire and Alexander J. Billig and William Sedley},
keywords = {dementia, hearing loss, medial temporal lobe, Alzheimer disease, auditory cognition},
abstract = {Summary
Epidemiological studies identify midlife hearing loss as an independent risk factor for dementia, estimated to account for 9% of cases. We evaluate candidate brain bases for this relationship. These bases include a common pathology affecting the ascending auditory pathway and multimodal cortex, depletion of cognitive reserve due to an impoverished listening environment, and the occupation of cognitive resources when listening in difficult conditions. We also put forward an alternate mechanism, drawing on new insights into the role of the medial temporal lobe in auditory cognition. In particular, we consider how aberrant activity in the service of auditory pattern analysis, working memory, and object processing may interact with dementia pathology in people with hearing loss. We highlight how the effect of hearing interventions on dementia depends on the specific mechanism and suggest avenues for work at the molecular, neuronal, and systems levels to pin this down.}
}
@article{BHATTI2020110611,
title = {General framework, opportunities and challenges for crowdsourcing techniques: A Comprehensive survey},
journal = {Journal of Systems and Software},
volume = {167},
pages = {110611},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110611},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220300893},
author = {Shahzad Sarwar Bhatti and Xiaofeng Gao and Guihai Chen},
keywords = {Crowdsourcing, Framework, Incentives, Decomposition, Aggregation, Reputation},
abstract = {Crowdsourcing, a distributed human problem-solving paradigm is an active research area which has attracted significant attention in the fields of computer science, business, and information systems. Crowdsourcing holds novelty with advantages like open innovation, scalability, and cost-efficiency. Although considerable research work is performed, however, a survey on the crowdsourcing process-technology has not been divulged yet. In this paper, we present a systematic survey of crowdsourcing in focussing emerging techniques and approaches for improving conventional and developing future crowdsourcing systems. We first present a simplified definition of crowdsourcing. Then, we propose a framework based on three major components, synthesize a wide spectrum of existing studies for various dimensions of the framework. According to the framework, we first introduce the initialization step, including task design, task settings, and incentive mechanisms. Next, in the implementation step, we look into task decomposition, crowd and platform selection, and task assignment. In the last step, we discuss different answer aggregation techniques, validation methods and reward tactics, and reputation management. Finally, we identify open issues and suggest possible research directions for the future.}
}
@article{SCHMALZLE2022,
title = {Harnessing Artificial Intelligence for Health Message Generation: The Folic Acid Message Engine},
journal = {Journal of Medical Internet Research},
volume = {24},
number = {1},
year = {2022},
issn = {1438-8871},
doi = {https://doi.org/10.2196/28858},
url = {https://www.sciencedirect.com/science/article/pii/S1438887122000905},
author = {Ralf Schmälzle and Shelby Wilcox},
keywords = {human-centered AI, campaigns, health communication, NLP, health promotion},
abstract = {Background
Communication campaigns using social media can raise public awareness; however, they are difficult to sustain. A barrier is the need to generate and constantly post novel but on-topic messages, which creates a resource-intensive bottleneck.
Objective
In this study, we aim to harness the latest advances in artificial intelligence (AI) to build a pilot system that can generate many candidate messages, which could be used for a campaign to suggest novel, on-topic candidate messages. The issue of folic acid, a B-vitamin that helps prevent major birth defects, serves as an example; however, the system can work with other issues that could benefit from higher levels of public awareness.
Methods
We used the Generative Pretrained Transformer-2 architecture, a machine learning model trained on a large natural language corpus, and fine-tuned it using a data set of autodownloaded tweets about #folicacid. The fine-tuned model was then used as a message engine, that is, to create new messages about this topic. We conducted a web-based study to gauge how human raters evaluate AI-generated tweet messages compared with original, human-crafted messages.
Results
We found that the Folic Acid Message Engine can easily create several hundreds of new messages that appear natural to humans. Web-based raters evaluated the clarity and quality of a human-curated sample of AI-generated messages as on par with human-generated ones. Overall, these results showed that it is feasible to use such a message engine to suggest messages for web-based campaigns that focus on promoting awareness.
Conclusions
The message engine can serve as a starting point for more sophisticated AI-guided message creation systems for health communication. Beyond the practical potential of such systems for campaigns in the age of social media, they also hold great scientific potential for the quantitative analysis of message characteristics that promote successful communication. We discuss future developments and obvious ethical challenges that need to be addressed as AI technologies for health persuasion enter the stage.}
}
@article{HASSANALY2021100955,
title = {Classification and computation of extreme events in turbulent combustion},
journal = {Progress in Energy and Combustion Science},
volume = {87},
pages = {100955},
year = {2021},
issn = {0360-1285},
doi = {https://doi.org/10.1016/j.pecs.2021.100955},
url = {https://www.sciencedirect.com/science/article/pii/S0360128521000538},
author = {Malik Hassanaly and Venkat Raman},
keywords = {Data-poor problems, Extreme events, Rare events},
abstract = {In the design of practical combustion systems, ensuring safety and reliability is an important requirement. For instance, reliably avoiding lean blowout, flame flashback or inlet unstart is critical for ensuring safe operation. Currently, the science of predicting such events is based on prior experience, limited modeling or diagnostic tools and purely statistical approaches. Even though computational and experimental tools for studying combustion devices have vastly advanced in the last three decades, the analysis of such failure events has not been pursued widely. While the use of data for model development and calibration is being widely accepted, the extension to failure events introduces numerous challenges. In particular, the focus here is on so-called data-poor problems, where the cost of generating data is extremely high and is not easily amenable to existing computational and experimental approaches. Data-poor problems are particularly relevant when related to extreme events (also called anomalous events) that can lead to catastrophic failure of the system. It is argued that transient events that describe such failure can have different causal mechanisms. To develop the scientific inference process, a classification of such problems is used to determine specific modeling paths as well as computational tools needed. Research opportunities in the emerging field of extreme event prediction are highlighted in order to identify critical and immediate needs.}
}
@article{SONY2021111347,
title = {A systematic review of convolutional neural network-based structural condition assessment techniques},
journal = {Engineering Structures},
volume = {226},
pages = {111347},
year = {2021},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2020.111347},
url = {https://www.sciencedirect.com/science/article/pii/S0141029620339481},
author = {Sandeep Sony and Kyle Dunphy and Ayan Sadhu and Miriam Capretz},
keywords = {Structural health monitoring, Artificial intelligence, Deep learning, CNN, Damage detection, Anomaly detection, Structural condition assessment},
abstract = {With recent advances in non-contact sensing technology such as cameras, unmanned aerial and ground vehicles, the structural health monitoring (SHM) community has witnessed a prominent growth in deep learning-based condition assessment techniques of structural systems. These deep learning methods rely primarily on convolutional neural networks (CNNs). The CNN networks are trained using a large number of datasets for various types of damage and anomaly detection and post-disaster reconnaissance. The trained networks are then utilized to analyze newer data to detect the type and severity of the damage, enhancing the capabilities of non-contact sensors in developing autonomous SHM systems. In recent years, a broad range of CNN architectures has been developed by researchers to accommodate the extent of lighting and weather conditions, the quality of images, the amount of background and foreground noise, and multiclass damage in the structures. This paper presents a detailed literature review of existing CNN-based techniques in the context of infrastructure monitoring and maintenance. The review is categorized into multiple classes depending on the specific application and development of CNNs applied to data obtained from a wide range of structures. The challenges and limitations of the existing literature are discussed in detail at the end, followed by a brief conclusion on potential future research directions of CNN in structural condition assessment.}
}
@article{YUN2020106636,
title = {Automated classification of patents: A topic modeling approach},
journal = {Computers & Industrial Engineering},
volume = {147},
pages = {106636},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106636},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220303703},
author = {Junghwan Yun and Youngjung Geum},
keywords = {Automatic patent classification, Latent Dirichlet allocation, LDA, Support vector machine, SVM},
abstract = {Due to the rapid increase in technological innovation and corresponding increase in patent applications, automatic patent classification systems are very helpful for both individual inventors and patent attorneys in classifying patents. However, previous studies have neglected the question of what content patents include and how to represent patent content effectively in a structured form to predict the patent class. In response, this study suggests a topic model based on support vector machine (SVM) prediction for automatic patent classification. This study considers two important issues for patent classification: text representation and class prediction. For text representation, we use the topic modeling technique and employ latent Dirichlet allocation (LDA). The result of LDA is then used as the input for the second aspect: class prediction. We use SVM prediction for automatic patent classification. We also suggest potential improvement strategies to enhance the prediction performance of our suggested approach. This study contributes to the field in that it can lead to the automatic classification of patents without the need for any expert judgment during the process.}
}
@incollection{BERMAN2022251,
title = {6 - The classification of life},
editor = {Jules J. Berman},
booktitle = {Classification Made Relevant},
publisher = {Academic Press},
pages = {251-341},
year = {2022},
isbn = {978-0-323-91786-5},
doi = {https://doi.org/10.1016/B978-0-323-91786-5.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917865000082},
author = {Jules J. Berman},
keywords = {Classification of living organisms, Tree of life, Periodic table of the elements, Phylogeny, Lineage, Apomorphy, Sister class},
abstract = {The Classification of All Living Organisms (also known as the Tree of Life and herein called the Classification of Life) is the most comprehensive, the most tested, the oldest, the best curated, and the most prized scientific document ever produced. In production for over two millennia, thousands of scientists have devoted their careers to this singular work. The Classification of Life does not impose order upon the natural world; it encapsulates the existing order and, in so doing, reveals to us the relationships among all forms of life, extant or extinct. Not surprisingly, the Classification of Life has become the touchstone for all the biological sciences, unifying the fields of zoology, botany, microbiology, embryology, anatomy, paleontology, geography, genetics, bioinformatics, and evolution. Nothing that we think we know about biological systems can be validated with any degree of confidence without first asking whether our conclusions are consistent with our current understanding of the Classification of Life. This chapter is devoted to examining the Classification of Life. By simple observations of its classes and their hierarchical relations, we will draw logical inferences, generate new hypotheses, and develop new ways of testing the structure of the Classification of Life.}
}
@article{TOMAZ2020100494,
title = {One does not simply … project a destination image within a participatory culture},
journal = {Journal of Destination Marketing & Management},
volume = {18},
pages = {100494},
year = {2020},
issn = {2212-571X},
doi = {https://doi.org/10.1016/j.jdmm.2020.100494},
url = {https://www.sciencedirect.com/science/article/pii/S2212571X20301165},
author = {Kolar Tomaž and Wattanacharoensil Walanchalee},
keywords = {Destination image, Internet memes, UGC format, Participatory culture, Online humor, Content analysis, Semiotic analysis},
abstract = {This study examines how internet memes, as an increasingly relevant and conceptually distinctive type of user-generated content (UGC), represent Thailand's destination image and how such representation differs from established destination image sources. For this purpose, participatory culture is first proposed as an alternative conceptual framework, followed by empirical research, which upgrades visual content analysis (VCA) with semiotic analysis. The findings of VCA reveal that memes yield a markedly different representation of Thailand as they introduce an entire cluster of peculiar and controversial themes, which are not depicted on destination marketing organization (DMO) and TripAdvisor photos. Semiotic analysis, in addition, uncovers that memetic representation is evocative and conveys a layer of symbolic notions and alluding connotations. In this manner, findings attest that memes are a semantically rich format and genre of UGC, which expands existing knowledge about destination-image formation and representation on social media. Managerial implications, limitations, and recommendations for future research in this domain are also discussed.}
}
@article{MAASS2021101909,
title = {Pairing conceptual modeling with machine learning},
journal = {Data & Knowledge Engineering},
volume = {134},
pages = {101909},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101909},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000367},
author = {Wolfgang Maass and Veda C. Storey},
keywords = {Conceptual modeling, Machine learning, Methodologies and tools, Models, Database management, Framework for incorporating conceptual modeling into data science projects, Artificial intelligence},
abstract = {Both conceptual modeling and machine learning have long been recognized as important areas of research. With the increasing emphasis on digitizing and processing large amounts of data for business and other applications, it would be helpful to consider how these areas of research can complement each other. To understand how they can be paired, we provide an overview of machine learning foundations and development cycle. We then examine how conceptual modeling can be applied to machine learning and propose a framework for incorporating conceptual modeling into data science projects. The framework is illustrated by applying it to a healthcare application. For the inverse pairing, machine learning can impact conceptual modeling through text and rule mining, as well as knowledge graphs. The pairing of conceptual modeling and machine learning in this way should help lay the foundations for future research.}
}
@article{SINGH20211835,
title = {Emerging Application of Nanorobotics and Artificial Intelligence To Cross the BBB: Advances in Design, Controlled Maneuvering, and Targeting of the Barriers},
journal = {ACS Chemical Neuroscience},
volume = {12},
number = {11},
pages = {1835-1853},
year = {2021},
issn = {1948-7193},
doi = {https://doi.org/10.1021/acschemneuro.1c00087},
url = {https://www.sciencedirect.com/science/article/pii/S1948719321000207},
author = {Ajay Vikram Singh and Vaisali Chandrasekar and Poonam Janapareddy and Divya Elsa Mathews and Peter Laux and Andreas Luch and Yin Yang and Beatriz Garcia-Canibano and Shidin Balakrishnan and Julien Abinahed and Abdulla {Al Ansari} and Sarada Prasad Dakua},
keywords = {Blood−brain barrier, nanorobots, transcytosis, machine learning and artificial intelligence, bioengineering, nanoparticles},
abstract = {The blood–brain barrier (BBB) is a prime focus for clinicians to maintain the homeostatic function in health and deliver the theranostics in brain cancer and number of neurological diseases. The structural hierarchy and in situ biochemical signaling of BBB neurovascular unit have been primary targets to recapitulate into the in vitro modules. The microengineered perfusion systems and development in 3D cellular and organoid culture have given a major thrust to BBB research for neuropharmacology. In this review, we focus on revisiting the nanoparticles based bimolecular engineering to enable them to maneuver, control, target, and deliver the theranostic payloads across cellular BBB as nanorobots or nanobots. Subsequently we provide a brief outline of specific case studies addressing the payload delivery in brain tumor and neurological disorders (e.g., Alzheimer’s disease, Parkinson’s disease, multiple sclerosis, etc.). In addition, we also address the opportunities and challenges across the nanorobots’ development and design. Finally, we address how computationally powered machine learning (ML) tools and artificial intelligence (AI) can be partnered with robotics to predict and design the next generation nanorobots to interact and deliver across the BBB without causing damage, toxicity, or malfunctions. The content of this review could be references to multidisciplinary science to clinicians, roboticists, chemists, and bioengineers involved in cutting-edge pharmaceutical design and BBB research.
}
}
@article{DENG2021105634,
title = {Players’ rights to game mods: Towards a more balanced copyright regime},
journal = {Computer Law & Security Review},
volume = {43},
pages = {105634},
year = {2021},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2021.105634},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921001072},
author = {Zhaoxia Deng and Yahong Li},
keywords = {Player contributed content, Game mods, Terms of service, Social benefits/harm, Right of modding, Community-based approach},
abstract = {In the context of video game, there is a notable convergence between the users and producers of content. There is also a tension between control over created content and innovative uses of that content, which arises from the gap existed between copyright law and the emerging practices of online communities. This paper examines a distinct form of player-contributed content, namely game Mods, through the perspective of social welfare rather than that of content creators. It argues that law is not the only factor affecting copyright owners’ decision-making behavior; social and economic factors also play an essential role. These factors explain why game developers may tolerate or even encourage minor alterations to their works but prohibit total conversion of the Mods. Given that the existing law and terms of service cannot serve as “effective cure” for regulating game Mods, this paper explores the social and economic factors that impact how game corporations address modding, framing these factors in a four-quadrant model according to the relative benefits and harm of Mods to game developers and users/modders. The inconsistency between the letter of the law and its practical application in the modding context suggests a need for law reform. Based on the findings of the above examinations, this paper proposes a two-pronged solution to the modding problem. The first prong concerns the social benefit of game Mods, aiming at changing the copyright regime from being exclusive to non-exclusive, which confers on gamers the legal right to modify video games without permission but obliges them to remunerate the original developers for commercial use of those Mods. The second prong concerns the potential social harm of game Mods and proposes a community-based approach, under which game operators are imposed a common law duty to monitor infringement and to ensure the fair implementation of game developers’ terms of service.}
}
@article{ADEL2021102012,
title = {Zipf's law applications in patent landscape analysis},
journal = {World Patent Information},
volume = {64},
pages = {102012},
year = {2021},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2020.102012},
url = {https://www.sciencedirect.com/science/article/pii/S0172219020301034},
author = {Michael E. Adel},
keywords = {Discrete pareto analysis, Patent landscape, Zipf's law, Benchmark, Consolidation, Fragmentation},
abstract = {New business insights are shown to be extractable from patent landscapes by the mathematical method of discrete Pareto analysis. By applying to patent publication distributions, a method analogous to that proposed by the linguist George Kingsley Zipf, metrics and methods of visualization are introduced which quantify scale, dominance and consolidation of a patent landscape. The key results of the method are illustrated in the Zipf plot of assignee patent publication count versus assignee rank for the lithography patent landscape shown below.}
}
@article{ALHASAN2021101933,
title = {Digital imaging, technologies and artificial intelligence applications during COVID-19 pandemic},
journal = {Computerized Medical Imaging and Graphics},
volume = {91},
pages = {101933},
year = {2021},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2021.101933},
url = {https://www.sciencedirect.com/science/article/pii/S0895611121000823},
author = {Mustafa Alhasan and Mohamed Hasaneen},
keywords = {Healthcare, Digital technologies, Artificial intelligence, Machine learning, COVID-19, Medical imaging},
abstract = {The advancement of technology remained an immersive interest for humankind throughout the past decades. Tech enterprises offered a stream of innovation to address the universal healthcare concerns. The novel coronavirus holds a substantial foothold of planet earth which is combatted by digital interventions across afflicted geographical boundaries and territories. This study aims to explore the trends of modern healthcare technologies and Artificial Intelligence (AI) during COVID-19 crisis, define the concepts and clinical role of AI in the mitigation of COVID-19, investigate and correlate the efficacy of AI-enabled technology in medical imaging during COVID-19 and determine advantages, drawbacks, and challenges of artificial intelligence during COVID-19 pandemic. The paper applied systematic review approach using a deliberated research protocol and Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) flow chart. Digital technologies can coordinate COVID-19 responses in a cascade fashion that extends from the clinical care facility to the exterior of the pending viral epicenter. With cases of healthcare robotics, aerial drones, and the internet of things as evidentiary examples. PCR tests and medical imaging are the frontier diagnostics of COVID-19. Computed tomography helped to correct the accuracy variation of PCR tests at a clinical sensitivity of 98 %. Artificial intelligence can enable autonomous COVID-19 responses using techniques like machine learning. Technology could be an endless system of innovation and opportunities when sourced effectively. Scientists can utilize technology to resolve global concerns challenging the history of tangible possibility. Digital interventions have enhanced the responses to COVID-19, magnified the role of medical imaging amid the COVID-19 crisis and have exposed healthcare professionals to the opportunity of contactless care.}
}
@article{RODRIGUEZBARROSO2020270,
title = {Federated Learning and Differential Privacy: Software tools analysis, the Sherpa.ai FL framework and methodological guidelines for preserving data privacy},
journal = {Information Fusion},
volume = {64},
pages = {270-292},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2020.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1566253520303213},
author = {Nuria Rodríguez-Barroso and Goran Stipcich and Daniel Jiménez-López and José Antonio Ruiz-Millán and Eugenio Martínez-Cámara and Gerardo González-Seco and M. Victoria Luzón and Miguel Angel Veganzones and Francisco Herrera},
keywords = {Federated learning, Differential privacy, Software framework,  Federated Learning framework},
abstract = {The high demand of artificial intelligence services at the edges that also preserve data privacy has pushed the research on novel machine learning paradigms that fit these requirements. Federated learning has the ambition to protect data privacy through distributed learning methods that keep the data in its storage silos. Likewise, differential privacy attains to improve the protection of data privacy by measuring the privacy loss in the communication among the elements of federated learning. The prospective matching of federated learning and differential privacy to the challenges of data privacy protection has caused the release of several software tools that support their functionalities, but they lack a unified vision of these techniques, and a methodological workflow that supports their usage. Hence, we present the Sherpa.ai Federated Learning framework that is built upon a holistic view of federated learning and differential privacy. It results from both the study of how to adapt the machine learning paradigm to federated learning, and the definition of methodological guidelines for developing artificial intelligence services based on federated learning and differential privacy. We show how to follow the methodological guidelines with the Sherpa.ai Federated Learning framework by means of a classification and a regression use cases.}
}
@article{AMEER20223220,
title = {From automation toward integration of process planning: a state-of-the-art review},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {3220-3225},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.145},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322021590},
author = {Muhammad Ameer and Mohammed Dahane},
keywords = {Computer-aided process plan (CAPP), Reconfigurable process planning (RPP), Manufacturing system design, performance indicators},
abstract = {The two main objective functions for designing the manufacturing system are, improving the manufacturing system's productivity and production quality. Process planning is an integral part of manufacturing system design. In this work, the study of process plan evolution over the period has been reviewed, keeping in mind the design objectives. Based on the technological advancements, the process plan evolution has been classified into two periods. The first evolution relates to automation, in which efforts are made to automate the manual activities of conventional process plans, which leads to the development of Computer-Aided Process Planning (CAPP). As long as the systems are deterministic with fixed structures and capabilities, CAPP is a good solution. Due to the uncertainty in the market for product demand, the new manufacturing systems are becoming more and more dynamic to handle the product variety demand. In that case, just automation is not enough to achieve the objective functions of system design. Designers have to consider the integration of the manufacturing system life cycle. So the second evolution of the process plan relates to, the consideration of performance indicators defined due to the system integration. For the second evolution, the literature review of the reconfigurable process plan (RPP) is performed considering both automation and integration of the system.}
}
@article{ELKASSAS2021113679,
title = {Automatic text summarization: A comprehensive survey},
journal = {Expert Systems with Applications},
volume = {165},
pages = {113679},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113679},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420305030},
author = {Wafaa S. El-Kassas and Cherif R. Salama and Ahmed A. Rafea and Hoda K. Mohamed},
keywords = {Automatic text summarization, Text summarization approaches, Text summarization techniques, Text summarization evaluation},
abstract = {Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.}
}
@article{LIN2020266,
title = {A novel quadruple generative adversarial network for semi-supervised categorization of low-resolution images},
journal = {Neurocomputing},
volume = {415},
pages = {266-285},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.05.050},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220308857},
author = {Zhongqi Lin and Jingdun Jia and Wanlin Gao and Feng Huang},
keywords = {Generative adversarial networks, Image super-resolution, Image categorization, Semi-supervised learning, Deep learning},
abstract = {In order to make utilization of unlabeled low-resolution (LR) images to shape discriminative models, we present quadruple generative adversarial network (Q-GAN), a game-theoretical framework for implementing semi-supervised categorization of LR images. It can realize photo-realistic image super-resolution (SR) and semi-supervised pattern recognition simultaneously. We consider our pipeline as a four-player optimization-based formulation, which consists of four vital components, i.e., a refiner for image SR and generation, a discriminator for identifying high-resolution (HR) samples and another for identifying true (original) samples, a classifier for label prediction. The refiner and two discriminators characterize the conditional distributions between images and labels, whilst the classifier solely focuses on predicting real image-label pairs. We select those high-quality super-solved images with ground-truth labels for data supplement. We customize the global optimization objective function as well as the training procedure to ensure model approximates the posterior distribution of latent variables given true data in a semi-supervised manner. Experimental results demonstrate that Q-GAN can simultaneously (1) deliver the promising categorization performance among state-of-the-arts, i.e., validation accuracy achieves 92.18% and testing accuracy achieves 90.63%, and (2) recover fine-grained textures with high peak signal-to-noise ratios (PNSRs) and structural similarities (SSIMs) from heavily downsampled testing images of hand-crafted dataset and public benchmarks.}
}
@article{KANGER2022102447,
title = {Deep Transitions: Towards a comprehensive framework for mapping major continuities and ruptures in industrial modernity},
journal = {Global Environmental Change},
volume = {72},
pages = {102447},
year = {2022},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2021.102447},
url = {https://www.sciencedirect.com/science/article/pii/S0959378021002260},
author = {Laur Kanger and Peeter Tinits and Anna-Kati Pahker and Kati Orru and Amaresh Kumar Tiwari and Silver Sillak and Artjoms Šeļa and Kristiina Vaik},
keywords = {Deep Transitions, Industrial modernity, Ideas, Institutions, Practices},
abstract = {The world is confronted by a socio-ecological emergency, requiring rapid and deep decarbonization of a broad range of socio-technical systems. A recent Deep Transitions framework argues that this fundamentally unsustainable trajectory has been generated by the co-evolutionary dynamics of multiple systems during the last 250 years. Altering this direction requires transformation in industrial modernity – a set of most fundamental ideas, institutions, and practices characterizing every industrial society to date. Although the proponents of the framework suggest that this shift has been unfolding since the 1960s, no attempts have been made to operationalize the concept of industrial modernity and to assess this claim. This paper develops a comprehensive multi-dimensional and multi-domain approach for the measurement of industrial modernity. As such it seeks to provide empirical evidence of long-term continuities and emerging ruptures in the dominant ideas, institutions, and practices of industrial societies along the domains of environment and technology. Using a methodologically novel approach where the text mining of newspapers is combined with data from various databases the paper provides results from three countries – Australia, Germany, Soviet Union/Russia – between 1900 and 2020. Despite considerable country-level differences the results show shifts in public environmental discourse from the 1960s, followed by institutional changes from the 1980s but with only a modest change in practices. We also observe some change in the direction of innovative activities and their regulation coupled with a resurgent optimism in technology-environment discourse. The findings tentatively suggest that industrial modernity might be in the process of hollowing out along ideational and institutional dimensions in the environmental domain but less so in the domain of technology and innovation.}
}
@article{ECHEVERRIA2021111037,
title = {Comparison of search strategies for feature location in software models},
journal = {Journal of Systems and Software},
volume = {181},
pages = {111037},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111037},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001345},
author = {Jorge Echeverría and Jaime Font and Francisca Pérez and Carlos Cetina},
keywords = {Feature location in models, Search strategies},
abstract = {Search-based model-driven engineering is the application of search-based techniques to specific problems that are related to software engineering that is driven using software models. In this work, we make use of measures from the literature to report feature location problems in models (size and volume of the model and density, multiplicity, and dispersion of the feature being located) and a set of search strategies (random search, iterated local search, hill climbing, an evolutionary algorithm, and a hybrid between an evolutionary algorithm and hill climbing). The goal is to analyze of the impact of different values that are used to describe the feature location problems and the performance obtained by the different search strategies. We apply the search strategies to 1895 feature location problems that are obtained from 40 industrial software product lines. This work shows that: 1) the best results overall are obtained by a hybrid between evolutionary algorithm and hill climbing; 2) the size of the search space has the greatest impact on the results obtained by the search strategies; and 3) the impact of each of the measures is not the same in the five search strategies. This work highlights the use of the search strategy that produces the best results. In addition, we provide recommendations on when to use each search strategy.}
}
@article{PREMAN2020121,
title = {Recent developments in stimuli-responsive polymer nanogels for drug delivery and diagnostics: A review},
journal = {European Journal of Pharmaceutics and Biopharmaceutics},
volume = {157},
pages = {121-153},
year = {2020},
issn = {0939-6411},
doi = {https://doi.org/10.1016/j.ejpb.2020.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S093964112030309X},
author = {Namitha K. Preman and Rashmi R. Barki and Anjali Vijayan and Sandesh G. Sanjeeva and Renjith P. Johnson},
keywords = {Polymer nanogel, Stimuli-responsiveness, Nanomedicine, Drug delivery, Gene delivery, Imaging},
abstract = {Polymer nanogels (NGs) are water-swellable, cross-linked 3D network structures with size typically range from 1 to 1000 nm. Especially, biocompatible and “smart” NGs engineered from stimuli-responsive polymers are attractive because of its capability to respond the endogenous biological triggers of pH, bioreduction, biomolecule recognition, as well as the exogenous stimuli-triggers like temperature and light. Importantly, on exposing to these physical or biochemical signals, the responsive NGs can be utilized for therapeutic delivery and diagonostic applications. In the past decade, substantial developments were achieved in the development of “smart” NGs for theranostic and diagnostic applications such as intracellular delivery of drug and nucleic acids, photodynamic/photothermal therapy, bioimaging and sensing. Herein, we exclusively review the recent exciting developments in synthetic methods as well as biomedical applications of successfully employed “smart” NGs which can respond to a single, dual or multiple stimulus- responsive triggers. The prospects in the application of the stimuli-responsive and multifunctional NGs also will be addressed in this review.}
}
@article{YANG2021107548,
title = {Perceptual similarity measurement based on generative adversarial neural networks in graphics design},
journal = {Applied Soft Computing},
volume = {110},
pages = {107548},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107548},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621004695},
author = {Bin Yang},
keywords = {Perceptual similarity, Plagiarism detection, Generative adversarial network, Logo design, Graphics design},
abstract = {Measuring the similarity between images is of paramount importance in computer vision. However, the commonly used pixelwise similarity metrics do not match well with perceptual similarity. The purpose of this paper is to propose a visual similarity measurement method, which can be effectively used for plagiarism detection in graphic design. Plagiarism detection of designs refers to the identification and determination of major similarities. It is difficult to carry out the similarity learning process in traditional deep neural network due to the insufficient of training samples. To overcome this problem, a novel scheme is proposed for measuring perceptual similarity of graphics by using a constraint Generative Adversarial Network (GAN) model. The generator of GAN is used to create similar graphics following the common plagiarism features of logo design. Unlike the traditional discriminator which judges the authenticity of the generated image and the original image, the modified discriminator is used to calculate the perceptual similarity of the graphics pair. In graphics design, plagiarism mainly focuses on the changes of shape, color and style, which has certain cognitive subjectivity. Therefore, design experts were invited to participate in a group of cognitive analysis experiments. A perceptual constraint model is established to limit the generation of plagiarized graphics according to “design and visual rationality”. Promising results demonstrate that the proposed method can be used for plagiarism detection of logo design. Given its effectiveness and conceptual simplicity, I hope it can serve as a baseline and contribute to the future research on plagiarism detection of artworks.}
}
@article{FENG2021509,
title = {Identifying opportunities for sustainable business models in manufacturing: Application of patent analysis and generative topographic mapping},
journal = {Sustainable Production and Consumption},
volume = {27},
pages = {509-522},
year = {2021},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2021.01.021},
url = {https://www.sciencedirect.com/science/article/pii/S235255092100021X},
author = {Jian Feng and Zhenfeng Liu and Lijie Feng},
keywords = {sustainable business model, business opportunity identification, patent analysis, business model canvas, generative topographic mapping, system dynamics, manufacturing},
abstract = {Early identification of business opportunities is critical for technology-based manufacturers seeking to develop new sustainable business models (SBMs) for future competitive advantages. However, there exists an insufficiency of identifying business opportunities compared to previous studies which have focused mainly on technology opportunities and service opportunities. To fill this research gap, this study proposes a new systematic approach to identify business opportunities for new SBMs based on information relating to the manufacturers' technologies and patents. To illustrate, an example in the mining machinery industry was examined as a case study. The results demonstrated that 255 patent documents relating to the product were collected. Next, latent Dirichlet allocation was used to generate 26 business topics, which were categorized into the 9 building blocks of the business model canvas (BMC). Then, generative topographic mapping (GTM) was applied to identify 13 vacuums and related technology-driven business opportunities on the basis of BMC-based patent-business vectors. Finally, dynamic business modelling was conducted, which integrated sustainable BMCs and system dynamics in order to evaluate and rank these business opportunities. The proposed approach can promote consensus building between the technology and business planning departments on developing technology-driven SBMs in both public and private sectors.}
}
@article{CHEN2021497,
title = {Topic analysis and development in knowledge graph research: A bibliometric review on three decades},
journal = {Neurocomputing},
volume = {461},
pages = {497-515},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.02.098},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221009528},
author = {Xieling Chen and Haoran Xie and Zongxi Li and Gary Cheng},
keywords = {Knowledge graphs, Bibliometric analysis, Structural topic modeling, Research topics, Scientific collaboration},
abstract = {Knowledge graph as a research topic is increasingly popular to represent structural relations between entities. Recent years have witnessed the release of various open-source and enterprise-supported knowledge graphs with dramatic growth in applying knowledge representation and reasoning into different areas like natural language processing and computer vision. This study aims to comprehensively explore the status and trends – particularly the thematic research structure – of knowledge graphs. Specifically, based on 386 research articles published from 1991 to 2020, we conducted analyses in terms of the (1) visualization of the trends of annual article and citation counts, (2) recognition of major institutions, countries/regions, and publication sources, (3) visualization of scientific collaborations of major institutions and countries/regions, and (4) detection of major research themes and their developmental tendencies. Interest in knowledge graph research has clearly increased from 1991 to 2020 and is continually expanding. China is the most prolific country in knowledge graph research. Moreover, countries/regions and institutions that have higher levels of international collaboration are more impactful. Several widely studied issues such as knowledge graph embedding, search and query based on knowledge graphs, and knowledge graphs for intangible cultural heritage are highlighted. Based on the results, we further summarize perspective directions and suggestions for researchers, practitioners, and project managers to facilitate future research on knowledge graphs.}
}
@article{LOUREIRO2021911,
title = {Artificial intelligence in business: State of the art and future research agenda},
journal = {Journal of Business Research},
volume = {129},
pages = {911-926},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320307451},
author = {Sandra Maria Correia Loureiro and João Guerreiro and Iis Tussyadiah},
keywords = {Artificial Intelligence, Intelligent agent, Business applications, Text mining, Research agenda, Future trends},
abstract = {This study provides an overview of state-of-the-art research on Artificial Intelligence in the business context and proposes an agenda for future research. First, by analyzing 404 relevant articles collected through Web of Science and Scopus, this article presents the evolution of research on AI in business over time, highlighting seminal works in the field, and the leading publication venues. Next, using a text-mining approach based on Latent Dirichlet Allocation, latent topics were extracted from the literature and comprehensively analyzed. The findings reveal 18 topics classified into four main clusters: societal impact of AI, organizational impact of AI, AI systems, and AI methodologies. This study then presents several main developmental trends and the resulting challenges, including robots and automated systems, Internet-of-Things and AI integration, law, and ethics, among others. Finally, a research agenda is proposed to guide the directions of future AI research in business addressing the identified trends and challenges.}
}
@article{HAN2021225,
title = {Pre-trained models: Past, present and future},
journal = {AI Open},
volume = {2},
pages = {225-250},
year = {2021},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666651021000231},
author = {Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Yuan Yao and Ao Zhang and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-Rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
keywords = {Pre-trained models, Language models, Transfer learning, Self-supervised learning, Natural language processing, Multimodal processing, Artificial intelligence},
abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.}
}
@article{GOMES2021120950,
title = {Ecosystem management: Past achievements and future promises},
journal = {Technological Forecasting and Social Change},
volume = {171},
pages = {120950},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120950},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521003826},
author = {Leonardo Augusto de Vasconcelos Gomes and Ximena Alejandra Flechas and Ana Lucia Figueiredo Facin and Felipe Mendes Borini},
keywords = {Ecosystem management, Innovation ecosystem, Distributed value creation, Supply chain management, New scientific field},
abstract = {The introduction of supply chain management (SCM) between the 1980s and 1990s represented a breakthrough in the field of management. SCM provides the analytical and theoretical background to design, plan, and manage production activities involving complex chains of firms. Nowadays, both production activities and incremental and radical innovation are organized through complex networks. Innovations, especially radical ones, have unique characteristics related to production flows, which generate new challenges for researchers and managers. In this study, we argue that similar to SCM, ecosystem management (EM) represents a potential breakthrough in the field. However, no general framework exists to address EM. To address this challenge, we perform a systematic literature review by adopting a hybrid approach that combines bibliometric analysis and content analysis. Our findings offer a historical perspective of how EM and the ecosystem concept have evolved over three generations. Building on and reaching beyond current scholarship, we propose a definition of ecosystem as a type of meta-organization. We also identify the goals, scope, and boundaries of EM. Our contributions invite scholars to explore old and new questions related to innovation and management in a novel way.}
}