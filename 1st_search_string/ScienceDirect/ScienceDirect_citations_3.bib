@article{SUN2024109346,
title = {A Fine Rendering High-Resolution Makeup Transfer network via inversion-editing strategy},
journal = {Engineering Applications of Artificial Intelligence},
volume = {138},
pages = {109346},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109346},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624015045},
author = {Zhaoyang Sun and Shengwu Xiong and Yaxiong Chen and Yi Rong},
keywords = {Makeup transfer, Inversion-editing strategy, Image translation, Artificial intelligence application},
abstract = {While current makeup transfer methods have made progress in realism and color fidelity, they struggle with capturing texture details and producing high-resolution images, limiting their practical utility. To address these challenges, we propose a Fine Rendering High-resolution Makeup Transfer (FRHMT) network, which leverages a powerful style-based generator and introduces a novel inversion-editing strategy tailored for makeup transfer. Concretely, in the inversion phase, considering the semantic decoupling properties in the latent space, we design a Hierarchical Residual Inversion (HRI), which projects the image onto high-dimensional feature maps in coarse layers and low-dimensional style codes in fine layers. This design effectively restores the content information of the image while maintaining flexibility in editing the makeup styles. In the editing phase, the Makeup Modulation Module (MMM) learns two mapping networks to adjust the latent variables of the source image based on those of the reference image. This modification occurs in fine layers to transfer the makeup information and preserve the content information. With new network structures and customized loss functions, our training eliminates cumbersome pseudo-paired data synthesis and unstable adversarial learning. Extensive experiments have demonstrated that our method outperforms existing methods in both image quality and makeup similarity through quantitative and qualitative analysis. Additionally, we address the lack of high-resolution data by collecting a dataset of 9716 face images with a resolution of 1024 × 1024. In conclusion, our framework offers a novel artificial intelligence (AI) implementation of makeup transfer in engineering, with the collected dataset holding substantial value for further advancements in AI research.}
}
@article{WANG2024123736,
title = {An exploration method for technology forecasting that combines link prediction with graph embedding: A case study on blockchain},
journal = {Technological Forecasting and Social Change},
volume = {208},
pages = {123736},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123736},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524005341},
author = {Liang Wang and Munan Li},
keywords = {Technology forecasting, Topic recognition, Link prediction, Graph representation learning, Emerging technology, Blockchain},
abstract = {To keep pace with the latest technological changes and advancements, predicting future technological trends and topics has become a critical approach for high-tech companies and policy-making institutions. In this paper, we proposed an explorative method that integrates link prediction and Node2Vec graph embedding to predict future technology topics using co-occurrence data from patent keywords. Specifically, this method collects and preprocesses patent datasets, constructs network graphs that depict relationships among different technology topics, and builds a supervised link prediction model based on the time series of the graph to identify future technology graphs. Furthermore, node2vec graph embedding is conducted to obtain node vector representations, and then the clustering algorithms can be improved to identify the relevant topics, which could be interpreted as future technology. A case study on blockchain is conducted to validate the feasibility and practicality of the method to demonstrate the application of the method. Through the comparison of machine learning methods, we selected the Random Forest (RF) model, which presents the highest accuracy, for our experiments. The results show that the proposed method can be used to effectively visualize potential future topics related to a specific technology. Compared to traditional methods such as Latent Dirichlet Allocation (LDA), our method can identify more unique and differentiated technological topics, significantly reducing topic overlap. Additionally, the reported method can illustrate the internal relationships of topics through subgraphs, helping readers better understand the core concepts of each topic and vividly displaying the structure and composition of the topics. Furthermore, the proposed method can also depict potential relationships between different technology topics, which can facilitate the visualization of new directions of research and development.}
}
@article{GABER2025104167,
title = {Zero day ransomware detection with Pulse: Function classification with Transformer models and assembly language},
journal = {Computers & Security},
volume = {148},
pages = {104167},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104167},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004723},
author = {Matthew Gaber and Mohiuddin Ahmed and Helge Janicke},
keywords = {Dynamic binary instrumentation, Malware analysis, Feature extraction, Ransomware, Transformers, LLM, AI, Assembly},
abstract = {Finding automated AI techniques to proactively defend against malware has become increasingly critical. The ability of an AI model to correctly classify novel malware is dependent on the quality of the features it is trained with and the authenticity of the features is dependent on the analysis tool. Peekaboo, a Dynamic Binary Instrumentation tool defeats evasive malware to capture its genuine behaviour. The ransomware Assembly instructions captured by Peekaboo, follow Zipf’s law, a principle also observed in natural languages, indicating Transformer models are particularly well-suited to binary classification. We propose Pulse, a novel framework for zero day ransomware detection with Transformer models and Assembly language. Pulse, trained with the Peekaboo ransomware and benign software data, uniquely identify truly new samples with high accuracy. Pulse eliminates any familiar functionality across the test and training samples, forcing the Transformer model to detect malicious behaviour based solely on context and novel Assembly instruction combinations.}
}
@article{DING2024109451,
title = {A survey of intelligent multimedia forensics for internet of things communications: Approaches, strategies, perspectives, and challenges for a sustainable future},
journal = {Engineering Applications of Artificial Intelligence},
volume = {138},
pages = {109451},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109451},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624016099},
author = {Weiping Ding and Mohamed Abdel-Basset and Ahmed M. Ali and Nour Moustafa},
keywords = {Multimedia forensics, Internet of things, Deep learning, Intrusion detection system application},
abstract = {Digital forensics is a proven method for collecting, preserving, reporting, analyzing, identifying, and presenting digital evidence from the original data, and it helps find evidence of cybercrimes. Intelligent multimedia forensics is a type of digital forensics and is essential because it is used to identify fake multimedia, including images, videos, audio, and text. This paper conducted a comprehensive survey for intelligent multimedia forensics, categorized into 3 classes: deep learning forensics, multimedia forensics, and network and Internet of Things forensics. In the first class, we provided a survey of the multiple attacks breaking the DL models, such as attacks in the training step, and the testing step, and crafted. In the craft attacks, we survey the three attacks: white box, gray box, and black box. We also provided some defense methods against DL attacks, training step attacks, and testing step attacks. In the second class, we offered a survey of multimedia forensics, including passive and active manipulation. In the third class, we provide a survey of network/IoT forensics, including the attacks in five layers: physical, data link, transport, application, and network. We also provided network/IoT attack detection in the third class using deep learning models. We applied AI models in various datasets and obtained higher accuracy and performance on the datasets from various used models. Finally, we offered the challenges and future direction for the researcher in this scope.}
}
@article{MAC2024106070,
title = {Bias and discrimination in ML-based systems of administrative decision-making and support},
journal = {Computer Law & Security Review},
volume = {55},
pages = {106070},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106070},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924001365},
author = {Trang Anh MAC},
keywords = {Bias, Discrimination, AI, Machine learning, Decision-making, Support},
abstract = {In 2020, the alleged wilful and gross negligence of four social workers, who did not notice and failed to report the risks to an eight-year-old boy's life from the violent abuses by his mother and her boyfriend back in 2013, ultimately leading to his death, had been heavily criticised.11*Trang Anh MAC, LLM. Digital Law, University of Paris XII Est-Créteil, reporter at AstraIA Gear. This paper is the English version of her master thesis, under supervision of Dr. Laurie MARGUET and Prof. Florent MADELAINE A. Reyes-Velarde, Charges dismissed against social workers linked to Gabriel Fernandez's killing, Los Angeles Times, 16 Jul 2020, available online at https://www.latimes.com/california/story/2020-07-15/charges-against-the-social-workers-linked-to-gabriel-fernandez-killing-will-be-dropped The documentary, Trials of Gabriel Fernandez in 2020,22https://www.imdb.com/title/tt11822998/ has discussed the Allegheny Family Screening Tool (AFST33Allegheny County, Allegheny Family Screening Tool, available online at https://www.alleghenycounty.us/Services/Human-Services-DHS/DHS-News-and-Events/Accomplishments-and-Innovations/Allegheny-Family-Screening-Tool), implemented by Allegheny County, US since 2016 to foresee involvement with the social services system. Rhema Vaithianathan44Bio of Prof. Rhema Vaithianathan. Available online at https://academics.aut.ac.nz/rhema.vaithianathan, the Centre for Social Data Analytics co-director, and the Children's Data Network55Our team, Children’s Data Network. Available online at https://www.datanetwork.org/people/ members, with Emily Putnam-Hornstein66Bio of PhD. Emily Putnam-Hornstein. Available online at https://www.datanetwork.org/people/#emily-putnam-hornstein, established the exemplary and screening tool, integrating and analysing enormous amounts of data details of the person allegedly associating to injustice to children, housed in DHS Data Warehouse77Allegheny County, DHS Data Warehouse. Available online at https://www.alleghenycounty.us/Services/Human-Services-DHS/DHS-News-and-Events/Accomplishments-and-Innovations/DHS-Data-Warehouse. They considered that may be the solution for the failure of the overwhelmed manual administrative systems. However, like other applications of AI in our modern world, in the public sector, Algorithmic Decisions Making and Support systems, it is also denounced because of the data and algorithmic bias.88N. LaGrone, Can AI Reduce Harm to Children?: Gabriel Fernandez and the Case for Machine Learning, 9 April 2020, available online at https://www.azavea.com/blog/2020/04/09/can-ai-reduce-harm-to-children/ This topic has been weighed up for the last few years but not has been put to an end yet. Therefore, this humble research is a glance through the problems - the bias and discrimination of AI based Administrative Decision Making and Support systems. At first, I determined the bias and discrimination, their blur boundary between two definitions from the legal perspective, then went into the details of the causes of bias in each stage of AI system development, mainly as the results of bias data sources and human decisions in the past, society and political contexts, and the developers’ ethics. In the same chapter, I presented the non-discrimination legal framework, including their application and convergence with the administration laws in regard to the automated decision making and support systems, as well as the involvement of ethics and regulations on personal data protection. In the next chapter, I tried to outline new proposals for potential solutions from both legal and technical perspectives. In respect to the former, my focus was fairness definitions and other current options for the developers, for example, the toolkits, benchmark datasets, debiased data, etc. For the latter, I reported the strategies and new proposals governing the datasets and AI systems development, implementation in the near future.}
}
@article{VOIGT2024301805,
title = {Re-imagen: Generating coherent background activity in synthetic scenario-based forensic datasets using large language models},
journal = {Forensic Science International: Digital Investigation},
volume = {50},
pages = {301805},
year = {2024},
note = {DFRWS APAC 2024 - Selected Papers from the 4th Annual Digital Forensics Research Conference APAC},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2024.301805},
url = {https://www.sciencedirect.com/science/article/pii/S266628172400129X},
author = {Lena L. Voigt and Felix Freiling and Christopher J. Hargreaves},
keywords = {Forensic datasets, User simulation, Large language models (LLM), Forensic disk image generation, Data synthesis, Digital forensic education, ChatGPT},
abstract = {Due to legal and privacy-related restrictions, the generation of synthetic data is recommended for creating datasets for digital forensic education and training. One challenge when synthesizing scenario-based forensic data is the creation of coherent background activity besides evidential actions. This work leverages the creative writing abilities of large language models (LLMs) to generate personas and actions that describe the background usage of a device consistent with the created persona. These actions are subsequently converted into a machine-readable format and executed on a virtualized device using VM control automation. We introduce Re-imagen, a framework that combines state-of-the-art LLMs and a recent unintrusive GUI automation tool to produce synthetic disk images that contain arguably coherent “wear-and-tear” artifacts that current synthesis platforms lack. While, for now, the focus is on the coherence of the generated background activity, we believe that the proposed approach is a step toward more realistic synthetic disk image generation.}
}
@article{LEE20241275,
title = {Advancing Autoencoder Architectures for Enhanced Anomaly Detection in Multivariate Industrial Time Series},
journal = {Computers, Materials and Continua},
volume = {81},
number = {1},
pages = {1275-1300},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.054826},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824007380},
author = {Byeongcheon Lee and Sangmin Kim and Muazzam Maqsood and Jihoon Moon and Seungmin Rho},
keywords = {Advanced anomaly detection, autoencoder innovations, unsupervised learning, industrial security, multivariate time series analysis},
abstract = {In the context of rapid digitization in industrial environments, how effective are advanced unsupervised learning models, particularly hybrid autoencoder models, at detecting anomalies in industrial control system (ICS) datasets? This study is crucial because it addresses the challenge of identifying rare and complex anomalous patterns in the vast amounts of time series data generated by Internet of Things (IoT) devices, which can significantly improve the reliability and safety of these systems. In this paper, we propose a hybrid autoencoder model, called ConvBiLSTM-AE, which combines convolutional neural network (CNN) and bidirectional long short-term memory (BiLSTM) to more effectively train complex temporal data patterns in anomaly detection. On the hardware-in-the-loop-based extended industrial control system dataset, the ConvBiLSTM-AE model demonstrated remarkable anomaly detection performance, achieving F1 scores of 0.78 and 0.41 for the first and second datasets, respectively. The results suggest that hybrid autoencoder models are not only viable, but potentially superior alternatives for unsupervised anomaly detection in complex industrial systems, offering a promising approach to improving their reliability and safety.}
}
@article{KAKRAN2025102572,
title = {Interconnectedness and return spillover among APEC currency exchange rates: A time-frequency analysis},
journal = {Research in International Business and Finance},
volume = {73},
pages = {102572},
year = {2025},
issn = {0275-5319},
doi = {https://doi.org/10.1016/j.ribaf.2024.102572},
url = {https://www.sciencedirect.com/science/article/pii/S0275531924003659},
author = {Shubham Kakran and Parminder Kaur Bajaj and Dharen Kumar Pandey and Ashish Kumar},
keywords = {Asia Pacific Economic Cooperation, COVID-19, Russia–Ukraine war, Silicon valley bank collapse, Spillover, Currency exchange rate},
abstract = {By combining TVP-VAR Model (time domain connectedness) and TVP-VAR based Baruník and Křehlík model (frequency domain connectedness), this study analyzes the impact of the COVID-19 pandemic, the Russia-Ukraine war, and the Silicon Valley Bank (SVB) collapse on the Asia Pacific Economic Cooperation (APEC) forum currency exchange rates. The results reveal that APEC currencies have time-varying effects (tend to cluster in appreciation and depreciation patterns in both the short and long term) and have generated higher total return spillover during COVID-19 (in the time domain) than the Russia-Ukraine war and SVB collapse. During COVID-19 (87.18 %) (total return spillover), impacts were more severe than the Russia-Ukraine crisis (79.49 %) and the Silicon Valley Bank collapse (75.55 %). Moreover, the South Korean won, Thai Bhat and Australian Dollar are identified as consistent shock transmitters, and Malaysian Ringgit, Philippine peso, Indonesian Rupiah, and Chinese Yuan as consistent shock receivers in the time domain. The findings have substantial repercussions for financial regulators and investors.}
}
@article{SERGEYUK2025107610,
title = {Using AI-based coding assistants in practice: State of affairs, perceptions, and ways forward},
journal = {Information and Software Technology},
volume = {178},
pages = {107610},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107610},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924002155},
author = {Agnia Sergeyuk and Yaroslav Golubev and Timofey Bryksin and Iftekhar Ahmed},
keywords = {LLMs, AI assistants, Software development lifecycle},
abstract = {Context:
The last several years saw the emergence of AI assistants for code — multi-purpose AI-based helpers in software engineering. As they become omnipresent in all aspects of software development, it becomes critical to understand their usage patterns.
Objective:
We aim to better understand how specifically developers are using AI assistants, why they are not using them in certain parts of their development workflow, and what needs to be improved in the future.
Methods:
In this work, we carried out a large-scale survey aimed at how AI assistants are used, focusing on specific software development activities and stages. We collected opinions of 481 programmers on five broad activities: (a) implementing new features, (b) writing tests, (c) bug triaging, (d) refactoring, and (e) writing natural-language artifacts, as well as their individual stages.
Results:
Our results provide a novel comparison of different stages where AI assistants are used that is both comprehensive and detailed. It highlights specific activities that developers find less enjoyable and want to delegate to an AI assistant, e.g., writing tests and natural-language artifacts. We also determine more granular stages where AI assistants are used, such as generating tests and generating docstrings, as well as less studied parts of the workflow, such as generating test data. Among the reasons for not using assistants, there are general aspects like trust and company policies, as well as more concrete issues like the lack of project-size context, which can be the focus of the future research.
Conclusion:
The provided analysis highlights stages of software development that developers want to delegate and that are already popular for using AI assistants, which can be a good focus for features aimed to help developers right now. The main reasons for not using AI assistants can serve as a guideline for future work.}
}
@article{GUPTA2024100685,
title = {Unleashing the prospective of blockchain-federated learning fusion for IoT security: A comprehensive review},
journal = {Computer Science Review},
volume = {54},
pages = {100685},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100685},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000698},
author = {Mansi Gupta and Mohit Kumar and Renu Dhir},
keywords = {Blockchain, Internet-of-Things, Security, Federated learning, Decentralization},
abstract = {Internet-of-things (IoT) is a revolutionary paragon that brings automation and easiness to human lives and improves their experience. Smart Homes, Healthcare, and Agriculture are some of their amazing use cases. These IoT applications often employ Machine Learning (ML) techniques to strengthen their functionality. ML can be used to analyze sensor data for various, including optimizing energy usage in smart homes, predicting maintenance needs in industrial equipment, personalized user experiences in wearable devices, and detecting anomalies for security monitoring. However, implementing centralized ML techniques is not viable because of the high cost of computing power and privacy issues since so much data is stored over a cloud server. To safeguard data privacy, Federated Learning (FL) has become a new paragon for centralized ML methods where FL,an ML variation sends a model to the user devices without the need to give private data to the third-party or central server, it is one of the promising solutions to address data leakage concerns. By saving raw data to the client itself and transferring only model updates or parameters to the central server, FL helps to reduce privacy leakage. However, it is still not attack-resistant. Blockchain offers a solution to protect FL-enabled IoT networks using smart contracts and consensus mechanisms. This manuscript reviews IoT applications and challenges, discusses FL techniques that can be used to train IoT networks while ensuring privacy, and analyzes existing work. To ensure the security and privacy of IoT applications, an integrated Blockchain-powered FL-based framework was introduced and studies existing research were done using these three powerful paradigms. Finally, the research challenges faced by the integrated platform are explored for future scope, along with the potential applications of IoT in conjunction with other cutting-edge technologies.}
}
@article{MARIONI2024106762,
title = {Productivity performance, distance to frontier and AI innovation: Firm-level evidence from Europe},
journal = {Journal of Economic Behavior & Organization},
volume = {228},
pages = {106762},
year = {2024},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2024.106762},
url = {https://www.sciencedirect.com/science/article/pii/S0167268124003767},
author = {Larissa da Silva Marioni and Ana Rincon-Aznar and Francesco Venturini},
keywords = {Artificial Intelligence, Multi-Factor Productivity, European firms, Local Projections Difference-in-Differences, Distance-to-Frontier},
abstract = {Using firm-level data from 15 European countries between 2011 and 2019, this paper examines the productivity effect associated with the development of Artificial Intelligence (AI), measured by patenting success in AI fields. By making advances in AI and expanding on their knowledge base, companies can optimise production tasks, and improve resource utilisation, ultimately leading to higher levels of efficiency. To investigate this, we develop a two-fold panel regression analysis estimated within a Difference-in-Differences (DiD) framework. First, we investigate whether firms that engage in AI innovation experience a productivity boost after developing the new technology, compared to similar firms which do not undertake AI innovation. To analyse this, we employ a novel event-analysis methodology that quantifies the effect of the treatment (AI innovation) on firm performance (productivity) using a Local Projections approach within the DiD setting. Second, we utilise a Distance-to-Frontier (DTF) regression framework in order to examine whether the productivity premium of AI is associated with a firm’s ability to absorb knowledge and learn from the technologies developed by market leaders. Our findings reveal that the productivity gains directly associated with AI are statistically significant and quantitatively important, ranging between 6.2 and 17% in the event analysis, and between 2.1 and 6% in the DTF framework. We also provide some evidence that the productivity benefits of AI might be greater for those firms further away from the frontier (between 0.3 and 0.7%). Our research demonstrates that Artificial Intelligence can play a crucial role in enhancing firm productivity in Europe, a result that is evident even in these early stages of the technology’s life cycle.}
}
@article{ABERNA2025510,
title = {PoWBWM: Proof of work consensus cryptographic blockchain-based adaptive watermarking system for tamper detection applications},
journal = {Alexandria Engineering Journal},
volume = {112},
pages = {510-537},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824011694},
author = {P. Aberna and L. Agilandeeswari},
keywords = {Convolution attention model, Fuzzy inference system, High dynamic range image (HDR), Proof of work consensus blockchain, Quaternion graph-based transform (QGBT)},
abstract = {Image tamper detection is a challenging area in multimedia research. The advances in photography technology have made it possible to capture real-time high-dynamic-range (HDR) images through an iPhone or an Android device, which highlights the need for rigorous research on HDR images for tamper detection, and localization. The algorithms built for standard images may affect the watermark visibility when it is applied to HDR images as this produces high perceptual variations in the image compared to the original. To tackle this problem, we presented a Proof of Work consensus blockchain watermarking scheme combined with a convolution attention model (PoWBWM) system for tamper detection and localization. The system utilizes a Convolution Attention model (CoAtNet) to generate robust watermarks. A quaternion graph-based transform (QGBT) for embedding, ensuring imperceptibility and robustness. A fuzzy inference system optimizes embedding regions and factors based on human visual system characteristics. The system's security is enhanced through blockchain's proof-of-work (consensus) mechanism, providing a semi-blind watermarking scheme that authenticates ownership and detects tampering efficiently. The security is ensured only when the embedded hash key is authentic with its previous block to proceed further extraction process. The proposed algorithm's performance is evaluated in terms of its visibility by Peak-Signal-to-Noise-Ratio (PSNR), Structural Similarity Index (SSIM), and the perceptual quality of an HDR image is additionally measured by the Visual Dynamic Predictor (VDP) metric. On the other hand, the robustness performance is measured by Normalized Correlation Coefficient (NCC) and Bit Error Rate (BER). The experimental results for CASIA images achieved the highest PSNR value of 63.84 dB, and the SSIM value of 1.000, whereas the maximum VDP value obtained for HDR images is 98.02. In comparison with the existing system, the experimental findings of the suggested model show an effective tamper detection watermarking system as well as a robust against both intentional and unintentional attacks with an average NCC value of 0.98.}
}
@article{BRUCKLACHER2025106716,
title = {Learning to segment self-generated from externally caused optic flow through sensorimotor mismatch circuits},
journal = {Neural Networks},
volume = {181},
pages = {106716},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106716},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024006403},
author = {Matthias Brucklacher and Giovanni Pezzulo and Francesco Mannella and Gaspare Galati and Cyriel M.A. Pennartz},
keywords = {Generative model, Object segmentation, Predictive coding, Optic flow, Sensorimotor},
abstract = {Efficient sensory detection requires the capacity to ignore task-irrelevant information, for example when optic flow patterns created by egomotion need to be disentangled from object perception. To investigate how this is achieved in the visual system, predictive coding with sensorimotor mismatch detection is an attractive starting point. Indeed, experimental evidence for sensorimotor mismatch signals in early visual areas exists, but it is not understood how they are integrated into cortical networks that perform input segmentation and categorization. Our model advances a biologically plausible solution by extending predictive coding models with the ability to distinguish self-generated from externally caused optic flow. We first show that a simple three neuron circuit produces experience-dependent sensorimotor mismatch responses, in agreement with calcium imaging data from mice. This microcircuit is then integrated into a neural network with two generative streams. The motor-to-visual stream consists of parallel microcircuits between motor and visual areas and learns to spatially predict optic flow resulting from self-motion. The second stream bidirectionally connects a motion-selective higher visual area (mHVA) to V1, assigning a crucial role to the abundant feedback connections to V1: the maintenance of a generative model of externally caused optic flow. In the model, area mHVA learns to segment moving objects from the background, and facilitates object categorization. Based on shared neurocomputational principles across species, the model also maps onto primate visual cortex. Our work extends Hebbian predictive coding to sensorimotor settings, in which the agent actively moves - and learns to predict the consequences of its own movements.}
}
@incollection{BRAGHIN2025871,
title = {Chapter 53 - Online Privacy},
editor = {John R. Vacca},
booktitle = {Computer and Information Security Handbook (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {871-890},
year = {2025},
isbn = {978-0-443-13223-0},
doi = {https://doi.org/10.1016/B978-0-443-13223-0.00053-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443132230000539},
author = {Chiara Braghin and Marco Cremonini},
keywords = {Cookies, Data protection, Generative AI, Informed consent, Online privacy, Predictive technology, Regulations, Surveillance, Web tracking},
abstract = {Privacy is fading away from the online world, with powerful actors that have worked to invade everyone's privacy for commercial and surveillance purposes. However, limiting the analysis on the role of those actors is overly simplistic because the state of online privacy is the result of many different contributions and an historical trend. In this chapter, we analyze several facets of the lack of online privacy, some idiosyncrasies exhibited by privacy advocates, together with characteristics of the industry mostly responsible of massive data collecting. An issue not sufficiently debated is the asserted effectiveness of data-centered predictive technologies, which should be openly inquired. We also introduce the prevalent market-oriented assumption and individualistic approach at the base of online privacy. The regulatory approach to online privacy is also considered. EU's GDPR is commonly considered the reference case of modern privacy regulations, but its success hinders critical aspects that require a close examination, from the quirks of the institutional decision process, to the flaws of the informed consent principle. A glimpse on the likely problematic future is provided with a discussion on privacy related aspects of European Union, United Kingdom, and China's proposed generative AI policies. The last part of this chapter is dedicated to discuss some privacy technologies, their ambitious goals, the actual effectiveness, and the limitations, which often have been severely underestimated. There is no technical silver bullet for privacy and it seems highly unlikely that it could ever exist, because it is too multifaceted as a problem, often contradictory, strained between contrasting interests.}
}
@article{SHANG2024108454,
title = {Droplet-based single-cell sequencing: Strategies and applications},
journal = {Biotechnology Advances},
volume = {77},
pages = {108454},
year = {2024},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2024.108454},
url = {https://www.sciencedirect.com/science/article/pii/S0734975024001484},
author = {Yuting Shang and Zhengzheng Wang and Liqing Xi and Yantao Wang and Meijing Liu and Ying Feng and Juan Wang and Qingping Wu and Xinran Xiang and Moutong Chen and Yu Ding},
keywords = {Chip, Different modes, Droplet microfluidics, Omics, Single cell level},
abstract = {Notable advancements in single-cell omics technologies have not only addressed longstanding challenges but also enabled unprecedented studies of cellular heterogeneity with unprecedented resolution and scale. These strides have led to groundbreaking insights into complex biological systems, paving the way for a more profound comprehension of human biology and diseases. The droplet microfluidic technology has become a crucial component in many single-cell sequencing workflows in terms of throughput, cost-effectiveness, and automation. Utilizing a microfluidic chip to encapsulate and profile individual cells within droplets has significantly improved single-cell research. Therefore, this review aims to comprehensively elaborate the droplet microfluidics-assisted omics methods from a single-cell perspective. The strategies for using droplet microfluidics in the realms of genomics, epigenomics, transcriptomics, and proteomics analyses are first introduced. On this basis, the focus then turns to the latest applications of this technology in different sequencing patterns, including mono- and multi-omics. Finally, the challenges and further perspectives of droplet-based single-cell sequencing in both foundational research and commercial applications are discussed.}
}
@article{HUANG2025102643,
title = {Identifying contextual content-based risk drivers for advanced risk management strategies},
journal = {Research in International Business and Finance},
volume = {73},
pages = {102643},
year = {2025},
issn = {0275-5319},
doi = {https://doi.org/10.1016/j.ribaf.2024.102643},
url = {https://www.sciencedirect.com/science/article/pii/S0275531924004367},
author = {Shirley Hsueh-Li Huang and Guo-Hsin Hu and Ming-Fu Hsu},
keywords = {Artificial intelligence, Risk management, Decision making, Text mining, Word embedding},
abstract = {This research proposes a profound contextual topic identifier that incorporates topic modelling and a word embedding technique to discover and quantify corporate risks from its self-identified risk disclosures and examines the association between each risk type and operating performance via artificial intelligence (AI) technique. Via topic modelling adoption, we are able to discover the most essential risks confronted by corporates in the near future and evaluate how they respond to these risks. To gain deeper insight, the study performs a bidirectional encoder representation from transformers (BERT) (one type of word embedding approach) to extract and quantify the semantic features embedded into each risk disclosure. The results show that operating performance significantly and positively relates to corporate-specific risks. This study offers solid and direct support for authorities that set accounting principles to encourage firm managers to add a new section on risk factors in annual reports.}
}
@article{BRIGHENTI2024118971,
title = {Bridge management systems: A review on current practice in a digitizing world},
journal = {Engineering Structures},
volume = {321},
pages = {118971},
year = {2024},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2024.118971},
url = {https://www.sciencedirect.com/science/article/pii/S0141029624015335},
author = {Francesca Brighenti and Valeria Francesca Caspani and Giancarlo Costa and Pier Francesco Giordano and Maria Pina Limongelli and Daniele Zonta},
keywords = {Bridge management system, Digitalization, Automation, Inspection, Structural health monitoring, Digital twin, Decision making, Life-cycle analysis},
abstract = {Bridges are subject to a plethora of deterioration phenomena, such as corrosion, fatigue, and damaging events (e.g., truck impacts and earthquakes) that can affect their performance and compromise functionality and safety. These challenges, along with the expansion of physical infrastructures and limited economic resources, underscore the need for effective management systems to enhance the efficiency of maintenance activities. To address this need, bridge operators have developed Bridge Management Systems (BMSs), which assist in ensuring safe operations while optimizing budget allocation and intervention strategies. Existing state-of-the-art studies on BMSs, dating back several years, primarily focus on specific aspects of BMSs and do not provide exhaustive insight into the implemented processes. Consequently, a comprehensive analysis of the entire process is currently lacking. This review organizes and discusses the key features of existing BMSs and introduces a novel definition of BMS modules—data management, diagnosis, prognosis, and decision-making—where consensus is currently lacking. The paper covers the historical and current practices of the most common BMSs, outlining the main principles of each phase along with their critical aspects and future trends.}
}
@incollection{GOLEMBOVSKA2025506,
title = {Technologies of Interactive Art},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {506-527},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00152-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001528},
author = {Olena Golembovska and Vyacheslav Kharchenko},
keywords = {Abstract painting, Artificial intelligence, Augmented and virtual reality, Big Data, Digital media, Information technologies, Interactive art, Internet of Things},
abstract = {The entry is devoted to interactive art and information technologies, the role of IT in creating new types of art, the authors of which are not only artists or creators, but also viewers. This becomes a new and important trend of modern types of interactive art, making it more democratic and insightful, accessible, and independent, flexible, and unpredictable, which is life itself. The entry provides a detailed definition of the main concepts for IT supported interactive art and its varieties and presents the types of IT used to create art objects. Cases of using IT in interactive art are presented on the example of the "Smart Gallery of Abstract Painting" project demonstrating the possibilities of using AR in abstract painting to create interactive paintings. New challenges and possibilities of Aritificial Intelligence and Augmented Reaility technologies application for development interactive art are discussed.}
}
@article{BOWERS202474,
title = {Reduced resting-state periodic beta power in adults who stutter is related to sensorimotor control of speech execution},
journal = {Cortex},
volume = {181},
pages = {74-92},
year = {2024},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2024.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0010945224002703},
author = {Andrew Bowers and Daniel Hudock},
keywords = {Independent component analysis, Event-related desynchronization, Parameterization, Resting-state},
abstract = {Objective
The primary aim of the current study was to determine whether adults who stutter (AWS) present with anomalous periodic beta (β) rhythms when compared to typically fluent adults in the eyes-open resting state. A second aim was to determine whether lower β power in the RS is related to a measure of β event-related desynchronization (ERD) during syllable sequence execution.
Methods
EEG data was collected from 128 channels in a 5 min, eyes-open resting state condition and from a syllable sequence repetition task. Temporal independent component analysis (ICA) was used to separate volume conducted EEG sources and to find a set of component weights common to the RS and syllable repetition task. Both traditional measures of power spectral density (PSD) and parameterized spectra were computed for components showing peaks in the β band (13–30 Hz). Parameterization was used to evaluate separable components adjusted for the 1/f part of the spectrum.
Results
ICA revealed frontal-parietal midline and lateral sensorimotor (μ) components common to the RS and a syllable repetition task with peaks in the β band. The entire spectrum for each component was modeled using the FOOOF algorithm. Independent samples t-tests revealed significantly lower periodic β in midline central-parietal and lateral sensorimotor components in AWS. Regression analysis suggested a significant relationship between left periodic sensorimotor β power in the RS and ERD during syllable sequence execution.
Conclusions
Findings suggest that periodic β peaks in the spectrum are related to hypothesized underlying pathophysiological differences in stuttering, including midline rhythms associated the default mode network (DMN) and lateral sensorimotor rhythms associated with the control of movement.}
}
@article{MA2025128916,
title = {Fully end-to-end EEG to speech translation using multi-scale optimized dual generative adversarial network with cycle-consistency loss},
journal = {Neurocomputing},
volume = {616},
pages = {128916},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128916},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224016874},
author = {Chen Ma and Yue Zhang and Yina Guo and Xin Liu and Hong Shangguan and Juan Wang and Luqing Zhao},
keywords = {Brain–computer interface (BCI), Fully end-to-end, Dual–dual generative adversarial network (Dual-DualGAN), Dual generative adversarial network based on multi-scale optimization and cycle-consistency loss (MSCC-dualGAN), Cross-domain},
abstract = {Decoding auditory evoked electroencephalographic (EEG) signals to correlate them with speech acoustic features and construct transitional signals between different domain signals is a challenging and fascinating research topic. Brain–computer interface (BCI) technologies that incorporate auditory evoked potentials (AEPs) can not only leverage encoder–decoder architectures for signal decoding, but also employ generative adversarial networks (GANs) to translate from human neural activity to speech (T-HNAS). However, in previous research, the cascading ratio of transitional signals leads to varying degrees of information loss in the two-domain signals, and the optimal ratio of transitional signals differs across datasets, impacting the translation effectiveness. To address these issues, an improved dual generative adversarial network based on multi-scale optimization and cycle-consistency loss (MSCC-DualGAN) is proposed. We leverage the feature of cycle consistency loss, which facilitates cross-modal signal conversion, to replace transitional signals and maintain the integrity of signals in both domains during the loss computation process. Multi-scale optimization is utilized to refine the details of signals downsampled by the network, improving the similarity between features, thus enabling efficient, fully end-to-end EEG to speech translation. Furthermore, to validate the efficacy of this network, we construct a new EEG dataset and conduct studies using metrics such as mel cepstral distortion (MCD), pearson correlation coefficient (PCC), and structural similarity index measure (SSIM). Experimental results demonstrate that this new network significantly outperforms previous methods on auditory stimulus datasets.}
}
@article{YEVSIKOV2024109414,
title = {CADefender: Detection of unknown malicious AutoLISP computer-aided design files using designated feature extraction and machine learning methods},
journal = {Engineering Applications of Artificial Intelligence},
volume = {138},
pages = {109414},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109414},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624015720},
author = {Alexander Yevsikov and Trivikram Muralidharan and Tomer Panker and Nir Nissim},
keywords = {Computer-aided design, Auto list processing, Machine learning, Malware detection, Feature extraction},
abstract = {Computer-aided design (CAD) files are used to create digital designs for various structures – from the smallest chips in the high-tech industry to large-scale buildings and bridges in the civil engineering space. We found that most exploits and malicious payloads are deployed through Auto List Processing (AutoLISP) source code (LSP) or Fast Load AutoLISP (FAS) files, which are non-executable files (NEFs) containing scripts in the AutoLISP language that are native to AutoCAD; While antivirus software is capable of detecting many malicious CAD files, the potential to improve protection by using a dedicated machine learning (ML) based detection solution remains, especially against unknown and sophisticated CAD malware. In this study, we are the first to propose designated feature extraction methods and a robust framework aimed at the detection of known and unknown AutoLISP malware using ML algorithms. To accomplish this, we examined the structure, functionality, and ecosystems of AutoLISP files and collected the largest known representative collection of LSP files consisting of 6418 malicious and benign files (labeled and verified). We then explored the use of two novel static-analysis-based feature extraction methods (knowledge-based and structural) designated for LSP files to extract a discriminative set of informative features, which can subsequently be used by ML models to detect malicious LSP files. These two feature extraction methods serve as the basis of the proposed detection framework, whose performance we comprehensively compare to both widely used antiviruses and baseline ML models based on existing feature extraction methods, including MinHash, Bidirectional Encoder Representations from Transformers (BERT), and n-gram. Our results highlight our methods' contributions to the detection of unknown AutoLISP malware and demonstrate their ability to outperform existing methods. The best performance in the task of unknown malicious LSP file detection was obtained by the Artificial Neural Networks (ANN) model trained on 100 knowledge-based features, which obtained a true positive rate (TPR) of 99.49% with a false positive rate (FPR) of 0.57%. Our framework's role in explainability is also highlighted, as we also present the prominent features that contribute most to the model's detection capabilities; this information can be used for explainability purposes. We conclude by evaluating the proposed framework's ability to detect a malicious file from an unknown AutoLISP malware family and by evaluating our framework on an additional independent test set that originated from another source, scenarios that are often faced by malware detection solutions.}
}
@article{CHEN2024102236,
title = {Recent advances in the integration of protein mechanics and machine learning},
journal = {Extreme Mechanics Letters},
volume = {72},
pages = {102236},
year = {2024},
issn = {2352-4316},
doi = {https://doi.org/10.1016/j.eml.2024.102236},
url = {https://www.sciencedirect.com/science/article/pii/S2352431624001160},
author = {Yen-Lin Chen and Shu-Wei Chang},
keywords = {Machine learning, Protein mechanics, Protein property prediction},
abstract = {Mechanics underlies protein properties and behavior. From a theoretical standpoint, it is possible to derive these based on physical rules. This is appealing because they provide insights into physiology and disease, as well as aid in protein engineering; however, the convoluted nature of the biological system and current computational speeds limit its feasibility. Machine learning (ML) architectures are known for their ability to make inferences on complex data, such as the relationship between protein mechanics, properties, and behavior. Substantial efforts have been made to learn such correlations in tasks such as the prediction of structure, stability, natural frequency, mechanical strength, folding rate, solubility, and function. Each of these properties is interconnected through protein mechanics, and it is not surprising that the methods used in these tasks overlap highly in model input and architecture. In this review, we evaluate ML methods for the seven aforementioned prediction tasks to identify current trends in ML research in the field of protein sciences, focusing on the input and model architecture of each method. A short overview of de novo protein design is also provided. Finally, we highlight trends in the application of ML methods in the field of protein science, as well as directions for future improvements.}
}
@article{JURISICA2024102006,
title = {Explainable biology for improved therapies in precision medicine: AI is not enough},
journal = {Best Practice & Research Clinical Rheumatology},
volume = {38},
number = {4},
pages = {102006},
year = {2024},
note = {Genetics of Rheumatic Diseases},
issn = {1521-6942},
doi = {https://doi.org/10.1016/j.berh.2024.102006},
url = {https://www.sciencedirect.com/science/article/pii/S1521694224000779},
author = {I Jurisica},
keywords = {Precision medicine, Rheumatoid arthritis, Artificial intelligence, Integrative computational biology},
abstract = {Technological advances and high-throughput bio-chemical assays are rapidly changing ways how we formulate and test biological hypotheses, and how we treat patients. Most complex diseases arise on a background of genetics, lifestyle and environment factors, and manifest themselves as a spectrum of symptoms. To fathom intricate biological processes and their changes from healthy to disease states, we need to systematically integrate and analyze multi-omics datasets, ontologies, and diverse annotations. Without proper management of such complex biological and clinical data, artificial intelligence (AI) algorithms alone cannot be effectively trained, validated, and successfully applied to provide trustworthy and patient-centric diagnosis, prognosis and treatment. Precision medicine requires to use multi-omics approaches effectively, and offers many opportunities for using AI, “big data” analytics, and integrative computational biology workflows. Advances in optical and biochemical assay technologies including sequencing, mass spectrometry and imaging modalities have transformed research by empowering us to simultaneously view all genes expressed, identify proteome-wide changes, and assess interacting partners of each individual protein within a dynamically changing biological system, at an individual cell level. While such views are already having an impact on our understanding of healthy and disease conditions, it remains challenging to extract useful information comprehensively and systematically from individual studies, ensure that signal is separated from noise, develop models, and provide hypotheses for further research. Data remain incomplete and are often poorly connected using fragmented biological networks. In addition, statistical and machine learning models are developed at a cohort level and often not validated at the individual patient level. Combining integrative computational biology and AI has the potential to improve understanding and treatment of diseases by identifying biomarkers and building explainable models characterizing individual patients. From systematic data analysis to more specific diagnostic, prognostic and predictive biomarkers, drug mechanism of action, and patient selection, such analyses influence multiple steps from prevention to disease characterization, and from prognosis to drug discovery. Data mining, machine learning, graph theory and advanced visualization may help identify diagnostic, prognostic and predictive biomarkers, and create causal models of disease. Intertwining computational prediction and modeling with biological experiments leads to faster, more biologically and clinically relevant discoveries. However, computational analysis results and models are going to be only as accurate and useful as correct and comprehensive are the networks, ontologies and datasets used to build them. High quality, curated data portals provide the necessary foundation for translational research. They help to identify better biomarkers, new drugs, precision treatments, and should lead to improved patient outcomes and their quality of life. Intertwining computational prediction and modeling with biological experiments, efficiently and effectively leads to more useful findings faster.}
}
@article{HERNANDEZPENALOZA2024100196,
title = {General context and relevant public datasets available for improving pathways in Paediatric Cancer applying Artificial Intelligence. A review},
journal = {EJC Paediatric Oncology},
volume = {4},
pages = {100196},
year = {2024},
issn = {2772-610X},
doi = {https://doi.org/10.1016/j.ejcped.2024.100196},
url = {https://www.sciencedirect.com/science/article/pii/S2772610X24000564},
author = {Gustavo Hernández-Peñaloza and Silvia Uribe and Francisco Moreno García and Norbert Graf and Federico Álvarez},
keywords = {Childhood cancer, Paediatric oncology, Childhood cancer patient, Artificial Intelligence, Data, Use of data, Machine Learning, Federated Learning},
abstract = {Due to the promise of transforming healthcare and medicine that Artificial Intelligence (AI) has posed, the number of applications has increased exponentially. These applications range from screening and disease diagnosis to prognosis, treatment planning, and follow-up. In complex topics such as childhood cancer, these techniques are being expanded with the ambition of improving the quality of care by allowing healthcare professionals to make more informed decisions. However, the adequate application of such techniques heavily depends on the data, which creates a set of challenges including collection, bias, and scarcity among others. Furthermore, ethical, legal, and regulatory frameworks increase even more the difficulties to develop AI-powered solutions. In this paper, we present an exhaustive literature review to identify and analyse public datasets targeting two common childhood cancer types, such as neuroblastoma and nephroblastoma. Moreover, the complex context for the development of AI- based software solutions is outlined. It includes the description of the most relevant techniques to address problems associated with data sharing and training. Finally, a set of code snippets is provided to perform exploratory analysis for the available data.}
}
@article{WALLER2024103940,
title = {Questionable devices: Applying a large language model to deliberate carbon removal},
journal = {Environmental Science & Policy},
volume = {162},
pages = {103940},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2024.103940},
url = {https://www.sciencedirect.com/science/article/pii/S1462901124002740},
author = {Dr. Laurie Waller and Dr. David Moats and Dr. Emily Cox and Dr. Rob Bellamy},
keywords = {Carbon removal, Deliberation, Devices, Publics, Experiments in participation, Large language models, Generative AI},
abstract = {This paper presents a device-centred approach to deliberation, developed in deliberative workshops appraising methods for removing carbon dioxide from the air. Our approach involved deploying the Large Language Model application ChatGPT (sometimes termed “generative AI”) to elicit questions and generate texts about carbon removal. We develop the notion of the “questionable” device to foreground the informational unruliness ChatGPT introduced into the deliberations. The analysis highlights occasions where the deliberative apparatus became a focus of collective critique, including over: issue definitions, expert-curated resources, lay identities and social classifications. However, in this set-up ChatGPT was all too often engaged unquestioningly as an instrument for informing discussion; its instrumental lure disguising the unruliness it introduced into the workshops. In concluding, we elaborate the notion of questionable devices and reflect on the way carbon removal has been “devised” as a field in want of informed deliberation.}
}
@article{TANGWARAGORN2024100680,
title = {Analyzing key drivers of digital transformation: A review and framework},
journal = {Journal of Industrial Information Integration},
volume = {42},
pages = {100680},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100680},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001237},
author = {Pattharin Tangwaragorn and Nuttirudee Charoenruk and Wattana Viriyasitavat and Chatpong Tangmanee and Prasert Kanawattanachai and Danupol Hoonsopon and Vitara Pungpapong and Ra-Pee Pattanapanyasat and Sawitree Boonpatcharanon and Phoranee Rhuwadhana},
keywords = {Digital transformation, Indicators, Systematic literature review (SLR), Survey},
abstract = {In the digital era, comprehending the factors that drive digital transformation (DT) is crucial for organizations aiming to navigate the evolving technological landscape. This systematic review focuses on identifying and analyzing the key drivers of DT, drawing insights from 19 recognized DT readiness indexes. We examine the dimensions and indicators used across these indexes, highlighting their relevance and frequency of occurrence to uncover the primary factors influencing DT. The findings are synthesized into a comprehensive framework that offers strategic guidelines for organizations. This framework not only aids in understanding the core drivers of DT but also provides practical recommendations for implementing effective DT strategies. By emphasizing the critical drivers rather than the indexes themselves, our study shifts the focus towards actionable insights that can guide entities, policymakers, and academics in the digital transformation journey. The urgency of DT, further accelerated by the COVID-19 pandemic, enhances the relevance of our findings in shaping resilient and adaptable digital strategies.}
}
@article{XIANG2025109761,
title = {Advancements and challenges in coverless image steganography: A survey},
journal = {Signal Processing},
volume = {228},
pages = {109761},
year = {2025},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2024.109761},
url = {https://www.sciencedirect.com/science/article/pii/S0165168424003815},
author = {Xuyu Xiang and Yang Tan and Jiaohua Qin and Yun Tan},
keywords = {Information security, Information hiding, Steganography, Coverless steganography},
abstract = {Coverless image steganography has emerged as a significant research direction in the field of steganography in recent years. Unlike traditional image steganography, it does not require modifying the cover image to achieve information hiding. This review aims to systematically summarize the research progress and challenges in coverless image steganography. Firstly, the paper introduces the basic principles and classification methods of coverless image steganography, including embedding methods based on low-level image features and those combining advanced semantic features from deep learning. Secondly, it discusses key research achievements in this field, such as novel embedding algorithms, efficient extraction methods, and robustness enhancement techniques against various attacks. Additionally, the review highlights major challenges faced by current coverless image steganography, including difficulties in secret information extraction, capacity limitations, and practicality issues, and explores potential solutions and future research directions. Through comprehensive analysis of existing literature, the review aims to provide researchers with a holistic perspective, fostering further development and application of coverless image steganography. The paper includes 124 key contributions, offering a comprehensive overview of coverless image steganography, covering its fundamental principles, research progress, challenges, and solutions.}
}
@article{ZHANG2024102823,
title = {A novel noiselayer-decoder driven blind watermarking network},
journal = {Displays},
volume = {85},
pages = {102823},
year = {2024},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2024.102823},
url = {https://www.sciencedirect.com/science/article/pii/S0141938224001872},
author = {Xiaorui Zhang and Rui Jiang and Wei Sun and Sunil Kr. Jha},
keywords = {Blind watermarking, Deep learning, Neural networks, Denoising diffusion probabilistic models},
abstract = {Most blind watermarking methods adopt the Encode-Noiselayer-Decoder network architecture, called END. However, there are issues that impact the imperceptibility and robustness of the watermarking, such as the encoder blindly embedding redundant features, adversarial training failing to simulate unknown noise effectively, and the limited capability of single-scale feature extraction. To address these challenges, we propose a new Noiselayer-Decoder-driven blind watermarking network, called ND-END, which leverages prior knowledge of the noise layer and features extracted by the decoder to guide the encoder for generating images with fewer redundant modifications, enhancing the imperceptibility. To effectively simulate the unknown noise caused during adversarial training, we introduce an unknown noise layer based on the guided denoising diffusion probabilistic model, which gradually modifies the mean value of the predicted noise during the image generation process. It produces unknown noise images that closely resemble the encoded images but can mislead the decoder. Moreover, we propose a multi-scale spatial-channel feature extraction method for extracting multi-scale message features from the noised image, which aids in message extraction. Experimental results demonstrate the effectiveness of our model, ND-END achieves a lower bit error rate while improving the peak signal-to-noise ratio by approximately 6 dB (from about 33.5 dB to 39.5 dB).}
}
@article{AMONEIT2024144011,
title = {Green chemistry and responsible research and innovation: Moving beyond the 12 principles},
journal = {Journal of Cleaner Production},
volume = {484},
pages = {144011},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.144011},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624034607},
author = {Madita Amoneit and Dagmara Weckowska and Stephanie Spahr and Olaf Wagner and Mohsen Adeli and Inka Mai and Rainer Haag},
keywords = {Circular economy, Green chemistry, Responsible research and innovation, Responsible roadmapping, SDGs},
abstract = {Green chemistry focuses on designing products and processes that minimize hazardous substances and address pollution, resource depletion, and climate change. Green chemistry products and processes could contribute to the transition to circular economy and reaching Sustainable Development Goals. However, green chemistry philosophy offers none or little guidance on social, ethical, economic, or political aspects that are inherent to complex transition processes. Such broad and future-oriented considerations are at the heart of ‘Responsible Research and Innovation’ (RRI) approach but to date the ideas of RRI and green chemistry remain largely unconnected. This study aims to shed light on how RRI and green chemistry approaches can be combined. A refined responsible roadmapping method is proposed to help researchers to go beyond the 12 principles of green chemistry and develop inter- and transdisciplinary research agendas that address technical, environmental as well as social, ethical, economic and political considerations. The method was piloted in three research projects aspiring to develop sustainable and safe chemical processes and their applications. The study demonstrates that at the early stage of research planning, the responsible roadmapping method can facilitate the integration of RRI and green chemistry practices and the development of interdisciplinary research plans, which address technical, environmental, socio-ethical, economic and political dimensions. The implications of our study for future research on roadmapping methods as well as for policy and innovation practice are discussed.}
}
@article{HOSSAIN2024951,
title = {Artificial Intelligence-Driven Vehicle Fault Diagnosis to Revolutionize Automotive Maintenance: A Review},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {141},
number = {2},
pages = {951-996},
year = {2024},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2024.056022},
url = {https://www.sciencedirect.com/science/article/pii/S152614922400290X},
author = {Md Naeem Hossain and Md Mustafizur Rahman and Devarajan Ramasamy},
keywords = {Artificial intelligence, machine learning, deep learning, vehicle fault diagnosis, predictive maintenance},
abstract = {Conventional fault diagnosis systems have constrained the automotive industry to damage vehicle maintenance and component longevity critically. Hence, there is a growing demand for advanced fault diagnosis technologies to mitigate the impact of these limitations on unplanned vehicular downtime caused by unanticipated vehicle breakdowns. Due to vehicles’ increasingly complex and autonomous nature, there is a growing urgency to investigate novel diagnosis methodologies for improving safety, reliability, and maintainability. While Artificial Intelligence (AI) has provided a great opportunity in this area, a systematic review of the feasibility and application of AI for Vehicle Fault Diagnosis (VFD) systems is unavailable. Therefore, this review brings new insights into the potential of AI in VFD methodologies and offers a broad analysis using multiple techniques. We focus on reviewing relevant literature in the field of machine learning as well as deep learning algorithms for fault diagnosis in engines, lifting systems (suspensions and tires), gearboxes, and brakes, among other vehicular subsystems. We then delve into some examples of the use of AI in fault diagnosis and maintenance for electric vehicles and autonomous cars. The review elucidates the transformation of VFD systems that consequently increase accuracy, economization, and prediction in most vehicular sub-systems due to AI applications. Indeed, the limited performance of systems based on only one of these AI techniques is likely to be addressed by combinations: The integration shows that a single technique or method fails its expectations, which can lead to more reliable and versatile diagnostic support. By synthesizing current information and distinguishing forthcoming patterns, this work aims to accelerate advancement in smart automotive innovations, conforming with the requests of Industry 4.0 and adding to the progression of more secure, more dependable vehicles. The findings underscored the necessity for cross-disciplinary cooperation and examined the total potential of AI in vehicle default analysis.}
}
@article{KATTNIG2024106053,
title = {Assessing trustworthy AI: Technical and legal perspectives of fairness in AI},
journal = {Computer Law & Security Review},
volume = {55},
pages = {106053},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106053},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924001195},
author = {Markus Kattnig and Alessa Angerschmid and Thomas Reichel and Roman Kern},
keywords = {Fairness, Non-discrimination, Group fairness, Individual fairness, AI Act, Trustworthy AI, Bias},
abstract = {Artificial Intelligence systems are used more and more nowadays, from the application of decision support systems to autonomous vehicles. Hence, the widespread use of AI systems in various fields raises concerns about their potential impact on human safety and autonomy, especially regarding fair decision-making. In our research, we primarily concentrate on aspects of non-discrimination, encompassing both group and individual fairness. Therefore, it must be ensured that decisions made by such systems are fair and unbiased. Although there are many different methods for bias mitigation, few of them meet existing legal requirements. Unclear legal frameworks further worsen this problem. To address this issue, this paper investigates current state-of-the-art methods for bias mitigation and contrasts them with the legal requirements, with the scope limited to the European Union and with a particular focus on the AI Act. Moreover, the paper initially examines state-of-the-art approaches to ensure AI fairness, and subsequently, outlines various fairness measures. Challenges of defining fairness and the need for a comprehensive legal methodology to address fairness in AI systems are discussed. The paper contributes to the ongoing discussion on fairness in AI and highlights the importance of meeting legal requirements to ensure fairness and non-discrimination for all data subjects.}
}
@article{CHANIPOSSE2024395,
title = {Origin and early diversification of a high Andean rove-beetle clade: Corisantis gen. nov., phylogeny, and historical biogeography},
journal = {Zoologischer Anzeiger},
volume = {313},
pages = {395-416},
year = {2024},
issn = {0044-5231},
doi = {https://doi.org/10.1016/j.jcz.2024.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0044523124001116},
author = {Mariana R. Chani-Posse and Maryzender E. Rodríguez-Melgarejo},
abstract = {A new genus, Corisantis gen. nov., within the Andean clade (AC) of the rove-beetle subtribe Philonthina, is described and illustrated, including 14 species. This new genus comprises six valid species formerly assigned to Belonuchus Nordmann (B. candens (Erichson, 1840) and B. pulchripennis Bernhauer, 1908) and Philonthus (Ph. auripennis Bernhauer, 1916; Ph. caliensis Bernhauer, 1916; Ph. excellens Bernhauer, 1916; and Ph. whymperi Sharp, 1891), along with nine newly described species: C. ancashensis sp. nov., C. bongarensis sp. nov., C. cajamarcanus sp. nov., C. columbiensis sp. nov., C. ecuatoriensis sp. nov., C. grandis sp. nov., C. levis sp. nov., C. magdalensis sp. nov., and C. napoensis sp. nov. The four species originally classified under Philonthus are reassigned to Corisantis with the following new combinations: C. auripennis (Bernhauer, 1916) comb. nov., C. caliensis (Bernhauer, 1916) comb. nov., C. excellens (Bernhauer, 1916) comb. nov., and C. whymperi (Sharp, 1891) comb. nov. Additionally, Belonuchus pulchripennis is synonymized under Belonuchus candens (=B. pulchripennis syn. nov.), which is also transferred to Corisantis as C. candens (Erichson, 1840) comb. nov. Lectotypes are designated for B. pulchripennis Bernhauer (1908), Philonthus candens Erichson (1840) and Philonthus whymperi Sharp (1891). This study provides diagnoses, redescriptions or descriptions, illustrations, a distribution map, and an identification key for the species of Corisantis. Phylogenetic analyses, employing three methodologies—Maximum Parsimony, Maximum Likelihood, and Bayesian Inference—consistently support the monophyly of Corisantis and confirm its sister-group relationship with Leptopeltoides Chani-Posse and Asenjo, with both genera being sister to Leptopeltus Bernhauer. Additionally, biogeographical analyses using two methodologies—BBM (Bayesian Binary MCMC) and S-DIVA (Statistical Dispersal-Vicariance Analysis)—suggest that the South American Transition Zone was the center of origin and early diversification for these three genera.}
}
@article{RIDDLE2024101625,
title = {Surgical Simulation: Virtual Reality to Artificial Intelligence},
journal = {Current Problems in Surgery},
volume = {61},
number = {11},
pages = {101625},
year = {2024},
issn = {0011-3840},
doi = {https://doi.org/10.1016/j.cpsurg.2024.101625},
url = {https://www.sciencedirect.com/science/article/pii/S0011384024001862},
author = {Elijah W. Riddle and Divya Kewalramani and Mayur Narayan and Daniel B. Jones}
}
@article{DICKSON2024103447,
title = {"Eat up. Save Earth." Alternative proteins and the myth of inevitable sustainability},
journal = {Journal of Rural Studies},
volume = {112},
pages = {103447},
year = {2024},
issn = {0743-0167},
doi = {https://doi.org/10.1016/j.jrurstud.2024.103447},
url = {https://www.sciencedirect.com/science/article/pii/S0743016724002511},
author = {Elissa Dickson and Nathan Clay},
keywords = {Agri-food tech, Alternative protein, Climate change, Livestock, Sustainability, Technology},
abstract = {The emerging agri-food tech sector promises to solve myriad environmental problems. This article considers the sociotechnical imaginaries that animate these claims. We focus on plant-based meat and dairy substitutes, or 'alternative proteins' (APs). To examine how APs are constructed as environmental solutions, we analyzed marketing materials, sustainability reports, and interviews. Our study illustrates how environmental metrics (Life Cycle Assessments) and corporate marketing make environmental issues legible to agri-industrial logics by reducing them to a narrow, technical issue: inefficient livestock. To critique this problem closure, we develop the concept of inevitable sustainability–where the increased adoption of a technology is equated with the assured reduction of environmental harm. We caution that APs support a neoliberal model of environmental governance that propagates apolitical and deterritorialized solutions. To reflect on the limits of agri food tech environmental fixes, we discuss three myths surrounding inevitable sustainability. We outline this concept's applicability to similar instances of environmental solutionism in agri-food tech and beyond.}
}
@article{2024e1,
title = {PDF of the Full Issue},
journal = {Annals of Emergency Medicine},
volume = {84},
number = {4, Supplement 1},
pages = {e1-e251},
year = {2024},
note = {ACEP Research Forum 2024},
issn = {0196-0644},
doi = {https://doi.org/10.1016/S0196-0644(24)01011-4},
url = {https://www.sciencedirect.com/science/article/pii/S0196064424010114}
}
@article{TURKI2024e38448,
title = {A framework for integrating biomedical knowledge in Wikidata with open biological and biomedical ontologies and MeSH keywords},
journal = {Heliyon},
volume = {10},
number = {19},
pages = {e38448},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e38448},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024144799},
author = {Houcemeddine Turki and Khalil Chebil and Bonaventure F.P. Dossou and Chris Chinenye Emezue and Abraham Toluwase Owodunni and Mohamed Ali {Hadj Taieb} and Mohamed {Ben Aouicha}},
keywords = {Wikidata, Open biological and biomedical ontologies, MeSH keywords, Biomedical relation identification, Crowdsourcing, PubMed},
abstract = {This study presents a comprehensive framework to enhance Wikidata as an open and collaborative knowledge graph by integrating Open Biological and Biomedical Ontologies (OBO) and Medical Subject Headings (MeSH) keywords from PubMed publications. The primary data sources include OBO ontologies and MeSH keywords, which were collected and classified using SPARQL queries for RDF knowledge graphs. The semantic alignment between OBO ontologies and Wikidata was evaluated, revealing significant gaps and distorted representations that necessitate both automated and manual interventions for improvement. We employed pointwise mutual information to extract biomedical relations among the 5000 most common MeSH keywords in PubMed, achieving an accuracy of 89.40 % for superclass-based classification and 75.32 % for relation type-based classification. Additionally, Integrated Gradients were utilized to refine the classification by removing irrelevant MeSH qualifiers, enhancing overall efficiency. The framework also explored the use of MeSH keywords to identify PubMed reviews supporting unsupported Wikidata relations, finding that 45.8 % of these relations were not present in PubMed, indicating potential inconsistencies in Wikidata. The contributions of this study include improved methodologies for enriching Wikidata with biomedical information, validated semantic alignments, and efficient classification processes. This work enhances the interoperability and multilingual capabilities of biomedical ontologies and demonstrates the critical role of MeSH keywords in verifying semantic relations, thereby contributing to the robustness and accuracy of collaborative biomedical knowledge graphs.}
}
@article{SUN20244065,
title = {RWNeRF: Robust Watermarking Scheme for Neural Radiance Fields Based on Invertible Neural Networks},
journal = {Computers, Materials and Continua},
volume = {80},
number = {3},
pages = {4065-4083},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.053115},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824006258},
author = {Wenquan Sun and Jia Liu and Weina Dong and Lifeng Chen and Fuqiang Di},
keywords = {Neural radiance fields, 3D scene, robust, watermarking, invertible neural networks},
abstract = {As neural radiance fields continue to advance in 3D content representation, the copyright issues surrounding 3D models oriented towards implicit representation become increasingly pressing. In response to this challenge, this paper treats the embedding and extraction of neural radiance field watermarks as inverse problems of image transformations and proposes a scheme for protecting neural radiance field copyrights using invertible neural network watermarking. Leveraging 2D image watermarking technology for 3D scene protection, the scheme embeds watermarks within the training images of neural radiance fields through the forward process in invertible neural networks and extracts them from images rendered by neural radiance fields through the reverse process, thereby ensuring copyright protection for both the neural radiance fields and associated 3D scenes. However, challenges such as information loss during rendering processes and deliberate tampering necessitate the design of an image quality enhancement module to increase the scheme’s robustness. This module restores distorted images through neural network processing before watermark extraction. Additionally, embedding watermarks in each training image enables watermark information extraction from multiple viewpoints. Our proposed watermarking method achieves a PSNR (Peak Signal-to-Noise Ratio) value exceeding 37 dB for images containing watermarks and 22 dB for recovered watermarked images, as evaluated on the Lego, Hotdog, and Chair datasets, respectively. These results demonstrate the efficacy of our scheme in enhancing copyright protection.}
}
@article{JIANG2024102883,
title = {When generative artificial intelligence meets multimodal composition: Rethinking the composition process through an AI-assisted design project},
journal = {Computers and Composition},
volume = {74},
pages = {102883},
year = {2024},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2024.102883},
url = {https://www.sciencedirect.com/science/article/pii/S8755461524000598},
author = {Jialei Jiang},
keywords = {Generative artificial intelligence, Multimodal composition process, Adobe Firefly, DALL·E, Wicked problems, Design, Writing studies},
abstract = {This study explores the integration of generative artificial intelligence (GenAI) design technologies, including Adobe Firefly and DALL·E, into the teaching and learning of multimodal composition. Through focus group discussions and case studies, this paper demonstrates the potential of GenAI in reshaping the various stages of the composition process, including invention, designing, and revising. The findings reveal that GenAI technologies have the potential to enhance students’ multimodal composition practices and offer alternative solutions to the wicked problems encountered during the design process. Specifically, GenAI facilitates invention by offering design inspirations and enriches designing by expanding, removing, and editing the student-produced design contents. The students in this study also shared their critical stance on the revision process by modifying and iterating their designs after their uses of GenAI. Through showcasing both the opportunities and challenges of GenAI technologies, this paper contributes to the ongoing scholarly conversations on multimodal composition and pedagogy. Moreover, the paper offers implications for the future research and teaching of GenAI-assisted multimodal composition projects, with the aim of encouraging thoughtful integration of GenAI technologies to foster critical AI literacy among college composition students.}
}
@article{BURIC2024e38287,
title = {Diagnosis of ophthalmologic diseases in canines based on images using neural networks for image segmentation},
journal = {Heliyon},
volume = {10},
number = {19},
pages = {e38287},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e38287},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024143186},
author = {Matija Buric and Sinisa Grozdanic and Marina Ivasic-Kos},
keywords = {U-net, Deep convolutional neural networks, Image segmentation, Image classification, Ophthalmological diseases in dogs, Eye diseases},
abstract = {The primary challenge in diagnosing ocular diseases in canines based on images lies in developing an accurate and reliable machine learning method capable of effectively segmenting and diagnosing these conditions through image analysis. Addressing this challenge, the study focuses on developing and rigorously evaluating a machine learning model for diagnosing ocular diseases in canines, employing the U-Net neural network architecture as a foundational element of this investigation. Through this extensive evaluation, the authors identified a model that exhibited good reliability, achieving prediction scores with an Intersection over Union (IoU) exceeding 80 %, as measured by the Jaccard index. The research methodology encompassed a systematic exploration of various neural network backbones (VGG, ResNet, Inception, EfficientNet) and the U-Net model, combined with an extensive model selection process and an in-depth analysis of a custom training dataset consisting of historical images of different medical symptoms and diseases in dog eyes. The results indicate a fairly high degree of accuracy in the segmentation and diagnosis of ocular diseases in canines, demonstrating the model's effectiveness in real-world applications. In conclusion, this potentially makes a significant contribution to the field by utilizing advanced machine-learning techniques to develop image-based diagnostic routines in veterinary ophthalmology. This model's successful development and validation offer a promising new tool for veterinarians and pet owners, enhancing early disease detection and improving health outcomes for canine patients.}
}
@article{ZHAO2024102847,
title = {A novel product shape design method integrating Kansei engineering and whale optimization algorithm},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102847},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102847},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004956},
author = {Xiang Zhao and Sharul {Azim Sharudin} and Han-Lu Lv},
keywords = {Product Shape Design, Kansei Engineering, Latent Dirichlet Allocation, Whale Optimization Algorithm, Whiskey Bottle Shape},
abstract = {The focus of consumer desire transitions from product functionality to emotional resonance in experience economy era, wherein emotional needs of users increasingly become a critical factor in product design. However, traditional approaches to product shape design often rely heavily on the designer’s intuition and experience, sometimes neglecting to incorporate emotional and humanistic elements into the product’s shape, thus resulting in inconsistencies in design results and quality. To address this challenge, this study introduces a novel method for emotionally driven product shape design that integrates Kansei engineering and the Whale Optimization Algorithm (WOA). This approach aims to fulfill consumer emotional demands related to product form and enhance overall user satisfaction. Firstly, the process utilized Python web crawlers to collect online product review texts and product images from e-commerce platforms. Next, Latent Dirichlet Allocation (LDA) and Analytic Hierarchy Process (AHP) were employed to extract representative emotional vocabularies, while representative samples were defined and deconstructed through clustering and morphological analysis. Then, semantic Differential (SD) questionnaires were distributed to collect consumer evaluations of product shape imagery, leading to the development of a user emotional prediction model for product shape. Then, WOA was introduced to optimize the performance of Back Propagation Neural Network (BPNN) and Support Vector Regression (SVR) models. Finally, Particle Swarm Optimization (PSO) and Seagull Optimization Algorithm (SOA) were employed to improve the prediction model, and the effect of these models was compared by the error method. This analysis explored the accuracy of these non-linear models in order to identify the optimal model for application in product form design cases. The scientific validity and effectiveness of this method were demonstrated utilizing whiskey bottle shape design as a case study. The results exhibit that under the WOA-BPNN model, the average error rates for four sets of perceptual words were 3.08%, 2.60%, 6.53%, and 5.70%, respectively. The WOA-based BPNN model outperformed other models in predictive ability, suggesting its utility as a valuable tool for designers during the front-end development stage of emotionally driven product form design.}
}
@article{KUMARI2024e00367,
title = {Evolution, integration, and challenges of 3D printing in pharmaceutical applications: A comprehensive review},
journal = {Bioprinting},
volume = {44},
pages = {e00367},
year = {2024},
issn = {2405-8866},
doi = {https://doi.org/10.1016/j.bprint.2024.e00367},
url = {https://www.sciencedirect.com/science/article/pii/S2405886624000393},
author = {Jyoti Kumari and Shalini Pandey and Krishna Kant Jangde and Palanirajan Vijayaraj Kumar and Dinesh Kumar Mishra},
keywords = {3D printing, Fused deposition modelling, Biodegradable filaments, Personalized medicine, Bioprinting},
abstract = {Three-dimensional (3D) printing involves fabricating objects from digital designs by sequentially layering materials along the X, Y, and Z axes. Although this technology has existed since the 1960s, its adoption in the pharmaceutical industry remains limited. This review examines the evolution of 3D printing and its emerging significance in pharmaceuticals. The technique offers numerous advantages, such as product customization, cost-effectiveness, and efficient material usage. Several methods—such as inkjet printing, extrusion printing, and beam-based printing—are employed, utilizing materials ranging from lactose and hydroxypropyl methylcellulose to bioinks like chitosan and hyaluronic acid. Among these techniques, fused deposition modelling (FDM) is particularly noteworthy for its versatility in both biodegradable and non-biodegradable applications. Advances in 3D printing have paved the way for innovative pharmaceutical uses, including the production of complex oral dosage forms, drug delivery systems, and medical devices such as prosthetics. More recent breakthroughs have extended into bioprinting, organ-on-a-chip technologies, and robotics. However, several challenges hinder broader adoption, including limited compatibility with thermosensitive materials, difficulties in scaling production, and maintaining quality control. Additionally, the lack of standardized regulatory and ethical frameworks for clinical approval complicates progress. This review explores the key 3D printing techniques, materials, and trends relevant to pharmaceuticals, while addressing resource constraints, intellectual property issues, and regulatory hurdles. It concludes by identifying future directions for research and development, emphasizing the need to optimize these technologies for widespread pharmaceutical applications.}
}
@article{DUAN2025109558,
title = {A novel anomaly detection and classification algorithm for application in tuyere images of blast furnace},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {109558},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109558},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624017160},
author = {Yifan Duan and Xiaojie Liu and Ran Liu and Xin Li and Hongwei Li and Hongyang Li and Yanqin Sun and Yujie Zhang and Qing Lv},
keywords = {Tuyere image recognition, Feature extraction and fusion, Edge detection, Knowledge integration and control, Key parameters},
abstract = {Traditional relying on manual experience to assess the tuyere status consumes significant human resources. In the era of intelligent blast furnaces and intensified smelting, this approach struggles to meet the demands for accuracy and real-time assessment, posing challenges to safety and efficiency of blast furnace production. Tuyere images exhibit high feature similarity, and the number of samples is often limited. Therefore, if a simple convolution operation is only used, it will be difficult to discern differences across various images. To address this challenge and cater to the requirements of intelligent tuyere status recognition across different steel enterprises, we designed a novel deep neural network algorithm called ES-SFRNet (Enhanced Sequential: Feature Fusion and Recognition Network), building upon our prior research. The algorithm concurrently modeled tuyere images alongside relevant time series data, comprising three components: Feature pre-extraction, Tuyere status recognition, and Generalization & Robustness. The first two modules focus on feature extraction and fusion of tuyere images, while leveraging edge detection information from the image, we developed a mathematical index Ar (Area Ratio) to serve as an auxiliary criterion for tuyere status recognition. Given the model's future scalability and multi-scenario application, the final module focuses on knowledge integration and parameter control. Test results reveal an overall accuracy rate of 99.3% for the ES-SFRNet algorithm, effectively capturing key parameters to facilitate on-site operations. In comparison to other mainstream object detection algorithms, our algorithm framework excels in tuyere image feature extraction and recognition, which can offer broad applications to Chinese blast furnace ironmaking industry.}
}
@article{DENEKE20247066,
title = {A conserved fertilization complex bridges sperm and egg in vertebrates},
journal = {Cell},
volume = {187},
number = {25},
pages = {7066-7078.e22},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2024.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S0092867424010936},
author = {Victoria E. Deneke and Andreas Blaha and Yonggang Lu and Johannes P. Suwita and Jonne M. Draper and Clara S. Phan and Karin Panser and Alexander Schleiffer and Laurine Jacob and Theresa Humer and Karel Stejskal and Gabriela Krssakova and Elisabeth Roitinger and Dominik Handler and Maki Kamoshita and Tyler D.R. Vance and Xinyin Wang and Joachim M. Surm and Yehu Moran and Jeffrey E. Lee and Masahito Ikawa and Andrea Pauli},
keywords = {vertebrate fertilization, sperm-egg interaction, male fertility, gamete adhesion, sperm proteome, AlphaFold, zebrafish, protein-protein interaction},
abstract = {Summary
Fertilization, the basis for sexual reproduction, culminates in the binding and fusion of sperm and egg. Although several proteins are known to be crucial for this process in vertebrates, the molecular mechanisms remain poorly understood. Using an AlphaFold-Multimer screen, we identified the protein Tmem81 as part of a conserved trimeric sperm complex with the essential fertilization factors Izumo1 and Spaca6. We demonstrate that Tmem81 is essential for male fertility in zebrafish and mice. In line with trimer formation, we show that Izumo1, Spaca6, and Tmem81 interact in zebrafish sperm and that the human orthologs interact in vitro. Notably, complex formation creates the binding site for the egg fertilization factor Bouncer in zebrafish. Together, our work presents a comprehensive model for fertilization across vertebrates, where a conserved sperm complex binds to divergent egg proteins—Bouncer in fish and JUNO in mammals—to mediate sperm-egg interaction.}
}
@article{ASGHAR20243591,
title = {Survey on Video Security: Examining Threats, Challenges, and Future Trends},
journal = {Computers, Materials and Continua},
volume = {80},
number = {3},
pages = {3591-3635},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.054654},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824006143},
author = {Ali Asghar and Amna Shifa and Mamoona Naveed Asghar},
keywords = {Attacks, threats, security services, video manipulation, video security},
abstract = {Videos represent the most prevailing form of digital media for communication, information dissemination, and monitoring. However, their widespread use has increased the risks of unauthorised access and manipulation, posing significant challenges. In response, various protection approaches have been developed to secure, authenticate, and ensure the integrity of digital videos. This study provides a comprehensive survey of the challenges associated with maintaining the confidentiality, integrity, and availability of video content, and examining how it can be manipulated. It then investigates current developments in the field of video security by exploring two critical research questions. First, it examine the techniques used by adversaries to compromise video data and evaluate their impact. Understanding these attack methodologies is crucial for developing effective defense mechanisms. Second, it explores the various security approaches that can be employed to protect video data, enhancing its transparency, integrity, and trustworthiness. It compares the effectiveness of these approaches across different use cases, including surveillance, video on demand (VoD), and medical videos related to disease diagnostics. Finally, it identifies potential research opportunities to enhance video data protection in response to the evolving threat landscape. Through this investigation, this study aims to contribute to the ongoing efforts in securing video data, providing insights that are vital for researchers, practitioners, and policymakers dedicated to enhancing the safety and reliability of video content in our digital world.}
}
@article{SANCHEZ2024e39766,
title = {Defining critical quality attributes and composition parameters for burn wound dressings: Antibiotic-anesthetic films as a model},
journal = {Heliyon},
volume = {10},
number = {22},
pages = {e39766},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e39766},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024157970},
author = {María Florencia Sanchez and Laura Carolina Luciani-Giacobbe and Fiamma Barbieri and María Eugenia Olivera},
keywords = {Film, Polyelectrolytes, Polymeric wound dressings, Quality by design, Risk analysis, Design space, Quality control},
abstract = {The management of wounds primarily revolves around pain relief, effective infection control and the promotion of tissue regeneration to prevent complications like chronic skin wounds. While polymeric bioactive films are innovative alternatives to conventional wound dressings, there exists a dearth of guidance regarding their quality control. This underscores the imperative need to establish precise critical quality attributes, a task undertaken within this study using an antibiotic-anesthetic film as a model. The aim was to establish the influence of critical composition and process parameters and optimize the formula. First, the quality target product profile was defined, and critical quality attributes were identified. Our material selection included ciprofloxacin hydrochloride (an antimicrobial), lidocaine hydrochloride (an anesthetic), as well as excipients, such as sodium alginate, sodium hyaluronate, carbomer and glycerol. The critical components were identified through a risk assessment matrix, and their influence on film composition was determined by experimental verification using Design-Expert® software. A full factorial design was employed to assess the effects of sodium hyaluronate, carbomer and glycerol (as independent variables) on transparency, homogeneity, folding capacity and handling. Following this, an optimized formulation was achieved and subjected to further characterization. These optimized antibiotic-anesthetic films exhibited uniform micro-distribution of components, ensuring dosage uniformity. Both ciprofloxacin hydrochloride and lidocaine hydrochloride displayed sustained release profiles, suggesting potential therapeutic benefits for skin wounds. Furthermore, the resistance and elongation properties were similar to those of human skin. Utilizing a QbD approach, we successfully developed an optimized antibiotic-anesthetic film that adhered to the essential critical quality attributes. This films exhibits potential utility as a wound dressing. The findings presented in this study establish a fundamental framework for delineating the critical quality attributes of dressing films and refining their formulation to optimize wound treatment.}
}
@article{MALI2024100514,
title = {DeepSecure watermarking: Hybrid Attention on Attention Net and Deep Belief Net based robust video authentication using Quaternion Curvelet Transform domain},
journal = {Egyptian Informatics Journal},
volume = {27},
pages = {100514},
year = {2024},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2024.100514},
url = {https://www.sciencedirect.com/science/article/pii/S111086652400077X},
author = {Satish D. Mali and Loganthan Agilandeeswari},
keywords = {Attention on Attention Network, Quaternion Curvelet Transform, Golden Section Fibonacci Tree Optimization, Deep Belief Network, Key Frame Extraction},
abstract = {Digital videos have entered every facet of people’s lives because of the rise of live-streaming platforms and the Internet’s expansion & popularity. Additionally, there are a tonne of pirated videos on the Internet that seriously violate the rights and interests of those who own copyrights to videos, hindering the growth of the video business. As a result, trustworthy video watermarking algorithms for copyright defense have emerged in response to consumer demand. To effectively watermark videos, this article proposes a robust feature extraction approach namely Attention on Attention Net (AoA Net). AoA Net extracts the robust features from the Deep Belief Network features of the cover video frames and then generates the score map that helps to identify the suitable location for embedding. The Golden Section Fibonacci Tree Optimization is used to identify the Key frames and then apply Quaternion Curvelet Transform (QCT) on those frames to obtain the QCT coefficients over which the watermark needs to be embedded. Thus, the embedding phase involves embedding the watermark on the obtained score map. Next, an Inverse QCT and the concatenation produce the watermarked video. The resultant video is now vulnerable to adversarial attacks when it is transferred over the Adversary Layer. Consequently, the embedded video is given to the decoder and the extraction phase, which performs key frame extraction and QCT. On the obtained QCT coefficients the similar AoA Net features are used to generate the score map and thus the watermark gets extracted. The performance of the devised technique is evaluated for various intentional and unintentional attacks, and it is assessed using PSNR, MSE, SSIM, BER, and NCC. Finally, the proposed method attains the enhanced visual quality outcome with an Average PSNR and SSIM of 64.33 and 0.9895 respectively. The robustness of the proposed AoADB_QCT attains an average NCC of 0.9999, and BER of 0.001251.}
}
@article{PALA2024100148,
title = {NFT price and sales characteristics prediction by transfer learning of visual attributes},
journal = {The Journal of Finance and Data Science},
volume = {10},
pages = {100148},
year = {2024},
issn = {2405-9188},
doi = {https://doi.org/10.1016/j.jfds.2024.100148},
url = {https://www.sciencedirect.com/science/article/pii/S2405918824000333},
author = {Mustafa Pala and Emre Sefer},
keywords = {NFTs, Blockchain, Deep learning, Transfer learning, Temporal price prediction},
abstract = {Non-fungible tokens (NFTs) are unique digital assets whose possession is defined over a blockchain. NFTs can represent multiple distinct objects such as art, images, videos, etc. There was a recent surge of interest in trading them which makes them another type of alternative investment. The inherent volatility of NFT prices, attributed to factors such as over-speculation, liquidity constraints, rarity, and market volatility, presents challenges for accurate price predictions. For such analysis and forecasting, machine learning methods offer a robust solution framework. Here, we focus on three related prediction problems over NFTs: Predicting NFTs sale price, inferring whether a given NFT will participate in a secondary sale, and predicting NFT's sale price change over time. We analyze and learn the visual characteristics of NFTs by deep pre-trained models and combine such visual knowledge with additional important non-visual attributes such as the sale history, seller's and buyer's centralities in the trading network, and collection's resale probability. We categorize input NFTs into six categories based on their characteristics. Across detailed experiments, we found visual attributes obtained from deep pre-trained models to increase the prediction performance in all cases, and EfficientNet seems to perform the best. In general, CNN and XGBoost consistently outperformed the rest of them across all categories. We also publish our novel NFT dataset with temporal price knowledge, which is the first dataset to have NFT prices over time rather than at a single time point. Our code and NFT datasets are publicly available at https://github.com/seferlab/deep_nft.}
}
@article{NANGIA2024107920,
title = {Molecular tweaking by generative cheminformatics and ligand–protein structures for rational drug discovery},
journal = {Bioorganic Chemistry},
volume = {153},
pages = {107920},
year = {2024},
issn = {0045-2068},
doi = {https://doi.org/10.1016/j.bioorg.2024.107920},
url = {https://www.sciencedirect.com/science/article/pii/S0045206824008253},
author = {Ashwini K. Nangia},
keywords = {Drug discovery, Ligand–protein, Artificial intelligence, Crystal structure, Natural product, Chemical synthesis},
abstract = {The purpose of this review is two-fold: (1) to summarize artificial intelligence and machine learning approaches and document the role of ligand–protein structures in directing drug discovery; (2) to present examples of drugs from the recent literature (past decade) of case studies where such strategies have been applied to accelerate the discovery pipeline. Compared to 50 years ago when drug discovery was largely a synthetic chemist driven research exercise, today a holistic approach needs to be adopted with seamless integration between synthetic and medicinal chemistry, supramolecular complexes, computations, artificial intelligence, machine learning, structural biology, chemical biology, diffraction analytical tools, drugs databases, and pharmacology. The urgency for an integrated and collaborative platform to accelerate drug discovery in an academic setting is emphasized.}
}
@article{SONG2025109572,
title = {Korean football in-game conversation state tracking dataset for dialogue and turn level evaluation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {109572},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109572},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624017305},
author = {Sangmin Song and Juhyoung Park and Juhwan Choi and Junho Lee and Kyohoon Jin and YoungBin Kim},
keywords = {Dialogue state tracking, Data annotation, Large language model},
abstract = {Recent research in dialogue state tracking has made significant progress in tracking user goals through dialogue-level and turn-level approaches, but existing research primarily focused on predicting dialogue-level belief states. In this study, we present the KICK: Korean football In-game Conversation state tracKing dataset, which introduces a conversation-based approach. This approach leverages the roles of casters and commentators within the self-contained context of sports broadcasting to examine how utterances impact the belief state at both the dialogue-level and turn-level. Towards this end, we propose a task that aims to track the states of a specific time turn and understand conversations during the entire game. The proposed dataset comprises 228 games and 2463 events over one season, with a larger number of tokens per dialogue and turn, making it more challenging than existing datasets. Experiments revealed that the roles and interactions of casters and commentators are important for improving the zero-shot state tracking performance. By better understanding role-based utterances, we identify distinct approaches to the overall game process and events at specific turns.}
}
@article{GROSS2025105142,
title = {The Government Patent Register: A new resource for measuring U.S. government-funded patenting},
journal = {Research Policy},
volume = {54},
number = {1},
pages = {105142},
year = {2025},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2024.105142},
url = {https://www.sciencedirect.com/science/article/pii/S0048733324001914},
author = {Daniel P. Gross and Bhaven N. Sampat},
keywords = {Public R&D, Patents, Patent policy, Patent data},
abstract = {We introduce new historical administrative data identifying U.S. government-funded patents since the early twentieth century. In addition to the funding agency, the data report whether the government has title to the patent (“title” patents) or funded a patent assigned to a private organization (“license” patents). The data include a large number of “license” patents that cannot be linked to government funding from patent text or other sources. Combining the historical data with modern administrative sources, we present a public, consolidated data series measuring U.S. government-funded patents — including funding agencies — through 2020, and we provide code to extend this series in the future. We use the data to document long-run patterns in U.S. government-funded patents and federal patent policy, propose ways in which these data can be used in future research, and discuss limitations of the data.}
}
@article{LUO2024133,
title = {Robust Deep Image Watermarking: A Survey},
journal = {Computers, Materials and Continua},
volume = {81},
number = {1},
pages = {133-160},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.055150},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824007458},
author = {Yuanjing Luo and Xichen Tan and Zhiping Cai},
keywords = {Deep image watermarking, multimedia security, data protection deep neural network},
abstract = {In the era of internet proliferation, safeguarding digital media copyright and integrity, especially for images, is imperative. Digital watermarking stands out as a pivotal solution for image security. With the advent of deep learning, watermarking has seen significant advancements. Our review focuses on the innovative deep watermarking approaches that employ neural networks to identify robust embedding spaces, resilient to various attacks. These methods, characterized by a streamlined encoder-decoder architecture, have shown enhanced performance through the incorporation of novel training modules. This article offers an in-depth analysis of deep watermarking’s core technologies, current status, and prospective trajectories, evaluating recent scholarly contributions across diverse frameworks. It concludes with an overview of the technical hurdles and prospects, providing essential insights for ongoing and future research endeavors in digital image watermarking.}
}
@article{NOVELLI2024106066,
title = {Generative AI in EU law: Liability, privacy, intellectual property, and cybersecurity},
journal = {Computer Law & Security Review},
volume = {55},
pages = {106066},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106066},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924001328},
author = {Claudio Novelli and Federico Casolari and Philipp Hacker and Giorgio Spedicato and Luciano Floridi},
keywords = {Generative AI, EU law, Liability, Privacy, Intellectual property, Cybersecurity},
abstract = {The complexity and emergent autonomy of Generative AI systems introduce challenges in predictability and legal compliance. This paper analyses some of the legal and regulatory implications of such challenges in the European Union context, focusing on four areas: liability, privacy, intellectual property, and cybersecurity. It examines the adequacy of the existing and proposed EU legislation, including the Artificial Intelligence Act (AIA), in addressing the challenges posed by Generative AI in general and LLMs in particular. The paper identifies potential gaps and shortcomings in the EU legislative framework and proposes recommendations to ensure the safe and compliant deployment of generative models.}
}
@incollection{CRUZ202525,
title = {Equity, Diversity, and Inclusion and Critical Libraries: A Historical Evolution of Awareness and Informed Practice},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {25-35},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00150-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001504},
author = {Jeffery Cruz},
keywords = {Affirmative action in libraries, Anti-discrimination in libraries, Critical librarianship, Decolonization in libraries, Diversity initiatives in libraries, Equal opportunity in libraries, Equity, diversity, and inclusion, Indigenization in libraries, Library and information services, Queering the library},
abstract = {In principle, libraries have espoused the values of equity, diversity, and inclusion (EDI) as evidenced by the broad range of diversity initiatives, commitments, conferences and literature within the library and information studies sector. More recently, concepts of equity and inclusion have played a role in library diversity initiatives in addition to newer concepts such as critical librarianship and the decolonization, Indigenization and queering of libraries. Starting from affirmative action and anti-discrimination policies and legislation in the 1960s, this entry reviews the historical evolution of EDI awareness and informed practice in the US and Australia over the past 65 years.}
}
@article{CRAIG20241173,
title = {The Future of Odontogenic Sinusitis},
journal = {Otolaryngologic Clinics of North America},
volume = {57},
number = {6},
pages = {1173-1181},
year = {2024},
note = {Odontogenic Sinusitis},
issn = {0030-6665},
doi = {https://doi.org/10.1016/j.otc.2024.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0030666524000938},
author = {John R. Craig and Rod W. Tataryn and Alberto M. Saibene},
keywords = {Odontogenic sinusitis, Multidisciplinary, Diagnostic criteria, Antibiotic stewardship, Coding, Research, Pediatrics, Patient selection}
}
@incollection{COLLINS202581,
title = {Copyright},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {81-91},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00053-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895000535},
author = {Perry Collins},
keywords = {Artificial intelligence, Berne Convention, Digital libraries, Fair dealing, Fair use, History of copyright, Infringement, Intellectual property, Plagiarism, Public domain, Traditional Cultural Expressions, Translation},
abstract = {Understanding the basics of copyright law is a crucial step toward understanding a broader information landscape that depends on the creation, dissemination, and reuse of cultural material. Despite its complexity, copyright is ubiquitous, and people around the world gain and share copyright every day as they write emails, record videos, and create new works of art and scholarship. With a focus on areas most applicable to library and information science professionals, this entry briefly considers a range of rights-related topics, including general information as well as more specialized starting points for inquiry. By examining areas such as copyright exceptions, digital libraries, open licensing, and contracts, the entry also reflects how copyright has permeated scholarly communication and impacts decision making around knowledge sharing on a global scale. Throughout, discussion considers how copyright law has been shaped, contested, and interpreted over time.}
}
@article{SHARMA2024104310,
title = {Distance distributions and runtime analysis of perceptual hashing algorithms},
journal = {Journal of Visual Communication and Image Representation},
volume = {104},
pages = {104310},
year = {2024},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2024.104310},
url = {https://www.sciencedirect.com/science/article/pii/S1047320324002669},
author = {Shivdutt Sharma},
keywords = {Perceptual hashing, Distance distributions, Image similarity},
abstract = {Perceptual image hashing refers to a class of algorithms that produce content-based image hashes. These systems use specialized perceptual hash algorithms like Phash, Microsoft’s PhotoDNA, or Facebook’s PDQ to generate a compact digest of an image file that can be roughly compared to a database of known illicit-content digests. Time taken by perceptual hashing algorithms to generate hash code has been computed. Then, we evaluated perceptual hashing algorithms on two million dataset of images. The produced nine variants of the original images were computed and then several distances were calculated. There have been several studies in the past, but in the existing literature size of the data is small and there are very few studies with hash code computation time and robustness tradeoff. This work shows that existing perceptual hashing algorithms are robust for most of the content-preserving operations and there is a tradeoff between computation time and robustness.}
}
@article{ALNAJI2024104180,
title = {The cultural significance of Syrian refugees' traditional childbirth and postpartum practices},
journal = {Midwifery},
volume = {139},
pages = {104180},
year = {2024},
issn = {0266-6138},
doi = {https://doi.org/10.1016/j.midw.2024.104180},
url = {https://www.sciencedirect.com/science/article/pii/S0266613824002638},
author = {Nada Alnaji and Bree Akesson and Alma Gottlieb},
keywords = {Cultural practices, Postpartum practices, Refugees' health},
abstract = {Problem
Childbirth and the postpartum period are critical times for both mothers and babies. Traditional cultural practices often play a significant role in providing support during this time. However, in exceptional circumstances, such as those faced by refugees giving birth in disrupted social environments, these practices may be inaccessible, leading to emotional distress and delayed physical recovery.
Aim
To explore the cultural significance of traditional motherhood practices in Syria that are still observed by some Syrian refugees in Lebanon.
Methods
The study used a phenomenological approach and included in-depth interviews with eight Syrian mothers residing in informal settlements in Lebanon.
Findings
Findings were organized around three themes: (1) Familial Support during the Postpartum Period, (2) Specific Cultural Practices during the Postpartum Period, and (3) Emotional Experiences during the Postpartum Period
Discussion
Understanding these cultural practices is essential for developing culturally sensitive interventions that can improve wellbeing of refugee mothers.}
}
@article{UKWATHTHA2024110294,
title = {A review of machine learning (ML) and explainable artificial intelligence (XAI) methods in additive manufacturing (3D Printing)},
journal = {Materials Today Communications},
volume = {41},
pages = {110294},
year = {2024},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2024.110294},
url = {https://www.sciencedirect.com/science/article/pii/S235249282402275X},
author = {Jeewanthi Ukwaththa and Sumudu Herath and D.P.P. Meddage},
keywords = {Additive manufacturing, 3D printing, Machine learning, Artificial intelligence, Metals, Materials},
abstract = {Additive Manufacturing (AM) (known as 3D printing) has modernised traditional manufacturing processes by enabling the layer-by-layer fabrication of complex geometries, along with advanced design capabilities, cost efficiency, and reduced production time. AM offers flexibility and customisation in product development, allowing for the deposition, solidification, or joining of materials based on computer-aided models. In recent years, the large-scale collection of AM-related data has facilitated the use of machine learning (ML) techniques to embed into AM processes and optimise quality. However, many advanced ML algorithms do not provide their underlying decision-making criteria, remaining as a black box. Alternatively, explainable artificial intelligence (XAI) methods have been employed to explain these black box models. Even though ML has been widely used in AM, the use of XAI in AM is still very limited. This paper provides a comprehensive review of the integration of ML and XAI (XAI for the first time) in AM processes, exploring current research progress and future prospects. The study outlines the various ML techniques and XAI applied in different domains of AM. Additionally, it examines ML and XAI applications across different AM technologies and life cycle stages, highlighting their functions in design optimisation, process monitoring, and sustainability analysis. Furthermore, the review discusses new developments, challenges, and future research directions in ML and XAI for AM, allowing for enhanced efficiency, innovation, and sustainability in AM processes.}
}
@article{RELLA2024100026,
title = {A stack made in heaven? Exploring AI-blockchain intersections and their implications for labour and value},
journal = {Progress in Economic Geography},
volume = {2},
number = {2},
pages = {100026},
year = {2024},
issn = {2949-6942},
doi = {https://doi.org/10.1016/j.peg.2024.100026},
url = {https://www.sciencedirect.com/science/article/pii/S2949694224000208},
author = {Ludovico Rella and Malcolm Campbell-Verduyn},
keywords = {Artificial intelligence, Blockchain, Labour, Technology, Value},
abstract = {How have socio-technical practices in blockchain and artificial intelligence (AI) communities shaped one another and society more widely? This article explores the different and overlapping materialities, practices, spaces and places that the two most hyped technologies of the 21st century are impacting and evolving within. Employing the concept and analogy of “the stack”, we show how Machine Learning (ML), and crypto-assets each developed separately and yet become deeply interconnected. In doing so, we pluralise the concept of the stack to trace how two techno-communities have cometh, collided and colluded (Three Cs) in ways that pose varying implications for labour and the enactment of value in hyper capitalist tech-driven economic geographies.}
}
@article{TYAGI2024109792,
title = {Deep learning models security: A systematic review},
journal = {Computers and Electrical Engineering},
volume = {120},
pages = {109792},
year = {2024},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2024.109792},
url = {https://www.sciencedirect.com/science/article/pii/S0045790624007195},
author = {Twinkle Tyagi and Amit Kumar Singh},
keywords = {Deep learning, Security, Privacy, Watermarking, Encryption, Fingerprint},
abstract = {Deep learning models and the digital records they generate have remarkably increased their adoption of many practical applications. While the success of deep learning in multimedia applications, especially images, helps tackle some of the most challenging problems, one of its copyright violations, ownership conflict, poses a grave concern for many potential applications. Many works on intellectual property protection for such models have proposed to verify ownership. Therefore, it is necessary to conduct a comprehensive study on the security of deep learning models to evaluate their strong background, state-of-the-art solutions, possible attacks, current limitations and notable improvements. This survey attempts to systematically discuss and summarise the recent advanced security solutions for deep learning models through watermarking, encryption and fingerprinting. Our study explores the recent applications, possible attacks, current limitations and notable suggestions regarding deep learning. It also comprehensively evaluates the recent research gaps and opportunities in detail to empower researchers and practitioners to provide additional secure solutions for deep learning models. This extensive survey is the first to consider model security through several notable techniques.}
}
@article{ZHOU2025112204,
title = {Exploring the problems, their causes and solutions of AI pair programming: A study on GitHub and Stack Overflow},
journal = {Journal of Systems and Software},
volume = {219},
pages = {112204},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112204},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224002486},
author = {Xiyu Zhou and Peng Liang and Beiqi Zhang and Zengyang Li and Aakash Ahmad and Mojtaba Shahin and Muhammad Waseem},
keywords = {GitHub copilot, GitHub issues, GitHub discussions, StackOverflow post, Problem, Cause, Solution},
abstract = {With the recent advancement of Artificial Intelligence (AI) and Large Language Models (LLMs), AI-based code generation tools become a practical solution for software development. GitHub Copilot, the AI pair programmer, utilizes machine learning models trained on a large corpus of code snippets to generate code suggestions using natural language processing. Despite its popularity in software development, there is limited empirical evidence on the actual experiences of practitioners who work with Copilot. To this end, we conducted an empirical study to understand the problems that practitioners face when using Copilot, as well as their underlying causes and potential solutions. We collected data from 473 GitHub issues, 706 GitHub discussions, and 142 Stack Overflow posts. Our results reveal that (1) Operation Issue and Compatibility Issue are the most common problems faced by Copilot users, (2) Copilot Internal Error, Network Connection Error, and Editor/IDE Compatibility Issue are identified as the most frequent causes, and (3) Bug Fixed by Copilot, Modify Configuration/Setting, and Use Suitable Version are the predominant solutions. Based on the results, we discuss the potential areas of Copilot for enhancement, and provide the implications for the Copilot users, the Copilot team, and researchers.}
}
@article{HU2025105082,
title = {Rethinking perceived constraints for people with chronic diseases: Developing and validating a scale for tourists with mild dementia},
journal = {Tourism Management},
volume = {107},
pages = {105082},
year = {2025},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2024.105082},
url = {https://www.sciencedirect.com/science/article/pii/S0261517724002012},
author = {Fangli Hu and Jun Wen and Danni Zheng and Yangyang Jiang and Haifeng Hou and Wei Wang},
keywords = {Perceived constraint, Dementia, Vulnerable tourist, Chronic disease, Scale development, Healthy aging, Accessible tourism},
abstract = {Although perceived constraints represent a well-documented concept, little research has specifically addressed vulnerable populations with chronic diseases. This study is among the first in tourism to explore perceived constraints for tourists with chronic diseases. It uses dementia as an example and draws on qualitative and quantitative data. We developed and validated a five-factor, 38-item scale to assess perceived constraints to outbound tourism for people with mild dementia. Factors include perceived incapability and uncertainties; dementia-friendly service access challenges; emotional fulfillment and adjustment challenges; medication management challenges; and travel procedures and financial challenges. Further investigation demonstrated that perceived constraints significantly contribute to this demographic's learned helplessness and negatively affect their future travel intentions. We have thus expanded accessible tourism beyond creating enjoyable experiences to fostering positive travel. Findings can inform experience design and encourage exploration of the travel behaviors of chronically ill individuals.}
}
@article{SAMAHA2024612,
title = {Leveraging the usage of blockchain toward trust-dominated manufacturing systems},
journal = {Journal of Manufacturing Systems},
volume = {77},
pages = {612-638},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S027861252400236X},
author = {Philip Samaha and Fadi {El Kalach} and Ramy Harik},
keywords = {Blockchain technology, Smart Manufacturing, Cybersecurity, Cyber-Physical Infrastructure, Supply Chain Management},
abstract = {Smart manufacturing has transformed the role of data in manufacturing, with a significant focus on secure data infrastructure. As factories engage with external data sources, cybersecurity becomes crucial. Blockchain technology is introduced to safeguard this infrastructure, ensuring secure and transparent data flow, which is vital for industries like pharmaceutical, aerospace, automotive, and electronics manufacturing. This review provides a comprehensive taxonomy of blockchain architectures, analyzing their working modes, strengths, and weaknesses while identifying appropriate use cases. It also examines consensus algorithms, categorizing them as either crash fault tolerant (CFT) or Byzantine fault tolerant (BFT) and further classifies them based on whether they are proof-based or voting-based. The review explores the intrinsic limitations of blockchain systems and highlights specific manufacturing challenges where blockchain can be instrumental. It also discusses the synergy between blockchain and cybersecurity, emphasizing how they work together to enhance security and accountability. The paper concludes by identifying private blockchain as the most suitable architecture for certain manufacturing applications, particularly in supply chain management and machinery control. A SWOT analysis is conducted on this architecture to provide a detailed understanding of its potential and challenges. The review suggests that while no single consensus algorithm is best universally, each has its own merits depending on the application. Lastly, the SWOT analysis serves as a catalyst for future research, guiding efforts to maximize blockchain’s strengths and mitigate its weaknesses in industrial contexts.}
}
@article{ESPANA2025102373,
title = {Ethical reasoning methods for ICT: What they are and when to use them},
journal = {Data & Knowledge Engineering},
volume = {155},
pages = {102373},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102373},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000971},
author = {Sergio España and Chris {van der Maaten} and Jens Gulden and Óscar Pastor},
keywords = {Conceptual modelling, Ethics, Ethical reasoning, Sustainability assessment, Method engineering, Situational factors},
abstract = {Information and communication technology (ICT) brings about numerous advantages across various domains of our lives. However, alongside these benefits, there is a growing awareness of its potential negative ethical, social, and environmental impacts. Consequently, stakeholders ranging from conceptual modellers to policy makers often find themselves grappling with ethical considerations stemming from ICT engineering and usage. This paper presents a review of 10 ethical reasoning methods suitable for the ICT domain. We have employed a method engineering technique to author metamodels for the methods, which were subsequently subjected to validation by experts proficient in the respective methods. Following a situational method engineering approach, we have also characterised each ethical reasoning method and validated the characterisation with the experts. This has allowed us to develop a tool that helps select the method that is most suitable for a given ethical reasoning situation. Furthermore, we deliberate on the practical application of ethical reasoning methods within conceptual modelling contexts. We are confident that we have laid the groundwork for further research into ethical reasoning of ICT, with a specific emphasis on its role during conceptual modelling.}
}
@article{MARKKANDEYAN2025100075,
title = {Novel hybrid deep learning based cyber security threat detection model with optimization algorithm},
journal = {Cyber Security and Applications},
volume = {3},
pages = {100075},
year = {2025},
issn = {2772-9184},
doi = {https://doi.org/10.1016/j.csa.2024.100075},
url = {https://www.sciencedirect.com/science/article/pii/S2772918424000419},
author = {S. Markkandeyan and A. Dennis Ananth and M. Rajakumaran and R.G. Gokila and R. Venkatesan and B. Lakshmi},
keywords = {Internet of things (IoT), Cyber-attacks, Security, Adaptive tensor flow deep neural network, Improved particle swarm optimization (IPSO), Enhanced long short term memory (E-LSTM)},
abstract = {In order to continuously provide services to the company, the Internet of Things (IoT) connects the hardware, software, storing data, and applications that could be utilized as a new port of entry for cyber-attacks. The privacy of IoT is presently very vulnerable to virus threats and software piracy. Threats like this have the potential to capture critical data, harming businesses' finances and reputations. We have suggested a hybrid Deep Learning (DL) strategy in this study to identify malware-infected programs and files that have been illegally distributed over the IoT environment. To detect illegal content utilizing Source code (SC) duplication, the Adaptive TensorFlow deep neural network with Improved Particle Swarm Optimization (IPSO) is suggested. This novel hybrid strategy improves cyber security by fusing cutting-edge DL with optimization methods, providing more effective and accurate detection. With a strong solution for real-time threat identification, the model handles the complexity of contemporary cyberthreats. To highlight the significance of the proxy regarding the SC duplication, the noisy data is filtered using the tokenization and weighting feature approaches. After that, duplication in SC is found using a DL method. To look into software piracy, the dataset was gathered via Google Code Jam (GCJ). Additionally, using the visual representation of color images, the Enhanced Long Short-Term Memory (E-LSTM) was employed to identify suspicious actions in the IoT environment. The Maling dataset is used to gather the malware samples required for testing. The experimental findings show that, in terms of categorization, the suggested method for evaluating cybersecurity threats in IoT surpasses conventional approaches.}
}
@article{STEPKOWSKI2024114803,
title = {Temporal alterations of the nascent proteome in response to mitochondrial stress},
journal = {Cell Reports},
volume = {43},
number = {10},
pages = {114803},
year = {2024},
issn = {2211-1247},
doi = {https://doi.org/10.1016/j.celrep.2024.114803},
url = {https://www.sciencedirect.com/science/article/pii/S2211124724011549},
author = {Tomasz M. Stępkowski and Vanessa Linke and Dorota Stadnik and Maciej Zakrzewski and Anna E. Zawada and Remigiusz A. Serwa and Agnieszka Chacinska},
keywords = {translation, mitochondria, protein synthesis, EEF1A, EEF1A1, elongation factor, nascent chain, proteomics, mass spectrometry, cellular stress},
abstract = {Summary
Under stress, protein synthesis is attenuated to preserve energy and mitigate challenges to protein homeostasis. Here, we describe, with high temporal resolution, the dynamic landscape of changes in the abundance of proteins synthesized upon stress from transient mitochondrial inner membrane depolarization. This nascent proteome was altered when global translation was attenuated by stress and began to normalize as translation was recovering. This transition was associated with a transient desynchronization of cytosolic and mitochondrial translation and recovery of cytosolic and mitochondrial ribosomal proteins. Further, the elongation factor EEF1A1 was downregulated upon mitochondrial stress, and its silencing mimicked the stress-induced nascent proteome remodeling, including alterations in the nascent respiratory chain proteins. Unexpectedly, the stress-induced alterations in the nascent proteome were independent of physiological protein abundance and turnover. In summary, we provide insights into the physiological and pathological consequences of mitochondrial function and dysfunction.}
}
@article{WU2024102756,
title = {Technology shock of ChatGPT, social attention and firm value: Evidence from China},
journal = {Technology in Society},
volume = {79},
pages = {102756},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102756},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X2400304X},
author = {Qinqin Wu and Qinqin Zhuang and Yitong Liu and Longyan Han},
keywords = {ChatGPT, Artificial intelligence, Event study, Difference-in-differences, Stock return, Social attention},
abstract = {The release of ChatGPT has attracted widespread attention and triggered fluctuations in the capital market. This study employs difference-in-differences (DID) and event study (ES) to investigate the effects of ChatGPT's release on the cumulative abnormal return (CAR) of listed companies in China. The results reveal that a series of ChatGPT launch events, including GPT-3.5 and GPT-4, have a significantly positive impact on the firm value of the companies focused on ChatGPT, with dynamic effects. In the initial two months after the release of ChatGPT, the Chinese stock market exhibited an undervaluation of GPT-focused companies, indicating information asymmetry and competitive substitution effect. With the widespread promotion of generative AI, social recognition of ChatGPT's potential value increased. This study verifies the moderation effect of social attention in strengthening ChatGPT's impact, demonstrating that a higher search index for ChatGPT enhances stock returns for GPT-focused companies. Heterogeneity tests reveal that the impact of ChatGPT is significantly positive for large or non-state-owned companies, while small or state-owned companies show no significant effect. From the perspective of labor structure, companies dominated by technical and production personnel experience positive effects, whereas those dominated by sales personnel do not. In the eastern regions with more favorable digital economic innovation environments, companies experience a notably positive impact. This paper provides a theoretical explanation and empirical evidence for the microeconomic impact of generative AI in the Chinese context, offering valuable insights for both government and firms.}
}
@article{2024276,
title = {Guide for Authors},
journal = {Intelligent Medicine},
volume = {4},
number = {4},
pages = {276-282},
year = {2024},
issn = {2667-1026},
doi = {https://doi.org/10.1016/S2667-1026(24)00078-0},
url = {https://www.sciencedirect.com/science/article/pii/S2667102624000780}
}
@article{BAJPAI2024109604,
title = {A novel methodology for anomaly detection in smart home networks via Fractional Stochastic Gradient Descent},
journal = {Computers and Electrical Engineering},
volume = {119},
pages = {109604},
year = {2024},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2024.109604},
url = {https://www.sciencedirect.com/science/article/pii/S0045790624005317},
author = {Abhishek Bajpai and Divyansh Chaurasia and Naveen Tiwari},
abstract = {With the rising integration of IoT devices within smart home environments, securing these interconnected systems against unauthorized access and cyber threats has become increasingly critical. This study introduces a novel methodology utilizing an advanced Artificial Neural Network (ANN) frame-work to enhance attack and anomaly detection capabilities within smart home networks. The objective is to develop a robust model that outperforms traditional detection methods and provides high accuracy and low false positive rates in identifying potential security threats. The research employs a pioneering approach to train the ANN by incorporating a Fractional Stochastic Gradient Descent optimizer grounded in Grunwald–Letnikov fractional calculus. This method was chosen for its potential to refine learning processes and improve detection accuracy over standard optimizers. The evaluation of the model was performed using the DS2OS traffic traces dataset, applying precision, sensitivity (recall), and specificity metrics to assess performance. The proposed model demonstrated exceptional performance with an accuracy of 0.9951. It significantly surpassed traditional methods like Logistic Regression and Support Vector Machines. The precision achieved was high, indicating a low rate of false positives, while the sensitivity and specificity values underscore the model’s ability to identify both typical and unconventional behaviours within the network accurately. This study introduces a robust and efficient ANN-based methodology for enhancing security in smart home IoT networks. Using a fractional stochastic gradient descent optimizer has proven effective in improving the model’s accuracy and reliability in detecting anomalies and attacks. The findings suggest significant implications for the future of IoT security, highlighting the potential for broader applications of fractional calculus in machine learning to enhance cybersecurity measures in various domains.}
}
@article{HASSAN2024100513,
title = {A review of AI for optimization of 3D printing of sustainable polymers and composites},
journal = {Composites Part C: Open Access},
volume = {15},
pages = {100513},
year = {2024},
issn = {2666-6820},
doi = {https://doi.org/10.1016/j.jcomc.2024.100513},
url = {https://www.sciencedirect.com/science/article/pii/S2666682024000823},
author = {Malik Hassan and Manjusri Misra and Graham W. Taylor and Amar K. Mohanty},
keywords = {3D printing, Artificial intelligence, Advanced manufacturing, Sustainability, Technological advancements},
abstract = {In recent years, 3D printing has experienced significant growth in the manufacturing sector due to its ability to produce intricate and customized components. The advent of Industry 4.0 further boosted this progress by seamlessly incorporating artificial intelligence (AI) in 3D printing processes. As a result, design precision and production efficiency have significantly improved. Although numerous studies have explored the integration of AI and 3D printing, the literature still lacks a comprehensive overview that emphasizes material selection and formulation, predictive modeling, design optimization, and quality control. To fully understand the impacts of these emerging technologies on advanced manufacturing, a thorough assessment is required. This review aims to examine the intersection of AI and 3D printing to create a technologically advanced and environment-friendly manufacturing environment. It examines factors such as material, process efficiency, and design enhancements to highlight the benefits of combining these technologies. By focusing on predictive modeling, material selection and quality control, this analysis aims to unlock the potential for a sustainable and efficient 3D printing process. This review provided a thorough analysis of the challenges and potential benefits, proving valuable for academics and practitioners alike. It presents solutions that may establish a foundation for sustained growth and outlines a strategy for leveraging 3D printing and AI capabilities in the manufacturing sector.}
}
@article{KIBRIYA2024109698,
title = {Privacy issues in Large Language Models: A survey},
journal = {Computers and Electrical Engineering},
volume = {120},
pages = {109698},
year = {2024},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2024.109698},
url = {https://www.sciencedirect.com/science/article/pii/S0045790624006256},
author = {Hareem Kibriya and Wazir Zada Khan and Ayesha Siddiqa and Muhammad Khurram Khan},
abstract = {In the fast-paced world of modern technology, the development of Large Language Models (LLMs) has increased drastically. However, this growth has also increased privacy concerns associated with these models. This paper investigates privacy concerns in the existing LLMs and their far-reaching implications. The paper categorizes privacy concerns of LLMs into two main groups: those occurring during training and those during inference, both of which can contribute to re-identification risks. Through an in-depth literature analysis, we have highlighted different requirements for safeguarding user privacy when interacting with LLMs. Moreover, this paper discusses the challenges that can arise in implementing privacy-preserving mechanisms in LLMs. It examines the complex interactions between ethical issues, legal requirements, and technology developments, highlighting the need for stakeholder collaboration to traverse this challenging environment successfully. This paper contributes to the ongoing discussion on the responsible development and deployment of LLMs. It aims to open the door for ethically acceptable Artificial Intelligence innovation processes by promoting a better awareness of privacy issues.}
}
@article{KLEIB2024100252,
title = {Current trends and future implications in the utilization of ChatGPT in nursing: A rapid review},
journal = {International Journal of Nursing Studies Advances},
volume = {7},
pages = {100252},
year = {2024},
issn = {2666-142X},
doi = {https://doi.org/10.1016/j.ijnsa.2024.100252},
url = {https://www.sciencedirect.com/science/article/pii/S2666142X24000791},
author = {Manal Kleib and Elizabeth Mirekuwaa Darko and Oluwadamilare Akingbade and Megan Kennedy and Precious Majekodunmi and Emma Nickel and Laura Vogelsang},
keywords = {ChatGPT, Generative artificial intelligence (Gen AI), Nursing education, Nursing practice, Nursing research, Rapid review},
abstract = {Background
The past decade has witnessed a surge in the development of artificial intelligence (AI)-based technology systems for healthcare. Launched in November 2022, ChatGPT (Generative Pre-trained Transformer), an AI-based Chatbot, is being utilized in nursing education, research and practice. However, little is known about its pattern of usage, which prompted this study.
Objective
To provide a concise overview of the existing literature on the application of ChatGPT in nursing education, practice and research.
Methods
A rapid review based on the Cochrane methodology was applied to synthesize existing literature. We conducted systematic searches in several databases, including CINAHL, Ovid Medline, Embase, Web of Science, Scopus, Education Search Complete, ERIC, and Cochrane CENTRAL, to ensure no publications were missed. All types of primary and secondary research studies, including qualitative, quantitative, mixed methods, and literature reviews published in the English language focused on the use of ChatGPT in nursing education, research, and practice, were included. Dissertations or theses, conference proceedings, government and other organizational reports, white papers, discussion papers, opinion pieces, editorials, commentaries, and published review protocols were excluded. Studies involving other healthcare professionals and/or students without including nursing participants were excluded. Studies exploring other language models without comparison to ChatGPT and those examining the technical specifications of ChatGPT were excluded. Data screening was completed in two stages: titles and abstract and full-text review, followed by data extraction and quality appraisal. Descriptive analysis and narrative synthesis were applied to summarize and categorize the findings.
Results
Seventeen studies were included: 15 (88.2 %) focused on nursing education and one each on nursing practice and research. Of the 17 included studies, 5 (29.4 %) were evaluation studies, 3 (17.6 %) were narrative reviews, 3 (17.6 %) were cross-sectional studies, 2 (11.8 %) were descriptive studies, and 1 (5.9 %) was a randomized controlled trial, quasi-experimental study, case study, and qualitative study, respectively.
Conclusion
This study has provided a snapshot of ChatGPT usage in nursing education, research, and practice. Although evidence is inconclusive, integration of ChatGPT should consider addressing ethical concerns and ongoing education on ChatGPT usage. Further research, specifically interventional studies, is recommended to ascertain and track the impact of ChatGPT in different contexts.}
}
@article{LI2025112233,
title = {An empirical study of AI techniques in mobile applications},
journal = {Journal of Systems and Software},
volume = {219},
pages = {112233},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112233},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224002772},
author = {Yinghua Li and Xueqi Dang and Haoye Tian and Tiezhu Sun and Zhijie Wang and Lei Ma and Jacques Klein and Tegawendé F. Bissyandé},
keywords = {AI apps, AI technologies, Analysis, Empirical study},
abstract = {The integration of artificial intelligence (AI) into mobile applications has significantly transformed various domains, enhancing user experiences and providing personalized services through advanced machine learning (ML) and deep learning (DL) technologies. AI-driven mobile apps typically refer to applications that leverage ML/DL technologies to perform key tasks such as image recognition and natural language processing. Despite existing research exploring how mobile apps exploit AI techniques, they have the following main limitations: (1) Most existing studies focus on DL-based apps, with limited research on ML-based apps. (2) Existing research typically focuses on investigating the apps and the technologies utilized in the apps, lacking user-level analysis. (3) The number of apps studied is limited, with only 1,000 to 2,000 ML/DL apps identified after filtering. To fill the gap, in this paper, we conducted the most extensive empirical study on AI applications, exploring on-device ML apps, on-device DL apps, and AI service-supported (cloud-based) apps. Our study encompasses 56,682 real-world AI applications, focusing on three crucial perspectives: (1) Application analysis, where we analyze the popularity of AI apps and investigate the update states of AI apps; (2) Framework and model analysis, where we analyze AI framework usage and AI model protection; (3) User analysis, where we examine user privacy protection and user review attitudes. Our study has strong implications for AI app developers, users, and AI R&D. On one hand, our findings highlight the growing trend of AI integration in mobile applications, demonstrating the widespread adoption of various AI frameworks and models. On the other hand, our findings emphasize the need for robust model protection to enhance app security. Additionally, our study highlights the importance of user privacy and presents user attitudes towards the AI technologies utilized in current AI apps. We provide our AI app dataset (currently the most extensive AI app dataset) as an open-source resource for future research on AI technologies utilized in mobile applications.}
}
@article{2024I,
title = {Full Issue PDF},
journal = {JACC: Basic to Translational Science},
volume = {9},
number = {9},
pages = {I-CIX},
year = {2024},
issn = {2452-302X},
doi = {https://doi.org/10.1016/S2452-302X(24)00319-X},
url = {https://www.sciencedirect.com/science/article/pii/S2452302X2400319X}
}
@incollection{MADLER2025108,
title = {Referencing and Citation Styles},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {108-120},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00213-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895002133},
author = {Aubrey M. Madler and Shamin Renwick},
keywords = {Attribution, Bibliography, Citation history, Citation style, Citing, Footnotes, In-text citation, Narrative citation, Parenthetical citation, Plagiarism, Reference list, References, Referencing, Works cited, Works consulted.},
abstract = {Referencing is the act of naming a person or organization responsible for creative or intellectual work. And it provides information that allows others to locate the original source. Referencing original sources helps to avoid plagiarizing academic, creative, and other works – either in written or in multi-media formats. Various citation styles dictate how to display identifying details about the source in a reference. Original styles were developed in the late 1880s, then as technology and information formats evolved, so did citation styles. The ever-evolving technological advances and changes in fields of study lead to releasing new style editions to address new information formats and professional standards of practice.}
}
@article{GUO2024100287,
title = {Chinese named entity recognition with multi-network fusion of multi-scale lexical information},
journal = {Journal of Electronic Science and Technology},
volume = {22},
number = {4},
pages = {100287},
year = {2024},
issn = {1674-862X},
doi = {https://doi.org/10.1016/j.jnlest.2024.100287},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X24000557},
author = {Yan Guo and Hong-Chen Liu and Fu-Jiang Liu and Wei-Hua Lin and Quan-Sen Shao and Jun-Shun Su},
keywords = {Bi-directional long short-term memory (BiLSTM), Chinese named entity recognition (CNER), Iterated dilated convolutional neural network (IDCNN), Multi-network integration, Multi-scale lexical features},
abstract = {Named entity recognition (NER) is an important part in knowledge extraction and one of the main tasks in constructing knowledge graphs. In today's Chinese named entity recognition (CNER) task, the BERT-BiLSTM-CRF model is widely used and often yields notable results. However, recognizing each entity with high accuracy remains challenging. Many entities do not appear as single words but as part of complex phrases, making it difficult to achieve accurate recognition using word embedding information alone because the intricate lexical structure often impacts the performance. To address this issue, we propose an improved Bidirectional Encoder Representations from Transformers (BERT) character word conditional random field (CRF) (BCWC) model. It incorporates a pre-trained word embedding model using the skip-gram with negative sampling (SGNS) method, alongside traditional BERT embeddings. By comparing datasets with different word segmentation tools, we obtain enhanced word embedding features for segmented data. These features are then processed using the multi-scale convolution and iterated dilated convolutional neural networks (IDCNNs) with varying expansion rates to capture features at multiple scales and extract diverse contextual information. Additionally, a multi-attention mechanism is employed to fuse word and character embeddings. Finally, CRFs are applied to learn sequence constraints and optimize entity label annotations. A series of experiments are conducted on three public datasets, demonstrating that the proposed method outperforms the recent advanced baselines. BCWC is capable to address the challenge of recognizing complex entities by combining character-level and word-level embedding information, thereby improving the accuracy of CNER. Such a model is potential to the applications of more precise knowledge extraction such as knowledge graph construction and information retrieval, particularly in domain-specific natural language processing tasks that require high entity recognition precision.}
}
@article{PAYEN2024199456,
title = {The cellular paraspeckle component SFPQ associates with the viral processivity factor ORF59 during lytic replication of Kaposi's Sarcoma-associated herpesvirus (KSHV)},
journal = {Virus Research},
volume = {349},
pages = {199456},
year = {2024},
issn = {0168-1702},
doi = {https://doi.org/10.1016/j.virusres.2024.199456},
url = {https://www.sciencedirect.com/science/article/pii/S0168170224001497},
author = {Shannon Harger Payen and Kayla Andrada and Evelyn Tara and Juli Petereit and Subhash C. Verma and Cyprian C. Rossetto},
keywords = {Kaposi's sarcoma-associated herpesvirus (KSHV), Splicing factor proline and glutamine rich (SFPQ), Non-POU domain-containing octamer-binding protein (NONO), Paraspeckles, DNA polymerase processivity factor},
abstract = {Kaposi's sarcoma-associated herpesvirus (KSHV) relies on many cellular proteins to complete replication and generate new virions. Paraspeckle nuclear bodies consisting of core ribonucleoproteins splicing factor proline/glutamine-rich (SFPQ), Non-POU domain-containing octamer-binding protein (NONO), and paraspeckle protein component 1 (PSPC1) along with the long non-coding RNA NEAT1, form a complex that has been speculated to play an important role in viral replication. Paraspeckle bodies are multifunctional and involved in various processes including gene expression, mRNA splicing, and anti-viral defenses. To better understand the role of SFPQ during KSHV replication, we performed SFPQ immunoprecipitation followed by mass spectrometry from KSHV-infected cells. Proteomic analysis showed that during lytic reactivation, SFPQ associates with viral proteins, including ORF10, ORF59, and ORF61. These results are consistent with a previously reported ORF59 proteomics assay identifying SFPQ. To test if the association between ORF59 and SFPQ is important for replication, we first identified the region of ORF59 that associates with SFPQ using a series of 50 amino acid deletion mutants of ORF59 in the KSHV BACmid system. By performing co-immunoprecipitations, we identified the region spanning amino acids 101–150 of ORF59 as the association domain with SFPQ. Using this information, we generated a dominant negative polypeptide of ORF59 encompassing amino acids 101–150, that disrupted the association between SFPQ and full-length ORF59, and decreased virus production. Interestingly, when we tested other human herpesvirus processivity factors (EBV BMRF1, HSV-1 UL42, and HCMV UL44) by transfection of each expression plasmid followed by co-immunoprecipitation, we found a conserved association with SFPQ. These are limited studies that remain to be done in the context of infection but suggest a potential association of SFPQ with processivity factors across multiple herpesviruses.}
}
@article{SHUKLA2024100605,
title = {An overview of blockchain research and future agenda: Insights from structural topic modeling},
journal = {Journal of Innovation & Knowledge},
volume = {9},
number = {4},
pages = {100605},
year = {2024},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2024.100605},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X24001446},
author = {Anuja Shukla and Poornima Jirli and Anubhav Mishra and Alok Kumar Singh},
keywords = {Structural topic modeling, Blockchain, Scenario building, Datatopia, Natural language processing, Emerging technologies},
abstract = {As a disruptive technology, blockchain has become a strategic priority for many businesses. A vast amount of research exists on blockchain's innovative nature and immense potential for multiple industries. This study aims to synthesize the existing research to classify the findings into various themes and propose avenues for further research. A total of 2,360 academic articles were analyzed using the text-mining method of structural topic modeling. The identified fifteen topics were mapped to the four quadrants of the Datatopia model, leading to the development of the Datatopia-blockchain (DBlock) framework. The results present future scenarios that provide an understanding of what is known about blockchain, its characteristics, and potential research areas. The contributions to the theory and implications to the practitioners are discussed in detail.}
}
@article{MUTLU2024102994,
title = {A digital narrative study concerning global crisis period: The pandemic's impact on the domestic responsibilities of women health workers in Türkiye},
journal = {Women's Studies International Forum},
volume = {107},
pages = {102994},
year = {2024},
issn = {0277-5395},
doi = {https://doi.org/10.1016/j.wsif.2024.102994},
url = {https://www.sciencedirect.com/science/article/pii/S0277539524001328},
author = {Cemre Gül Mutlu and Funda Dağ},
keywords = {COVID-19, Female health worker, Unpaid domestic work, Digital storytelling, Digital narrative, Interpretative phenomenological analysis},
abstract = {The COVID-19 pandemic has exerted a profound influence on female healthcare workers, resulting in a notable escalation in the time allocated to domestic duties and unpaid household labor. This study delves into the professional and personal experiences of these women through the medium of digital narratives. Employing interpretive analysis within a phenomenological framework, the research scrutinized six female healthcare workers, all of whom were mothers of children under the age of 18. The investigation revealed that the pandemic has exacerbated existing gender disparities, manifesting in an increased burden of unpaid domestic responsibilities coupled with a concomitant reduction in personal time. Moreover, the utilization of digital narratives emerged as a multifaceted tool, not only facilitating socialization among healthcare workers but also nurturing their well-being and fostering the development of digital literacy skills.}
}
@article{PARAYIL2025216200,
title = {A review on defect engineered NIR persistent luminescence through transition metal ion (Cr, Mn, Fe and Ni) doping: Wider perspective covering synthesis, characterization, fundamentals and applications},
journal = {Coordination Chemistry Reviews},
volume = {522},
pages = {216200},
year = {2025},
issn = {0010-8545},
doi = {https://doi.org/10.1016/j.ccr.2024.216200},
url = {https://www.sciencedirect.com/science/article/pii/S0010854524005460},
author = {Reshmi T. Parayil and Santosh K. Gupta and M. Mohapatra},
keywords = {NIR persistent luminescence, defect engineering, Cr/Mn/Fe/Ni doping, Luminescence, Phosphors},
abstract = {Persistent luminescence is an optical phenomenon where materials continue to emit light after the cessation of the excitation source which leads to different applications in areas like bioimaging, information storage, anticounterfeiting, etc. This review focuses on the latest advancements in near-infrared (NIR) persistent luminescence (PersL) materials doped with Cr3+, Mn4+, Mn2+, Fe3+ and Ni2+along with recent advances in the synthesis and mechanisms associated with the afterglow. A comprehensive discussion on the various types of defects and their importance in NIR PersL materials is also included, along with a section dedicated to the techniques used to characterize these defects and application of NIR PersL materials in different areas. The review also examines the different strategies to improve the NIR PersL. It starts with a brief description of the history of the PersL and then discusses the reported NIR PersL phosphors activated by manganese, chromium, iron and nickel ions. Understanding the mechanism associated with PersL is very important to develop a novel PersL phosphor, so the review discussed the role of defects and traps in PersL along with different models which include the conduction band model, oxygen vacancy model, and quantum tunneling model which is followed by few main applications of PersL materials and culminated by concluding and associated challenges and future directions in this ever-growing field.}
}
@article{SEBASTIANO2024177478,
title = {Gene expression provides mechanistic insights into a viral disease in seabirds},
journal = {Science of The Total Environment},
volume = {957},
pages = {177478},
year = {2024},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2024.177478},
url = {https://www.sciencedirect.com/science/article/pii/S0048969724076356},
author = {Manrico Sebastiano and Olivier Chastel and Marcel Eens and David Costantini},
keywords = {Gene expression, Immune responses, Diseases, Wild animals, Frigatebirds, Metabolism},
abstract = {Wild animals are exposed to a variety of anthropogenic stressors that may result in loss of physiological homeostasis. One main consequence of this stress exposure is the increased vulnerability to pathogens. We addressed the hypothesis that energetic unbalance and alterations of immune effectors are key proximate mechanisms underlying this vulnerability, by quantifying the gene expression of magnificent frigatebird Fregata magnificens chicks affected by a highly lethal viral disease, whose appearance is favoured by food limitation in this species. A comparison between chicks with and without visible clinical signs of the disease using strict threshold of significance (p-value adjusted<0.05 and log2 fold-change above 1 or below −1) revealed 86 upregulated and 9 downregulated genes in sick chicks. The main differentially expressed genes with several fold difference between healthy and sick chicks were linked to biotic and external stimuli, inflammation and antifungal/antibacterial activity, signaling, and hydrolase activity. We further followed the chicks for several weeks, to identify chicks that became sick over the course of the study, to assess how the gene expression profile of chicks may predict the response to the disease. A comparison between chicks that remained always healthy and chicks that showed the appearance of visible clinical signs of the disease revealed 4 upregulated and 8 downregulated genes in chicks that became sick. The main differentially expressed genes with several fold difference between the two phenotypes were linked to cell development and differentiation, metabolism, and immunity. The results of our study suggest that alterations of the energetic machinery and of specific immune effectors (e.g. toll-like receptor, tetraspanins) underlie the impact of a viral disease on a free-living vertebrate. Our study contributes to a more comprehensive understanding of the host-pathogen interaction in wild animals and the physiological pathways involved, and provides insights for effective wildlife disease monitoring and management strategies.}
}
@article{SMITH2025102219,
title = {Estimating the density of urban trees in 1890s Leeds and Edinburgh using object detection on historical maps},
journal = {Computers, Environment and Urban Systems},
volume = {115},
pages = {102219},
year = {2025},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2024.102219},
url = {https://www.sciencedirect.com/science/article/pii/S0198971524001480},
author = {Eleanor S. Smith and Christopher Fleet and Stuart King and William Mackaness and Hannah Walker and Catherine E. Scott},
keywords = {Historical maps, Urban forests, Object detection, Machine learning, Template matching},
abstract = {We present a new end-to-end methodology for extracting symbols from historical maps and demonstrate an application of the method to extract details of the urban forests of Leeds and Edinburgh in the UK using Ordnance Survey maps from the 1890s. The methods presented allow tree symbols on 1:500 scale maps to be efficiently extracted, with our object detection model achieving an F1-score of 0.945. The results for each city are presented on the National Library of Scotland website and have been used to generate an estimate of 37 ± 1 tree symbols per hectare for Leeds in 1888–90 and 40 ± 1 tree symbols per hectare for Edinburgh in 1893–94. This is the first time that quantitative data has been obtained for historical urban tree counts in these two cities. The method presented can be expanded to other UK towns and cities and is a valuable tool for learning about the past, and changes to both the natural and built environment over time, aiding decisions on future tree planting. We discuss the process used to automate the generation of training data and to train a machine learning model to extract the symbols, comparing it with other possible models. This discussion provides context on how best to tackle similar problems of symbol extraction from historical maps and the issues that may arise in such automated analysis, alongside factors that must be considered when using historical maps as a data source.}
}
@incollection{BRENTS2025441,
title = {Community Engagement, Building, and Outreach},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {441-450},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00170-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032395689500170X},
author = {Madison Brents and Meghan Sprabary and Abby Stovall and Kristin Wolski},
keywords = {Academic libraries, Artificial intelligence, Assessment, Censorship, Community building, Corporate libraries, Engagement, Library history, Marketing, Outreach, Programming, Public libraries, Rural libraries, School libraries, Social media, Virtual spaces},
abstract = {Library employees spread awareness of library resources and services to its community through engagement, building, and outreach activities. In some libraries, specific positions exist for this role and in others, these tasks are part of an all-encompassing job. Library employees aim to reach infrequent or non-users of libraries but also maintain positive relationships with recurring library patrons. Library employees in this role encounter more social interaction compared to other library jobs. This entry outlines the definitions and concept of community, engagement, and outreach, as well as the historical context, current state, and future trends in different types of libraries.}
}
@article{SOMPOLGRUNK2024105801,
title = {Strategic alignment of BIM and big data through systematic analysis and model development},
journal = {Automation in Construction},
volume = {168},
pages = {105801},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105801},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524005375},
author = {Apeesada Sompolgrunk and Saeed Banihashemi and Hamed Golzad and Khuong {Le Nguyen}},
keywords = {BIM, Big data, Strategic alignment model (SAM), Industry 4.0, Construction industry},
abstract = {Organisations increasingly rely on data-driven strategies, utilising analytics to achieve competitive advantages. This paper systematically investigates the integration of big data into Building Information Modeling (BIM) within the Architecture, Engineering, and Construction (AEC) sectors, named “big BIM data.” Employing mixed methods of systematic and bibliometric analysis, it synthesises findings from 125 records published 2013–23. While many studies are at preliminary stages with conceptual or small-scale experimental approaches, the paper categorises its results into four domains: AEC organisational infrastructure, big BIM data (IT) infrastructure, AEC organisational strategic domain, and big BIM data (IT) strategic domain, aligned with the Strategic Alignment Model (SAM), exploring organisational competencies, governance factors, and strategic frameworks. This paper introduces the AEC Organisational - Big BIM Data SAM as the research agenda to implement big BIM data utilisation across AEC industry. This framework thoroughly addresses organisational dynamics while emphasising interconnectedness among individual projects, organisational tiers, and industry-wide standards.}
}
@article{DHAR2024100679,
title = {Digital to quantum watermarking: A journey from past to present and into the future},
journal = {Computer Science Review},
volume = {54},
pages = {100679},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100679},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000637},
author = {Swapnaneel Dhar and Aditya Kumar Sahu},
keywords = {Classical or digital watermarking, Quantum watermarking, Quantum entanglement, Quantum superposition},
abstract = {With the amplification of digitization, the surge in multimedia content, such as text, video, audio, and images, is incredible. Concomitantly, the incidence of multimedia tampering is also apparently increasing. Digital watermarking (DW) is the means of achieving privacy and authentication of the received content while preserving integrity and copyright. Literature has produced a plethora of state-of-the-art DW techniques to achieve the right balance between its performance measuring parameters, including high imperceptibility, increased watermarking ability, and tamper-free recovery. Meanwhile, during the vertex of DW, scientific advances in quantum computing led to the emergence of quantum-based watermarking. Though quantum watermarking (QW) is in its nascent stage, it has become captivating among researchers to dive deep inside it. This study not only investigates the performance of existing DW techniques but also extensively assesses the recently devised QW techniques. It further presents how the principles of quantum entanglement and superposition can be decisive in achieving superior immunity against several watermarking attacks. To the best of our knowledge, this study is the unique one to present a comprehensive review of both DW as well as QW techniques. Therefore, the facts presented in this study could be a baseline for the researchers to devise a novel DW or QW technique.}
}
@article{HOSSAIN2025125643,
title = {AuthorNet: Leveraging attention-based early fusion of transformers for low-resource authorship attribution},
journal = {Expert Systems with Applications},
volume = {262},
pages = {125643},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125643},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424025107},
author = {Md. Rajib Hossain and Mohammed Moshiul Hoque and M. Ali Akber Dewan and Enamul Hoque and Nazmul Siddique},
keywords = {Natural language processing, Fine-tuning, Hyperparameters tuning, Early fusion, Multi-head attention, Low resource language, Authorship attribution},
abstract = {Authorship Attribution (AA) is crucial for identifying the author of a given text from a pool of suspects, especially with the widespread use of the internet and electronic devices. However, most AA research has primarily focused on high-resource languages like English, leaving low-resource languages such as Bengali relatively unexplored. Challenges faced in this domain include the absence of benchmark corpora, a lack of context-aware feature extractors, limited availability of tuned hyperparameters, and OOV issues. To address these challenges, this study introduces AuthorNet for authorship attribution using attention-based early fusion of transformer-based language models, i.e., concatenation of an embeddings output of two existing models that were fine-tuned. AuthorNet consists of three key modules: Feature extraction, Fine-tuning and selection of best-performing models, and Attention-based early fusion. To evaluate the performance of AuthorNet, a number of experiments using four benchmark corpora have been conducted. The results demonstrated exceptional accuracy: 98.86 ± 0.01%, 99.49 ± 0.01%, 97.91 ± 0.01%, and 99.87 ± 0.01% for four corpora. Notably, AuthorNet outperformed all foundation models, achieving accuracy improvements ranging from 0.24% to 2.92% across the four corpora.}
}
@article{NAKANO2024100188,
title = {Advancing state of health estimation for electric vehicles: Transformer-based approach leveraging real-world data},
journal = {Advances in Applied Energy},
volume = {16},
pages = {100188},
year = {2024},
issn = {2666-7924},
doi = {https://doi.org/10.1016/j.adapen.2024.100188},
url = {https://www.sciencedirect.com/science/article/pii/S266679242400026X},
author = {Kosaku Nakano and Sophia Vögler and Kenji Tanaka},
keywords = {Lithium-ion battery, Electric vehicle, State of health estimation, Deep learning, Transformer},
abstract = {The widespread adoption of electric vehicles (EVs) underscores the urgent need for innovative approaches to estimate their lithium-ion batteries’ state of health (SOH), which is crucial for ensuring safety and efficiency. This study introduces SOH-TEC, a transformer encoder-based model that processes raw time-series battery and vehicle-related data from a single EV trip to estimate the SOH. Unlike conventional methods that rely on lab-experimented battery cycle data, SOH-TEC utilizes real-world EV operation data, enhancing practical application. The model is trained and evaluated on a real-world dataset collected over nearly three years from three EVs. This dataset includes reliable SOH labels obtained through periodic constant-current full-discharge tests using a chassis dynamometer. Despite the challenges posed by noisy EV real-world data, the model shows high accuracy, with a mean absolute error of 0.72% and a root mean square error of 1.17%. Moreover, our proposed pre-training strategies with unlabeled data, particularly SOH ordinal comparison, significantly enhance the model’s performance; using only 50% of the labeled data achieves results nearly identical to those obtained with the full dataset. Self-attention map analysis reveals that the model primarily focuses on stationary or consistent driving periods to estimate SOH. While the study is constrained by a dataset featuring repetitive driving patterns, it highlights the significant potential of transformer for SOH estimation in EVs and offers valuable insights for future data collection and model development.}
}
@article{BAKAS2024e589,
title = {Artificial Intelligence for Response Assessment in Neuro Oncology (AI-RANO), part 2: recommendations for standardisation, validation, and good clinical practice},
journal = {The Lancet Oncology},
volume = {25},
number = {11},
pages = {e589-e601},
year = {2024},
issn = {1470-2045},
doi = {https://doi.org/10.1016/S1470-2045(24)00315-2},
url = {https://www.sciencedirect.com/science/article/pii/S1470204524003152},
author = {Spyridon Bakas and Philipp Vollmuth and Norbert Galldiks and Thomas C Booth and Hugo J W L Aerts and Wenya Linda Bi and Benedikt Wiestler and Pallavi Tiwari and Sarthak Pati and Ujjwal Baid and Evan Calabrese and Philipp Lohmann and Martha Nowosielski and Rajan Jain and Rivka Colen and Marwa Ismail and Ghulam Rasool and Janine M Lupo and Hamed Akbari and Joerg C Tonn and David Macdonald and Michael Vogelbaum and Susan M Chang and Christos Davatzikos and Javier E Villanueva-Meyer and Raymond Y Huang},
abstract = {Summary
Technological advancements have enabled the extended investigation, development, and application of computational approaches in various domains, including health care. A burgeoning number of diagnostic, predictive, prognostic, and monitoring biomarkers are continuously being explored to improve clinical decision making in neuro-oncology. These advancements describe the increasing incorporation of artificial intelligence (AI) algorithms, including the use of radiomics. However, the broad applicability and clinical translation of AI are restricted by concerns about generalisability, reproducibility, scalability, and validation. This Policy Review intends to serve as the leading resource of recommendations for the standardisation and good clinical practice of AI approaches in health care, particularly in neuro-oncology. To this end, we investigate the repeatability, reproducibility, and stability of AI in response assessment in neuro-oncology in studies on factors affecting such computational approaches, and in publicly available open-source data and computational software tools facilitating these goals. The pathway for standardisation and validation of these approaches is discussed with the view of trustworthy AI enabling the next generation of clinical trials. We conclude with an outlook on the future of AI-enabled neuro-oncology.}
}
@article{MASCITTI2024106037,
title = {The Metaverse impact on the politics means},
journal = {Computer Law & Security Review},
volume = {55},
pages = {106037},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106037},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924001031},
author = {Matías Mascitti},
keywords = {Metaverse, Hypothetical scenario, Rule of law, Democracy, Human rights, Adaptation},
abstract = {Here, I pose a hypothetical scenario starring the Metaverse arrival in fifteen years. First, I describe this network of networks. Then, I provide some notes on the Metaverse impact on the politics means in the constitutional states, i.e., rule of law, democracy, and human rights. Next, I propose some measures to fit that political means to the Metaverse ecosystem. Hence, they will serve as the basis for the Metaverse regulation in advance and -in turn- they will be useful as a starting point for the academic debate and will enlighten us for the analysis of concepts and institutions that today require reforms because they are not suitable for the regulation of intersubjective conducts in the digital era; e.g., civil liability for damage caused by robotics and autonomous systems, 'unlimited' power of Terms of service imposition by largest internet platforms, real democracy weakness, data and privacy protection by the use of extended reality tools, non-personal data protection.}
}
@incollection{EGAN202598,
title = {Plagiarism: History, Culture, and Prevention},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {98-107},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00018-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895000183},
author = {Laura Egan},
keywords = {Academic integrity, AI chatbots, Artificial intelligence, ChatGPT, Common knowledge, Contract cheating, Copyright, Cultural differences, Large language models, Plagiarism, Plagiarism consequences, Plagiarism detection tools, Self-plagiarism},
abstract = {This entry covers the origins of the word plagiarism, how it differentiates from common knowledge and copyright, and defines different types of plagiarism. It addresses how perceptions of plagiarism have changed over time, variations in how different cultures view plagiarism, and how technological advances have led to increases in plagiarism. One section focuses on plagiarism and academic integrity, providing examples of incidents and their consequences, as well as non-academic examples. Later sections identify how training and skillsets can reduce plagiarism, as well as providing a detailed list of plagiarism detection tools. Finally, the entry addresses plagiarism issues related to using artificial intelligence Large Language Models (LLMs), such as ChatGPT.}
}
@article{SHARMA2024997,
title = {Phenotypic approaches for CNS drugs},
journal = {Trends in Pharmacological Sciences},
volume = {45},
number = {11},
pages = {997-1017},
year = {2024},
issn = {0165-6147},
doi = {https://doi.org/10.1016/j.tips.2024.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0165614724001883},
author = {Raahul Sharma and Caitlin R.M. Oyagawa and Hamid Abbasi and Michael Dragunow and Daniel Conole},
keywords = {CNS disorders, phenotypic drug discovery, screening platforms, compound library, chemical proteomics, artificial intelligence},
abstract = {Central nervous system (CNS) drug development is plagued by high clinical failure rate. Phenotypic assays promote clinical translation of drugs by reducing complex brain diseases to measurable, clinically valid phenotypes. We critique recent platforms integrating patient-derived brain cells, which most accurately recapitulate CNS disease phenotypes, with higher throughput models, including immortalized cells, to balance validity and scalability. These platforms were screened with conventional commercial chemogenomic compound libraries. We explore emerging library curation strategies to improve hit rate and quality, and screening novel fragment libraries as alternatives, for more tractable drug target deconvolution. The clinically relevant models used in these platforms could harbor important, unidentified drug targets, so we review evolving agnostic target deconvolution approaches, including chemical proteomics and artificial intelligence (AI), which aid in phenotypic screening hit mechanism elucidation, thereby facilitating rational hit-to-drug optimization.}
}
@article{SHI2024127520,
title = {Serum trace elements and osteoarthritis: A meta-analysis and Mendelian randomization study},
journal = {Journal of Trace Elements in Medicine and Biology},
volume = {86},
pages = {127520},
year = {2024},
issn = {0946-672X},
doi = {https://doi.org/10.1016/j.jtemb.2024.127520},
url = {https://www.sciencedirect.com/science/article/pii/S0946672X24001408},
author = {Haoyan Shi and Haochen Wang and Minghao Yu and Jianbang Su and Ze Zhao and Tianqi Gao and Qian Zhang and Yingliang Wei},
keywords = {Osteoarthritis, Trace elements, Copper, Meta-analysis, Mendelian Randomization},
abstract = {Objective
This study aims to establish the correlation between shifts in serum trace element (TE) levels and the progression of osteoarthritis (OA), while also exploring the underlying causal relationship between these variables.
Methods
An investigation was conducted, which included a systematic review, a meta-analysis of observational studies, and a two-sample Mendelian randomization (MR) study.
Results
This meta-analysis revealed significant differences in serum levels of copper, manganese, cadmium, and selenium between OA patients and healthy controls, after adjusting for heterogeneity. Specifically, significant disparities were observed for copper (SMD 0.118 [95 % CI: 0.061 ∼ 0.175], P < 0.001), manganese (SMD −0.180 [95 % CI: −0.326 ∼ −0.034], P = 0.016), cadmium (SMD 0.227 [95 % CI: 0.131 ∼ 0.322], P < 0.001), and selenium (SMD −0.138 [95 % CI: −0.209 ∼ −0.068], P < 0.001), while zinc levels did not show a significant difference (SMD −0.02 [95 % CI: −0.077 ∼ 0.038], P = 0.503). Further, MR analysis suggested a causal link between genetically predicted serum copper level changes and OA development, but not for other TEs.
Conclusion
The study suggests that there is an association between the occurrence of OA and variations in serum levels of copper, manganese, cadmium, and selenium. Elevated serum copper may play a pivotal role. Further research is needed to explore the therapeutic potential of TE level modulation in OA management.}
}
@article{THOMPSON20244147,
title = {Zero-shot counting with a dual-stream neural network model},
journal = {Neuron},
volume = {112},
number = {24},
pages = {4147-4158.e5},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324007293},
author = {Jessica A.F. Thompson and Hannah Sheahan and Tsvetomira Dumbalska and Julian D. Sandbrink and Manuela Piazza and Christopher Summerfield},
keywords = {enumeration, visual reasoning, PPC, numerical cognition, dorsal stream, enactive cognition, neural networks, zero-shot generalization, attention, structure learning},
abstract = {Summary
To understand a visual scene, observers need to both recognize objects and encode relational structure. For example, a scene comprising three apples requires the observer to encode concepts of “apple” and “three.” In the primate brain, these functions rely on dual (ventral and dorsal) processing streams. Object recognition in primates has been successfully modeled with deep neural networks, but how scene structure (including numerosity) is encoded remains poorly understood. Here, we built a deep learning model, based on the dual-stream architecture of the primate brain, which is able to count items “zero-shot”—even if the objects themselves are unfamiliar. Our dual-stream network forms spatial response fields and lognormal number codes that resemble those observed in the macaque posterior parietal cortex. The dual-stream network also makes successful predictions about human counting behavior. Our results provide evidence for an enactive theory of the role of the posterior parietal cortex in visual scene understanding.}
}
@article{WHITROCK20241610,
title = {Does using artificial intelligence take the person out of personal statements? We can't tell},
journal = {Surgery},
volume = {176},
number = {6},
pages = {1610-1616},
year = {2024},
issn = {0039-6060},
doi = {https://doi.org/10.1016/j.surg.2024.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0039606024005907},
author = {Jenna N. Whitrock and Catherine G. Pratt and Michela M. Carter and Ryan C. Chae and Adam D. Price and Carla F. Justiniano and Robert M. {Van Haren} and Latifa S. Silski and Ralph C. Quillin and Shimul A. Shah},
abstract = {Background
Use of artificial intelligence to generate personal statements for residency is currently not permitted but is difficult to monitor. This study sought to evaluate the ability of surgical residency application reviewers to identify artificial intelligence–generated personal statements and to understand perceptions of this practice.
Methods
Three personal statements were generated using ChatGPT, and 3 were written by medical students who previously matched into surgery residency. Blinded participants at a single institution were instructed to read all personal statements and identify which were generated by artificial intelligence; they then completed a survey exploring their opinions regarding artificial intelligence use.
Results
Of the 30 participants, 50% were faculty (n = 15) and 50% were residents (n = 15). Overall, experience ranged from 0 to 20 years (median, 2 years; interquartile range, 1–6.25 years). Artificial intelligence–derived personal statements were identified correctly only 59% of the time, with 3 (10%) participants identifying all the artificial intelligence–derived personal statements correctly. Artificial intelligence–generated personal statements were labeled as the best 60% of the time and the worst 43.3% of the time. When asked whether artificial intelligence use should be allowed in personal statements writing, 66.7% (n = 20) said no and 30% (n = 9) said yes. When asked if the use of artificial intelligence would impact their opinion of an applicant, 80% (n = 24) said yes, and 20% (n = 6) said no. When survey questions and ability to identify artificial intelligence–generated personal statements were evaluated by faculty/resident status and experience, no differences were noted (P > .05).
Conclusion
This study shows that surgical faculty and residents cannot reliably identify artificial intelligence–generated personal statements and that concerns exist regarding the impact of artificial intelligence on the application process.}
}
@article{FEI2025104126,
title = {LaAeb: A comprehensive log-text analysis based approach for insider threat detection},
journal = {Computers & Security},
volume = {148},
pages = {104126},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104126},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004310},
author = {Kexiong Fei and Jiang Zhou and Yucan Zhou and Xiaoyan Gu and Haihui Fan and Bo Li and Weiping Wang and Yong Chen},
keywords = {Insider threat detection, Log analysis, Anomaly detection, Cyber security, User behavior analysis},
abstract = {Insider threats have increasingly become a critical issue that modern enterprises and organizations faced. They are mainly initiated by insider attackers, which may cause disastrous impacts. Numerous research studies have been conducted for insider threat detection. However, most of them are limited due to a small number of malicious samples. Moreover, as existing methods often concentrate on feature information or statistical characteristics for anomaly detection, they still lack effective use of comprehensive textual content information contained in logs and thus will affect detection efficiency. We propose LaAeb, a novel unsupervised insider threat detection framework that leverages rich linguistic information in log contents to enable conventional methods, such as an Isolation Forest-based anomaly detection, to better detect insider threats besides using various features and statistical information. To find malicious acts under different scenarios, we consider three patterns of insider threats, including attention, emotion, and behavior anomaly. The attention anomaly detection analyzes textual contents of operation objects (e.g., emails and web pages) in logs to detect threats, where the textual information reflects the areas that employees focus on. When the attention seriously deviates from daily work, an employee may involve malicious acts. The emotion anomaly detection analyzes all dialogs between every two employees’ daily communicated texts and uses the degree of negative to find potential psychological problems. The behavior anomaly detection analyzes the operations of logs to detect threats. It utilizes information acquired from attention and emotion anomalies as ancillary features, integrating them with features and statistics extracted from log operations to create log embeddings. With these log embeddings, LaAeb employs anomaly detection algorithm like Isolation Forest to analyze an employee’s malicious operations, and further detects the employee’s behavior anomaly by considering all employees’ acts in the same department. Finally, LaAeb consolidates detection results of three patterns indicative of insider threats in a comprehensive manner. We implement the prototype of LaAeb and test it on CERT and LANL datasets. Our evaluations demonstrate that compared with state-of-the-art unsupervised methods, LaAeb reduces FPR by 50% to reach 0.05 on CERT dataset under the same AUC (0.93), and gets the best AUC (0.97) with 0.06 higher value on LANL dataset.}
}
@article{PARAMBIL2024100327,
title = {Integrating AI-based and conventional cybersecurity measures into online higher education settings: Challenges, opportunities, and prospects},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100327},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100327},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001309},
author = {Medha Mohan Ambali Parambil and Jaloliddin Rustamov and Soha Galalaldin Ahmed and Zahiriddin Rustamov and Ali Ismail Awad and Nazar Zaki and Fady Alnajjar},
keywords = {Higher education, Online education systems, E-learning systems, Cybersecurity, Artificial intelligence, Ethics and privacy},
abstract = {The rapid adoption of online learning in higher education has resulted in significant cybersecurity challenges. As educational institutions increasingly rely on digital platforms, they are facing cyber threats that can compromise sensitive data and disrupt operations. This systematic literature review explores the integration of artificial intelligence (AI) into traditional methods to address cybersecurity risks in online higher education. The review integrates a qualitative synthesis of relevant literature and a quantitative meta-analysis using PRISMA guidelines, ensuring comprehensive insights into the integration process. The most prevalent cybersecurity threats are examined, and the effectiveness of AI-based and conventional approaches in mitigating these challenges is compared. Additionally, the most effective AI techniques in cybersecurity solutions are analyzed, and their performance is compared across different contexts. Furthermore, the review considers the key ethical and technical considerations associated with integrating AI into traditional cybersecurity methods. The findings reveal that while AI-based techniques offer promising solutions for threat detection, authentication, and privacy preservation, their successful implementation requires careful consideration of data privacy, fairness, transparency, and robustness. The importance of interdisciplinary collaboration, continuous monitoring of AI models—by automated systems and humans—and the need for comprehensive guidelines to ensure responsible and ethical use of AI in cybersecurity are highlighted. The findings of this review provide actionable insights for educational institutions, educators, and students, helping to facilitate the development of secure and resilient online learning environments. The identified ethical and technical considerations can serve as a foundation for the responsible integration of AI into cybersecurity within the online higher-education sector.}
}
@article{MA2024101100,
title = {The bounded intelligence of AI: Superficiality and deceivability},
journal = {Organizational Dynamics},
pages = {101100},
year = {2024},
issn = {0090-2616},
doi = {https://doi.org/10.1016/j.orgdyn.2024.101100},
url = {https://www.sciencedirect.com/science/article/pii/S0090261624000731},
author = {Hao Ma and Mengyue Su},
keywords = {Artificial intelligence, Bounded intelligence, Superficiality, Deceivability, Coping strategy},
abstract = {Artificial Intelligence (AI) is typically designed to replace or augment human beings in making and executing decisions. In the domain of organizational management, AI has already found wide applications in many areas such as HR functions, marketing campaigns, competitive analysis, and strategy formulations, exerting increasing impacts on a host of executives and management professionals. Accurate and timely replicating or capturing the know-how of human decisions and actions becomes imperative, as AI has to be, at least as intelligent as, or even more intelligent than, those human actors whose data are being fed to AI during its training and learning iterations. Just as human beings are subject to bounded rationality due to their limited capacities in information processing, AI, too, is subject to bounded intelligence, in the sense that it could appear not as smart as expected, due to its limited capacities in replicating and capturing human expertise, to say nothing of augmenting or extending it. Such bounded intelligence of AI hinges on two underlying factors: superficiality and deceivability. Superficiality refers to the inability of AI to fully replicate human expertise during its learning process due to natural or technical barriers such as imperfect capturability, relational embeddedness, and time sensitivity. Deceivability refers to the inability of AI to accurately capture human expertise during its learning process due to human-actor-created barriers such as passive evasion, deliberate manipulation, and malicious sabotage. Understanding such boundedness of AI and the specific mechanisms of superficiality and deceivability will help us better appreciate the usefulness and limitations of AI and take measures to both remedy and utilize the bounded intelligence of AI.}
}
@incollection{HARMENING2025423,
title = {Chapter 24 - Information Security Essentials for IT Managers: Protecting Mission-Critical Systems},
editor = {John R. Vacca},
booktitle = {Computer and Information Security Handbook (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {423-432},
year = {2025},
isbn = {978-0-443-13223-0},
doi = {https://doi.org/10.1016/B978-0-443-13223-0.00024-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443132230000242},
author = {Jim Harmening},
keywords = {Access control, Application security, Cybersecurity, Cybersecurity framework, Cybersecurity risk management, Cybersecurity supply chain risk management, Encryption, Information security management, Risk management, Security architecture},
abstract = {Good policies and procedures are essential to the success of every information technology (IT) manager. Being agile in a constantly changing world will help you succeed in the long run. Knowing your assets, protecting them, and utilizing a risk-based approach to your security plans, policies, and procedures will make you prosper. Make sure you consider how you are handling data. The three key principles of data confidentiality, integrity, and availability will come in handy.}
}
@article{KULIKOVA2024,
title = {Semantic search using protein large language models detects class II microcins in bacterial genomes},
journal = {mSystems},
volume = {9},
number = {10},
year = {2024},
issn = {2379-5077},
doi = {https://doi.org/10.1128/msystems.01044-24},
url = {https://www.sciencedirect.com/science/article/pii/S2379507724002952},
author = {Anastasiya V. Kulikova and Jennifer K. Parker and Bryan W. Davies and Claus O. Wilke},
keywords = {class II microcin, protein large language model, embedding},
abstract = {ABSTRACT

Class II microcins are antimicrobial peptides that have shown some potential as novel antibiotics. However, to date, only 10 class II microcins have been described, and the discovery of novel microcins has been hampered by their short length and high sequence divergence. Here, we ask if we can use numerical embeddings generated by protein large language models to detect microcins in bacterial genome assemblies and whether this method can outperform sequence-based methods such as BLAST. We find that embeddings detect known class II microcins much more reliably than does BLAST and that any two microcins tend to have a small distance in embedding space even though they typically are highly diverged at the sequence level. In data sets of Escherichia coli, Klebsiella spp., and Enterobacter spp. genomes, we further find novel putative microcins that were previously missed by sequence-based search methods.
IMPORTANCE
Antibiotic resistance is becoming an increasingly serious problem in modern medicine, but the development pipeline for conventional antibiotics is not promising. Therefore, alternative approaches to combat bacterial infections are urgently needed. One such approach may be to employ naturally occurring antibacterial peptides produced by bacteria to kill competing bacteria. A promising class of such peptides are class II microcins. However, only a small number of class II microcins have been discovered to date, and the discovery of further such microcins has been hampered by their high sequence divergence and short length, which can cause sequence-based search methods to fail. Here, we demonstrate that a more robust method for microcin discovery can be built on the basis of a protein large language model, and we use this method to identify several putative novel class II microcins.
Antibiotic resistance is becoming an increasingly serious problem in modern medicine, but the development pipeline for conventional antibiotics is not promising. Therefore, alternative approaches to combat bacterial infections are urgently needed. One such approach may be to employ naturally occurring antibacterial peptides produced by bacteria to kill competing bacteria. A promising class of such peptides are class II microcins. However, only a small number of class II microcins have been discovered to date, and the discovery of further such microcins has been hampered by their high sequence divergence and short length, which can cause sequence-based search methods to fail. Here, we demonstrate that a more robust method for microcin discovery can be built on the basis of a protein large language model, and we use this method to identify several putative novel class II microcins.}
}
@article{DAS2024126506,
title = {Organic two-dimensional nanostructures: Harnessing soft matter for multifunctional applications},
journal = {Journal of Molecular Liquids},
volume = {416},
pages = {126506},
year = {2024},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2024.126506},
url = {https://www.sciencedirect.com/science/article/pii/S0167732224025650},
author = {Tarak Nath Das and Sourav Moyra and Russel Aliamintakath Sharafudheen and Arghya Ghosh and Aparna Ramesh and Tapas Kumar Maji and Goutam Ghosh},
keywords = {Organic 2D nanostructures, Molecular engineering, Supramolecular self-assembly, Energy harvesting materials, Biocompatible 2D assemblies},
abstract = {Over the past two decades, two-dimensional (2D) materials have garnered substantial interest because of their distinctive atomic-scale layered structures, which are associated with a variety of applications, ranging from materials science to biomedical processes. However, in the case of inorganic 2D materials, challenges related to large-scale production and the toxicity associated with heavy metals greatly restrict their applications. Hence, the development of organic 2D systems has garnered significant interest, offering immense potential, as an almost infinite range of molecules can be designed and synthesized with predictable functionalities. Thus, developing a range of techniques that provide advanced, customizable synthesis methods for producing intrinsically flexible, lightweight, and easily processable 2D nanomaterials is crucial. In this context, various non-covalent interactions, including hydrogen bonding (H-bonding), π-stacking, electrostatic forces, coordination linkages, and van der Waals forces, play a vital role in stabilizing the overall structure, which can be tuned to manipulate the structure–property relationship. The high surface area and active site exposure of these 2D systems are key to their advancement in applications in materials science, nanodevices, optoelectronics, energy and environmental science, and biomedical fields. This review discusses the development of non-covalently and covalently linked organic 2D nano assemblies, highlighting various synthetic approaches and their potential applications in the current context. Beginning with the increasing dimensionality of small molecular self-assembled nanostructures, the discussion progresses to various interfacial synthetic approaches for creating covalently linked systems with precise control over their dimensionality and crystallinity. Specifically, we presented an overview of supramolecular 2D assemblies of small organic molecules, peptide self-assembly, covalent organic frameworks (COFs), and coordination polymers (CPs), offering insights into potential future research directions.}
}