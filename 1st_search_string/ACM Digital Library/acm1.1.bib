@article{10.1007/s00165-021-00549-0,
author = {Dubslaff, Clemens and Koopmann, Patrick and Turhan, Anni-Yasmin},
title = {Enhancing Probabilistic Model Checking with Ontologies},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00549-0},
doi = {10.1007/s00165-021-00549-0},
abstract = {Probabilistic model checking (PMC) is a well-established method for the quantitative analysis of state based operational models such as Markov decision processes. Description logics (DLs) provide a well-suited formalism to describe and reason about knowledge and are used as basis for the web ontology language (OWL). We investigate how such knowledge described by DLs can be integrated into the PMC process, introducing ontology-mediated PMC. Specifically, we propose ontologized programs as a formalism that links ontologies to behaviors specified by probabilistic guarded commands, the de-facto standard input formalism for PMC tools such as Prism. Through DL reasoning, inconsistent states in the modeled system can be detected. We present three ways to resolve these inconsistencies, leading to different Markov decision process semantics. We analyze the computational complexity of checking whether an ontologized program is consistent under these semantics. Further, we present and implement a technique for the quantitative analysis of ontologized programs relying on standard DL reasoning and PMC tools. This way, we enable the application of PMC techniques to analyze knowledge-intensive systems.We evaluate our approach and implementation on amulti-server systemcase study,where different DL ontologies are used to provide specifications of different server platforms and situations the system is executed in.},
journal = {Form. Asp. Comput.},
month = dec,
pages = {885–921},
numpages = {37},
keywords = {Probabilisticmodel checking, Ontologies, Description logics, Ontology-mediated verification, Context dependent systems analysis}
}

@inproceedings{10.1109/ASE.2019.00011,
author = {Kang, Hong Jin and Bissyand\'{e}, Tegawend\'{e} F. and Lo, David},
title = {Assessing the generalizability of code2vec token embeddings},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00011},
doi = {10.1109/ASE.2019.00011},
abstract = {Many Natural Language Processing (NLP) tasks, such as sentiment analysis or syntactic parsing, have benefited from the development of word embedding models. In particular, regardless of the training algorithms, the learned embeddings have often been shown to be generalizable to different NLP tasks. In contrast, despite recent momentum on word embeddings for source code, the literature lacks evidence of their generalizability beyond the example task they have been trained for.In this experience paper, we identify 3 potential downstream tasks, namely code comments generation, code authorship identification, and code clones detection, that source code token embedding models can be applied to. We empirically assess a recently proposed code token embedding model, namely code2vec's token embeddings. Code2vec was trained on the task of predicting method names, and while there is potential for using the vectors it learns on other tasks, it has not been explored in literature. Therefore, we fill this gap by focusing on its generalizability for the tasks we have identified. Eventually, we show that source code token embeddings cannot be readily leveraged for the downstream tasks. Our experiments even show that our attempts to use them do not result in any improvements over less sophisticated methods. We call for more research into effective and general use of code embeddings.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1–12},
numpages = {12},
keywords = {big code, code embeddings, distributed representations},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1109/TASLP.2022.3190732,
author = {Gupta, Chitralekha and Li, Haizhou and Goto, Masataka},
title = {Deep Learning Approaches in Topics of Singing Information Processing},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3190732},
doi = {10.1109/TASLP.2022.3190732},
abstract = {Singing, the vocal productionof musical tones, is one of the most important elements of music. Addressing the needs of real-world applications, the study of technologies related to singing voices has become an increasingly active area of research. In this paper, we provide a comprehensive overview of the recent developments in the field of singing information processing, specifically in the topics of singing skill evaluation, singing voice synthesis, singing voice separation, and lyrics synchronization and transcription. We will especially focus on deep learning approaches including modern representation learning techniques for singing voices. We will also provide an overview of contributions in public datasets for singing voice research.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {2422–2451},
numpages = {30}
}

@book{10.1145/3107990,
editor = {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip R. and Sonntag, Daniel and Potamianos, Gerasimos and Kr\"{u}ger, Antonio},
title = {The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition - Volume 2},
year = {2018},
isbn = {9781970001716},
publisher = {Association for Computing Machinery and Morgan \&amp; Claypool},
volume = {21},
abstract = {The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces: user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces that often include biosignals.This edited collection is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This second volume of the handbook begins with multimodal signal processing, architectures, and machine learning. It includes recent deep-learning approaches for processing multisensorial and multimodal user data and interaction, as well as context-sensitivity. A further highlight is processing of information about users' states and traits, an exciting emerging capability in next-generation user interfaces. These chapters discuss real-time multimodal analysis of emotion and social signals from various modalities and perception of affective expression by users. Further chapters discuss multimodal processing of cognitive state using behavioral and physiological signals to detect cognitive load, domain expertise, deception, and depression. This collection of chapters provides walk-through examples of system design and processing, information on tools and practical resources for developing and evaluating new systems, and terminology, and tutorial support for mastering this rapidly expanding field. In the final section of this volume, experts exchange views on the timely and controversial challenge topic of multimodal deep learning. The discussion focuses on how multimodal-multisensor interfaces are most likely to advance human performance during the next decade.}
}

@article{10.1145/3134599,
author = {Goodfellow, Ian and McDaniel, Patrick and Papernot, Nicolas},
title = {Making machine learning robust against adversarial inputs},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3134599},
doi = {10.1145/3134599},
abstract = {Such inputs distort how machine-learning-based systems are able to function in the world as it is.},
journal = {Commun. ACM},
month = jun,
pages = {56–66},
numpages = {11}
}

@article{10.1145/3152823,
author = {Xiao, Chang and Zhang, Cheng and Zheng, Changxi},
title = {FontCode: Embedding Information in Text Documents Using Glyph Perturbation},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {0730-0301},
url = {https://doi.org/10.1145/3152823},
doi = {10.1145/3152823},
abstract = {We introduce FontCode, an information embedding technique for text documents. Provided a text document with specific fonts, our method embeds user-specified information in the text by perturbing the glyphs of text characters while preserving the text content. We devise an algorithm to choose unobtrusive yet machine-recognizable glyph perturbations, leveraging a recently developed generative model that alters the glyphs of each character continuously on a font manifold. We then introduce an algorithm that embeds a user-provided message in the text document and produces an encoded document whose appearance is minimally perturbed from the original document. We also present a glyph recognition method that recovers the embedded information from an encoded document stored as a vector graphic or pixel image, or even on a printed paper. In addition, we introduce a new error-correction coding scheme that rectifies a certain number of recognition errors. Lastly, we demonstrate that our technique enables a wide array of applications, using it as a text document metadata holder, an unobtrusive optical barcode, a cryptographic message embedding scheme, and a text document signature.},
journal = {ACM Trans. Graph.},
month = feb,
articleno = {15},
numpages = {16},
keywords = {Font manifold, error correction coding, glyph perturbation, text document signature}
}

@article{10.1145/3158369,
author = {Zhang, Ligang and Verma, Brijesh and Tjondronegoro, Dian and Chandran, Vinod},
title = {Facial Expression Analysis under Partial Occlusion: A Survey},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3158369},
doi = {10.1145/3158369},
abstract = {Automatic machine-based Facial Expression Analysis (FEA) has made substantial progress in the past few decades driven by its importance for applications in psychology, security, health, entertainment, and human–computer interaction. The vast majority of completed FEA studies are based on nonoccluded faces collected in a controlled laboratory environment. Automatic expression recognition tolerant to partial occlusion remains less understood, particularly in real-world scenarios. In recent years, efforts investigating techniques to handle partial occlusion for FEA have seen an increase. The context is right for a comprehensive perspective of these developments and the state of the art from this perspective. This survey provides such a comprehensive review of recent advances in dataset creation, algorithm development, and investigations of the effects of occlusion critical for robust performance in FEA systems. It outlines existing challenges in overcoming partial occlusion and discusses possible opportunities in advancing the technology. To the best of our knowledge, it is the first FEA survey dedicated to occlusion and aimed at promoting better-informed and benchmarked future work.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {25},
numpages = {49},
keywords = {Facial expression analysis, emotion recognition, overview, partial occlusion, survey}
}

@inproceedings{10.1145/3167132.3167263,
author = {Carvalho, Tiago and Cardoso, Jo\~{a}o M. P.},
title = {An approach based on a DSL + API for programming runtime adaptivity and autotuning concerns},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167263},
doi = {10.1145/3167132.3167263},
abstract = {In the context of compiler optimizations, tuning of parameters and selection of algorithms, runtime adaptivity and autotuning are becoming increasingly important, especially due to the complexity of applications, workloads, computing devices and execution environments. For identifying and specifying adaptivity, different phases are required: analysis of program hotspots and adaptivity opportunities, code restructuring, and programming of adaptivity strategies. These phases usually require different tools and modifications to the source code that may result in difficult to maintain and error prone code. This paper presents a flexible approach to support the different phases when developing adaptive applications. The approach is based on a single domain-specific language (DSL), able to specify and evaluate multiple strategies and to maintain a separation of concerns. We describe the requirements and the design of the DSL, an accompanying API, and of a Java-to-Java compiler that implements the approach. In addition, we present and evaluate the use of the approach to specify runtime adaptivity strategies in the context of Java programs, especially when considering runtime autotuning of optimization parameters and runtime selection of algorithms. Although simple, the case studies shown truly demonstrate the main advantages of the approach in terms of the programming model and of the performance impact.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1211–1220},
numpages = {10},
keywords = {aspect-oriented programming, autotuning, runtime adaptivity},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3173574.3173889,
author = {Schlesinger, Ari and O'Hara, Kenton P. and Taylor, Alex S.},
title = {Let's Talk About Race: Identity, Chatbots, and AI},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173889},
doi = {10.1145/3173574.3173889},
abstract = {Why is it so hard for chatbots to talk about race? This work explores how the biased contents of databases, the syntactic focus of natural language processing, and the opaque nature of deep learning algorithms cause chatbots difficulty in handling race-talk. In each of these areas, the tensions between race and chatbots create new opportunities for people and machines. By making the abstract and disparate qualities of this problem space tangible, we can develop chatbots that are more capable of handling race-talk in its many forms. Our goal is to provide the HCI community with ways to begin addressing the question, how can chatbots handle race-talk in new and improved ways?},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {artificial intelligence, chatbots, race},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3176258.3176306,
author = {Pagani, Fabio and Dell'Amico, Matteo and Balzarotti, Davide},
title = {Beyond Precision and Recall: Understanding Uses (and Misuses) of Similarity Hashes in Binary Analysis},
year = {2018},
isbn = {9781450356329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176258.3176306},
doi = {10.1145/3176258.3176306},
abstract = {Fuzzy hashing algorithms provide a convenient way of summarizing in a compact form the content of files, and of looking for similarities between them. Because of this, they are widely used in the security and forensics communities to look for similarities between binary program files; one version of them, ssdeep, is the de facto standard to share information about known malware.Fuzzy hashes are quite pervasive, but no study so far answers conclusively the question of which (if any) fuzzy hashing algorithms are suited to detect similarities between programs, where we consider as similar those programs that have code or libraries in common. We measure how four popular algorithms perform in different scenarios: when they are used to correlate statically-compiled files with the libraries they use, when compiled with different flags or different compilers, and when applied to programs that share a large part of their source code. Perhaps more importantly, we provide interpretations that explain the reasons why results vary, sometimes widely, among apparently very similar use cases.We find that the low-level details of the compilation process, together with the technicalities of the hashing algorithms, can explain surprising results such as similarities dropping to zero with the change of a single assembly instruction. More in general, we see that ssdeep, the de facto standard for this type of analysis, performs definitely worse than alternative algorithms; we also find that the best choice of algorithm to use varies depending on the particularities of the use case scenario.},
booktitle = {Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy},
pages = {354–365},
numpages = {12},
keywords = {approximate matching, binary analysis, fuzzy hash, malware},
location = {Tempe, AZ, USA},
series = {CODASPY '18}
}

@article{10.1145/3178112,
author = {Bakerman, Jordan and Pazdernik, Karl and Wilson, Alyson and Fairchild, Geoffrey and Bahran, Rian},
title = {Twitter Geolocation: A Hybrid Approach},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3178112},
doi = {10.1145/3178112},
abstract = {Geotagging Twitter messages is an important tool for event detection and enrichment. Despite the availability of both social media content and user network information, these two features are generally utilized separately in the methodology. In this article, we create a hybrid method that uses Twitter content and network information jointly as model features. We use Gaussian mixture models to map the raw spatial distribution of the model features to a predicted field. This approach is scalable to large datasets and provides a natural representation of model confidence. Our method is tested against other approaches and we achieve greater prediction accuracy. The model also improves both precision and coverage.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {34},
numpages = {17},
keywords = {Gaussian mixture model, Geotag, Twitter, comprehensive accuracy error, prediction region area, simple accuracy error}
}

@inproceedings{10.1145/3183428.3183430,
author = {Simm, W. A. and Samreen, F. and Bassett, R. and Ferrario, M. A. and Blair, G. and Whittle, J. and Young, P. J.},
title = {SE in ES: opportunities for software engineering and cloud computing in environmental science},
year = {2018},
isbn = {9781450356619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183428.3183430},
doi = {10.1145/3183428.3183430},
abstract = {New and emergent computing architectures and software engineering practices provide an opportunity for environmental models to be deployed more efficiently and democratically. In this paper we aim to capture the software engineering practices of environmental scientists, highlight opportunities for software engineering and work towards developing a domain specific language for the configuration and deployment of environmental models. We hold a series of interviews with environmental scientists involved in developing and deploying computer based environmental models about the approach taken in engineering models, and describe a case study in deploying an environmental model (WRF: Weather Research Forecasting) on a cloud architecture. From these studies we find a number of opportunities for A) software engineering methods and tools such as Domain Specific Languages to play a role in abstracting from underlying computing complexity, and for B) new architectures to increase efficiency and availability of deployment. Together, we propose they will allow scientists to concentrate on fundamental science rather than specifics of the underlying computing.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Society},
pages = {61–70},
numpages = {10},
keywords = {WRF, cloud computing, environmental modelling, environmental science, model driven engineering},
location = {Gothenburg, Sweden},
series = {ICSE-SEIS '18}
}

@book{10.1145/3191315,
editor = {Kifer, Michael and Liu, Yanhong Annie},
title = {Declarative Logic Programming: Theory, Systems, and Applications},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan \&amp; Claypool},
volume = {20},
abstract = {Logic Programming (LP) is at the nexus of knowledge representation, AI, mathematical logic, databases, and programming languages. It allows programming to be more declarative, by specifying “what” to do instead of “how” to do it. This field is fascinating and intellectually stimulating due to the fundamental interplay among theory, systems, and applications brought about by logic. Several books cover the basics of LP but they focus mostly on the Prolog language. There is generally a lack of accessible collections of articles covering the key aspects of LP, such as the well-founded vs. stable semantics for negation, constraints, object-oriented LP, updates, probabilistic LP, and implementation methods, including top-down vs. bottom-up evaluation and tabling.For systems, the situation is even less satisfactory, lacking expositions of LP inference machinery that supports tabling and other state-of-the-art implementation techniques. There is also a dearth of articles about systems that support truly declarative languages, especially those that tie into first-order logic, mathematical programming, and constraint programming. Also rare are surveys of challenging application areas of LP, such as bioinformatics, natural language processing, verification, and planning, as well as analysis of LP applications based on language abstractions and implementations methods.The goal of this book is to help fill in the void in the literature with state-of-the-art surveys on key aspects of LP. Much attention was paid to making these surveys accessible to researchers, practitioners, and graduate students alike.}
}

@inproceedings{10.1145/3194747.3194749,
author = {Smith, Spencer},
title = {Beyond software carpentry},
year = {2018},
isbn = {9781450357487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194747.3194749},
doi = {10.1145/3194747.3194749},
abstract = {About 20 years ago the need for scientists and engineers to have basic knowledge of software development skills and tools became apparent. Without these so-called software carpentry skills, developers were wasting time and compromising the quality of their work. Since that time great progress has been made with software carpentry, as evidenced by the growing understanding of the importance of tools, and by the growth of the namesake Software Carpentry foundation and other similar projects. With scientific software developers now prepared to move forward, we should turn our attention to the next logical step after carpentry: Software Engineering (SE) applied to Scientific Computing Software (SCS). Past attempts with SE for SCS have not always been successful; therefore, this paper proposes a vision for future success, including SE specifically adapting ideas to SCS, SCS recognizing the value of software artifacts other than the code, and all parties increasing the emphasis on empirical evidence and the quality of replicability. Several ideas are proposed for turning the proposed vision into a reality, including promoting requirements documentation for replicability, building assurance cases for correctness (and other qualities), and automatic generation of all documentation and code.},
booktitle = {Proceedings of the International Workshop on Software Engineering for Science},
pages = {32–39},
numpages = {8},
keywords = {assurance cases, document generation, requirements analysis, scientific computing, software engineering},
location = {Gothenburg, Sweden},
series = {SE4Science '18}
}

@inproceedings{10.1145/3196494.3196536,
author = {Gambs, S\'{e}bastien and Lolive, Julien and Robert, Jean-Marc},
title = {Entwining Sanitization and Personalization on Databases},
year = {2018},
isbn = {9781450355766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196494.3196536},
doi = {10.1145/3196494.3196536},
abstract = {In the last decade, a lot of research has been done to prevent the illegal distribution of digital content, \% in the context in which the proprietary content is a medium such as musical works and movies. However, only few works have tackled this problem for databases, and even less for databases containing personal and sensitive information (emphe.g, a medical database). In this work, we address this latter issue by proposing \o{}uralgo (for Sanitization and Personalization of Databases ), an approach in which the owner of a database personalizes it before distributing it to ensure that a malicious buyer can be traced back in case of an illegal redistribution. Our novel solution entwines the personalization step with a sanitization mechanism to prevent the leak of personal information and limit the privacy risks. Thus, our objective is to release a sanitized and personalized database, both to protect the privacy of the concerned individuals and to prevent the illegal redistribution, even from a collusion of malicious buyers.},
booktitle = {Proceedings of the 2018 on Asia Conference on Computer and Communications Security},
pages = {207–219},
numpages = {13},
keywords = {fingerprinting, sanitization, traitor-tracing},
location = {Incheon, Republic of Korea},
series = {ASIACCS '18}
}

@proceedings{10.1145/3202918,
title = {SIGGRAPH '18: ACM SIGGRAPH 2018 Art Gallery},
year = {2018},
isbn = {9781450357784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Building upon an exciting and eclectic selection of creative practices mediated through technologies that represent the sophistication of our times, the SIGGRAPH 2018 Art Gallery will embrace the narratives of the indigenous communities near Vancouver and throughout Canada as a source of inspiration.The exhibition will feature contemporary media artworks, art pieces by indigenous communities, and other traces of technologically mediated ludic practices.The exhibition aims to articulate myth and technology, science and art, the deep past and the computational present, and will coalesce around the theme of Origins. Media and technological creative expressions will explore principles such as the origins of cosmos, the origins of life, the origins of human presence on the planet, the origins of people that occupy the territories of the Americas, and the origins of people who are still living in the vast territories of the Arctic.Additionally, the venue intends to rekindle the original spark that ignited the collaborative spirit in the community of engineers, scientists, and artists who came together to create the first SIGGRAPH conference in the early 1970s.},
location = {Vancouver, British Columbia, Canada}
}

@inproceedings{10.1145/3209219.3209236,
author = {Galke, Lukas and Mai, Florian and Vagliano, Iacopo and Scherp, Ansgar},
title = {Multi-Modal Adversarial Autoencoders for Recommendations of Citations and Subject Labels},
year = {2018},
isbn = {9781450355896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209219.3209236},
doi = {10.1145/3209219.3209236},
abstract = {We present multi-modal adversarial autoencoders for recommendation and evaluate them on two different tasks: citation recommendation and subject label recommendation. We analyze the effects of adversarial regularization, sparsity, and different input modalities. By conducting 408 experiments, we show that adversarial regularization consistently improves the performance of autoencoders for recommendation. We demonstrate, however, that the two tasks differ in the semantics of item co-occurrence in the sense that item co-occurrence resembles relatedness in case of citations, yet implies diversity in case of subject labels. Our results reveal that supplying the partial item set as input is only helpful, when item co-occurrence resembles relatedness. When facing a new recommendation task it is therefore crucial to consider the semantics of item co-occurrence for the choice of an appropriate model.},
booktitle = {Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization},
pages = {197–205},
numpages = {9},
keywords = {adversarial autoencoders, multi-modal, neural networks, recommender systems, sparsity},
location = {Singapore, Singapore},
series = {UMAP '18}
}

@inproceedings{10.1145/3213846.3213848,
author = {Cummins, Chris and Petoumenos, Pavlos and Murray, Alastair and Leather, Hugh},
title = {Compiler fuzzing through deep learning},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213848},
doi = {10.1145/3213846.3213848},
abstract = {Random program generation — fuzzing — is an effective technique for discovering bugs in compilers but successful fuzzers require extensive development effort for every language supported by the compiler, and often leave parts of the language space untested. We introduce DeepSmith, a novel machine learning approach to accelerating compiler validation through the inference of generative models for compiler inputs. Our approach infers a learned model of the structure of real world code based on a large corpus of open source code. Then, it uses the model to automatically generate tens of thousands of realistic programs. Finally, we apply established differential testing methodologies on them to expose bugs in compilers. We apply our approach to the OpenCL programming language, automatically exposing bugs with little effort on our side. In 1,000 hours of automated testing of commercial and open source compilers, we discover bugs in all of them, submitting 67 bug reports. Our test cases are on average two orders of magnitude smaller than the state-of-the-art, require 3.03\texttimes{} less time to generate and evaluate, and expose bugs which the state-of-the-art cannot. Our random program generator, comprising only 500 lines of code, took 12 hours to train for OpenCL versus the state-of-the-art taking 9 man months to port from a generator for C and 50,000 lines of code. With 18 lines of code we extended our program generator to a second language, uncovering crashes in Solidity compilers in 12 hours of automated testing.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {95–105},
numpages = {11},
keywords = {Compiler Fuzzing, Deep Learning, Differential Testing},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@proceedings{10.1145/3214834,
title = {SIGGRAPH '18: ACM SIGGRAPH 2018 Courses},
year = {2018},
isbn = {9781450358095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Learn New Concepts and SkillsSIGGRAPH courses are learning sessions in which experts from all areas of computer graphics technology and interactive techniques share their knowledge. Course presenters distill key concepts and ideas into self-contained lessons. Courses may lie anywhere on a continuum from conceptual and theoretical to practical and applied.Courses are presented in both long (3.25 hour) or short (1.5 hour) sessions and may include elements of interactive demonstration, performance, or other imaginative approaches to teaching.},
location = {Vancouver, British Columbia, Canada}
}

@inproceedings{10.1145/3219819.3220000,
author = {Yu, Wenchao and Zheng, Cheng and Cheng, Wei and Aggarwal, Charu C. and Song, Dongjin and Zong, Bo and Chen, Haifeng and Wang, Wei},
title = {Learning Deep Network Representations with Adversarially Regularized Autoencoders},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220000},
doi = {10.1145/3219819.3220000},
abstract = {The problem of network representation learning, also known as network embedding, arises in many machine learning tasks assuming that there exist a small number of variabilities in the vertex representations which can capture the "semantics" of the original network structure. Most existing network embedding models, with shallow or deep architectures, learn vertex representations from the sampled vertex sequences such that the low-dimensional embeddings preserve the locality property and/or global reconstruction capability. The resultant representations, however, are difficult for model generalization due to the intrinsic sparsity of sampled sequences from the input network. As such, an ideal approach to address the problem is to generate vertex representations by learning a probability density function over the sampled sequences. However, in many cases, such a distribution in a low-dimensional manifold may not always have an analytic form. In this study, we propose to learn the network representations with adversarially regularized autoencoders (NetRA). NetRA learns smoothly regularized vertex representations that well capture the network structure through jointly considering both locality-preserving and global reconstruction constraints. The joint inference is encapsulated in a generative adversarial training process to circumvent the requirement of an explicit prior distribution, and thus obtains better generalization performance. We demonstrate empirically how well key properties of the network structure are captured and the effectiveness of NetRA on a variety of tasks, including network reconstruction, link prediction, and multi-label classification.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2663–2671},
numpages = {9},
keywords = {autoencoder, gans, generative adversarial networks, network embedding},
location = {London, United Kingdom},
series = {KDD '18}
}

@proceedings{10.1145/3229147,
title = {Expressive '18: Proceedings of the Joint Symposium on Computational Aesthetics and Sketch-Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering},
year = {2018},
isbn = {9781450358927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The Expressive conference series was born out of three workshops; CAe - Computational Aesthetics, SBIM - Sketch Based Interfaces Modelling and animation and NPAR - Non-Photo Realistic Animation and Rendering. The amalgamation has brought together artists, scientists, researchers and practitioners to showcase cutting-edge research and artistic innovation in these disciplines. We are jointly sponsored by Eurographics and ACM SIGGRAPH and are grateful for a generous donation from Disney Research. The conference is now in the fourteenth year of running under the Expressive banner.},
location = {Victoria, British Columbia, Canada}
}

@inproceedings{10.1145/3233027.3233047,
author = {El-Sharkawy, Sascha and Dhar, Saura Jyoti and Krafczyk, Adam and Duszynski, Slawomir and Beichter, Tobias and Schmid, Klaus},
title = {Reverse engineering variability in an industrial product line: observations and lessons learned},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233047},
doi = {10.1145/3233027.3233047},
abstract = {Ideally, a variability model is a correct and complete representation of product line features and constraints among them. Together with a mapping between features and code, this ensures that only valid products can be configured and derived. However, in practice the modeled constraints might be neither complete nor correct, which causes problems in the configuration and product derivation phases. This paper presents an approach to reverse engineer variability constraints from the implementation, and thus improve the correctness and completeness of variability models.We extended the concept of feature effect analysis [22] to extract variability constraints from code artifacts of the Bosch PS-EC large-scale product line. We present an industrial application of the approach and discuss its required modifications to handle non-Boolean variability and heterogeneous artifact types.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {215–225},
numpages = {11},
keywords = {reverse engineering, software product lines, static analysis, variability modeling},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@book{10.1145/3233795,
editor = {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip R. and Sonntag, Daniel and Potamianos, Gerasimos and Kr\"{u}ger, Antonio},
title = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan \&amp; Claypool},
abstract = {The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces---user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces.This three-volume handbook is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This third volume focuses on state-of-the-art multimodal language and dialogue processing, including semantic integration of modalities. The development of increasingly expressive embodied agents and robots has become an active test-bed for coordinating multimodal dialogue input and output, including processing of language and nonverbal communication. In addition, major application areas are featured for commercializing multimodal-multisensor systems, including automotive, robotic, manufacturing, machine translation, banking, communications, and others. These systems rely heavily on software tools, data resources, and international standards to facilitate their development. For insights into the future, emerging multimodal-multisensor technology trends are highlighted for medicine, robotics, interaction with smart spaces, and similar topics. Finally, this volume discusses the societal impact of more widespread adoption of these systems, such as privacy risks and how to mitigate them. The handbook chapters provide a number of walk-through examples of system design and processing, information on practical resources for developing and evaluating new systems, and terminology and tutorial support for mastering this emerging field. In the final section of this volume, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces need to be equipped to most effectively advance human performance during the next decade.}
}

@inproceedings{10.1145/3235765.3235821,
author = {van Rozen, Riemer and Heijn, Quinten},
title = {Measuring quality of grammars for procedural level generation},
year = {2018},
isbn = {9781450365710},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3235765.3235821},
doi = {10.1145/3235765.3235821},
abstract = {Grammar-based procedural level generation raises the productivity of level designers for games such as dungeon crawl and platform games. However, the improved productivity comes at cost of level quality assurance. Authoring, improving and maintaining grammars is difficult because it is hard to predict how each grammar rule impacts the overall level quality, and tool support is lacking. We propose a novel metric called Metric of Added Detail (MAD) that indicates if a rule adds or removes detail with respect to its phase in the transformation pipeline, and Specification Analysis Reporting (SAnR) for expressing level properties and analyzing how qualities evolve in level generation histories. We demonstrate MAD and SAnR using a prototype of a level generator called Ludoscope Lite. Our preliminary results show that problematic rules tend to break SAnR properties and that MAD intuitively raises flags. MAD and SAnR augment existing approaches, and can ultimately help designers make better levels and level generators.},
booktitle = {Proceedings of the 13th International Conference on the Foundations of Digital Games},
articleno = {56},
numpages = {8},
keywords = {PCG, automated game design, domain-specific languages, game development, grammars, level design, metrics, quality},
location = {Malm\"{o}, Sweden},
series = {FDG '18}
}

@article{10.1145/3236009,
author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
title = {A Survey of Methods for Explaining Black Box Models},
year = {2018},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3236009},
doi = {10.1145/3236009},
abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {93},
numpages = {42},
keywords = {Open the black box, explanations, interpretability, transparent models}
}

@article{10.1145/3236762,
author = {Bra\v{c}evac, Oliver and Amin, Nada and Salvaneschi, Guido and Erdweg, Sebastian and Eugster, Patrick and Mezini, Mira},
title = {Versatile event correlation with algebraic effects},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ICFP},
url = {https://doi.org/10.1145/3236762},
doi = {10.1145/3236762},
abstract = {We present the first language design to uniformly express variants of n-way joins over asynchronous event streams from different domains, e.g., stream-relational algebra, event processing, reactive and concurrent programming. We model asynchronous reactive programs and joins in direct style, on top of algebraic effects and handlers. Effect handlers act as modular interpreters of event notifications, enabling fine-grained control abstractions and customizable event matching. Join variants can be considered as cartesian product computations with ”degenerate” control flow, such that unnecessary tuples are not materialized a priori. Based on this computational interpretation, we decompose joins into a generic, naive enumeration procedure of the cartesian product, plus variant-specific extensions, represented in terms of user-supplied effect handlers. Our microbenchmarks validate that this extensible design avoids needless materialization. Alongside a formal semantics for joining and prototypes in Koka and multicore OCaml, we contribute a systematic comparison of the covered domains and features.},
journal = {Proc. ACM Program. Lang.},
month = jul,
articleno = {67},
numpages = {31},
keywords = {Koka, algebraic effect handlers, asynchrony, complex event processing, event correlation, joins, multicore OCaml}
}

@article{10.1145/3236778,
author = {undefinedcibior, Adam and Kammar, Ohad and Ghahramani, Zoubin},
title = {Functional programming for modular Bayesian inference},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ICFP},
url = {https://doi.org/10.1145/3236778},
doi = {10.1145/3236778},
abstract = {We present an architectural design of a library for Bayesian modelling and inference in modern functional programming languages. The novel aspect of our approach are modular implementations of existing state-of-the-art inference algorithms. Our design relies on three inherently functional features: higher-order functions, inductive data-types, and support for either type-classes or an expressive module system. We provide a performant Haskell implementation of this architecture, demonstrating that high-level and modular probabilistic programming can be added as a library in sufficiently expressive languages. We review the core abstractions in this architecture: inference representations, inference transformations, and inference representation transformers. We then implement concrete instances of these abstractions, counterparts to particle filters and Metropolis-Hastings samplers, which form the basic building blocks of our library. By composing these building blocks we obtain state-of-the-art inference algorithms: Resample-Move Sequential Monte Carlo, Particle Marginal Metropolis-Hastings, and Sequential Monte Carlo Squared. We evaluate our implementation against existing probabilistic programming systems and find it is already competitively performant, although we conjecture that existing functional programming optimisation techniques could reduce the overhead associated with the abstractions we use. We show that our modular design enables deterministic testing of inherently stochastic Monte Carlo algorithms. Finally, we demonstrate using OCaml that an expressive module system can also implement our design.},
journal = {Proc. ACM Program. Lang.},
month = jul,
articleno = {83},
numpages = {29},
keywords = {Anglican, Bayesian inference, Markov Chain Monte Carlo, Monte Carlo samplers, Sequential Monte Carlo, WebPPL, functional programming, higher-order functions, inductive types, machine learning, module systems, monad transformers, monads, probabilistic programming, type-classes}
}

@inproceedings{10.1145/3242587.3242650,
author = {Liu, Thomas F. and Craft, Mark and Situ, Jason and Yumer, Ersin and Mech, Radomir and Kumar, Ranjitha},
title = {Learning Design Semantics for Mobile Apps},
year = {2018},
isbn = {9781450359481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242587.3242650},
doi = {10.1145/3242587.3242650},
abstract = {Recently, researchers have developed black-box approaches to mine design and interaction data from mobile apps. Although the data captured during this interaction mining is descriptive, it does not expose the design semantics of UIs: what elements on the screen mean and how they are used. This paper introduces an automatic approach for generating semantic annotations for mobile app UIs. Through an iterative open coding of 73k UI elements and 720 screens, we contribute a lexical database of 25 types of UI components, 197 text button concepts, and 135 icon classes shared across apps. We use this labeled data to learn code-based patterns to detect UI components and to train a convolutional neural network that distinguishes between icon classes with 94\% accuracy. To demonstrate the efficacy of our approach at scale, we compute semantic annotations for the 72k unique UIs in the Rico dataset, assigning labels for 78\% of the total visible, non-redundant elements.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {569–579},
numpages = {11},
keywords = {design semantics, machine learning, mobile app design},
location = {Berlin, Germany},
series = {UIST '18}
}

@inproceedings{10.1145/3242671.3242705,
author = {Muscat, Alexander and Duckworth, Jonathan},
title = {WORLD4: Designing Ambiguity for First-Person Exploration Games},
year = {2018},
isbn = {9781450356244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242671.3242705},
doi = {10.1145/3242671.3242705},
abstract = {In this paper we present the design and evaluation of a first-person walker digital game called WORLD4. Walkers are a sub-genre of 3D games that typically include minimal player interaction, slow paced game play, and ambiguous goals. Walking is the primary means of interaction in walker games, rather than prioritize 'skill-based' mechanics. However, the design of these game environments is not well understood and challenges many accepted game design conventions. We have designed WORLD4, a multi-dimensional first-person exploration game, to explore how ambiguity might support exploratory game play experiences in virtual environments. 14 participants playtest WORLD4 and analysis of the data identified three descriptive themes specific to the walker game player experience: 1) designing partial inscrutability; 2) shifting meaning; and 3) facilitating subversion of expectations. We use these themes to describe a set of prescriptive design strategies that may assist designers in designing for ambiguity in exploratory game environments.},
booktitle = {Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play},
pages = {341–351},
numpages = {11},
keywords = {ambiguity, exploration, game design, game environments, level design, navigation, walker, walking simulator},
location = {Melbourne, VIC, Australia},
series = {CHI PLAY '18}
}

@inproceedings{10.1145/3242671.3242712,
author = {Allison, Fraser and Carter, Marcus and Gibbs, Martin and Smith, Wally},
title = {Design Patterns for Voice Interaction in Games},
year = {2018},
isbn = {9781450356244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242671.3242712},
doi = {10.1145/3242671.3242712},
abstract = {Voice interaction is increasingly common in digital games, but it remains a notoriously difficult modality to design a satisfying experience for. This is partly due to limitations of speech recognition technology, and partly due to the inherent awkwardness we feel when performing some voice actions. We present a pattern language for voice interaction elements in games, to help game makers explore and describe common approaches to this design challenge. We define 25 design patterns, based on a survey of 449 videogames and 22 audiogames that use the player's voice as an input to affect the game state. The patterns express how games frame and structure voice input, and how voice input is used for selection, navigation, control and performance actions. Finally, we argue that academic research has been overly concentrated on a single one of these design patterns, due to an instrumental research focus and a lack of interest in the fictive dimension of videogames.},
booktitle = {Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play},
pages = {5–17},
numpages = {13},
keywords = {design patterns, game design, interaction design, pattern language, speech recognition, voice control, voice interaction},
location = {Melbourne, VIC, Australia},
series = {CHI PLAY '18}
}

@inproceedings{10.1145/3269206.3271669,
author = {Hooi, Bryan and Akoglu, Leman and Eswaran, Dhivya and Pandey, Amritanshu and Jereminov, Marko and Pileggi, Larry and Faloutsos, Christos},
title = {ChangeDAR: Online Localized Change Detection for Sensor Data on a Graph},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271669},
doi = {10.1145/3269206.3271669},
abstract = {Given electrical sensors placed on the power grid, how can we automatically determine when electrical components (e.g. power lines) fail? Or, given traffic sensors which measure the speed of vehicles passing over them, how can we determine when traffic accidents occur? Both these problems involve detecting change points in a set of sensors on the nodes or edges of a graph. To this end, we propose ChangeDAR (Change Detection And Resolution), which detects changes in an online manner, and reports when and where the change occurred in the graph.Our contributions are: 1) Algorithm : we propose novel information-theoretic optimization objectives for scoring and detecting localized changes, and propose two algorithms, ChangeDAR-S and ChangeDAR-D respectively, to optimize them. 2) Theoretical Guarantees : we show that both methods provide constant-factor approximation guarantees (Theorems 5.2 and 6.2). 3) Effectiveness : in experiments, ChangeDAR detects traffic accidents and power line failures with 75\% higher F-measure than comparable baselines. 4) Scalability : ChangeDAR is online and near-linear in the graph size and the number of time ticks.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {507–516},
numpages = {10},
keywords = {change detection, graph, localized, online, sensor data},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3269206.3271756,
author = {Bonchi, Francesco and Gullo, Francesco and Mishra, Bud and Ramazzotti, Daniele},
title = {Probabilistic Causal Analysis of Social Influence},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271756},
doi = {10.1145/3269206.3271756},
abstract = {Mastering the dynamics of social influence requires separating, in a database of information propagation traces, the genuine causal processes from temporal correlation, i.e., homophily and other spurious causes. However, most studies to characterize social influence, and, in general, most data-science analyses focus on correlations, statistical independence, or conditional independence. Only recently, there has been a resurgence of interest in "causal data science,'' e.g., grounded on causality theories. In this paper we adopt a principled causal approach to the analysis of social influence from information-propagation data, rooted in the theory of probabilistic causation. Our approach consists of two phases. In the first one, in order to avoid the pitfalls of misinterpreting causation when the data spans a mixture of several subtypes ("Simpson's paradox''), we partition the set of propagation traces into groups, in such a way that each group is as less contradictory as possible in terms of the hierarchical structure of information propagation. To achieve this goal, we borrow the notion of "agony'' and define the Agony-bounded Partitioning problem, which we prove being hard, and for which we develop two efficient algorithms with approximation guarantees. In the second phase, for each group from the first phase, we apply a constrained MLE approach to ultimately learn a minimal causal topology. Experiments on synthetic data show that our method is able to retrieve the genuine causal arcs w.r.t. a ground-truth generative model. Experiments on real data show that, by focusing only on the extracted causal structures instead of the whole social graph, the effectiveness of predicting influence spread is significantly improved.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1003–1012},
numpages = {10},
keywords = {causal analysis, data mining, social influence, social networks},
location = {Torino, Italy},
series = {CIKM '18}
}

@article{10.1145/3274296,
author = {Calefato, Fabio and Iaffaldano, Giuseppe and Lanubile, Filippo and Maiorano, Federico},
title = {Investigating Crowd Creativity in Online Music Communities},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274296},
doi = {10.1145/3274296},
abstract = {Crowd creativity is typically associated with peer-production communities focusing on artistic products like animations, video games, and music, but less frequently to Open Source Software (OSS), despite the fact that also developers must be creative to come up with new solutions to their technical challenges. In this paper, we conduct a study to further the understanding of which factors from prior work in both OSS and art communities are predictive of successful collaboration - defined as reuse of previous songs - in three different songwriting communities, namely Songtree, Splice, and ccMixter. The main findings from this study confirm that the success of collaborations is associated with high community status of recognizable authors and low degree of derivativity of songs.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {27},
numpages = {21},
keywords = {ccmixter, collaborative songwriting, crowd creativity, online communities, open source, remix, reuse, social computing, songtree, splice}
}

@article{10.1145/3274375,
author = {Li, Hanlin and Alarcon, Bodhi and Milkes Espinosa, Sara and Hecht, Brent},
title = {Out of Site: Empowering a New Approach to Online Boycotts},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274375},
doi = {10.1145/3274375},
abstract = {GrabYourWallet, #boycottNRA and other online boycott campaigns have attracted substantial public interest in recent months. However, a number of significant challenges are preventing online boycotts from reaching their potential. In particular, complex webs of brands and subsidiaries can make it difficult for participants to conform to the goals of a boycott. Similarly, participants and organizers have limited visibility into a boycott's progress. This affects their ability to use sociotechnical innovations from social computing to incentivize participation. To address these challenges, this paper makes a system contribution: a new boycott tool called Out of Site. Out of Site uses lightweight automation to remove obstacles to successful online boycotts. We describe the design challenges associated with Out of Site and report results from two phases of deployment with the GrabYourWallet and Stop Animal Testing boycott communities. Our findings highlight the potential of boycott-assisting technologies and inform the design of this new class of technologies. Finally, like is the case for many systems in social computing, while we designed Out of Site for pro-social uses, there are a number of easily predictable ways in which the system can be leveraged for anti-social purposes (e.g. exacerbating filter bubble issues, empowering boycotts of businesses owned by racial, ethnic, and religious minorities). As such, we developed for this project a new, very straightforward design approach that treats preventing these anti-social uses as a top-tier design concern. This approach stands in contrast to the status quo of ignoring potential anti-social uses and/or considering them to be a secondary design priority. We discuss how our simple approach may help other research projects reduce their potential negative impacts with minimal burden.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {106},
numpages = {28},
keywords = {boycott, collective action, negative impacts, social computing systems}
}

@article{10.1145/3276463,
author = {Abuzaid, Firas and Bailis, Peter and Ding, Jialin and Gan, Edward and Madden, Samuel and Narayanan, Deepak and Rong, Kexin and Suri, Sahaana},
title = {MacroBase: Prioritizing Attention in Fast Data},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0362-5915},
url = {https://doi.org/10.1145/3276463},
doi = {10.1145/3276463},
abstract = {As data volumes continue to rise, manual inspection is becoming increasingly untenable. In response, we present MacroBase, a data analytics engine that prioritizes end-user attention in high-volume fast data streams. MacroBase enables efficient, accurate, and modular analyses that highlight and aggregate important and unusual behavior, acting as a search engine for fast data. MacroBase is able to deliver order-of-magnitude speedups over alternatives by optimizing the combination of explanation (i.e., feature selection) and classification tasks and by leveraging a new reservoir sampler and heavy-hitters sketch specialized for fast data streams. As a result, MacroBase delivers accurate results at speeds of up to 2M events per second per query on a single core. The system has delivered meaningful results in production, including at a telematics company monitoring hundreds of thousands of vehicles.},
journal = {ACM Trans. Database Syst.},
month = dec,
articleno = {15},
numpages = {45},
keywords = {Streaming, analytics, database}
}

@article{10.1145/3276517,
author = {Pradel, Michael and Sen, Koushik},
title = {DeepBugs: a learning approach to name-based bug detection},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {OOPSLA},
url = {https://doi.org/10.1145/3276517},
doi = {10.1145/3276517},
abstract = {Natural language elements in source code, e.g., the names of variables and functions, convey useful information. However, most existing bug detection tools ignore this information and therefore miss some classes of bugs. The few existing name-based bug detection approaches reason about names on a syntactic level and rely on manually designed and tuned algorithms to detect bugs. This paper presents DeepBugs, a learning approach to name-based bug detection, which reasons about names based on a semantic representation and which automatically learns bug detectors instead of manually writing them. We formulate bug detection as a binary classification problem and train a classifier that distinguishes correct from incorrect code. To address the challenge that effectively learning a bug detector requires examples of both correct and incorrect code, we create likely incorrect code examples from an existing corpus of code through simple code transformations. A novel insight learned from our work is that learning from artificially seeded bugs yields bug detectors that are effective at finding bugs in real-world code. We implement our idea into a framework for learning-based and name-based bug detection. Three bug detectors built on top of the framework detect accidentally swapped function arguments, incorrect binary operators, and incorrect operands in binary operations. Applying the approach to a corpus of 150,000 JavaScript files yields bug detectors that have a high accuracy (between 89\% and 95\%), are very efficient (less than 20 milliseconds per analyzed file), and reveal 102 programming mistakes (with 68\% true positive rate) in real-world code.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {147},
numpages = {25},
keywords = {Bug detection, JavaScript, Machine learning, Name-based program analysis, Natural language}
}

@inproceedings{10.1145/3276954.3276963,
author = {Basman, Antranig and Lewis, Clayton and Clark, Colin},
title = {The open authorial principle: supporting networks of authors in creating externalisable designs},
year = {2018},
isbn = {9781450360319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3276954.3276963},
doi = {10.1145/3276954.3276963},
abstract = {We introduce a new principle, the open authorial principle, that characterises desirable properties of languages supporting networks of authors. We survey the growth in generosity of authorial systems, in a progression starting with traditional object-orientation, continuing through aspect-oriented, subject-oriented, context-oriented and dependency injection systems, and concluding with the most recent generation of highly dynamic systems such as Korz and Newspeak. We follow the implications of our principle for the externalisation of application designs, resulting from the need to promote the representation of differences between programs as valid programs themselves. This raises conceptual and practical parallels with technologies and idioms supporting the web, such as REST, realised document structures supported by the DOM, and the negotiated space of CSS selectors. These parallels lead to a quite different organisation for the language and runtime of an openly authorable system, which emphasises a publicly addressable cellular structure and a largely static dispatch.},
booktitle = {Proceedings of the 2018 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {29–43},
numpages = {15},
keywords = {context awareness, integration domain, open authorial principle, reuse},
location = {Boston, MA, USA},
series = {Onward! 2018}
}

@proceedings{10.1145/3277644,
title = {SA '18: SIGGRAPH Asia 2018 Courses},
year = {2018},
isbn = {9781450360265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {At SIGGRAPH Asia 2018 hundreds of visitors will attend its courses to broaden and deepen their knowledge and to learn the secrets of new directions. The Crossover can also be offered via the presenters being from different backgrounds (e.g., computer science, art, animation, medicine, science and others), with each giving their unique perspective on the topic.},
location = {Tokyo, Japan}
}

@inproceedings{10.1145/3278122.3278128,
author = {Khalaj, Ebrahim and Abi-Antoun, Marwan},
title = {Inferring ownership domains from refinements},
year = {2018},
isbn = {9781450360456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278122.3278128},
doi = {10.1145/3278122.3278128},
abstract = {Ownership type qualifiers clarify aliasing invariants that cannot be directly expressed in mainstream programming languages. Adding qualifiers to code, however, often involves significant overhead and difficult interaction. We propose an analysis to infer qualifiers in the code based on developer refinements that express strict encapsulation, logical containment and architectural tiers. Refinements include: &lt;pre&gt;makeOwnedBy&lt;/pre&gt;, to make an object strictly encapsulated by another; &lt;pre&gt;makePartOf&lt;/pre&gt;, to make an object logically contained in another; &lt;pre&gt;makePeer&lt;/pre&gt;, to make two objects peers; &lt;pre&gt;makeParam&lt;/pre&gt;, to make an object more accessible than the above choices; or &lt;pre&gt;makeShared&lt;/pre&gt;, to allow an object to be globally aliased. If the code as-written matches the requested refinements, the analysis generates qualifiers that type-check; otherwise, it reports that the refinements do not match the code, so developers must investigate unexpected aliasing, change their understanding of the code and pick different refinements, or change the code and re-run the analysis. We implement the analysis and confirm that refinements generate precise qualifiers that express strict encapsulation, logical containment and architectural tiers.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {53–65},
numpages = {13},
keywords = {ownership type systems, type inference},
location = {Boston, MA, USA},
series = {GPCE 2018}
}

@article{10.1145/3393934.3278128,
author = {Khalaj, Ebrahim and Abi-Antoun, Marwan},
title = {Inferring ownership domains from refinements},
year = {2020},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/3393934.3278128},
doi = {10.1145/3393934.3278128},
abstract = {Ownership type qualifiers clarify aliasing invariants that cannot be directly expressed in mainstream programming languages. Adding qualifiers to code, however, often involves significant overhead and difficult interaction. We propose an analysis to infer qualifiers in the code based on developer refinements that express strict encapsulation, logical containment and architectural tiers. Refinements include: &lt;pre&gt;makeOwnedBy&lt;/pre&gt;, to make an object strictly encapsulated by another; &lt;pre&gt;makePartOf&lt;/pre&gt;, to make an object logically contained in another; &lt;pre&gt;makePeer&lt;/pre&gt;, to make two objects peers; &lt;pre&gt;makeParam&lt;/pre&gt;, to make an object more accessible than the above choices; or &lt;pre&gt;makeShared&lt;/pre&gt;, to allow an object to be globally aliased. If the code as-written matches the requested refinements, the analysis generates qualifiers that type-check; otherwise, it reports that the refinements do not match the code, so developers must investigate unexpected aliasing, change their understanding of the code and pick different refinements, or change the code and re-run the analysis. We implement the analysis and confirm that refinements generate precise qualifiers that express strict encapsulation, logical containment and architectural tiers.},
journal = {SIGPLAN Not.},
month = apr,
pages = {53–65},
numpages = {13},
keywords = {ownership type systems, type inference}
}

@inproceedings{10.1145/3278122.3278137,
author = {Liu, Yin and An, Kijin and Tilevich, Eli},
title = {RT-trust: automated refactoring for trusted execution under real-time constraints},
year = {2018},
isbn = {9781450360456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278122.3278137},
doi = {10.1145/3278122.3278137},
abstract = {Real-time systems must meet strict timeliness requirements. These systems also often need to protect their critical program information (CPI) from adversarial interference and intellectual property theft. Trusted execution environments (TEE) execute CPI tasks on a special-purpose processor, thus providing hardware protection. However, adapting a system written to execute in environments without TEE requires partitioning the code into the regular and trusted parts. This process involves complex manual program transformations that are not only laborious and intellectually tiresome, but also hard to validate and verify for the adherence to real-time constraints. To address these problems, this paper presents novel program analyses and transformation techniques, accessible to the developer via a declarative meta-programming model. The developer declaratively specifies the CPI portion of the system. A custom static analysis checks CPI specifications for validity, while probe-based profiling helps identify whether the transformed system would continue to meet the original real-time constraints, with a feedback loop suggesting how to modify the code, so its CPI can be isolated. Finally, an automated refactoring isolates the CPI portion for TEE-based execution, communicated with through generated calls to the TEE API. We have evaluated our approach by successfully enabling the trusted execution of the CPI portions of several microbenchmarks and a drone autopilot. Our approach shows the promise of declarative meta-programming in reducing the programmer effort required to adapt systems for trusted execution under real-time constraints.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {175–187},
numpages = {13},
keywords = {declarative meta-programming, program analyses, real-time systems, software refactoring, trusted execution},
location = {Boston, MA, USA},
series = {GPCE 2018}
}

@article{10.1145/3393934.3278137,
author = {Liu, Yin and An, Kijin and Tilevich, Eli},
title = {RT-trust: automated refactoring for trusted execution under real-time constraints},
year = {2020},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/3393934.3278137},
doi = {10.1145/3393934.3278137},
abstract = {Real-time systems must meet strict timeliness requirements. These systems also often need to protect their critical program information (CPI) from adversarial interference and intellectual property theft. Trusted execution environments (TEE) execute CPI tasks on a special-purpose processor, thus providing hardware protection. However, adapting a system written to execute in environments without TEE requires partitioning the code into the regular and trusted parts. This process involves complex manual program transformations that are not only laborious and intellectually tiresome, but also hard to validate and verify for the adherence to real-time constraints. To address these problems, this paper presents novel program analyses and transformation techniques, accessible to the developer via a declarative meta-programming model. The developer declaratively specifies the CPI portion of the system. A custom static analysis checks CPI specifications for validity, while probe-based profiling helps identify whether the transformed system would continue to meet the original real-time constraints, with a feedback loop suggesting how to modify the code, so its CPI can be isolated. Finally, an automated refactoring isolates the CPI portion for TEE-based execution, communicated with through generated calls to the TEE API. We have evaluated our approach by successfully enabling the trusted execution of the CPI portions of several microbenchmarks and a drone autopilot. Our approach shows the promise of declarative meta-programming in reducing the programmer effort required to adapt systems for trusted execution under real-time constraints.},
journal = {SIGPLAN Not.},
month = apr,
pages = {175–187},
numpages = {13},
keywords = {declarative meta-programming, program analyses, real-time systems, software refactoring, trusted execution}
}

@inproceedings{10.1145/3278721.3278777,
author = {Henderson, Peter and Sinha, Koustuv and Angelard-Gontier, Nicolas and Ke, Nan Rosemary and Fried, Genevieve and Lowe, Ryan and Pineau, Joelle},
title = {Ethical Challenges in Data-Driven Dialogue Systems},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278777},
doi = {10.1145/3278721.3278777},
abstract = {The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {123–129},
numpages = {7},
keywords = {adversarial examples, bias, computers and society, dialogue systems, ethics and safety, machine learning, natural language processing, privacy, reinforcement learning, reproducibility, security},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@article{10.1145/3281745,
author = {Sluganovic, Ivo and Roeschlin, Marc and Rasmussen, Kasper B. and Martinovic, Ivan},
title = {Analysis of Reflexive Eye Movements for Fast Replay-Resistant Biometric Authentication},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {2471-2566},
url = {https://doi.org/10.1145/3281745},
doi = {10.1145/3281745},
abstract = {Eye tracking devices have recently become increasingly popular as an interface between people and cons-umer-grade electronic devices. Due to the fact that human eyes are fast, responsive, and carry information unique to an individual, analyzing person’s gaze is particularly attractive for rapid biometric authentication. Unfortunately, previous proposals for gaze-based authentication systems either suffer from high error rates or requires long authentication times.We build on the fact that some eye movements can be reflexively and predictably triggered and develop an interactive visual stimulus for elicitation of reflexive eye movements that support the extraction of reliable biometric features in a matter of seconds, without requiring any memorization or cognitive effort on the part of the user. As an important benefit, our stimulus can be made unique for every authentication attempt and thus incorporated in a challenge-response biometric authentication system. This allows us to prevent replay attacks, which are possibly the most applicable attack vectors against biometric authentication.Using a gaze tracking device, we build a prototype of our system and perform a series of systematic user experiments with 30 participants from the general public. We thoroughly analyze various system parameters and evaluate the performance and security guarantees under several different attack scenarios. The results show that our system matches or surpasses existing gaze-based authentication methods in achieved equal error rates (6.3\%) while achieving significantly lower authentication times (5s).},
journal = {ACM Trans. Priv. Secur.},
month = nov,
articleno = {4},
numpages = {30},
keywords = {Eye movement biometrics, challenge-response biometrics, reflexive eye movements, user authentication}
}

@proceedings{10.1145/3283254,
title = {SA '18: SIGGRAPH Asia 2018 Technical Briefs},
year = {2018},
isbn = {9781450360623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The SIGGRAPH Asia 2018 Technical Briefs is a premier forum for presenting the latest developments and research still in progress. Leading international experts in academia and industry present work that showcase actual implementations of research ideas, work at the cross-roads of computer graphics with computer vision, machine learning, HCI, VR, CAD, visualization and many others!The Technical Briefs \&amp; Posters program spans a wide range of topics including:•Geometry and Modeling•Animation and Visual Effects•Computer Vision and Image Understanding•Learning techniques for Computer Graphics•Image and Video Processing Applications•Human-Computer Interaction•Video Gaming•Computer-Aided Design•Information Visualization and Scientific Visualization•Virtual Reality, Augmented Reality, and Mixed Reality•Multimedia Applications•Web Graphics and Mobile Graphics•Computer Graphics Education},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3283289,
title = {SA '18: SIGGRAPH Asia 2018 Posters},
year = {2018},
isbn = {9781450360630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The SIGGRAPH Asia Posters program is an interactive forum for innovative ideas that are not yet fully polished, high-impact practical contributions, and offer a behind-the-scenes view of new commercial and artistic work, as well as solutions that help solve challenging problems.Contributors are expected to exhibit graphic displays of incremental, preliminary, partial, and innovative insights that are important but not fully developed. Posters are displayed throughout the conference days. They will be organized in sessions by technical areas or thematic focus. Presenters will also discuss their work in scheduled sessions.Come by to see the work-in-progress and thought-provoking ideas, techniques, and applications in technical research at SIGGRAPH Asia 2018!},
location = {Tokyo, Japan}
}

@article{10.1145/3287051,
author = {Jin, Haojian and Liu, Minyi and Dodhia, Kevan and Li, Yuanchun and Srivastava, Gaurav and Fredrikson, Matthew and Agarwal, Yuvraj and Hong, Jason I.},
title = {Why Are They Collecting My Data? Inferring the Purposes of Network Traffic in Mobile Apps},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3287051},
doi = {10.1145/3287051},
abstract = {Many smartphone apps collect potentially sensitive personal data and send it to cloud servers. However, most mobile users have a poor understanding of why their data is being collected. We present MobiPurpose, a novel technique that can take a network request made by an Android app and then classify the data collection purposes, as one step towards making it possible to explain to non-experts the data disclosure contexts. Our purpose inference works by leveraging two observations: 1) developer naming conventions (e.g., URL paths) of ten offer hints as to data collection purposes, and 2) external knowledge, such as app metadata and information about the domain name, are meaningful cues that can be used to infer the behavior of different traffic requests. MobiPurpose parses each traffic request body into key-value pairs, and infers the data type and data collection purpose of each key-value pair using a combination of supervised learning and text pattern bootstrapping. We evaluated MobiPurpose's effectiveness using a dataset cross-labeled by ten human experts. Our results show that MobiPurpose can predict the data collection purpose with an average precision of 84\% (among 19 unique categories).},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {173},
numpages = {27},
keywords = {Contextual Integrity, Mobile Privacy, Privacy in Context, Purposes of Data Collection}
}

@article{10.1145/3290348,
author = {Gorinova, Maria I. and Gordon, Andrew D. and Sutton, Charles},
title = {Probabilistic programming with densities in SlicStan: efficient, flexible, and deterministic},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {POPL},
url = {https://doi.org/10.1145/3290348},
doi = {10.1145/3290348},
abstract = {Stan is a probabilistic programming language that has been increasingly used for real-world scalable projects. However, to make practical inference possible, the language sacrifices some of its usability by adopting a block syntax, which lacks compositionality and flexible user-defined functions. Moreover, the semantics of the language has been mainly given in terms of intuition about implementation, and has not been formalised.  This paper provides a formal treatment of the Stan language, and introduces the probabilistic programming language SlicStan --- a compositional, self-optimising version of Stan. Our main contributions are (1) the formalisation of a core subset of Stan through an operational density-based semantics; (2) the design and semantics of the Stan-like language SlicStan, which facilities better code reuse and abstraction through its compositional syntax, more flexible functions, and information-flow type system; and (3) a formal, semantic-preserving procedure for translating SlicStan to Stan.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {35},
numpages = {30},
keywords = {information flow analysis, probabilistic programming}
}

@inproceedings{10.1145/3290605.3300271,
author = {Brown, Anna and Chouldechova, Alexandra and Putnam-Hornstein, Emily and Tobin, Andrew and Vaithianathan, Rhema},
title = {Toward Algorithmic Accountability in Public Services: A Qualitative Study of Affected Community Perspectives on Algorithmic Decision-making in Child Welfare Services},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300271},
doi = {10.1145/3290605.3300271},
abstract = {Algorithmic decision-making systems are increasingly being adopted by government public service agencies. Researchers, policy experts, and civil rights groups have all voiced concerns that such systems are being deployed without adequate consideration of potential harms, disparate impacts, and public accountability practices. Yet little is known about the concerns of those most likely to be affected by these systems. We report on workshops conducted to learn about the concerns of affected communities in the context of child welfare services. The workshops involved 83 study participants including families involved in the child welfare system, employees of child welfare agencies, and service providers. Our findings indicate that general distrust in the existing system contributes significantly to low comfort in algorithmic decision-making. We identify strategies for improving comfort through greater transparency and improved communication strategies. We discuss the implications of our study for accountable algorithm design for child welfare applications.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {algorithmic accountability, algorithmic bias, automated decision systems, child welfare services, decision-support, participatory design},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3290605.3300307,
author = {Chivukula, Shruthi Sai and Gray, Colin M. and Brier, Jason A.},
title = {Analyzing Value Discovery in Design Decisions Through Ethicography},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300307},
doi = {10.1145/3290605.3300307},
abstract = {HCI scholarship is increasingly concerned with the ethical impact of socio-technical systems. Current theoretically driven approaches that engage with ethics generally prescribe only abstract approaches by which designers might consider values in the design process. However, there is little guidance on methods that promote value discovery, which might lead to more specific examples of relevant values in specific design contexts. In this paper, we elaborate a method for value discovery, identifying how values impact the designer's decision making. We demonstrate the use of this method, called Ethicography, in describing value discovery and use throughout the design process. We present analysis of design activity by user experience (UX) design students in two lab protocol conditions, describing specific human values that designers considered for each task, and visualizing the interplay of these values. We identify opportunities for further research, using the Ethicograph method to illustrate value discovery and translation into design solutions.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {ethicography, ethics, value discovery, values},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3290605.3300688,
author = {Gui, Xinning and Chen, Yunan},
title = {Making Healthcare Infrastructure Work: Unpacking the Infrastructuring Work of Individuals},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300688},
doi = {10.1145/3290605.3300688},
abstract = {The U.S. healthcare infrastructure is fragmented with various breakdowns. Patients or caregivers have to rely on their own to overcome barriers and fix breakdowns in order to obtain necessary service, that is, infrastructuring work to make the healthcare infrastructure work for them. So far little attention has been paid to such infrastructuring work in healthcare. We present an interview study of 32 U.S. parents of young children to discuss the work of infrastructuring our participants carry out to deal with breakdowns within the healthcare infrastructure. We report how they repaired unexpected failures happening at the individual level, aligned components at organizational and cross-organizational level, and circumvented infrastructural constraints (e.g., policy and financial ones) that were perceived as ambiguous and demanding. We discuss infrastructuring work in light of the literature on patients' and caregivers' work, reflect upon the notion of patient engagement, and explore nuances along several dimensions of infrastructuring work.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {healthcare consumers, healthcare infrastructure, infrastructuring work, new parents, patient work, patients and caregivers},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3297858.3304019,
author = {Banerjee, Subho S. and Kalbarczyk, Zbigniew T. and Iyer, Ravishankar K.},
title = {AcMC 2 : Accelerating Markov Chain Monte Carlo Algorithms for Probabilistic Models},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304019},
doi = {10.1145/3297858.3304019},
abstract = {Probabilistic models (PMs) are ubiquitously used across a variety of machine learning applications. They have been shown to successfully integrate structural prior information about data and effectively quantify uncertainty to enable the development of more powerful, interpretable, and efficient learning algorithms. This paper presents AcMC2, a compiler that transforms PMs into optimized hardware accelerators (for use in FPGAs or ASICs) that utilize Markov chain Monte Carlo methods to infer and query a distribution of posterior samples from the model. The compiler analyzes statistical dependencies in the PM to drive several optimizations to maximally exploit the parallelism and data locality available in the problem. We demonstrate the use of AcMC2 to implement several learning and inference tasks on a Xilinx Virtex-7 FPGA. AcMC2-generated accelerators provide a 47-100\texttimes{} improvement in runtime performance over a 6-core IBM Power8 CPU and a 8-18\texttimes{} improvement over an NVIDIA K80 GPU. This corresponds to a 753-1600\texttimes{} improvement over the CPU and 248-463\texttimes{} over the GPU in performance-per-watt terms.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {515–528},
numpages = {14},
keywords = {accelerator, markov chain monte carlo, probabilistic graphical models, probabilistic programming},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings{10.1145/3299869.3314043,
author = {Korn, Flip and Wang, Xuezhi and Wu, You and Yu, Cong},
title = {Automatically Generating Interesting Facts from Wikipedia Tables},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3314043},
doi = {10.1145/3299869.3314043},
abstract = {Modern search engines provide contextual information surrounding query entities beyond ten blue links in the form of information cards. Among the various attributes displayed about entities there has been recent interest in providing fun facts. Obtaining such trivia at a large scale is, however, non-trivial: hiring professional content creators is expensive and extracting statements from the Web is prone to uninteresting, out-of-context and/or unreliable facts.In this paper we show how fun facts can be mined from superlative tables in Wikipedia, whose rows are ranked according to some statistics, to provide a large volume of reliable and interesting content. We employ a template-based approach to semi-automatically generate natural language statements as fun facts. We show how to bootstrap and streamline the process for faster and cheaper task completion. However, the content contained in these tables is dynamic. Therefore, we address the problem of automatically maintaining the pairing of templates to tables as the tables are updated over time. Fun facts produced by our work is now part of Google's production search results.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {349–361},
numpages = {13},
keywords = {dynamic maintenance, fun facts generation, superlative tables},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.1145/3299869.3319855,
author = {Wang, Pei and He, Yeye},
title = {Uni-Detect: A Unified Approach to Automated Error Detection in Tables},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3319855},
doi = {10.1145/3299869.3319855},
abstract = {Data errors are ubiquitous in tables. Extensive research in this area has resulted in a rich variety of techniques, each often targeting a specific type of errors, e.g., numeric outliers, constraint violations, etc. While these diverse techniques clearly improve data quality, it places a significant burden on humans to configure these techniques with suitable rules and parameters for each data set. For example, an expert is expected to define suitable functional-dependencies between column pairs, or tune appropriate thresholds for outlier-detection algorithms, all of which are specific to one individual data set. As a result, users today often hire experts to cleanse only their high-value data sets. We propose sj, a unified framework to automatically detect diverse types of errors. Our approach employs a novel "what-if'' analysis that performs local data perturbations to reason about data abnormality, leveraging classical hypothesis-tests on a large corpus of tables. We test sj on a wide variety of tables including Wikipedia tables, and make surprising discoveries of thousands of FD violations, numeric outliers, spelling mistakes, etc., with better accuracy than existing algorithms specifically designed for each type of errors. For example, for spelling mistakes, sj outperforms the state-of-the-art spell-checker from a commercial search engine.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {811–828},
numpages = {18},
keywords = {constraints, data quality, error detection, outliers},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.1145/3305366.3328026,
author = {Glassner, Andrew},
title = {Deep learning: a crash course},
year = {2019},
isbn = {9781450363075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3305366.3328026},
doi = {10.1145/3305366.3328026},
abstract = {Concepts, terminology, structures, no math, no code. Free open-source libraries do the hard work. My background: consultant, writer, director, etc.},
booktitle = {ACM SIGGRAPH 2019 Courses},
articleno = {9},
numpages = {550},
location = {Los Angeles, California},
series = {SIGGRAPH '19}
}

@article{10.1145/3306346.3323023,
author = {Bau, David and Strobelt, Hendrik and Peebles, William and Wulff, Jonas and Zhou, Bolei and Zhu, Jun-Yan and Torralba, Antonio},
title = {Semantic photo manipulation with a generative image prior},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3323023},
doi = {10.1145/3306346.3323023},
abstract = {Despite the recent success of GANs in synthesizing images conditioned on inputs such as a user sketch, text, or semantic labels, manipulating the high-level attributes of an existing natural photograph with GANs is challenging for two reasons. First, it is hard for GANs to precisely reproduce an input image. Second, after manipulation, the newly synthesized pixels often do not fit the original image. In this paper, we address these issues by adapting the image prior learned by GANs to image statistics of an individual image. Our method can accurately reconstruct the input image and synthesize new content, consistent with the appearance of the input image. We demonstrate our interactive system on several semantic image editing tasks, including synthesizing new objects consistent with background, removing unwanted objects, and changing the appearance of an object. Quantitative and qualitative comparisons against several existing methods demonstrate the effectiveness of our method.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {59},
numpages = {11},
keywords = {deep learning, generative adversarial networks, image editing, vision for graphics}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/3308558.3313591,
author = {Xu, Mengwei and Liu, Jiawei and Liu, Yuanqiang and Lin, Felix Xiaozhu and Liu, Yunxin and Liu, Xuanzhe},
title = {A First Look at Deep Learning Apps on Smartphones},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313591},
doi = {10.1145/3308558.3313591},
abstract = {To bridge the knowledge gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do their deep learning models look like. Our study has strong implications for app developers, smartphone vendors, and deep learning R&amp;D. On one hand, our findings paint a promising picture of deep learning for smartphones, showing the prosperity of mobile deep learning frameworks as well as the prosperity of apps building their cores atop deep learning. On the other hand, our findings urge optimizations on deep learning models deployed on smartphones, protection of these models, and validation of research ideas on these models.},
booktitle = {The World Wide Web Conference},
pages = {2125–2136},
numpages = {12},
keywords = {Deep Learning, Empirical Study, Mobile Apps},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@inproceedings{10.1145/3308560.3316502,
author = {Tavabi, Nazgol and Bartley, Nathan and Abeliuk, Andres and Soni, Sandeep and Ferrara, Emilio and Lerman, Kristina},
title = {Characterizing Activity on the Deep and Dark Web},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316502},
doi = {10.1145/3308560.3316502},
abstract = {The deep and darkweb (d2web) refers to limited access web sites that require registration, authentication, or more complex encryption protocols to access them. These web sites serve as hubs for a variety of illicit activities: to trade drugs, stolen user credentials, hacking tools, and to coordinate attacks and manipulation campaigns. Despite its importance to cyber crime, the d2web has not been systematically investigated. In this paper, we study a large corpus of messages posted to 80 d2web forums over a period of more than a year. We identify topics of discussion using LDA and use a non-parametric HMM to model the evolution of topics across forums. Then, we examine the dynamic patterns of discussion and identify forums with similar patterns. We show that our approach surfaces hidden similarities across different forums and can help identify anomalous events in this rich, heterogeneous data.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {206–213},
numpages = {8},
keywords = {Beta Process, Cluster Time Series, Cyber Crime, Cyber Security, D2web, Darkweb, Deepweb, LDA, Non-Parametric HMM, multivariate Time Series},
location = {San Francisco, USA},
series = {WWW '19}
}

@book{10.1145/3310205,
author = {Ilyas, Ihab F. and Chu, Xu},
title = {Data Cleaning},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.}
}

@article{10.1145/3310275,
author = {Bader, Patrick and Voit, Alexandra and Le, Huy Viet and Wo\'{z}niak, Pawe\l{} W. and Henze, Niels and Schmidt, Albrecht},
title = {WindowWall: Towards Adaptive Buildings with Interactive Windows as Ubiquitous Displays},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3310275},
doi = {10.1145/3310275},
abstract = {As architects usually decide on the shape and look of windows during the design of buildings, opportunities for interactive windows have not been systematically explored yet. In this work, we extend the vision of sustainable and comfortable adaptive buildings using interactive smart windows. We systematically explore the design space of interactive windows to chart requirements, constraints, and challenges. To that end, we built proof-of-concept prototypes of smart windows with fine-grained control of transparency. In two studies, we explored user attitudes towards interactive windows and elicited control methods. We found that users understand and see potential for interactive windows at home. We provide specific usage contexts and specify interactions that may facilitate domestic applications. Our work illustrates the concept of interactive smart windows and provides insights regarding their design, development, and user controls for adaptive walls. We identify design dimensions and challenges to stimulate further development in the domain of adaptive buildings.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = mar,
articleno = {11},
numpages = {42},
keywords = {Smart windows, adaptive buildings, ambient information systems, elicitation, see-through displays}
}

@inproceedings{10.1145/3313831.3376275,
author = {Dove, Graham and Fayard, Anne-Laure},
title = {Monsters, Metaphors, and Machine Learning},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376275},
doi = {10.1145/3313831.3376275},
abstract = {Machine learning (ML) poses complex challenges for user experience (UX) designers. Typically unpredictable and opaque, it may produce unforeseen outcomes detrimental to particular groups or individuals, yet simultaneously promise amazing breakthroughs in areas as diverse as medical diagnosis and universal translation. This results in a polarized view of ML, which is often manifested through a technology-as-monster metaphor. In this paper, we acknowledge the power and potential of this metaphor by resurfacing historic complexities in human-monster relations. We (re)introduce these liminal and ambiguous creatures, and discuss their relation to ML. We offer a background to designers' use of metaphor, and show how the technology-as-monster metaphor can generatively probe and (re)frame the questions ML poses. We illustrate the effectiveness of this approach through a detailed discussion of an early-stage generative design workshop inquiring into ML approaches to supporting student mental health and well-being.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–17},
numpages = {17},
keywords = {generative metaphor, machine learning, monster theory, ux design},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376425,
author = {Vasquez, Joshua and Twigg-Smith, Hannah and Tran O'Leary, Jasper and Peek, Nadya},
title = {Jubilee: An Extensible Machine for Multi-tool Fabrication},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376425},
doi = {10.1145/3313831.3376425},
abstract = {We present Jubilee, an open-source hardware machine with automatic tool-changing and interchangeable bed plates. As digital fabrication tools have become more broadly accessible, tailoring those machines to new users and novel workflows has become central to HCI research. However, the lack of hardware infrastructure makes custom application development cumbersome. We identify a need for an extensible platform to allow HCI researchers to develop workflows for fabrication, material exploration, and other applications. Jubilee addresses this need. It can automatically and repeatably change tools in the same operation. It can be built with a combination of simple 3D-printed and readily available parts. It has several standard head designs for a variety of applications including 3D printing, syringe-based liquid handling, imaging, and plotting. We present Jubilee with a comprehensive set of assembly instructions and kinematic mount templates for user-designed tools and bed plates. Finally we demonstrate Jubilee's multi-tool workflow functionality with a series of example applications.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {digital fabrication, multi-tool workflows, toolchanging},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376525,
author = {Kristensson, Per Ola and Lilley, James and Black, Rolf and Waller, Annalu},
title = {A Design Engineering Approach for Quantitatively Exploring Context-Aware Sentence Retrieval for Nonspeaking Individuals with Motor Disabilities},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376525},
doi = {10.1145/3313831.3376525},
abstract = {Nonspeaking individuals with motor disabilities typically have very low communication rates. This paper proposes a design engineering approach for quantitatively exploring context-aware sentence retrieval as a promising complementary input interface, working in tandem with a word-prediction keyboard. We motivate the need for complementary design engineering methodology in the design of augmentative and alternative communication and explain how such methods can be used to gain additional design insights. We then study the theoretical performance envelopes of a context-aware sentence retrieval system, identifying potential keystroke savings as a function of the parameters of the subsystems, such as the accuracy of the underlying auto-complete word prediction algorithm and the accuracy of sensed context information under varying assumptions. We find that context-aware sentence retrieval has the potential to provide users with considerable improvements in keystroke savings under reasonable parameter assumptions of the underlying subsystems. This highlights how complementary design engineering methods can reveal additional insights into design for augmentative and alternative communication.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {augmentative and alternative communication, context-aware text entry, design engineering, information retrieval, sentence prediction, text entry},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376637,
author = {Nebeling, Michael and Lewis, Katy and Chang, Yu-Cheng and Zhu, Lihan and Chung, Michelle and Wang, Piaoyang and Nebeling, Janet},
title = {XRDirector: A Role-Based Collaborative Immersive Authoring System},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376637},
doi = {10.1145/3313831.3376637},
abstract = {Immersive authoring is an increasingly popular technique to design AR/VR scenes because design and testing can be done concurrently. Most existing systems, however, are single-user and limited to either AR or VR, thus constrained in the interaction techniques. We present XRDirector, a role-based collaborative immersive authoring system that enables designers to freely express interactions using AR and VR devices as puppets to manipulate virtual objects in 3D physical space. In XRDirector, we adapt roles known from filmmaking to structure the authoring process and help coordinate multiple designers in immersive authoring tasks. We study how novice AR/VR creators can take advantage of the roles and modes in XRDirector to prototype complex scenes with animated 3D characters, light effects, and camera movements, and also simulate interactive system behavior in a Wizard of Oz style. XRDirector's design was informed by case studies around complex 3D movie scenes and AR/VR games, as well as workshops with novice AR/VR creators. We show that XRDirector makes it easier and faster to create AR/VR scenes without the need for coding, characterize the issues in coordinating designers between AR and VR, and identify the strengths and weaknesses of each role and mode to mitigate the issues.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {ar/vr, immersive authoring, mixed-reality collaboration},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376708,
author = {Seering, Joseph and Luria, Michal and Ye, Connie and Kaufman, Geoff and Hammer, Jessica},
title = {It Takes a Village: Integrating an Adaptive Chatbot into an Online Gaming Community},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376708},
doi = {10.1145/3313831.3376708},
abstract = {While the majority of research in chatbot design has focused on creating chatbots that engage with users one-on-one, less work has focused on the design of conversational agents for online communities. In this paper we present results from a three week test of a social chatbot in an established online community. During this study, the chatbot "grew up" from "birth" through its teenage years, engaging with community members and "learning" vocabulary from their conversations. We discuss the design of this chatbot, how users' interactions with it evolved over the course of the study, and how it impacted the community as a whole. We discuss how we addressed challenges in developing a chatbot whose vocabulary could be shaped by users, and conclude with implications for the role of machine learning in social interactions in online communities and potential future directions for design of community-based chatbots.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {AI, babybot, chatbot, community interaction, interaction design, long-term study, machine learning, twitch},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3314221.3314594,
author = {Campagna, Giovanni and Xu, Silei and Moradshahi, Mehrad and Socher, Richard and Lam, Monica S.},
title = {Genie: a generator of natural language semantic parsers for virtual assistant commands},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314594},
doi = {10.1145/3314221.3314594},
abstract = {To understand diverse natural language commands, virtual assistants today are trained with numerous labor-intensive, manually annotated sentences. This paper presents a methodology and the Genie toolkit that can handle new compound commands with significantly less manual effort. We advocate formalizing the capability of virtual assistants with a Virtual Assistant Programming Language (VAPL) and using a neural semantic parser to translate natural language into VAPL code. Genie needs only a small realistic set of input sentences for validating the neural model. Developers write templates to synthesize data; Genie uses crowdsourced paraphrases and data augmentation, along with the synthesized data, to train a semantic parser. We also propose design principles that make VAPL languages amenable to natural language translation. We apply these principles to revise ThingTalk, the language used by the Almond virtual assistant. We use Genie to build the first semantic parser that can support compound virtual assistants commands with unquoted free-form parameters. Genie achieves a 62\% accuracy on realistic user inputs. We demonstrate Genie’s generality by showing a 19\% and 31\% improvement over the previous state of the art on a music skill, aggregate functions, and access control.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {394–410},
numpages = {17},
keywords = {data augmentation, data engineering, semantic parsing, training data generation, virtual assistants},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@article{10.1145/3314414,
author = {Wang, Huandong and Li, Yong and Zeng, Sihan and Wang, Gang and Zhang, Pengyu and Hui, Pan and Jin, Depeng},
title = {Modeling Spatio-Temporal App Usage for a Large User Population},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3314414},
doi = {10.1145/3314414},
abstract = {With the wide adoption of mobile devices, it becomes increasingly important to understand how users use mobile apps. Knowing when and where certain apps are used is instrumental for app developers to improve app usability and for Internet service providers (ISPs) to optimize their network services. However, modeling spatio-temporal patterns of app usage has been a challenging problem due to the complicated usage behavior and the very limited personal data. In this paper, we propose a Bayesian mixture model to capture when, where and what apps are used and predict future app usage. To solve the challenge of data sparsity, we apply a hierarchical Dirichlet process to leverage the shared spatio-temporal patterns to accurately model users with insufficient data. We then evaluate our model using a large dataset of app usage traces involving 1.7 million users over 3503 apps. Our analysis shows a clear correlation between the user's location and the apps being used. Extensive evaluations show that our model can accurately predict users' future locations and app usage, outperforming the state-of-the-art algorithms by 11.7\% and 11.1\%, respectively. In addition, our model can be used to synthesize app usage traces that do not leak user privacy while preserving the key data statistical properties.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {27},
numpages = {23},
keywords = {Bayesian mixture model, app usage, spatio-temporal pattern}
}

@inproceedings{10.1145/3315507.3330200,
author = {Tahboub, Ruby Y. and Wu, Xilun and Essertel, Gr\'{e}gory M. and Rompf, Tiark},
title = {Towards compiling graph queries in relational engines},
year = {2019},
isbn = {9781450367189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3315507.3330200},
doi = {10.1145/3315507.3330200},
abstract = {The increasing demand for graph query processing has prompted the addition of support for graph workloads on top of standard relational database management systems (RDBMS). Although this appears like a good idea --- after all, graphs are just relations --- performance is typically suboptimal since graph workloads are naturally iterative and rely extensively on efficient traversal of adjacency structures that are not typically implemented in an RDBMS. Adding such specialized adjacency structures is not at all straightforward due to the complexity of typical RDBMS implementations. The iterative nature of graph queries also practically requires a form of runtime compilation and native code generation which adds another dimension of complexity to the RDBMS implementation and any potential extensions.  In this paper, we demonstrate how the idea of the first Futamura projection, which links interpreted query engines and compilers through specialization, can be applied to compile graph workloads in an efficient way that simplifies the construction of relational engines which also support graph workloads. We extend the LB2 main-memory query compiler with graph adjacency structures and operators. We implement a subset of the Datalog logical query language evaluation to enable processing graph and recursive queries efficiently. The graph extension matches, and sometimes outperforms, best-of-breed low-level graph engines.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Symposium on Database Programming Languages},
pages = {30–41},
numpages = {12},
keywords = {Futamura Projections, Graph Query Engines, Query Compilation},
location = {Phoenix, AZ, USA},
series = {DBPL 2019}
}

@article{10.1145/3317612,
author = {Zhang, Zheng and Huang, Minlie and Zhao, Zhongzhou and Ji, Feng and Chen, Haiqing and Zhu, Xiaoyan},
title = {Memory-Augmented Dialogue Management for Task-Oriented Dialogue Systems},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3317612},
doi = {10.1145/3317612},
abstract = {Dialogue management (DM) is responsible for predicting the next action of a dialogue system according to the current dialogue state and thus plays a central role in task-oriented dialogue systems. Since DM requires having access not only to local utterances but also to the global semantics of the entire dialogue session, modeling the long-range history information is a critical issue. To this end, we propose MAD, a novel memory-augmented dialogue management model that employs a memory controller and two additional memory structures (i.e., a slot-value memory and an external memory). The slot-value memory tracks the dialogue state by memorizing and updating the values of semantic slots (i.e., cuisine, price, and location), and the external memory augments the representation of hidden states of traditional recurrent neural networks by storing more context information. To update the dialogue state efficiently, we also propose slot-level attention on user utterances to extract specific semantic information for each slot. Experiments show that our model can obtain state-of-the-art performance and outperforms existing baselines.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {34},
numpages = {30},
keywords = {Dialogue management, attention, dialogue state, memory network, neural network}
}

@inproceedings{10.1145/3318464.3389701,
author = {Tahboub, Ruby Y. and Rompf, Tiark},
title = {Architecting a Query Compiler for Spatial Workloads},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389701},
doi = {10.1145/3318464.3389701},
abstract = {Modern location-based applications rely extensively on the efficient processing of spatial data and queries. Spatial query engines are commonly engineered as an extension to a relational database or a cluster-computing framework. Large parts of the spatial processing runtime is spent on evaluating spatial predicates and traversing spatial indexing structures. Typical high-level implementations of these spatial structures incur significant interpretive overhead, which increases latency and lowers throughput. A promising idea to improve the performance of spatial workloads is to leverage native code generation techniques that have become popular in relational query engines. However, architecting a spatial query compiler is challenging since spatial processing has fundamentally different execution characteristics from relational workloads in terms of data dimensionality, indexing structures, and predicate evaluation. In this paper, we discuss the underlying reasons why standard query compilation techniques are not fully effective when applied to spatial workloads, and we demonstrate how a particular style of query compilation based on techniques borrowed from partial evaluation and generative programming manages to avoid most of these difficulties by extending the scope of custom code generation into the data structures layer. We extend the LB2 main-memory query compiler, a relational engine developed in this style, with spatial data types, predicates, indexing structures, and operators. We show that the spatial extension matches the performance of specialized library code and outperforms relational and map-reduce extensions.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2103–2118},
numpages = {16},
keywords = {code generation, extensible compilers, futamura projections, query compilation, spatial query processing, staging},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3319535.3363200,
author = {Weinshel, Ben and Wei, Miranda and Mondal, Mainack and Choi, Euirim and Shan, Shawn and Dolin, Claire and Mazurek, Michelle L. and Ur, Blase},
title = {Oh, the Places You've Been! User Reactions to Longitudinal Transparency About Third-Party Web Tracking and Inferencing},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3363200},
doi = {10.1145/3319535.3363200},
abstract = {Internet companies track users' online activity to make inferences about their interests, which are then used to target ads and personalize their web experience. Prior work has shown that existing privacy-protective tools give users only a limited understanding and incomplete picture of online tracking. We present Tracking Transparency, a privacy-preserving browser extension that visualizes examples of long-term, longitudinal information that third-party trackers could have inferred from users' browsing. The extension uses a client-side topic modeling algorithm to categorize pages that users visit and combines this with data about the web trackers encountered over time to create these visualizations. We conduct a longitudinal field study in which 425 participants use one of six variants of our extension for a week. We find that, after using the extension, participants have more accurate perceptions of the extent of tracking and also intend to take privacy-protecting actions.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {149–166},
numpages = {18},
keywords = {third-party online tracking, transparency, usable privacy, user study},
location = {London, United Kingdom},
series = {CCS '19}
}

@inproceedings{10.1145/3320269.3384731,
author = {Song, Congzheng and Shokri, Reza},
title = {Membership Encoding for Deep Learning},
year = {2020},
isbn = {9781450367509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320269.3384731},
doi = {10.1145/3320269.3384731},
abstract = {Machine learning as a service (MLaaS), and algorithm marketplaces are on a rise. Data holders can easily train complex models on their data using third party provided learning codes. Training accurate ML models requires massive labeled data and advanced learning algorithms. The resulting models are considered as intellectual property of the model owners and their copyright should be protected. Also, MLaaS needs to be trusted not to embed secret information about the training data into the model, such that it could be later retrieved when the model is deployed.In this paper, we present membership encoding for training deep neural networks and encoding the membership information, i.e. whether a data point is used for training, for a subset of training data. Membership encoding has several applications in different scenarios, including robust watermarking for model copyright protection, and also the risk analysis of stealthy data embedding privacy attacks. Our encoding algorithm can determine the membership of significantly redacted data points, and is also robust to model compression and fine-tuning. It also enables encoding a significant fraction of the training set, with negligible drop in the model's prediction accuracy.},
booktitle = {Proceedings of the 15th ACM Asia Conference on Computer and Communications Security},
pages = {344–356},
numpages = {13},
keywords = {copyright protection, machine learning, membership inference},
location = {Taipei, Taiwan},
series = {ASIA CCS '20}
}

@inproceedings{10.1145/3322276.3322366,
author = {Leit\~{a}o, Roxanne},
title = {Anticipating Smart Home Security and Privacy Threats with Survivors of Intimate Partner Abuse},
year = {2019},
isbn = {9781450358507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322276.3322366},
doi = {10.1145/3322276.3322366},
abstract = {This paper presents a design-led qualitative study investigating the (mis)use of digital technologies as tools for stalking, threats, and harassment within the context of intimate partner abuse (IPA). Results from interviews and domestic abuse forum data are reported on and set the foundation for a series of codesign workshops. The workshops invite participants to creatively anticipate smart home attack vectors, based on their lived experiences of IPA. Three workshops with seven IPA survivors and eleven professional support workers are detailed in this paper. Findings are organised into three phases through which survivors' privacy and security needs can be understood: 1) initial purchasing and configuring of smart home devices; 2) daily usage and; 3) (re-)securing devices after abuse has been identified. The speculative attack-vectors and design ideas generated by participants expose, for the first time, survivors' understanding of smart home security and privacy, as well as their needs, concerns, and requirements.},
booktitle = {Proceedings of the 2019 on Designing Interactive Systems Conference},
pages = {527–539},
numpages = {13},
keywords = {codesign, interpersonal privacy, intimate partner abuse, smart homes},
location = {San Diego, CA, USA},
series = {DIS '19}
}

@inproceedings{10.1145/3324884.3416546,
author = {Gros, David and Sezhiyan, Hariharan and Devanbu, Prem and Yu, Zhou},
title = {Code to comment "translation": data, metrics, baselining \&amp; evaluation},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416546},
doi = {10.1145/3324884.3416546},
abstract = {The relationship of comments to code, and in particular, the task of generating useful comments given the code, has long been of interest. The earliest approaches have been based on strong syntactic theories of comment-structures, and relied on textual templates. More recently, researchers have applied deep-learning methods to this task---specifically, trainable generative translation models which are known to work very well for Natural Language translation (e.g., from German to English). We carefully examine the underlying assumption here: that the task of generating comments sufficiently resembles the task of translating between natural languages, and so similar models and evaluation metrics could be used. We analyze several recent code-comment datasets for this task: CodeNN, DeepCom, FunCom, and DocString. We compare them with WMT19, a standard dataset frequently used to train state-of-the-art natural language translators. We found some interesting differences between the code-comment data and the WMT19 natural language data. Next, we describe and conduct some studies to calibrate BLEU (which is commonly used as a measure of comment quality). using "affinity pairs" of methods, from different projects, in the same project, in the same class, etc; Our study suggests that the current performance on some datasets might need to be improved substantially. We also argue that fairly naive information retrieval (IR) methods do well enough at this task to be considered a reasonable baseline. Finally, we make some suggestions on how our findings might be used in future research in this area.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {746–757},
numpages = {12},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3328526.3329582,
author = {Yan, Hao and Das, Sanmay and Lavoie, Allen and Li, Sirui and Sinclair, Betsy},
title = {The Congressional Classification Challenge: Domain Specificity and Partisan Intensity},
year = {2019},
isbn = {9781450367929},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328526.3329582},
doi = {10.1145/3328526.3329582},
abstract = {In this paper, we study the effectiveness and generalizability of techniques for classifying partisanship and ideology from text in the context of US politics. In particular, we are interested in how well measures of partisanship transfer across domains as well as the potential to rely upon measures of partisan intensity as a proxy for political ideology. We construct novel datasets of English texts from (1) the Congressional Record, (2) prominent conservative and liberal media websites, and (3) conservative and liberal wikis, and apply text classification algorithms to evaluate domain specificity via a domain adaptation technique. Surprisingly, we find that the cross-domain learning performance, benchmarking the ability to generalize from one of these datasets to another, is in general poor, even though the algorithms perform very well in within-dataset cross-validation tests. While party affiliation of legislators is not predictable based on models learned from other sources, we do find some ability to predict the leanings of the media and crowdsourced websites based on models learned from the Congressional Record. This predictivity is different across topics, and itself a priori predictable based on within-topic cross-validation results. Temporally, phrases tend to move from politicians to the media, helping to explain this predictivity. Finally, when we compare legislators themselves across different media (the Congressional Record and press releases), we find that while party affiliation is highly predictable, within-party ideology is completely unpredictable. Legislators are communicating different messages through different channels while clearly signaling party identity systematically across all channels. Choice of language is a clearly strategic act, among both legislators and the media, and we must therefore proceed with extreme caution in extrapolating from language to partisanship or ideology across domains.},
booktitle = {Proceedings of the 2019 ACM Conference on Economics and Computation},
pages = {71–89},
numpages = {19},
keywords = {domain adaptation, partisanship, political ideology, political science, text classification},
location = {Phoenix, AZ, USA},
series = {EC '19}
}

@inproceedings{10.1145/3328778.3366817,
author = {Karbasian, Habib and Johri, Aditya},
title = {Insights for Curriculum Development: Identifying Emerging Data Science Topics through Analysis of Q&amp;A Communities},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3366817},
doi = {10.1145/3328778.3366817},
abstract = {Updating curricula in new computer science domains is a critical challenge faced by many instructors and programs. In this paper we present an approach for identifying emerging topics and issues in Data Science by using Question and Answer (Q&amp;A) sites as a resource. Q&amp;A sites provide a useful online platform for discussion of topics and through the sharing of information they become a valuable corpus of knowledge. We applied latent Dirichlet allocation (LDA), a statistical topic modeling technique, to analyze data science related threads from from two popular Q&amp;A communities "Stack Exchange and Reddit". We uncovered both important topics as well as useful examples that can be incorporated into teaching. In addition to technical topics, our analysis also identified topics related to professional development. We believe that approaches such as these are critical in order to update curriculum and bridge the workplace-school divide in teaching of newer topics such as data science. Given the pace of technical development and frequent changes in the field, this is an inventive and effective method to keep teaching up to date. We also discuss the limitations of this approach whereby topics of importance such as data ethics are largely missing from online discussions.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {192–198},
numpages = {7},
keywords = {curriculum development, online q&amp;a platforms, reddit, stackexchange, text mining, topic modeling},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.1145/3331543.3342577,
author = {Quick, Donya and Thomas, Kelland},
title = {A functional model of jazz improvisation},
year = {2019},
isbn = {9781450368117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331543.3342577},
doi = {10.1145/3331543.3342577},
abstract = {We present a model of jazz improvisation where short-term decision making by each performer is modeled as a function from contexts to music. Contexts can be shared, such as an agreed-upon chord progression, or they can also be private---a current state for each musician. We formalize this model in Haskell to generate potentially infinitely long jazz improvisations, and we have also used the same model in Python to support real-time human-computer interaction through jazz.},
booktitle = {Proceedings of the 7th ACM SIGPLAN International Workshop on Functional Art, Music, Modeling, and Design},
pages = {11–21},
numpages = {11},
keywords = {functional programming, generative music, improvisation, jazz},
location = {Berlin, Germany},
series = {FARM 2019}
}

@inproceedings{10.1145/3337722.3341835,
author = {Miller, Mitchell and Mendonca, Sean and Philliber, Nathan and Khosmood, Foaad},
title = {Panoptyk: information driven MMO engine},
year = {2019},
isbn = {9781450372176},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3337722.3341835},
doi = {10.1145/3337722.3341835},
abstract = {Project Panoptyk is a game engine designed to run Massive Multiplayer Online (MMO) games with information creation, sharing, and exchange as the central gameplay focus. This engine is a work in progress, intended to serve as a platform for simulating human/robot interaction, as well as automatic generation of game assets, quests, and real-estate. The project also aims to create an open platform allowing indie and research communities to experiment with MMO concepts. In pursuit of these goals, we identify and address a number of challenges that have traditionally made it difficult for independent designers or researchers to be competitive in creation of new MMO games.},
booktitle = {Proceedings of the 14th International Conference on the Foundations of Digital Games},
articleno = {59},
numpages = {4},
location = {San Luis Obispo, California, USA},
series = {FDG '19}
}

@inproceedings{10.1145/3337722.3341845,
author = {Karth, Isaac and Smith, Adam M.},
title = {Addressing the fundamental tension of PCGML with discriminative learning},
year = {2019},
isbn = {9781450372176},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3337722.3341845},
doi = {10.1145/3337722.3341845},
abstract = {Procedural content generation via machine learning (PCGML) is typically framed as the task of fitting a generative model to full-scale examples of a desired content distribution. This approach presents a fundamental tension: the more design effort expended to produce detailed training examples for shaping a generator, the lower the return on investment from applying PCGML in the first place. In response, we propose the use of discriminative models, which capture the validity of a design rather the distribution of the content, trained on positive and negative example design fragments. Through a modest modification of WaveFunctionCollapse, a commercially-adopted PCG approach that we characterize as using elementary machine learning, we demonstrate a new mode of control for learning-based generators. We demonstrate how an artist might craft a focused set of additional positive and negative design fragments by critique of the generator's previous outputs. This interaction mode bridges PCGML with mixed-initiative design assistance tools by working with a machine to define a space of valid designs rather than just one new design.},
booktitle = {Proceedings of the 14th International Conference on the Foundations of Digital Games},
articleno = {89},
numpages = {9},
keywords = {PGCML, constraint solving, design tools, machine learning, mixed-initiative interface, procedural content generation, procedural content generation machine learning},
location = {San Luis Obispo, California, USA},
series = {FDG '19}
}

@inproceedings{10.1145/3338533.3366577,
author = {Chen, Peng and Song, Yuqing and Yuan, Deqi and Liu, Zhe},
title = {Feature fusion adversarial learning network for liver lesion classification},
year = {2020},
isbn = {9781450368414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338533.3366577},
doi = {10.1145/3338533.3366577},
abstract = {The number of training data is the key bottleneck in achieving good results for medical image analysis and especially in deep learning. Due to small medical training data, deep learning models often fail to mine useful features and have serious over-fitting problems. In this paper, we propose a clean and effective feature fusion adversarial learning network to mine useful features and relieve over-fitting problems. Firstly, we train a fully convolution autoencoder network with unsupervised learning to mine useful feature maps from our liver lesion data. Secondly, these feature maps will be transferred to our adversarial SENet network for liver lesion classification. Our experiments on liver lesion classification in CT show an average accuracy as 85.47\% compared with the baseline training scheme, which demonstrate our proposed method can mime useful features and relieve over-fitting problem. It can assist physicians in the early detection and treatment of liver lesions.},
booktitle = {Proceedings of the 1st ACM International Conference on Multimedia in Asia},
articleno = {21},
numpages = {7},
keywords = {Adversarial learning, Feature fusion, Liver lesions, Medical},
location = {Beijing, China},
series = {MMAsia '19}
}

@inproceedings{10.1145/3341105.3373933,
author = {Dahiya, Manjeet and Samatia, Devendra and Rustogi, Kabir},
title = {Learning locality maps from noisy geospatial labels},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373933},
doi = {10.1145/3341105.3373933},
abstract = {E-commerce and logistics operations produce a vast amount of geospatial data labelled with postal addresses. The data has great potential to mine geospatial knowledge, and we demonstrate that regional maps can be automatically built using the same. We propose an algorithm to construct non-overlapping polygons of the localities at a city level. The algorithm involves non-parametric spatial probability modelling of the localities followed by locality classification of the cells in a hexagonal grid. We show that our algorithm is capable of handling noise, which is significantly high in our setting due to the small scale of localities. A property about the noise and the correct information is presented such that our algorithm infers a correct locality polygon. We quantitatively measure the accuracy of our system by comparing its output with the available ground truth. We also discuss multiple applications of the generated maps in the context of e-commerce and logistics operations.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {601–608},
numpages = {8},
keywords = {geospatial mining, labelled geospatial data, localities, maps, polygons, postal addresses},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@article{10.1145/3341702,
author = {Walia, Rajan and Narayanan, Praveen and Carette, Jacques and Tobin-Hochstadt, Sam and Shan, Chung-chieh},
title = {From high-level inference algorithms to efficient code},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {ICFP},
url = {https://doi.org/10.1145/3341702},
doi = {10.1145/3341702},
abstract = {Probabilistic programming languages are valuable because they allow domain experts to express probabilistic models and inference algorithms without worrying about irrelevant details. However, for decades there remained an important and popular class of probabilistic inference algorithms whose efficient implementation required manual low-level coding that is tedious and error-prone. They are algorithms whose idiomatic expression requires random array variables that are latent or whose likelihood is conjugate. Although that is how practitioners communicate and compose these algorithms on paper, executing such expressions requires eliminating the latent variables and recognizing the conjugacy by symbolic mathematics. Moreover, matching the performance of handwritten code requires speeding up loops by more than a constant factor. We show how probabilistic programs that directly and concisely express these desired inference algorithms can be compiled while maintaining efficiency. We introduce new transformations that turn high-level probabilistic programs with arrays into pure loop code. We then make great use of domain-specific invariants and norms to optimize the code, and to specialize and JIT-compile the code per execution. The resulting performance is competitive with manual implementations.},
journal = {Proc. ACM Program. Lang.},
month = jul,
articleno = {98},
numpages = {30},
keywords = {arrays, collapsed Gibbs sampling, conjugacy, loop optimization, map-reduce, marginalization, multidimensional distributions, plates, probabilistic programs}
}

@inproceedings{10.1145/3342195.3387555,
author = {Chaudhary, Shubham and Ramjee, Ramachandran and Sivathanu, Muthian and Kwatra, Nipun and Viswanatha, Srinidhi},
title = {Balancing efficiency and fairness in heterogeneous GPU clusters for deep learning},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387555},
doi = {10.1145/3342195.3387555},
abstract = {We present Gandivafair, a distributed, fair share scheduler that balances conflicting goals of efficiency and fairness in GPU clusters for deep learning training (DLT). Gandivafair provides performance isolation between users, enabling multiple users to share a single cluster, thus, maximizing cluster efficiency. Gandivafair is the first scheduler that allocates cluster-wide GPU time fairly among active users.Gandivafair achieves efficiency and fairness despite cluster heterogeneity. Data centers host a mix of GPU generations because of the rapid pace at which newer and faster GPUs are released. As the newer generations face higher demand from users, older GPU generations suffer poor utilization, thus reducing cluster efficiency. Gandivafair profiles the variable marginal utility across various jobs from newer GPUs, and transparently incentivizes users to older GPUs by a novel resource trading mechanism that maximizes cluster efficiency without affecting fairness guarantees of any user. With a prototype implementation and evaluation in a heterogeneous 200-GPU cluster, we show that Gandivafair achieves both fairness and efficiency under realistic multi-user workloads.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {1},
numpages = {16},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@article{10.1145/3344255,
author = {Pisani, Paulo Henrique and Mhenni, Abir and Giot, Romain and Cherrier, Estelle and Poh, Norman and Ferreira de Carvalho, Andr\'{e} Carlos Ponce de Leon and Rosenberger, Christophe and Amara, Najoua Essoukri Ben},
title = {Adaptive Biometric Systems: Review and Perspectives},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3344255},
doi = {10.1145/3344255},
abstract = {With the widespread of computing and mobile devices, authentication using biometrics has received greater attention. Although biometric systems usually provide good solutions, the recognition performance tends to be affected over time due to changing conditions and aging of biometric data, which results in intra-class variability. Adaptive biometric systems, which adapt the biometric reference over time, have been proposed to deal with such intra-class variability. This article provides the most up-to-date and complete discussion on adaptive biometrics systems we are aware of, including formalization, terminology, sources or variations that motivates the use of adaptation, adaptation strategies, evaluation methodology, and open challenges. This field of research is sometimes referred to as template update.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {102},
numpages = {38},
keywords = {Adaptive biometric systems, biometric reference adaptation, evaluation methodology, template aging, template update}
}

@inproceedings{10.1145/3355047.3359415,
author = {Glassner, Andrew},
title = {Deep learning: a crash course},
year = {2019},
isbn = {9781450369411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3355047.3359415},
doi = {10.1145/3355047.3359415},
abstract = {The topics for Part 1 of 4.},
booktitle = {SIGGRAPH Asia 2019 Courses},
articleno = {120},
numpages = {588},
location = {Brisbane, Queensland, Australia},
series = {SA '19}
}

@proceedings{10.1145/3355088,
title = {SA '19: SIGGRAPH Asia 2019 Technical Briefs},
year = {2019},
isbn = {9781450369459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Brisbane, QLD, Australia}
}

@inproceedings{10.1145/3357384.3357980,
author = {Ai, Qingyao and Hill, Daniel N. and Vishwanathan, S. V. N. and Croft, W. Bruce},
title = {A Zero Attention Model for Personalized Product Search},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357980},
doi = {10.1145/3357384.3357980},
abstract = {Product search is one of the most popular methods for people to discover and purchase products on e-commerce websites. Because personal preferences often have an important influence on the purchase decision of each customer, it is intuitive that personalization should be beneficial for product search engines. While synthetic experiments from previous studies show that purchase histories are useful for identifying the individual intent of each product search session, the effect of personalization on product search in practice, however, remains mostly unknown. In this paper, we formulate the problem of personalized product search and conduct large-scale experiments with search logs sampled from a commercial e-commerce search engine. Results from our preliminary analysis show that the potential of personalization depends on query characteristics, interactions between queries, and user purchase histories. Based on these observations, we propose a Zero Attention Model for product search that automatically determines when and how to personalize a user-query pair via a novel attention mechanism. Empirical results on commercial product search logs show that the proposed model not only significantly outperforms state-of-the-art personalized product retrieval models, but also provides important information on the potential of personalization in each product search session.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {379–388},
numpages = {10},
keywords = {attention mechanism, personalization, product search},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3357766.3359540,
author = {Jeanjean, Pierre and Combemale, Benoit and Barais, Olivier},
title = {From DSL specification to interactive computer programming environment},
year = {2019},
isbn = {9781450369817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357766.3359540},
doi = {10.1145/3357766.3359540},
abstract = {The adoption of Domain-Specific Languages (DSLs) relies on the capacity of language workbenches to automate the development of advanced and customized environments. While DSLs are usually well tailored for the main scenarios, the cost of developing mature tools prevents the ability to develop additional capabilities for alternative scenarios targeting specific tasks (e.g., API testing) or stakeholders (e.g., education). In this paper, we propose an approach to automatically generate interactive computer programming environments from existing specifications of textual interpreted DSLs. The approach provides abstractions to complement the DSL specification, and combines static analysis and language transformations to automate the transformation of the language syntax, the execution state and the execution semantics. We evaluate the approach over a representative set of DSLs, and demonstrate the ability to automatically transform a textual syntax to load partial programs limited to a single statement, and to derive a Read-Eval-Print-Loop (REPL) from the specification of a language interpreter.},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {167–178},
numpages = {12},
keywords = {domain specific languages, language engineering, repl},
location = {Athens, Greece},
series = {SLE 2019}
}

@article{10.1145/3359134,
author = {Frey, Seth and Krafft, P. M. and Keegan, Brian C.},
title = {"This Place Does What It Was Built For": Designing Digital Institutions for Participatory Change},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359134},
doi = {10.1145/3359134},
abstract = {Whether we recognize it or not, the Internet is rife with exciting and original institutional forms that are transforming social organization on and offline. Governing these Internet platforms and other digital institutions has posed a challenge for engineers and managers, many of whom have little exposure to the relevant history or theory of institutional design. The dominant guiding practices for the design of digital institutions to date in human-computer interaction, computer-supported cooperative work, and the tech industry at large have been an incentive-focused behavioral engineering paradigm encompassing atheoretical approaches such as emulation, A/B-testing, engagement maximization, and piecemeal issue-driven engineering. One institutional analysis framework that has been useful in the study of traditional institutions comes from scholars of natural resource management, particularly that community of economists, anthropologists, and environmental and political scientists focused around the work of Elinor Ostrom, known collectively as the "Ostrom Workshop." A key finding from this community that has yet to be broadly incorporated into the design of many digital institutions is the importance of including participatory change mechanisms in what is called a "constitutional layer" of institutional design. The institutional rules that compose a constitutional layer facilitate stakeholder participation in the ongoing process of institutional design change. We explore to what extent consideration of constitutional layers is met or could be better met in three varied cases of digital institutions: cryptocurrencies, cannabis informatics, and amateur Minecraft server governance. Examining such highly varied cases allows us to demonstrate the broad relevance of constitutional layers in many different types of digital institutions.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {32},
numpages = {31},
keywords = {computational social science, digital democracy, digital institutions, institutional analysis, institutional design, knowledge commons, resource management}
}

@article{10.1145/3359176,
author = {Muri\'{c}, Goran and Abeliuk, Andres and Lerman, Kristina and Ferrara, Emilio},
title = {Collaboration Drives Individual Productivity},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359176},
doi = {10.1145/3359176},
abstract = {How does the number of collaborators affect individual productivity? Results of prior research have been conflicting, with some studies reporting an increase in individual productivity as the number of collaborators grows, while other studies showing that the free-rider effect skews the effort invested by individuals, making larger groups less productive. The difference between these schools of thought is substantial: if a super-scaling effect exists, as suggested by former studies, then as groups grow, their productivity will increase even faster than their size, super-linearly improving their efficiency. We address this question by studying two planetary-scale collaborative systems: GitHub and Wikipedia. By analyzing the activity of over 2 million users on these platforms, we discover that the interplay between group size and productivity exhibits complex, previously-unobserved dynamics: the productivity of smaller groups scales super-linearly with group size, but saturates at larger sizes. This effect is not an artifact of the heterogeneity of productivity: the relation between group size and productivity holds at the individual level. People tend to do more when collaborating with more people. We propose a generative model of individual productivity that captures the non-linearity in collaboration effort. The proposed model is able to explain and predict group work dynamics in GitHub and Wikipedia by capturing their maximally informative behavioral features, and it paves the way for a principled, data-driven science of collaboration.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {74},
numpages = {24},
keywords = {collaboration, github, productivity, software development, teamwork, wikipedia}
}

@article{10.1145/3359221,
author = {Mulligan, Deirdre K. and Kroll, Joshua A. and Kohli, Nitin and Wong, Richmond Y.},
title = {This Thing Called Fairness: Disciplinary Confusion Realizing a Value in Technology},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359221},
doi = {10.1145/3359221},
abstract = {The explosion in the use of software in important sociotechnical systems has renewed focus on the study of the way technical constructs reflect policies, norms, and human values. This effort requires the engagement of scholars and practitioners from many disciplines. And yet, these disciplines often conceptualize the operative values very differently while referring to them using the same vocabulary. The resulting conflation of ideas confuses discussions about values in technology at disciplinary boundaries. In the service of improving this situation, this paper examines the value of shared vocabularies, analytics, and other tools that facilitate conversations about values in light of these disciplinary specific conceptualizations, the role such tools play in furthering research and practice, outlines different conceptions of "fairness" deployed in discussions about computer systems, and provides an analytic tool for interdisciplinary discussions and collaborations around the concept of fairness. We use a case study of risk assessments in criminal justice applications to both motivate our effort--describing how conflation of different concepts under the banner of "fairness" led to unproductive confusion--and illustrate the value of the fairness analytic by demonstrating how the rigorous analysis it enables can assist in identifying key areas of theoretical, political, and practical misunderstanding or disagreement, and where desired support alignment or collaboration in the absence of consensus.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {119},
numpages = {36},
keywords = {analytic, fairness, infrastructure, interdisciplinary, values}
}

@article{10.1145/3359272,
author = {Yadav, Deepika and Malik, Prerna and Dabas, Kirti and Singh, Pushpendra},
title = {Feedpal: Understanding Opportunities for Chatbots in Breastfeeding Education of Women in India},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359272},
doi = {10.1145/3359272},
abstract = {Use of chatbots in different spheres of life is continuously increasing since a couple of years. We attempt to understand the potential of chatbots for breastfeeding education by conducting an Wizard-of-Oz experiment with 22 participants. Our participants included breastfeeding mothers and community health workers from the slum areas of Delhi, India.We prototyped our chatbot as an interactive question-answering application and analyzed users' interaction patterns, perceptions, and contexts of use. The chatbot use cases emerged primarily as the first line of support. The participants, especially the mothers, were enthusiastic with the opportunity to ask questions and get reliable answers. We also observed the influencing role of female relative, e.g. mothers-in-law, in breastfeeding practices. Our analysis of user information-seeking suggests that a majority of questions (88\%) are of nature that can be answered by a chatbot application. We further observe that the queries are embedded deeply into myths and existing belief systems. Therefore requiring the designers to focus on subtle aspects for providing information such as positive reinforcement and contextual sensitivity. Further, we discuss, different societal and ethical issues associated with Chatbot usage for a public health topic such as breastfeeding education.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {170},
numpages = {30},
keywords = {asha, breastfeeding, chatbot, chw, hci4d, india}
}

@article{10.1145/3359626,
author = {De Guzman, Jaybie A. and Thilakarathna, Kanchana and Seneviratne, Aruna},
title = {Security and Privacy Approaches in Mixed Reality: A Literature Survey},
year = {2019},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3359626},
doi = {10.1145/3359626},
abstract = {Mixed reality (MR) technology development is now gaining momentum due to advances in computer vision, sensor fusion, and realistic display technologies. With most of the research and development focused on delivering the promise of MR, the privacy and security implications of this technology are yet to be thoroughly investigated. This survey article aims to put in to light these risks and to look into the latest security and privacy work on MR. Specifically, we list and review the different protection approaches that have been proposed to ensure user and data security and privacy in MR. We extend the scope to include work on related technologies such as augmented reality, virtual reality, and human-computer interaction as crucial components, if not the origins, of MR, as well as numerous related work from the larger area of mobile devices, wearables, and Internet-of-Things. We highlight the lack of investigation, implementation, and evaluation of data protection approaches in MR. Further challenges and directions on MR security and privacy are also discussed.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {110},
numpages = {37},
keywords = {Mixed reality, augmented reality, privacy, security}
}

@proceedings{10.1145/3359996,
title = {VRST '19: Proceedings of the 25th ACM Symposium on Virtual Reality Software and Technology},
year = {2019},
isbn = {9781450370011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Parramatta, NSW, Australia}
}

@inproceedings{10.1145/3362743.3362960,
author = {Aloufi, Ranya and Haddadi, Hamed and Boyle, David},
title = {Emotion Filtering at the Edge},
year = {2019},
isbn = {9781450370110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362743.3362960},
doi = {10.1145/3362743.3362960},
abstract = {Voice controlled devices and services have become very popular in the consumer IoT. Cloud-based speech analysis services extract information from voice inputs using speech recognition techniques. Services providers can thus build very accurate profiles of users' demographic categories, personal preferences, emotional states, etc., and may therefore significantly compromise their privacy. To address this problem, we have developed a privacy-preserving intermediate layer between users and cloud services to sanitize voice input directly at edge devices. We use CycleGAN-based speech conversion to remove sensitive information from raw voice input signals before regenerating neutralized signals for forwarding. We implement and evaluate our emotion filtering approach using a relatively cheap Raspberry Pi 4, and show that performance accuracy is not compromised at the edge. Signals generated at the edge are shown to differ only slightly (~0.16\%) from cloud-based approaches for speech recognition. Experimental evaluation of generated signals show that identification of the emotional state of a speaker can be reduced by ~91\%.},
booktitle = {Proceedings of the 1st Workshop on Machine Learning on Edge in Sensor Systems},
pages = {1–6},
numpages = {6},
keywords = {Internet of Things (IoT), Speech Analysis, Voice Privacy, Voice Synthesis},
location = {New York, NY, USA},
series = {SenSys-ML 2019}
}

@inproceedings{10.1145/3368089.3417058,
author = {Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel},
title = {IntelliCode compose: code generation using transformer},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417058},
doi = {10.1145/3368089.3417058},
abstract = {In software development through integrated development environments (IDEs), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and APIs, or arguments. In this paper, we introduce IntelliCode Compose – a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages state-of-the-art generative transformer model trained on 1.2 billion lines of source code in Python, C#, JavaScript and TypeScript programming languages. IntelliCode Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code IDE and Azure Notebook. Our best model yields an average edit similarity of 86.7\% and a perplexity of 1.82 for Python programming language.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1433–1443},
numpages = {11},
keywords = {Code completion, naturalness of software, neural networks},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3369412.3395059,
author = {Agarwal, Shruti and Farid, Hany},
title = {Photo Forensics From Rounding Artifacts},
year = {2020},
isbn = {9781450370509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369412.3395059},
doi = {10.1145/3369412.3395059},
abstract = {Many aspects of JPEG compression have been successfully used in the domain of photo forensics. Adding to this literature, we describe a JPEG artifact that can arise depending upon seemingly innocuous implementation details in a JPEG encoder. We describe the nature of these artifacts and show how a generic JPEG encoder can be configured to explain a wide range of these artifacts found in real-world cameras. We also describe an algorithm to simultaneously estimate the nature of these artifacts and localize inconsistencies that can arise from a wide range of image manipulations.},
booktitle = {Proceedings of the 2020 ACM Workshop on Information Hiding and Multimedia Security},
pages = {103–114},
numpages = {12},
keywords = {JPEG compression, image analysis, image forensics},
location = {Denver, CO, USA},
series = {IH&amp;MMSec '20}
}

@article{10.1145/3369813,
author = {Haladjian, Juan},
title = {The Wearables Development Toolkit: An Integrated Development Environment for Activity Recognition Applications},
year = {2020},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3369813},
doi = {10.1145/3369813},
abstract = {Although the last two decades have seen an increasing number of activity recognition applications with wearable devices, there is still a lack of tools specifically designed to support their development. The development of activity recognition algorithms for wearable devices is particularly challenging because of the several requirements that have to be met simultaneously (e.g., low energy consumption, small and lightweight, accurate recognition). Activity recognition applications are usually developed in a series of iterations to annotate sensor data and to analyze, develop and assess the performance of a recognition algorithm. This paper presents the Wearables Development Toolkit, an Integrated Development Environment designed to lower the entrance barrier to the development of activity recognition applications with wearables. It specifically focuses on activity recognition using on-body inertial sensors. The toolkit offers a repository of high-level reusable components and a set of tools with functionality to annotate data, to analyze and develop activity recognition algorithms and to assess their recognition and computational performance. We demonstrate the versatility of the toolkit with three applications and describe how we developed it incrementally based on two user studies.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {134},
numpages = {26},
keywords = {Development Environment, Flow-based programming, Human Activity Recognition, Machine Learning, Toolkit, Wearables}
}

@article{10.1145/3372266,
author = {Chelini, Lorenzo and Zinenko, Oleksandr and Grosser, Tobias and Corporaal, Henk},
title = {Declarative Loop Tactics for Domain-specific Optimization},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3372266},
doi = {10.1145/3372266},
abstract = {Increasingly complex hardware makes the design of effective compilers difficult. To reduce this problem, we introduce Declarative Loop Tactics, which is a novel framework of composable program transformations based on an internal tree-like program representation of a polyhedral compiler. The framework is based on a declarative C++ API built around easy-to-program matchers and builders, which provide the foundation to develop loop optimization strategies. Using our matchers and builders, we express computational patterns and core building blocks, such as loop tiling, fusion, and data-layout transformations, and compose them into algorithm-specific optimizations. Declarative Loop Tactics (Loop Tactics for short) can be applied to many domains. For two of them, stencils and linear algebra, we show how developers can express sophisticated domain-specific optimizations as a set of composable transformations or calls to optimized libraries. By allowing developers to add highly customized optimizations for a given computational pattern, we expect our approach to reduce the need for DSLs and to extend the range of optimizations that can be performed by a current general-purpose compiler.},
journal = {ACM Trans. Archit. Code Optim.},
month = dec,
articleno = {55},
numpages = {25},
keywords = {Loop tactics, declarative loop optimizations, polyhedral model}
}

@inproceedings{10.1145/3372297.3417880,
author = {Zanella-B\'{e}guelin, Santiago and Wutschitz, Lukas and Tople, Shruti and R\"{u}hle, Victor and Paverd, Andrew and Ohrimenko, Olga and K\"{o}pf, Boris and Brockschmidt, Marc},
title = {Analyzing Information Leakage of Updates to Natural Language Models},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3417880},
doi = {10.1145/3372297.3417880},
abstract = {To continuously improve quality and reflect changes in data, machine learning applications have to regularly retrain and update their core models. We show that a differential analysis of language model snapshots before and after an update can reveal a surprising amount of detailed information about changes in the training data. We propose two new metrics---differential score and differential rank---for analyzing the leakage due to updates of natural language models. We perform leakage analysis using these metrics across models trained on several different datasets using different methods and configurations. We discuss the privacy implications of our findings, propose mitigation strategies and evaluate their effect.},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {363–375},
numpages = {13},
keywords = {machine learning, natural language, neural networks, privacy},
location = {Virtual Event, USA},
series = {CCS '20}
}

@inproceedings{10.1145/3373625.3417024,
author = {Bragg, Danielle and Koller, Oscar and Caselli, Naomi and Thies, William},
title = {Exploring Collection of Sign Language Datasets: Privacy, Participation, and Model Performance},
year = {2020},
isbn = {9781450371032},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373625.3417024},
doi = {10.1145/3373625.3417024},
abstract = {As machine learning algorithms continue to improve, collecting training data becomes increasingly valuable. At the same time, increased focus on data collection may introduce compounding privacy concerns. Accessibility projects in particular may put vulnerable populations at risk, as disability status is sensitive, and collecting data from small populations limits anonymity. To help address privacy concerns while maintaining algorithmic performance on machine learning tasks, we propose privacy-enhancing distortions of training datasets. We explore this idea through the lens of sign language video collection, which is crucial for advancing sign language recognition and translation. We present a web study exploring signers’ concerns in contributing to video corpora and their attitudes about using filters, and a computer vision experiment exploring sign language recognition performance with filtered data. Our results suggest that privacy concerns may exist in contributing to sign language corpora, that filters (especially expressive avatars and blurred faces) may impact willingness to participate, and that training on more filtered data may boost recognition accuracy in some cases.},
booktitle = {Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {33},
numpages = {14},
keywords = {data collection, machine learning, privacy, sign language},
location = {Virtual Event, Greece},
series = {ASSETS '20}
}

@inproceedings{10.1145/3375627.3375815,
author = {Shevlane, Toby and Dafoe, Allan},
title = {The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375815},
doi = {10.1145/3375627.3375815},
abstract = {There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {173–179},
numpages = {7},
keywords = {ai governance, disclosure of research, misuse of ai, publication norms},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3377814.3381703,
author = {Ahmed, Umair Z. and Srivastava, Nisheeth and Sindhgatta, Renuka and Karkare, Amey},
title = {Characterizing the pedagogical benefits of adaptive feedback for compilation errors by novice programmers},
year = {2020},
isbn = {9781450371247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377814.3381703},
doi = {10.1145/3377814.3381703},
abstract = {Can automated adaptive feedback for correcting erroneous programs help novice programmers learn to code better? In a large-scale experiment, we compare student performance when tutored by human tutors, and when receiving automated adaptive feedback. The automated feedback was designed using one of two well-known instructional principles: (i) presenting the correct solution for the immediate problem, or (ii) presenting generated examples or analogies that guide towards the correct solution. We report empirical results from a large-scale (N = 480, 10,000 + person hour) experiment assessing the efficacy of these automated compilation-error feedback tools. Using the survival analysis on error rates of students measured over seven weeks, we found that automated feedback allows students to resolve errors in their code more efficiently than students receiving manual feedback. However, we also found that this advantage is primarily logistical and not conceptual; the performance benefit seen during lab assignments disappeared during exams wherein feedback of any kind was withdrawn. We further found that the performance advantage of automated feedback over human tutors increases with problem complexity, and that feedback via example and specific repair have distinct, non-overlapping relative advantages for different categories of programming errors. Our results offer a clear and granular delimitation of the pedagogical benefits of automated feedback in teaching programming to novices.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering Education and Training},
pages = {139–150},
numpages = {12},
keywords = {compilation errors, example generation, intelligent tutoring systems, introductory programming, program repair, user study},
location = {Seoul, South Korea},
series = {ICSE-SEET '20}
}

@article{10.1145/3377869,
author = {Burton, Ren\'{e}e},
title = {Unsupervised Learning Techniques for Malware Characterization: Understanding Certain DNS-based DDoS Attacks},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3377869},
doi = {10.1145/3377869},
abstract = {This article details data science research in the area of Cyber Threat Intelligence applied to a specific type of Distributed Denial of Service (DDoS) attack. We study a DDoS technique prevalent in the Domain Name System (DNS) for which little malware have been recovered. Using data from a globally distributed set of a passive collectors (pDNS), we create a statistical classifier to identify these attacks and then use unsupervised learning to investigate the attack events and the malware that generates them. The first known major study of this technique, this work demonstrates that current attacks have little resemblance to earlier published descriptions and identifies several features of the attacks. Through a combination of text and time-series features, we are able to characterize the dominant malware and demonstrate that the number of global-scale attack systems is relatively small.},
journal = {Digital Threats},
month = aug,
articleno = {14},
numpages = {26},
keywords = {Domain name service (DNS), botnet, clustering, data science, ddos attacks, malware, threat intelligence}
}

@inproceedings{10.1145/3379155.3391314,
author = {Barz, Michael and Stauden, Sven and Sonntag, Daniel},
title = {Visual Search Target Inference in Natural Interaction Settings with Machine Learning},
year = {2020},
isbn = {9781450371339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379155.3391314},
doi = {10.1145/3379155.3391314},
abstract = {Visual search is a perceptual task in which humans aim at identifying a search target object such as a traffic sign among other objects. Search target inference subsumes computational methods for predicting this target by tracking and analyzing overt behavioral cues of that person, e.g., the human gaze and fixated visual stimuli. We present a generic approach to inferring search targets in natural scenes by predicting the class of the surrounding image segment. Our method encodes visual search sequences as histograms of fixated segment classes determined by SegNet, a deep learning image segmentation model for natural scenes. We compare our sequence encoding and model training (SVM) to a recent baseline from the literature for predicting the target segment. Also, we use a new search target inference dataset. The results show that, first, our new segmentation-based sequence encoding outperforms the method from the literature, and second, that it enables target inference in natural settings.},
booktitle = {ACM Symposium on Eye Tracking Research and Applications},
articleno = {1},
numpages = {8},
keywords = {Machine Learning, Mobile Eyetracking, Search Target Inference, Visual Attention},
location = {Stuttgart, Germany},
series = {ETRA '20 Full Papers}
}

@inproceedings{10.1145/3379337.3415867,
author = {Yang, Humphrey and Qian, Kuanren and Liu, Haolin and Yu, Yuxuan and Gu, Jianzhe and McGehee, Matthew and Zhang, Yongjie Jessica and Yao, Lining},
title = {SimuLearn: Fast and Accurate Simulator to Support Morphing Materials Design and Workflows},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415867},
doi = {10.1145/3379337.3415867},
abstract = {Morphing materials allow us to create new modalities of interaction and fabrication by leveraging the materials? dynamic behaviors. Yet, despite the ongoing rapid growth of computational tools within this realm, current developments are bottlenecked by the lack of an effective simulation method. As a result, existing design tools must trade-off between speed and accuracy to support a real-time interactive design scenario. In response, we introduce SimuLearn, a data-driven method that combines finite element analysis and machine learning to create real-time (0.61 seconds) and truthful (97\% accuracy) morphing material simulators. We use mesh-like 4D printed structures to contextualize this method and prototype design tools to exemplify the design workflows and spaces enabled by a fast and accurate simulation method. Situating this work among existing literature, we believe SimuLearn is a timely addition to the HCI CAD toolbox that can enable the proliferation of morphing materials.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {71–84},
numpages = {14},
keywords = {4d printing, computational fabrication., design tool, machine learning, shape-changing interface, simulation},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3383313.3412252,
author = {Huang, Jin and Oosterhuis, Harrie and de Rijke, Maarten and van Hoof, Herke},
title = {Keeping Dataset Biases out of the Simulation: A Debiased Simulator for Reinforcement Learning based Recommender Systems},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3412252},
doi = {10.1145/3383313.3412252},
abstract = {Reinforcement learning for recommendation (RL4Rec) methods are increasingly receiving attention as an effective way to improve long-term user engagement. However, applying RL4Rec online comes with risks: exploration may lead to periods of detrimental user experience. Moreover, few researchers have access to real-world recommender systems. Simulations have been put forward as a solution where user feedback is simulated based on logged historical user data, thus enabling optimization and evaluation without being run online. While simulators do not risk the user experience and are widely accessible, we identify an important limitation of existing simulation methods. They ignore the interaction biases present in logged user data, and consequently, these biases affect the resulting simulation. As a solution to this issue, we introduce a debiasing step in the simulation pipeline, which corrects for the biases present in the logged data before it is used to simulate user behavior. To evaluate the effects of bias on RL4Rec simulations, we propose a novel evaluation approach for simulators that considers the performance of policies optimized with the simulator. Our results reveal that the biases from logged data negatively impact the resulting policies, unless corrected for with our debiasing method. While our debiasing methods can be applied to any simulator, we make our complete pipeline publicly available as the Simulator for OFfline leArning and evaluation (SOFA): the first simulator that accounts for interaction biases prior to optimization and evaluation.},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {190–199},
numpages = {10},
keywords = {Interaction bias, Recommender systems, Reinforcement learning, Simulation},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@inproceedings{10.1145/3383455.3422550,
author = {Wu, Hanwei and Gattami, Ather and Flierl, Markus},
title = {Conditional mutual information-based contrastive loss for financial time series forecasting},
year = {2021},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383455.3422550},
doi = {10.1145/3383455.3422550},
abstract = {We present a representation learning framework for financial time series forecasting. One challenge of using deep learning models for finance forecasting is the shortage of available training data when using small datasets. Direct trend classification using deep neural networks trained on small datasets is susceptible to the overfitting problem. In this paper, we propose to first learn compact representations from time series data, then use the learned representations to train a simpler model for predicting time series movements. We consider a class-conditioned latent variable model. We train an encoder network to maximize the mutual information between the latent variables and the trend information conditioned on the encoded observed variables. We show that conditional mutual information maximization can be approximated by a contrastive loss. Then, the problem is transformed into a classification task of determining whether two encoded representations are sampled from the same class or not. This is equivalent to performing pairwise comparisons of the training datapoints, and thus, improves the generalization ability of the encoder network. We use deep autoregressive models as our encoder to capture long-term dependencies of the sequence data. Empirical experiments indicate that our proposed method has the potential to advance state-of-the-art performance.},
booktitle = {Proceedings of the First ACM International Conference on AI in Finance},
articleno = {9},
numpages = {7},
location = {New York, New York},
series = {ICAIF '20}
}

@inproceedings{10.1145/3385010.3385014,
author = {J Bidwell, Nicola},
title = {Wireless in the Weather-world and Community Networks Made to Last},
year = {2020},
isbn = {9781450377003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385010.3385014},
doi = {10.1145/3385010.3385014},
abstract = {I describe grassroots innovation and recursive engagement in Argentina by members of rural Community Networks (CNs), or decentralized telecommunications that people build and operate themselves. Hackers began the CNs to resist the dominant internet regime through peer collaboration but, these days, their members’ technical competences, life experiences and perspectives on communality and solidarity are diverse. I apply Tim Ingold's concepts in analysing links between technology, social relations and bodies in the CNs, and how co-design of an app unfolded along paths of growth and repair. The WiFi network materialises the liveliness of processes that go into forming it, and the app materialises CN members’ efforts to enact awareness of their own nodes to maintain the network. The Argentine CNs and Ingold's perspective on commoning illustrate that being aware of local details and adapting to the ways things are going in the living world is vital to making shared resources last.},
booktitle = {Proceedings of the 16th Participatory Design Conference 2020 - Participation(s) Otherwise - Volume 1},
pages = {126–136},
numpages = {11},
keywords = {Commoning, Community Network, Grassroots innovation, Ingold, Mesh, Meshwork, Recursive engagement, WiFi},
location = {Manizales, Colombia},
series = {PDC '20}
}

@article{10.1145/3386330,
author = {Clinger, William D. and Wand, Mitchell},
title = {Hygienic macro technology},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3386330},
doi = {10.1145/3386330},
abstract = {The fully parenthesized Cambridge Polish syntax of Lisp, originally regarded as a temporary expedient to be replaced by more conventional syntax, possesses a peculiar virtue: A read procedure can parse it without knowing the syntax of any expressions, statements, definitions, or declarations it may represent. The result of that parsing is a list structure that establishes a standard representation for uninterpreted abstract syntax trees. This representation provides a convenient basis for macro processing, which allows the programmer to specify that some simple piece of abstract syntax should be replaced by some other, more complex piece of abstract syntax. As is well-known, this yields an abstraction mechanism that does things that procedural abstraction cannot, such as introducing new binding structures. The existence of that standard representation for uninterpreted abstract syntax trees soon led Lisp to a greater reliance upon macros than was common in other high-level languages. The importance of those features is suggested by the ten pages devoted to macros in an earlier ACM HOPL paper, “The Evolution of Lisp.” However, na'ive macro expansion was a leaky abstraction, because the movement of a piece of syntax from one place to another might lead to the accidental rebinding of a program’s identifiers. Although this problem was recognized in the 1960s, it was 20 years before a reliable solution was discovered, and another 10 before a solution was discovered that was reliable, flexible, and efficient. In this paper, we summarize that early history with greater focus on hygienic macros, and continue the story by describing the further development, adoption, and influence of hygienic and partially hygienic macro technology in Scheme. The interplay between the desire for standardization and the development of new algorithms is a major theme of that story. We then survey the ways in which hygienic macro technology has been adapted into recent non-parenthetical languages. Finally, we provide a short history of attempts to provide a formal account of macro processing.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {80},
numpages = {110},
keywords = {Lisp, Scheme, hygiene, macro}
}

@inproceedings{10.1145/3388440.3412410,
author = {Singh, Ritambhara and Demetci, Pinar and Bonora, Giancarlo and Ramani, Vijay and Lee, Choli and Fang, He and Duan, Zhijun and Deng, Xinxian and Shendure, Jay and Disteche, Christine and Noble, William Stafford},
title = {Unsupervised manifold alignment for single-cell multi-omics data},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3412410},
doi = {10.1145/3388440.3412410},
abstract = {Integrating single-cell measurements that capture different properties of the genome is vital to extending our understanding of genome biology. This task is challenging due to the lack of a shared axis across datasets obtained from different types of single-cell experiments. For most such datasets, we lack corresponding information among the cells (samples) and the measurements (features). In this scenario, unsupervised algorithms that are capable of aligning single-cell experiments are critical to learning an in silico co-assay that can help draw correspondences among the cells. Maximum mean discrepancy-based manifold alignment (MMD-MA) is such an unsupervised algorithm. Without requiring correspondence information, it can align single-cell datasets from different modalities in a common shared latent space, showing promising results on simulations and a small-scale single-cell experiment with 61 cells. However, it is essential to explore the applicability of this method to larger single-cell experiments with thousands of cells so that it can be of practical interest to the community. In this paper, we apply MMD-MA to two recent datasets that measure transcriptome and chromatin accessibility in ~2000 single cells. To scale the runtime of MMD-MA to a more substantial number of cells, we extend the original implementation to run on GPUs. We also introduce a method to automatically select one of the user-defined parameters, thus reducing the hyperparameter search space. We demonstrate that the proposed extensions allow MMD-MA to accurately align state-of-the-art single-cell experiments.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {40},
numpages = {10},
keywords = {manifold alignment, single cells, unsupervised learning},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1145/3388769.3407509,
author = {Glassner, Andrew},
title = {ML/DL roundup},
year = {2020},
isbn = {9781450379724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388769.3407509},
doi = {10.1145/3388769.3407509},
booktitle = {ACM SIGGRAPH 2020 Courses},
articleno = {22},
numpages = {306},
location = {Virtual Event, USA},
series = {SIGGRAPH '20}
}

@article{10.1145/3392047,
author = {Krishna, Ravi and Mu, Norman and Keutzer, Kurt},
title = {Applying Text Analytics to the Mind-section Literature of the Tibetan Tradition of the Great Perfection},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3392047},
doi = {10.1145/3392047},
abstract = {Over the past decade, through a mixture of optical character recognition and manual input, there is now a growing corpus of Tibetan literature available as e-texts in Unicode format. With the creation of such a corpus, the techniques of text analytics that have been applied in the analysis of English and other modern languages may now be applied to Tibetan. In this work, we narrow our focus to examine a modest portion of that literature, the Mind-section portion of the literature of the Tibetan tradition of the Great Perfection. Here, we will use the lens of text analytics tools based on machine learning techniques to investigate a number of questions of interest to scholars of this and related traditions of the Great Perfection. It has been necessary for us to participate in all portions of this process: corpora identification and text edition selection, rendering the text as e-texts in Unicode using both Optical Character Recognition and manual entry, data cleaning and transformation, implementation of software for text analysis, and interpretation of results. For this reason, we hope this study can serve as a model for other low-resource languages that are just beginning to approach the problem of providing text analytics for their language.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {21},
numpages = {32},
keywords = {Digital humanities, text analytics, natural language processing for low-resource languages}
}

@inproceedings{10.1145/3394486.3403193,
author = {Mandros, Panagiotis and Kaltenpoth, David and Boley, Mario and Vreeken, Jilles},
title = {Discovering Functional Dependencies from Mixed-Type Data},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403193},
doi = {10.1145/3394486.3403193},
abstract = {Given complex data collections, practitioners can perform non-parametric functional dependency discovery (FDD) to uncover relationships between variables that were previously unknown. However, known FDD methods are applicable to nominal data, and in practice non-nominal variables are discretized, e.g., in a pre-processing step. This is problematic because, as soon as a mix of discrete and continuous variables is involved, the interaction of discretization with the various dependency measures from the literature is poorly understood. In particular, it is unclear whether a given discretization method even leads to a consistent dependency estimate. In this paper, we analyze these fundamental questions and derive formal criteria as to when a discretization process applied to a mixed set of random variables leads to consistent estimates of mutual information. With these insights, we derive an estimator framework applicable to any task that involves estimating mutual information from multivariate and mixed-type data. Last, we extend with this framework a previously proposed FDD approach for reliable dependencies. Experimental evaluation shows that the derived reliable estimator is both computationally and statistically efficient, and leads to effective FDD algorithms for mixed-type data.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {1404–1414},
numpages = {11},
keywords = {functional dependency discovery, mixed data, mutual information},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3394486.3403206,
author = {Xue, Yuan and Zhou, Denny and Du, Nan and Dai, Andrew M. and Xu, Zhen and Zhang, Kun and Cui, Claire},
title = {Deep State-Space Generative Model For Correlated Time-to-Event Predictions},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403206},
doi = {10.1145/3394486.3403206},
abstract = {Capturing the inter-dependencies among multiple types of clinically-critical events is critical not only to accurate future event prediction, but also to better treatment planning. In this work, we propose a deep latent state-space generative model to capture the interactions among different types of correlated clinical events (e.g., kidney failure, mortality) by explicitly modeling the temporal dynamics of patients' latent states. Based on these learned patient states, we further develop a new general discrete-time formulation of the hazard rate function to estimate the survival distribution of patients with significantly improved accuracy. Extensive evaluations over real EMR data show that our proposed model compares favorably to various state-of-the-art baselines. Furthermore, our method also uncovers meaningful insights about the latent correlations among mortality and different types of organ failures.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {1552–1562},
numpages = {11},
keywords = {generative model, state space model, survival analysis},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3394486.3403304,
author = {Nguyen, Viet-An and Shi, Peibei and Ramakrishnan, Jagdish and Weinsberg, Udi and Lin, Henry C. and Metz, Steve and Chandra, Neil and Jing, Jane and Kalimeris, Dimitris},
title = {CLARA: Confidence of Labels and Raters},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403304},
doi = {10.1145/3394486.3403304},
abstract = {Large online services employ thousands of people to label content for applications such as video understanding, natural language processing, and content policy enforcement. While labelers typically reach their decisions by following a well-defined "protocol'', humans may still make mistakes. A common countermeasure is to have multiple people review the same content; however, this process is often time-intensive and requires accurate aggregation of potentially noisy decisions.In this paper, we present CLARA (Confidence of Labels and Raters), a system developed and deployed at Facebook for aggregating reviewer decisions and estimating their uncertainty. We perform extensive validations and describe the deployment of CLARA for measuring the base rate of policy violations, quantifying reviewers' performance, and improving their efficiency. In our experiments, we found that CLARA (a) provides an unbiased estimator of violation rates that is robust to changes in reviewer quality, with accurate confidence intervals, (b) provides an accurate assessment of reviewers' performance, and (c) improves efficiency by reducing the number of reviews based on the review certainty, and enables the operational selection of a threshold on the cost/accuracy efficiency frontier.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2542–2552},
numpages = {11},
keywords = {bayesian methods, crowdsourcing, probabilistic models},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3397271.3401180,
author = {Zou, Jie and Chen, Yifan and Kanoulas, Evangelos},
title = {Towards Question-based Recommender Systems},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401180},
doi = {10.1145/3397271.3401180},
abstract = {Conversational and question-based recommender systems have gained increasing attention in recent years, with users enabled to converse with the system and better control recommendations. Nevertheless, research in the field is still limited, compared to traditional recommender systems. In this work, we propose a novel Question-based recommendation method, Qrec, to assist users to find items interactively, by answering automatically constructed and algorithmically chosen questions. Previous conversational recommender systems ask users to express their preferences over items or item facets. Our model, instead, asks users to express their preferences over descriptive item features. The model is first trained offline by a novel matrix factorization algorithm, and then iteratively updates the user and item latent factors online by a closed-form solution based on the user answers. Meanwhile, our model infers the underlying user belief and preferences over items to learn an optimal question-asking strategy by using Generalized Binary Search, so as to ask a sequence of questions to the user. Our experimental results demonstrate that our proposed matrix factorization model outperforms the traditional Probabilistic Matrix Factorization model. Further, our proposed Qrec model can greatly improve the performance of state-of-the-art baselines, and it is also effective in the case of cold-start user and item recommendations.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {881–890},
numpages = {10},
keywords = {cold-start problem, conversational recommender systems, matrix factorization, question-based recommender systems},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1145/3398020,
author = {Qian, Bin and Su, Jie and Wen, Zhenyu and Jha, Devki Nandan and Li, Yinhao and Guan, Yu and Puthal, Deepak and James, Philip and Yang, Renyu and Zomaya, Albert Y. and Rana, Omer and Wang, Lizhe and Koutny, Maciej and Ranjan, Rajiv},
title = {Orchestrating the Development Lifecycle of Machine Learning-based IoT Applications: A Taxonomy and Survey},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3398020},
doi = {10.1145/3398020},
abstract = {Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML techniques unlock the potential of IoT with intelligence, and IoT applications increasingly feed data collected by sensors into ML models, thereby employing results to improve their business processes and services. Hence, orchestrating ML pipelines that encompass model training and implication involved in the holistic development lifecycle of an IoT application often leads to complex system integration. This article provides a comprehensive and systematic survey of the development lifecycle of ML-based IoT applications. We outline the core roadmap and taxonomy and subsequently assess and compare existing standard techniques used at individual stages.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {82},
numpages = {47},
keywords = {IoT, deep learning, machine learning, orchestration}
}

@inproceedings{10.1145/3402942.3402945,
author = {Gellel, Alexander and Sweetser, Penny},
title = {A Hybrid Approach to Procedural Generation of Roguelike Video Game Levels},
year = {2020},
isbn = {9781450388078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3402942.3402945},
doi = {10.1145/3402942.3402945},
abstract = {Algorithmic generation of data, known as procedural content generation, is an attractive prospect within the game development industry as a means of creating infinitely fresh and varied content. In this paper, we present an approach to level generation for roguelike dungeon style levels, based on our examination of the suite of existing approaches used in formal research. Our generator aims to create simple dungeon style level layouts that are always playable. We utilise a hybrid technique combining context free grammars to generate a description of levels and a cellular automata inspired process to generate the physical space. The generator proves successful at consistently generating dungeon layouts that maintain completability at all times with sufficient variation, when accounting for the occasional need for corrective actions. We conclude that there is substantial value in hybrid approaches to automated level design and propose a new heuristic by which to assess dungeon style level content.},
booktitle = {Proceedings of the 15th International Conference on the Foundations of Digital Games},
articleno = {3},
numpages = {10},
keywords = {PCG, context free grammars, dungeons, game design, procedural content generation, roguelike},
location = {Bugibba, Malta},
series = {FDG '20}
}

@inproceedings{10.1145/3404835.3462872,
author = {Su, Zhan and Dou, Zhicheng and Zhu, Yutao and Qin, Xubo and Wen, Ji-Rong},
title = {Modeling Intent Graph for Search Result Diversification},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462872},
doi = {10.1145/3404835.3462872},
abstract = {Search result diversification aims to offer diverse documents that cover as many intents as possible. Most existing implicit diversification approaches model diversity through the similarity of document representation, which is indirect and unnatural. To handle the diversity more precisely, we measure the similarity of documents by their similarity of the intent coverage. Specifically, we build a classifier to judge whether two different documents contain the same intent based on the document's content. Then we construct an intent graph to present the complicated relationship of documents and the query. On the intent graph, documents are connected if they are similar, while the query and the document are gradually connected based on the document selection result. Then we employ graph convolutional networks (GCNs) to update the representation of the query and each document by aggregating its neighbors. By this means, we can obtain the context-aware query representation and the intent-aware document representations through the dynamic intent graph during the document selection process. Furthermore, these representations and intent graph features are fused into diversity features. Combined with the traditional relevance features, we obtain the final ranking score that balances the relevance and the diversity. Experimental results show that this implicit diversification model significantly outperforms all existing implicit diversification methods, and it can even beat the state-of-the-art explicit models.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {736–746},
numpages = {11},
keywords = {graph neural network, intent graph, search result diversification},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3404835.3462966,
author = {Li, Yunqi and Chen, Hanxiong and Xu, Shuyuan and Ge, Yingqiang and Zhang, Yongfeng},
title = {Towards Personalized Fairness based on Causal Notion},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462966},
doi = {10.1145/3404835.3462966},
abstract = {Recommender systems are gaining increasing and critical impacts on human and society since a growing number of users use them for information seeking and decision making. Therefore, it is crucial to address the potential unfairness problems in recommendations. Just like users have personalized preferences on items, users' demands for fairness are also personalized in many scenarios. Therefore, it is important to providepersonalized fair recommendations for users to satisfy theirpersonalized fairness demands. Besides, previous works on fair recommendation mainly focus on association-based fairness. However, it is important to advance from associative fairness notions to causal fairness notions for assessing fairness more properly in recommender systems. Based on the above considerations, this paper focuses on achieving personalized counterfactual fairness for users in recommender systems. To this end, we introduce a framework for achieving counterfactually fair recommendations through adversary learning by generating feature-independent user embeddings for recommendation. The framework allows recommender systems to achieve personalized fairness for users while also covering non-personalized situations. Experiments on two real-world datasets with shallow and deep recommendation algorithms show that our method can generate fairer recommendations for users with a desirable recommendation performance.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1054–1063},
numpages = {10},
keywords = {adversary learning, counterfactual fairness, personalized fairness, recommender system},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3411497.3420220,
author = {Le, Minh-Ha and Khan, Md Sakib Nizam and Tsaloli, Georgia and Carlsson, Niklas and Buchegger, Sonja},
title = {AnonFACES: Anonymizing Faces Adjusted to Constraints on Efficacy and Security},
year = {2020},
isbn = {9781450380867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411497.3420220},
doi = {10.1145/3411497.3420220},
abstract = {Image data analysis techniques such as facial recognition can threaten individuals' privacy. Whereas privacy risks often can be reduced by adding noise to the data, this approach reduces the utility of the images. For this reason, image de-identification techniques typically replace directly identifying features (e.g., faces, car number plates) present in the data with synthesized features, while still preserving other non-identifying features. As of today, existing techniques mostly focus on improving the naturalness of the generated synthesized images, without quantifying their impact on privacy. In this paper, we propose the first methodology and system design to quantify, improve, and tune the privacy-utility trade-off, while simultaneously also improving the naturalness of the generated images. The system design is broken down into three components that address separate but complementing challenges. This includes a two-step cluster analysis component to extract low-dimensional feature vectors representing the images (embedding) and to cluster the images into fixed-sized clusters. While the importance of good clustering mostly has been neglected in previous work, we find that our novel approach of using low-dimensional feature vectors can improve the privacy-utility trade-off by better clustering similar images. The use of these embeddings has been found particularly useful when wanting to ensure high naturalness and utility of the synthetically generated images. By combining improved clustering and incorporating StyleGAN, a state-of-the-art Generative Neural Network, into our solution, we produce more realistic synthesized faces than prior works, while also better preserving properties such as age, gender, skin tone, or even emotional expressions. Finally, our iterative tuning method exploits non-linear relations between privacy and utility to identify good privacy-utility trade-offs. We note that an example benefit of these improvements is that our solution allows car manufacturers to train their autonomous vehicles while complying with privacy laws.},
booktitle = {Proceedings of the 19th Workshop on Privacy in the Electronic Society},
pages = {87–100},
numpages = {14},
keywords = {image de-identification, k-anonymity, privacy},
location = {Virtual Event, USA},
series = {WPES'20}
}

@proceedings{10.1145/3411763,
title = {CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@inproceedings{10.1145/3411764.3445226,
author = {Ali, Safinah and DiPaola, Daniella and Lee, Irene and Hong, Jenna and Breazeal, Cynthia},
title = {Exploring Generative Models with Middle School Students},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445226},
doi = {10.1145/3411764.3445226},
abstract = {Applications of generative models such as Generative Adversarial Networks (GANs) have made their way to social media platforms that children frequently interact with. While GANs are associated with ethical implications pertaining to children, such as the generation of Deepfakes, there are negligible efforts to educate middle school children about generative AI. In this work, we present a generative models learning trajectory (LT), educational materials, and interactive activities for young learners with a focus on GANs, creation and application of machine-generated media, and its ethical implications. The activities were deployed in four online workshops with 72 students (grades 5-9). We found that these materials enabled children to gain an understanding of what generative models are, their technical components and potential applications, and benefits and harms, while reflecting on their ethical implications. Learning from our findings, we propose an improved learning trajectory for complex socio-technical systems.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {678},
numpages = {13},
keywords = {AI Education, Artificial Intelligence, Generative Adversarial Networks, Generative Machine Learning},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445408,
author = {Hamidi, Foad and Stamato, Lydia and Scheifele, Lisa and Hammond, Rian Ciela Visscher and Asgarali-Hoffman, S. Nisa},
title = {“Turning the Invisible Visible”: Transdisciplinary Bioart Explorations in Human-DNA Interaction},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445408},
doi = {10.1145/3411764.3445408},
abstract = {Hybrid interactive systems that combine living and digital components can engage, educate, and inform users, and are of growing interest in the HCI community. Advances in synthetic biology are transforming what is possible to do with these living media interfaces (LMIs). Bioart is a practice in which artists, often using synthetic biology methods, work with living organisms to creatively explore the human relationship with nonhuman organisms. We present results from an interview study with expert bioartists as well as our hands-on experience in a bioart project where we created poetry-infused wine by encoding and inserting a Persian Sufi poem into the DNA sequence of living yeast cells. We find that engaging in bioart practice generates transdisciplinary fluency with implications for access and activism and our understanding of the qualities of living media. We further explore the qualitative aspects of interacting directly with DNA and implications for sustainable futures.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {592},
numpages = {15},
keywords = {DIYbio, Living organisms, bioart, biotechnology, community science, synthetic biology, transgenic art},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445441,
author = {Chen, Yuxin and Yang, Zhuolin and Abbou, Ruben and Lopes, Pedro and Zhao, Ben Y. and Zheng, Haitao},
title = {User Authentication via Electrical Muscle Stimulation},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445441},
doi = {10.1145/3411764.3445441},
abstract = {We propose a novel modality for active biometric authentication: electrical muscle stimulation (EMS). To explore this, we engineered an interactive system, which we call ElectricAuth, that stimulates the user’s forearm muscles with a sequence of electrical impulses (i.e., EMS challenge) and measures the user’s involuntary finger movements (i.e., response to the challenge). ElectricAuth leverages EMS’s intersubject variability, where the same electrical stimulation results in different movements in different users because everybody’s physiology is unique (e.g., differences in bone and muscular structure, skin resistance and composition, etc.). As such, ElectricAuth allows users to login without memorizing passwords or PINs. ElectricAuth’s challenge-response structure makes it secure against data breaches and replay attacks, a major vulnerability facing today’s biometrics such as facial recognition and fingerprints. Furthermore, ElectricAuth never reuses the same challenge twice in authentications – in just one second of stimulation it encodes one of 68M possible challenges. In our user studies, we found that ElectricAuth resists: (1) impersonation attacks (false acceptance rate: 0.17\% at 5\% false rejection rate); (2) replay attacks (false acceptance rate: 0.00\% at 5\% false rejection rate); and, (3) synthesis attacks (false acceptance rates: 0.2-2.5\%). Our longitudinal study also shows that ElectricAuth produces consistent results over time and across different humidity and muscle conditions.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {6},
numpages = {15},
keywords = {biometric authentication, electrical muscle stimulation, wearable},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445653,
author = {Twigg-Smith, Hannah and O'Leary, Jasper Tran and Peek, Nadya},
title = {Tools, Tricks, and Hacks: Exploring Novel Digital Fabrication Workflows on #PlotterTwitter},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445653},
doi = {10.1145/3411764.3445653},
abstract = {As digital fabrication machines become widespread, online communities have provided space for diverse practitioners to share their work, troubleshoot, and socialize. These communities pioneer increasingly novel fabrication workflows, and it is critical that we understand and conceptualize these workflows beyond traditional manufacturing models. To this end, we conduct a qualitative study of #PlotterTwitter, an online community developing custom hardware and software tools to create artwork with computer-controlled drawing machines known as plotters. We documented and analyzed emergent themes where the traditional interpretation of digital fabrication workflows fails to capture important nuances and nascent directions. We find that #PlotterTwitter makers champion creative exploration of interwoven digital and physical materials over a predictable series of steps. We discuss how this challenges long-running views of digital fabrication and propose design implications for future frameworks and toolkits to account for this breadth of practice.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {594},
numpages = {15},
keywords = {fabrication, maker culture},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445682,
author = {Li, Jingyi and Hashim, Sonia and Jacobs, Jennifer},
title = {What We Can Learn From Visual Artists About Software Development},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445682},
doi = {10.1145/3411764.3445682},
abstract = {This paper explores software’s role in visual art production by examining how artists use and develop software. We conducted interviews with professional artists who were collaborating with software developers, learning software development, and building and maintaining software. We found artists were motivated to learn software development for intellectual growth and access to technical communities. Artists valued efficient workflows through skilled manual execution and personal software development, but avoided high-level forms of software automation. Artists identified conflicts between their priorities and those of professional developers and computational art communities, which influenced how they used computational aesthetics in their work. These findings contribute to efforts in systems engineering research to integrate end-user programming and creativity support across software and physical media, suggesting opportunities for artists as collaborators. Artists’ experiences writing software can guide technical implementations of domain-specific representations, and their experiences in interdisciplinary production can aid inclusive community building around computational tools.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {314},
numpages = {14},
keywords = {creativity support tools, software development, visual art},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3414685.3417836,
author = {Henter, Gustav Eje and Alexanderson, Simon and Beskow, Jonas},
title = {MoGlow: probabilistic and controllable motion synthesis using normalising flows},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3414685.3417836},
doi = {10.1145/3414685.3417836},
abstract = {Data-driven modelling and synthesis of motion is an active research area with applications that include animation, games, and social robotics. This paper introduces a new class of probabilistic, generative, and controllable motion-data models based on normalising flows. Models of this kind can describe highly complex distributions, yet can be trained efficiently using exact maximum likelihood, unlike GANs or VAEs. Our proposed model is autoregressive and uses LSTMs to enable arbitrarily long time-dependencies. Importantly, is is also causal, meaning that each pose in the output sequence is generated without access to poses or control inputs from future time steps; this absence of algorithmic latency is important for interactive applications with real-time motion control. The approach can in principle be applied to any type of motion since it does not make restrictive, task-specific assumptions regarding the motion or the character morphology. We evaluate the models on motion-capture datasets of human and quadruped locomotion. Objective and subjective results show that randomly-sampled motion from the proposed method outperforms task-agnostic baselines and attains a motion quality close to recorded motion capture.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {236},
numpages = {14},
keywords = {data dropout, footstep analysis, generative models, glow, machine learning, normalising flows}
}

@article{10.1145/3414685.3417840,
author = {Bhunia, Ayan Kumar and Das, Ayan and Muhammad, Umar Riaz and Yang, Yongxin and Hospedales, Timothy M. and Xiang, Tao and Gryaditskaya, Yulia and Song, Yi-Zhe},
title = {Pixelor: a competitive sketching AI agent. so you think you can sketch?},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3414685.3417840},
doi = {10.1145/3414685.3417840},
abstract = {We present the first competitive drawing agent Pixelor that exhibits humanlevel performance at a Pictionary-like sketching game, where the participant whose sketch is recognized first is a winner. Our AI agent can autonomously sketch a given visual concept, and achieve a recognizable rendition as quickly or faster than a human competitor. The key to victory for the agent's goal is to learn the optimal stroke sequencing strategies that generate the most recognizable and distinguishable strokes first. Training Pixelor is done in two steps. First, we infer the stroke order that maximizes early recognizability of human training sketches. Second, this order is used to supervise the training of a sequence-to-sequence stroke generator. Our key technical contributions are a tractable search of the exponential space of orderings using neural sorting; and an improved Seq2Seq Wasserstein (S2S-WAE) generator that uses an optimal-transport loss to accommodate the multi-modal nature of the optimal stroke distribution. Our analysis shows that Pixelor is better than the human players of the Quick, Draw! game, under both AI and human judging of early recognition. To analyze the impact of human competitors' strategies, we conducted a further human study with participants being given unlimited thinking time and training in early recognizability by feedback from an AI judge. The study shows that humans do gradually improve their strategies with training, but overall Pixelor still matches human performance. The code and the dataset are available at http://sketchx.ai/pixelor.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {166},
numpages = {15},
keywords = {AI games, early recognition, neural search, recurrent neural network, sketch-generation}
}

@article{10.1145/3415173,
author = {Kou, Yubo and Gui, Xinning},
title = {Mediating Community-AI Interaction through Situated Explanation: The Case of AI-Led Moderation},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415173},
doi = {10.1145/3415173},
abstract = {Artificial intelligence (AI) has become prevalent in our everyday technologies and impacts both individuals and communities. The explainable AI (XAI) scholarship has explored the philosophical nature of explanation and technical explanations, which are usually driven by experts in lab settings and can be challenging for laypersons to understand. In addition, existing XAI research tends to focus on the individual level. Little is known about how people understand and explain AI-led decisions in the community context. Drawing from XAI and activity theory, a foundational HCI theory, we theorize how explanation is situated in a community's shared values, norms, knowledge, and practices, and how situated explanation mediates community-AI interaction. We then present a case study of AI-led moderation, where community members collectively develop explanations of AI-led decisions, most of which are automated punishments. Lastly, we discuss the implications of this framework at the intersection of CSCW, HCI, and XAI.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {102},
numpages = {27},
keywords = {accountability, ai, artificial intelligence, automated moderation, community-ai interaction, explainable ai, explanation, human-ai interaction, online community., transparency}
}

@article{10.1145/3415178,
author = {Seering, Joseph},
title = {Reconsidering Self-Moderation: the Role of Research in Supporting Community-Based Models for Online Content Moderation},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415178},
doi = {10.1145/3415178},
abstract = {Research in online content moderation has a long history of exploring different forms that moderation can take, including both user-driven moderation models on community-based platforms like Wikipedia, Facebook Groups, and Reddit, and centralized corporate moderation models on platforms like Twitter and Instagram. In this work I review different approaches to moderation research with the goal of providing a roadmap for researchers studying community self-moderation. I contrast community-based moderation research with platforms and policies-focused moderation research, and argue that the former has an important role to play in shaping discussions about the future of online moderation. I provide six guiding questions for future research that, if answered, can support the development of a form of user-driven moderation that is widely implementable across a variety of social spaces online, offering an alternative to the corporate moderation models that dominate public debate and discussion.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {107},
numpages = {28},
keywords = {governance, harassment, hate speech, online communities, moderation, platforms, social networks}
}

@article{10.1145/3415219,
author = {Halfaker, Aaron and Geiger, R. Stuart},
title = {ORES: Lowering Barriers with Participatory Machine Learning in Wikipedia},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415219},
doi = {10.1145/3415219},
abstract = {Algorithmic systems---from rule-based bots to machine learning classifiers---have a long history of supporting the essential work of content moderation and other curation work in peer production projects. From counter-vandalism to task routing, basic machine prediction has allowed open knowledge projects like Wikipedia to scale to the largest encyclopedia in the world, while maintaining quality and consistency. However, conversations about how quality control should work and what role algorithms should play have generally been led by the expert engineers who have the skills and resources to develop and modify these complex algorithmic systems. In this paper, we describe ORES: an algorithmic scoring service that supports real-time scoring of wiki edits using multiple independent classifiers trained on different datasets. ORES decouples several activities that have typically all been performed by engineers: choosing or curating training data, building models to serve predictions, auditing predictions, and developing interfaces or automated agents that act on those predictions. This meta-algorithmic system was designed to open up socio-technical conversations about algorithms in Wikipedia to a broader set of participants. In this paper, we discuss the theoretical mechanisms of social change ORES enables and detail case studies in participatory machine learning around ORES from the 5 years since its deployment.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {148},
numpages = {37},
keywords = {algorithms, fairness, governance, machine learning, reflection, transparency, wikipedia}
}

@inproceedings{10.1145/3415263.3419155,
author = {J\"{o}rg, Sophie and Ye, Yuting and Mueller, Franziska and Neff, Michael and Zordan, Victor},
title = {Virtual hands in VR: motion capture, synthesis, and perception},
year = {2020},
isbn = {9781450381123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415263.3419155},
doi = {10.1145/3415263.3419155},
abstract = {We use our hands every day: to grasp a cup of coffee, write text on a keyboard, or signal that we are about to say something important. We use our hands to interact with our environment and to help us communicate with each other without thinking about it. Wouldn't it be great to be able to do the same in virtual reality? However, accurate hand motions are not trivial to capture. In this course, we present the current state of the art when it comes to virtual hands. Starting with current examples for controlling and depicting hands in virtual reality (VR), we dive into the latest methods and technologies to capture hand motions. As hands can currently not be captured in every situation and as constraints stopping us from intersecting with objects are typically not available in VR, we present research on how to synthesize hand motions and simulate grasping motions. Finally, we provide an overview of our knowledge of how virtual hands are being perceived, resulting in practical tips on how to represent and handle virtual hands.Our goals are (a) to present a broad state of the art of the current usage of hands in VR, (b) to provide more in-depth knowledge about the functioning of current hand motion tracking and hand motion synthesis methods, (c) to give insights on our perception of hand motions in VR and how to use those insights when developing new applications, and finally (d) to identify gaps in knowledge that might be investigated next. While the focus of this course is on VR, many parts also apply to augmented reality, mixed reality, and character animation in general, and some content originates from these areas.},
booktitle = {SIGGRAPH Asia 2020 Courses},
articleno = {2},
numpages = {32},
location = {Virtual Event},
series = {SA '20}
}

@inproceedings{10.1145/3419249.3420106,
author = {van Berkel, Niels and Papachristos, Eleftherios and Giachanou, Anastasia and Hosio, Simo and Skov, Mikael B.},
title = {A Systematic Assessment of National Artificial Intelligence Policies: Perspectives from the Nordics and Beyond},
year = {2020},
isbn = {9781450375795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419249.3420106},
doi = {10.1145/3419249.3420106},
abstract = {Echoing the evolving interest and impact of artificial intelligence on society, governments are increasingly looking for ways to strategically position themselves as both innovators and regulators in this new domain. One of the most explicit and accessible ways in which governments outline these plans is through national strategy and policy documents. We follow a systematic search strategy to identify national AI policy documents across twenty-five countries. Through an analysis of these documents, including topic modelling, clustering, and reverse topic-search, we provide an overview of the topics discussed in national AI policies and contrast the differences between countries. Furthermore, we analyse the frequency of eleven ethical principles across our corpus. Our paper outlines implications of the differences between geographical and cultural clusters in relation to the future development of artificial intelligence applications.},
booktitle = {Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society},
articleno = {10},
numpages = {12},
keywords = {AI, Artificial Intelligence, ethics, national guidelines, policy, strategy, topic modelling},
location = {Tallinn, Estonia},
series = {NordiCHI '20}
}

@inproceedings{10.1145/3424771.3424786,
author = {Santos, Jo\~{a}o and Correia, Filipe Figueiredo},
title = {A Review of Pattern Languages for Software Documentation},
year = {2020},
isbn = {9781450377690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424771.3424786},
doi = {10.1145/3424771.3424786},
abstract = {Software documentation is an important part of the captured knowledge of a software project and documentation patterns have often been used as a systematic way to describe good practices on software documentation. Still, many software teams are challenged by what to document, how to keep the documentation consistent and how to make their consumers aware of the relevant documents. A literature review was done over 14 publications and identified 16 quality attributes and 114 patterns about software documentation. This knowledge was analysed and classified and led to the proposal of new categories and relationships between the existing patterns. These are depicted as a new pattern map that provides a new perspective of documentation patterns and can be used to guide teams in adopting software documentation practices.},
booktitle = {Proceedings of the European Conference on Pattern Languages of Programs 2020},
articleno = {27},
numpages = {14},
keywords = {documentation engineering, documentation issues, documentation patterns, quality attributes, software documentation},
location = {Virtual Event, Germany},
series = {EuroPLoP '20}
}

@article{10.1145/3425780,
author = {Mirsky, Yisroel and Lee, Wenke},
title = {The Creation and Detection of Deepfakes: A Survey},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3425780},
doi = {10.1145/3425780},
abstract = {Generative deep learning algorithms have progressed to a point where it is difficult to tell the difference between what is real and what is fake. In 2018, it was discovered how easy it is to use this technology for unethical and malicious applications, such as the spread of misinformation, impersonation of political leaders, and the defamation of innocent individuals. Since then, these “deepfakes” have advanced significantly.In this article, we explore the creation and detection of deepfakes and provide an in-depth view as to how these architectures work. The purpose of this survey is to provide the reader with a deeper understanding of (1) how deepfakes are created and detected, (2) the current trends and advancements in this domain, (3) the shortcomings of the current defense solutions, and (4) the areas that require further research and attention.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {7},
numpages = {41},
keywords = {Deepfake, deep fake, face swap, generative AI, impersonation, reenactment, replacement, social engineering}
}

@inproceedings{10.1145/3425898.3426952,
author = {Collie, Bruce and Woodruff, Jackson and O'Boyle, Michael F. P.},
title = {Modeling black-box components with probabilistic synthesis},
year = {2020},
isbn = {9781450381741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425898.3426952},
doi = {10.1145/3425898.3426952},
abstract = {This paper is concerned with synthesizing programs based on black-box oracles: we are interested in the case where there exists an executable implementation of a component or library, but its internal structure is unknown. We are provided with just an API or function signature, and aim to synthesize a program with equivalent behavior.  To attack this problem, we detail Presyn: a program synthesizer designed for flexible interoperation with existing programs and compiler toolchains. Presyn uses high-level imperative control-flow structures and a pair of cooperating predictive models to efficiently narrow the space of potential programs. These models can be trained effectively on small corpora of synthesized examples.  We evaluate Presyn against five leading program synthesizers on a collection of 112 synthesis benchmarks collated from previous studies and real-world software libraries. We show that Presyn is able to synthesize a wider range of programs than each of them with less human input. We demonstrate the application of our approach to real-world code and software engineering problems with two case studies: accelerator library porting and detection of duplicated library reimplementations.},
booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {1–14},
numpages = {14},
keywords = {black box oracle, model program synthesis, probabilistic},
location = {Virtual, USA},
series = {GPCE 2020}
}

@inproceedings{10.1145/3425898.3426954,
author = {Rouxel, Benjamin and Schultz, Ulrik Pagh and Akesson, Benny and Holst, Jesper and J\o{}rgensen, Ole and Grelck, Clemens},
title = {PReGO: a generative methodology for satisfying real-time requirements on COTS-based systems: definition and experience report},
year = {2020},
isbn = {9781450381741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425898.3426954},
doi = {10.1145/3425898.3426954},
abstract = {Satisfying real-time requirements in cyber-physical systems is challenging as timing behaviour depends on the application software, the embedded hardware, as well as the execution environment. This challenge is exacerbated as real-world, industrial systems often use unpredictable hardware and software libraries or operating systems with timing hazards and proprietary device drivers. All these issues limit or entirely prevent the application of established real-time analysis techniques.  In this paper we propose PReGO, a generative methodology for satisfying real-time requirements in industrial commercial-off-the-shelf (COTS) systems. We report on our experience in applying PReGO to a use-case: a Search \&amp; Rescue application running on a fixed-wing drone with COTS components, including an NVIDIA Jetson board and a stock Ubuntu/Linux. We empirically evaluate the impact of each integration step and demonstrate the effectiveness of our methodology in meeting real-time application requirements in terms of deadline misses and energy consumption.},
booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {70–83},
numpages = {14},
keywords = {Methodology, Real-Time Systems deployment, Use-case study},
location = {Virtual, USA},
series = {GPCE 2020}
}

@inproceedings{10.1145/3425898.3426961,
author = {Parreaux, Lionel and Shaikhha, Amir},
title = {Multi-stage programming in the large with staged classes},
year = {2020},
isbn = {9781450381741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425898.3426961},
doi = {10.1145/3425898.3426961},
abstract = {Multi-stage programming (MSP) holds great promise, allowing the reliable generation of specialized, partially-evaluated code with static type- and scope-safety guarantees. Yet, we argue that MSP has not reached its full potential yet, as it has been traditionally limited to generating expressions, and has lacked principled facilities for generating modular programs and data structures. In that sense, we argue that MSP has been reserved for programming “in the small,” focused on generating efficient kernels of computation on the scale of single function bodies. We present a novel technique called staged classes, which extends MSP with the ability to manipulate class definitions as first-class constructs. This lets programmers use MSP “in the large,” on the level of applications, rather than mere functions. This way, applications can be designed in an abstract and modular way without runtime cost, as staged classes guarantee the removal of all staging-time abstractions, resulting in the generation of efficient specialized modules and data structures. We describe the design of a prototype relational database system in Scala, which uses several stages of runtime compilation to maximize the efficiency of query execution and data storage. We also show that staged classes can be used for defining type- and scope-safe implementations of type providers.},
booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {35–49},
numpages = {15},
keywords = {Multi-stage programming, Squid, Staged classes},
location = {Virtual, USA},
series = {GPCE 2020}
}

@inproceedings{10.1145/3427228.3427230,
author = {Rosenberg, Ishai and Shabtai, Asaf and Elovici, Yuval and Rokach, Lior},
title = {Query-Efficient Black-Box Attack Against Sequence-Based Malware Classifiers},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427230},
doi = {10.1145/3427228.3427230},
abstract = {In this paper, we present a generic, query-efficient black-box attack against API call-based machine learning malware classifiers. We generate adversarial examples by modifying the malware’s API call sequences and non-sequential features (printable strings), and these adversarial examples will be misclassified by the target malware classifier without affecting the malware’s functionality. In contrast to previous studies, our attack minimizes the number of malware classifier queries required. In addition, in our attack, the attacker must only know the class predicted by the malware classifier; attacker knowledge of the malware classifier’s confidence score is optional. We evaluate the attack effectiveness when attacks are performed against a variety of malware classifier architectures, including recurrent neural network (RNN) variants, deep neural networks, support vector machines, and gradient boosted decision trees. Our attack success rate is around 98\% when the classifier’s confidence score is known and 64\% when just the classifier’s predicted class is known. We implement four state-of-the-art query-efficient attacks and show that our attack requires fewer queries and less knowledge about the attacked model’s architecture than other existing query-efficient attacks, making it practical for attacking cloud-based malware classifiers at a minimal cost.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {611–626},
numpages = {16},
keywords = {Adversarial Example, Decision-Based Attack, Machine Learning as a Service, Malware Classification, Recurrent Neural Networks, Score-Based Attack},
location = {Austin, USA},
series = {ACSAC '20}
}

@inproceedings{10.1145/3427228.3427285,
author = {Pu, Jiameng and Mangaokar, Neal and Wang, Bolun and Reddy, Chandan K and Viswanath, Bimal},
title = {NoiseScope: Detecting Deepfake Images in a Blind Setting},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427285},
doi = {10.1145/3427228.3427285},
abstract = {Recent advances in Generative Adversarial Networks (GANs) have significantly improved the quality of synthetic images or deepfakes. Photorealistic images generated by GANs start to challenge the boundary of human perception of reality, and brings new threats to many critical domains, e.g., journalism, and online media. Detecting whether an image is generated by GAN or a real camera has become an important yet under-investigated area. In this work, we propose a blind detection approach called NoiseScope for discovering GAN images among other real images. A blind approach requires no a priori access to GAN images for training, and demonstrably generalizes better than supervised detection schemes. Our key insight is that, similar to images from cameras, GAN images also carry unique patterns in the noise space. We extract such patterns in an unsupervised manner to identify GAN images. We evaluate NoiseScope on 11 diverse datasets containing GAN images, and achieve up to 99.68\% F1 score in detecting GAN images. We test the limitations of NoiseScope against a variety of countermeasures, observing that NoiseScope holds robust or is easily adaptable.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {913–927},
numpages = {15},
keywords = {Blind Detection, Clustering, Deepfakes, Machine Learning},
location = {Austin, USA},
series = {ACSAC '20}
}

@article{10.1145/3428194,
author = {Brachth\"{a}user, Jonathan Immanuel and Schuster, Philipp and Ostermann, Klaus},
title = {Effects as capabilities: effect handlers and lightweight effect polymorphism},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428194},
doi = {10.1145/3428194},
abstract = {Effect handlers have recently gained popularity amongst programming language researchers. Existing type- and effect systems for effect handlers are often complicated and potentially hinder a wide-spread adoption.  We present the language Effekt with the goal to close the gap between research languages with effect handlers and languages for working programmers. The design of Effekt revolves around a different view of effects and effect types. Traditionally, effect types express which side effects a computation might have. In Effekt, effect types express which capabilities a computation requires from its context. While this new point in the design space of effect systems impedes reasoning about purity, we demonstrate that it simplifies the treatment of effect polymorphism and the related issues of effect parametricity and effect encapsulation. To guarantee effect safety, we separate functions from values and treat all functions as second-class. We define the semantics of Effekt as a translation to System Xi, a calculus in explicit capability-passing style.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {126},
numpages = {30},
keywords = {algebraic effects, effect handlers, effect polymorphism}
}

@article{10.1145/3428205,
author = {Wang, Yu and Wang, Ke and Gao, Fengjuan and Wang, Linzhang},
title = {Learning semantic program embeddings with graph interval neural network},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428205},
doi = {10.1145/3428205},
abstract = {Learning distributed representations of source code has been a challenging task for machine learning models. Earlier works treated programs as text so that natural language methods can be readily applied. Unfortunately, such approaches do not capitalize on the rich structural information possessed by source code. Of late, Graph Neural Network (GNN) was proposed to learn embeddings of programs from their graph representations. Due to the homogeneous (i.e. do not take advantage of the program-specific graph characteristics) and expensive (i.e. require heavy information exchange among nodes in the graph) message-passing procedure, GNN can suffer from precision issues, especially when dealing with programs rendered into large graphs. In this paper, we present a new graph neural architecture, called Graph Interval Neural Network (GINN), to tackle the weaknesses of the existing GNN. Unlike the standard GNN, GINN generalizes from a curated graph representation obtained through an abstraction method designed to aid models to learn. In particular, GINN focuses exclusively on intervals (generally manifested in looping construct) for mining the feature representation of a program, furthermore, GINN operates on a hierarchy of intervals for scaling the learning to large graphs.  We evaluate GINN for two popular downstream applications: variable misuse prediction and method name prediction. Results show in both cases GINN outperforms the state-of-the-art models by a comfortable margin. We have also created a neural bug detector based on GINN to catch null pointer deference bugs in Java code. While learning from the same 9,000 methods extracted from 64 projects, GINN-based bug detector significantly outperforms GNN-based bug detector on 13 unseen test projects. Next, we deploy our trained GINN-based bug detector and Facebook Infer, arguably the state-of-the-art static analysis tool, to scan the codebase of 20 highly starred projects on GitHub. Through our manual inspection, we confirm 38 bugs out of 102 warnings raised by GINN-based bug detector compared to 34 bugs out of 129 warnings for Facebook Infer. We have reported 38 bugs GINN caught to developers, among which 11 have been fixed and 12 have been confirmed (fix pending). GINN has shown to be a general, powerful deep neural network for learning precise, semantic program embeddings.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {137},
numpages = {27},
keywords = {Control-flow graphs, Graph neural networks, Intervals, Null pointer dereference detection, Program embeddings}
}

@article{10.1145/3432939,
author = {Fedosov, Anton and Lampinen, Airi and Odom, William and Huang, Elaine M.},
title = {A Dozen Stickers on a Mailbox: Physical Encounters and Digital Interactions in a Local Sharing Community},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3432939},
doi = {10.1145/3432939},
abstract = {Many non-profit peer-to-peer exchange arrangements and profit-driven, multi-sided online marketplaces leverage underutilized resources, such as tools, to optimize their use to capacity. They often rely on a digital platform in pursuit of their social aspirations and/or economic objectives. We report on a field study of a local sharing community that employs a set of stickers illustrating different household items, typically placed on community members' mailboxes, along with complementary digital tools. The stickers are used to communicate the availability of resources among neighbors to facilitate social encounters and to encourage sustainable use and re-use of shared resources. Through in-depth qualitative interviews with sixteen participants, we describe the opportunities and limitations of this approach to peer-to-peer exchange. We offer insights for designers of resource sharing communities into facilitating face-to-face encounters and the online interactions needed to support them.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {240},
numpages = {23},
keywords = {community informatics, interaction design, peer-to-peer exchange, qualitative study, reciprocity, resource sharing, sharing economy, trust}
}

@inproceedings{10.1145/3433210.3453086,
author = {Lucas, Keane and Sharif, Mahmood and Bauer, Lujo and Reiter, Michael K. and Shintre, Saurabh},
title = {Malware Makeover: Breaking ML-based Static Analysis by Modifying Executable Bytes},
year = {2021},
isbn = {9781450382878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433210.3453086},
doi = {10.1145/3433210.3453086},
abstract = {Motivated by the transformative impact of deep neural networks (DNNs) in various domains, researchers and anti-virus vendors have proposed DNNs for malware detection from raw bytes that do not require manual feature engineering. In this work, we propose an attack that interweaves binary-diversification techniques and optimization frameworks to mislead such DNNs while preserving the functionality of binaries. Unlike prior attacks, ours manipulates instructions that are a functional part of the binary, which makes it particularly challenging to defend against. We evaluated our attack against three DNNs in white- and black-box settings, and found that it often achieved success rates near 100\%. Moreover, we found that our attack can fool some commercial anti-viruses, in certain cases with a success rate of 85\%. We explored several defenses, both new and old, and identified some that can foil over 80\% of our evasion attempts. However, these defenses may still be susceptible to evasion by attacks, and so we advocate for augmenting malware-detection systems with methods that do not rely on machine learning.},
booktitle = {Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security},
pages = {744–758},
numpages = {15},
keywords = {adversarial machine learning, malware, neural networks, security},
location = {Virtual Event, Hong Kong},
series = {ASIA CCS '21}
}

@inproceedings{10.1145/3433667.3433670,
author = {Park, Daniel and Yener, B\"{u}lent},
title = {A survey on practical adversarial examples for malware classifiers},
year = {2021},
isbn = {9781450389747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433667.3433670},
doi = {10.1145/3433667.3433670},
abstract = {Machine learning based solutions have been very helpful in solving problems that deal with immense amounts of data, such as malware detection and classification. However, deep neural networks have been found to be vulnerable to adversarial examples, or inputs that have been purposefully perturbed to result in an incorrect label. Researchers have shown that this vulnerability can be exploited to create evasive malware samples. However, many proposed attacks do not generate an executable and instead generate a feature vector. To fully understand the impact of adversarial examples on malware detection, we review practical attacks against malware classifiers that generate executable adversarial malware examples. We also discuss current challenges in this area of research, as well as suggestions for improvement and future research directions.},
booktitle = {Reversing and Offensive-Oriented Trends Symposium},
pages = {23–35},
numpages = {13},
keywords = {SoK, adversarial examples, machine learning, malware, survey},
location = {Vienna, Austria},
series = {ROOTS'20}
}

@inproceedings{10.1145/3437992.3439934,
author = {Annenkov, Danil and Milo, Mikkel and Nielsen, Jakob Botsch and Spitters, Bas},
title = {Extracting smart contracts tested and verified in Coq},
year = {2021},
isbn = {9781450382991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437992.3439934},
doi = {10.1145/3437992.3439934},
abstract = {We implement extraction of Coq programs to functional languages based on MetaCoq's certified erasure. As part of this, we implement an optimisation pass removing unused arguments. We prove the pass correct wrt. a conventional call-by-value operational semantics of functional languages. We apply this to two functional smart contract languages, Liquidity and Midlang, and to the functional language Elm.  Our development is done in the context of the ConCert framework that enables smart contract verification. We contribute a verified boardroom voting smart contract featuring maximum voter privacy such that each vote is kept private except under collusion of all other parties.  We also integrate property-based testing into ConCert using QuickChick and our development is the first to support testing properties of interacting smart contracts. We test several complex contracts such as a DAO-like contract, an escrow contract, an implementation of a Decentralized Finance (DeFi) contract which includes a custom token standard (Tezos FA2), and more.  In total, this gives us a way to write dependent programs in Coq, test them semi-automatically, verify, and then extract to functional smart contract languages, while retaining a small trusted computing base of only MetaCoq and the pretty-printers into these languages.},
booktitle = {Proceedings of the 10th ACM SIGPLAN International Conference on Certified Programs and Proofs},
pages = {105–121},
numpages = {17},
keywords = {Coq, blockchain, certified programming, code extraction, formal verification, proof assistants, property-based testing, smart contracts, software correctness},
location = {Virtual, Denmark},
series = {CPP 2021}
}

@inproceedings{10.1145/3441000.3441032,
author = {Larsen-Ledet, Ida and Borowski, Marcel},
title = {“It Looks Like You Don’t Agree”: Idiosyncratic Practices and Preferences in Collaborative Writing},
year = {2021},
isbn = {9781450389754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441000.3441032},
doi = {10.1145/3441000.3441032},
abstract = {This paper addresses collaborative writing in academia. Recent research has indicated that while many tools for collaborative writing exist and continue to be developed, co-writers frequently employ workarounds and cumbersome substitutions to accommodate their writing approaches and collaborative needs. As part of a process to address these issues, we conducted a co-design study on collaborative academic writing with 18 participants. The paper details a three-stage co-design approach developed for this purpose. During this three-stage workshop series, the participants discussed needs, frustrations, and desires in their experiences with collaborative writing. These discussions revealed how participants’ different ways of practicing and experiencing collaborative writing entail contrasting needs that are difficult to balance. Based on an analysis of discussions and artifacts from the workshops, we argue that researchers and designers should aim to support diverse practices and propose a protocol for examining and drawing on the contradictions that arise from co-writers’ idiosyncratic preferences.},
booktitle = {Proceedings of the 32nd Australian Conference on Human-Computer Interaction},
pages = {339–354},
numpages = {16},
keywords = {academic writing, co-design, collaborative writing, guidelines, individual needs, trade-offs},
location = {Sydney, NSW, Australia},
series = {OzCHI '20}
}

@inproceedings{10.1145/3445814.3446700,
author = {Gan, Yu and Liang, Mingyu and Dev, Sundar and Lo, David and Delimitrou, Christina},
title = {Sage: practical and scalable ML-driven performance debugging in microservices},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446700},
doi = {10.1145/3445814.3446700},
abstract = {Cloud applications are increasingly shifting from large monolithic services to complex graphs of loosely-coupled microservices. Despite the advantages of modularity and elasticity microservices offer, they also complicate cluster management and performance debugging, as dependencies between tiers introduce backpressure and cascading QoS violations. Prior work on performance debugging for cloud services either relies on empirical techniques, or uses supervised learning to diagnose the root causes of performance issues, which requires significant application instrumentation, and is difficult to deploy in practice.  We present Sage, a machine learning-driven root cause analysis system for interactive cloud microservices that focuses on practicality and scalability. Sage leverages unsupervised ML models to circumvent the overhead of trace labeling, captures the impact of dependencies between microservices to determine the root cause of unpredictable performance online, and applies corrective actions to recover a cloud service’s QoS. In experiments on both dedicated local clusters and large clusters on Google Compute Engine we show that Sage consistently achieves over 93\% accuracy in correctly identifying the root cause of QoS violations, and improves performance predictability.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {135–151},
numpages = {17},
keywords = {Bayesian network, QoS, cloud computing, counterfactual, microservices, performance debugging, variational autoencoder},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@article{10.1145/3445972,
author = {Groh, Matthew and Epstein, Ziv and Obradovich, Nick and Cebrian, Manuel and Rahwan, Iyad},
title = {Human detection of machine-manipulated media},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3445972},
doi = {10.1145/3445972},
abstract = {Technologies for manipulating and faking online media may outpace people's ability to tell the difference.},
journal = {Commun. ACM},
month = sep,
pages = {40–47},
numpages = {8}
}

@inproceedings{10.1145/3447526.3472064,
author = {Zang, Xiaoxue and Xu, Ying and Chen, Jindong},
title = {Multimodal Icon Annotation For Mobile Applications},
year = {2021},
isbn = {9781450383288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447526.3472064},
doi = {10.1145/3447526.3472064},
abstract = {Annotating user interfaces (UIs) that involves localization and classification of meaningful UI elements on a screen is a critical step for many mobile applications such as screen readers and voice control of devices. Annotating object icons, such as menu, search, and arrow backward, is especially challenging due to the lack of explicit labels on screens, their similarity to pictures, and their diverse shapes. Existing studies either use view hierarchy or pixel based methods to tackle the task. Pixel based approaches are more popular as view hierarchy features on mobile platforms are often incomplete or inaccurate, however it leaves out instructional information in the view hierarchy such as resource-ids or content descriptions. We propose a novel deep learning based multi-modal approach that combines the benefits of both pixel and view hierarchy features as well as leverages the state-of-the-art object detection techniques. In order to demonstrate the utility provided, we create a high quality UI dataset by manually annotating the most commonly used 29 icons in Rico, a large scale mobile design dataset consisting of 72k UI screenshots. The experimental results indicate the effectiveness of our multi-modal approach. Our model not only outperforms a widely used object classification baseline but also pixel based object detection models. Our study sheds light on how to combine view hierarchy with pixel features for annotating UI elements.},
booktitle = {Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction},
articleno = {8},
numpages = {11},
keywords = {Deep learning, Mobile interface, User interface understanding},
location = {Toulouse \&amp; Virtual, France},
series = {MobileHCI '21}
}

@article{10.1145/3447541,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh},
title = {TabReformer: Unsupervised Representation Learning for Erroneous Data Detection},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2691-1922},
url = {https://doi.org/10.1145/3447541},
doi = {10.1145/3447541},
abstract = {Error detection is a crucial preliminary phase in any data analytics pipeline. Existing error detection techniques typically target specific types of errors. Moreover, most of these detection models either require user-defined rules or ample hand-labeled training examples. Therefore, in this article, we present TabReformer, a model that learns bidirectional encoder representations for tabular data. The proposed model consists of two main phases. In the first phase, TabReformer follows encoder architecture with multiple self-attention layers to model the dependencies between cells and capture tuple-level representations. Also, the model utilizes a Gaussian Error Linear Unit activation function with the Masked Data Model objective to achieve deeper probabilistic understanding. In the second phase, the model parameters are fine-tuned for the task of erroneous data detection. The model applies a data augmentation module to generate more erroneous examples to represent the minority class. The experimental evaluation considers a wide range of databases with different types of errors and distributions. The empirical results show that our solution can enhance the recall values by 32.95\% on average compared with state-of-the-art techniques while reducing the manual effort by up to 48.86\%.},
journal = {ACM/IMS Trans. Data Sci.},
month = may,
articleno = {18},
numpages = {29},
keywords = {Error detection, transformers, bidirectional encoder, data augmentation}
}

@inproceedings{10.1145/3447548.3467198,
author = {Bai, Ching-Yuan and Lin, Hsuan-Tien and Raffel, Colin and Kan, Wendy Chi-wen},
title = {On Training Sample Memorization: Lessons from Benchmarking Generative Modeling with a Large-scale Competition},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467198},
doi = {10.1145/3447548.3467198},
abstract = {Many recent developments on generative models for natural images have relied on heuristically-motivated metrics that can be easily gamed by memorizing a small sample from the true distribution or training a model directly to improve the metric. In this work, we critically evaluate the gameability of these metrics by designing and deploying a generative modeling competition. Our competition received over 11000 submitted models. The competitiveness between participants allowed us to investigate both intentional and unintentional memorization in generative modeling. To detect intentional memorization, we propose the "Memorization-Informed Frechet Inception Distance" (MiFID) as a new memorization-aware metric and design benchmark procedures to ensure that winning submissions made genuine improvements in perceptual quality. Furthermore, we manually inspect the code for the 1000 top-performing models to understand and label different forms of memorization. Our analysis reveals that unintentional memorization is a serious and common issue in popular generative models. The generated images and our memorization labels of those models as well as code to compute MiFID are released to facilitate future studies on benchmarking generative models.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2534–2542},
numpages = {9},
keywords = {benchmark, competition, computer vision, datasets, generative models, memorization, neural networks},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3447548.3467265,
author = {Luss, Ronny and Chen, Pin-Yu and Dhurandhar, Amit and Sattigeri, Prasanna and Zhang, Yunfeng and Shanmugam, Karthikeyan and Tu, Chun-Chen},
title = {Leveraging Latent Features for Local Explanations},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467265},
doi = {10.1145/3447548.3467265},
abstract = {As the application of deep neural networks proliferates in numerous areas such as medical imaging, video surveillance, and self driving cars, the need for explaining the decisions of these models has become a hot research topic, both at the global and local level. Locally, most explanation methods have focused on identifying relevance of features, limiting the types of explanations possible. In this paper, we investigate a new direction by leveraging latent features to generate contrastive explanations; predictions are explained not only by highlighting aspects that are in themselves sufficient to justify the classification, but also by new aspects which if added will change the classification. The key contribution of this paper lies in how we add features to rich data in a formal yet humanly interpretable way that leads to meaningful results. Our new definition of "addition" uses latent features to move beyond the limitations of previous explanations and resolve an open question laid out in Dhurandhar, et. al. (2018), which creates local contrastive explanations but is limited to simple datasets such as grayscale images. The strength of our approach in creating intuitive explanations that are also quantitatively superior to other methods is demonstrated on three diverse image datasets (skin lesions, faces, and fashion apparel). A user study with 200 participants further exemplifies the benefits of contrastive information, which can be viewed as complementary to other state-of-the-art interpretability methods.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {1139–1149},
numpages = {11},
keywords = {deep learning, explainability},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3447548.3467402,
author = {Bagherian, Dawna and Gornet, James and Bernstein, Jeremy and Ni, Yu-Li and Yue, Yisong and Meister, Markus},
title = {Fine-Grained System Identification of Nonlinear Neural Circuits},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467402},
doi = {10.1145/3447548.3467402},
abstract = {We study the problem of sparse nonlinear model recovery of high dimensional compositional functions. Our study is motivated by emerging opportunities in neuroscience to recover fine-grained models of biological neural circuits using collected measurement data. Guided by available domain knowledge in neuroscience, we explore conditions under which one can recover the underlying biological circuit that generated the training data. Our results suggest insights of both theoretical and practical interests. Most notably, we find that a sign constraint on the weights is a necessary condition for system recovery, which we establish both theoretically with an identifiability guarantee and empirically on simulated biological circuits. We conclude with a case study on retinal ganglion cell circuits using data collected from mouse retina, showcasing the practical potential of this approach.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {14–24},
numpages = {11},
keywords = {neural networks, neuroscience, nonlinear system identification},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3447568.3448544,
author = {Badji, Aliou and Dieng, Youssou and Diop, Ibrahima and Cisse, Papa Alioune and Diouf, Boubacar},
title = {Automatic Speaker Recognition (ASR): Application in the monitoring of PLHIV in the cross-border area between the Gambia, Guinea-Bissau and Senegal},
year = {2021},
isbn = {9781450376556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447568.3448544},
doi = {10.1145/3447568.3448544},
abstract = {This article focuses on the study and optimization of the monitoring system for People Living with HIV (PLWHIV) in the cross-border area between the Gambia, Senegal and Guinea Bissau. Difficulties in adherence to treatment are accentuated, among other things, by the stigmatization of HIV patients and the high mobility of the population. These two situations and the absence of a reliable identification system for PLWHIV lead to restarting treatment for certain patients, which can cause resistance.This article presents the problem of automatic speaker recognition in the monitoring of PLWHA to avoid resumption of treatment and reduce the number of patients on ART lost to removing duplicates of patients in follow-up. From the technical point of view, we presented the problem of identification by speech. We proposed a state of the arts pretty full on art methodology s existing. We proposed a model with a first implementation allowing to make the first analyzes. We also presented a set of related works.},
booktitle = {Proceedings of the 10th International Conference on Information Systems and Technologies},
articleno = {36},
numpages = {9},
keywords = {ASR, HIV, Mel-Frequency Cepstral Coefficient (MFCC), PLWHA},
location = {Lecce, Italy},
series = {ICIST '20}
}

@article{10.1145/3447822,
author = {Steinbuss, Georg and B\"{o}hm, Klemens},
title = {Generating Artificial Outliers in the Absence of Genuine Ones — A Survey},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3447822},
doi = {10.1145/3447822},
abstract = {By definition, outliers are rarely observed in reality, making them difficult to detect or analyze. Artificial outliers approximate such genuine outliers and can, for instance, help with the detection of genuine outliers or with benchmarking outlier-detection algorithms. The literature features different approaches to generate artificial outliers. However, systematic comparison of these approaches remains absent. This surveys and compares these approaches. We start by clarifying the terminology in the field, which varies from publication to publication, and we propose a general problem formulation. Our description of the connection of generating outliers to other research fields like experimental design or generative models frames the field of artificial outliers. Along with offering a concise description, we group the approaches by their general concepts and how they make use of genuine instances. An extensive experimental study reveals the differences between the generation approaches when ultimately being used for outlier detection. This survey shows that the existing approaches already cover a wide range of concepts underlying the generation, but also that the field still has potential for further development. Our experimental study does confirm the expectation that the quality of the generation approaches varies widely, for example, in terms of the dataset they are used on. Ultimately, to guide the choice of the generation approach in a specific context, we propose an appropriate general-decision process. In summary, this survey comprises, describes, and connects all relevant work regarding the generation of artificial outliers and may serve as a basis to guide further research in the field.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {30},
numpages = {37},
keywords = {Artificial outlier, anomalies, artificial data, outlier detection}
}

@inproceedings{10.1145/3447993.3483275,
author = {Liu, Zikun and Singh, Gagandeep and Xu, Chenren and Vasisht, Deepak},
title = {FIRE: enabling reciprocity for FDD MIMO systems},
year = {2021},
isbn = {9781450383424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447993.3483275},
doi = {10.1145/3447993.3483275},
abstract = {Massive MIMO forms a crucial component for 5G because of its ability to improve quality of service and support multiple streams simultaneously. However, for real-world MIMO deployments, estimating the downlink wireless channel from each antenna on the base station to every client device is a critical bottleneck, especially for the widely used frequency duplexed designs that cannot utilize reciprocity. Typically, this channel estimation requires explicit feedback from client devices and is prohibitive for large antenna deployments. In this paper, we present FIRE, a system that uses an end-to-end machine learning approach to enable accurate channel estimation without requiring any feedback from client devices. FIRE is interpretable, accurate, and has low compute overhead. We show that FIRE can successfully support MIMO transmissions in a real-world testbed and achieves SNR improvement over 10 dB in MIMO transmissions compared to the current state-of-the-art.},
booktitle = {Proceedings of the 27th Annual International Conference on Mobile Computing and Networking},
pages = {628–641},
numpages = {14},
location = {New Orleans, Louisiana},
series = {MobiCom '21}
}

@inproceedings{10.1145/3448300.3467827,
author = {Wang, Han and Mu\~{n}oz-Gonz\'{a}lez, Luis and Eklund, David and Raza, Shahid},
title = {Non-IID data re-balancing at IoT edge with peer-to-peer federated learning for anomaly detection},
year = {2021},
isbn = {9781450383493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448300.3467827},
doi = {10.1145/3448300.3467827},
abstract = {The increase of the computational power in edge devices has enabled the penetration of distributed machine learning technologies such as federated learning, which allows to build collaborative models performing the training locally in the edge devices, improving the efficiency and the privacy for training of machine learning models, as the data remains in the edge devices. However, in some IoT networks the connectivity between devices and system components can be limited, which prevents the use of federated learning, as it requires a central node to orchestrate the training of the model. To sidestep this, peer-to-peer learning appears as a promising solution, as it does not require such an orchestrator. On the other side, the security challenges in IoT deployments have fostered the use of machine learning for attack and anomaly detection. In these problems, under supervised learning approaches, the training datasets are typically imbalanced, i.e. the number of anomalies is very small compared to the number of benign data points, which requires the use of re-balancing techniques to improve the algorithms' performance. In this paper, we propose a novel peer-to-peer algorithm,P2PK-SMOTE, to train supervised anomaly detection machine learning models in non-IID scenarios, including mechanisms to locally re-balance the training datasets via synthetic generation of data points from the minority class. To improve the performance in non-IID scenarios, we also include a mechanism for sharing a small fraction of synthetic data from the minority class across devices, aiming to reduce the risk of data de-identification. Our experimental evaluation in real datasets for IoT anomaly detection across a different set of scenarios validates the benefits of our proposed approach.},
booktitle = {Proceedings of the 14th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {153–163},
numpages = {11},
keywords = {anomaly detection, federated learning, imbalanced data, non-IID data},
location = {Abu Dhabi, United Arab Emirates},
series = {WiSec '21}
}

@article{10.1145/3449168,
author = {Jiang, Jialun Aaron and Wade, Kandrea and Fiesler, Casey and Brubaker, Jed R.},
title = {Supporting Serendipity: Opportunities and Challenges for Human-AI Collaboration in Qualitative Analysis},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449168},
doi = {10.1145/3449168},
abstract = {Qualitative inductive methods are widely used in CSCW and HCI research for their ability to generatively discover deep and contextualized insights, but these inherently manual and human-resource-intensive processes are often infeasible for analyzing large corpora. Researchers have been increasingly interested in ways to apply qualitative methods to "big" data problems, hoping to achieve more generalizable results from larger amounts of data while preserving the depth and richness of qualitative methods. In this paper, we describe a study of qualitative researchers' work practices and their challenges, with an eye towards whether this is an appropriate domain for human-AI collaboration and what successful collaborations might entail. Our findings characterize participants' diverse methodological practices and nuanced collaboration dynamics, and identify areas where they might benefit from AI-based tools. While participants highlight the messiness and uncertainty of qualitative inductive analysis, they still want full agency over the process and believe that AI should not interfere. Our study provides a deep investigation of task delegability in human-AI collaboration in the context of qualitative analysis, and offers directions for the design of AI assistance that honor serendipity, human agency, and ambiguity.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {94},
numpages = {23},
keywords = {ai, human-ai collaboration, interview, qualitative research}
}

@article{10.1145/3449240,
author = {Liu, Michael Xieyang and Kittur, Aniket and Myers, Brad A.},
title = {To Reuse or Not To Reuse? A Framework and System for Evaluating Summarized Knowledge},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449240},
doi = {10.1145/3449240},
abstract = {As the amount of information online continues to grow, a correspondingly important opportunity is for individuals to reuse knowledge which has been summarized by others rather than starting from scratch. However, appropriate reuse requires judging the relevance, trustworthiness, and thoroughness of others' knowledge in relation to an individual's goals and context. In this work, we explore augmenting judgements of the appropriateness of reusing knowledge in the domain of programming, specifically of reusing artifacts that result from other developers' searching and decision making. Through an analysis of prior research on sensemaking and trust, along with new interviews with developers, we synthesized a framework for reuse judgements. The interviews also validated that developers express a desire for help with judging whether to reuse an existing decision. From this framework, we developed a set of techniques for capturing the initial decision maker's behavior and visualizing signals calculated based on the behavior, to facilitate subsequent consumers' reuse decisions, instantiated in a prototype system called Strata. Results of a user study suggest that the system significantly improves the accuracy, depth, and speed of reusing decisions. These results have implications for systems involving user-generated content in which other users need to evaluate the relevance and trustworthiness of that content.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {166},
numpages = {35},
keywords = {decision making, developer tools, knowledge reuse, sensemaking}
}

@inproceedings{10.1145/3450508.3464564,
author = {Marshall, Carl S.},
title = {Practical machine learning for rendering: from research to deployment},
year = {2021},
isbn = {9781450383615},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450508.3464564},
doi = {10.1145/3450508.3464564},
abstract = {• Give insights into closing the gap between taking a research neural model to deployment• Understand the challenges in development, training, deployment, and iteration of neural networks for rendering• Show practical use cases, tools, and networks to start your path toward neural rendering in production software},
booktitle = {ACM SIGGRAPH 2021 Courses},
articleno = {10},
numpages = {239},
location = {Virtual Event, USA},
series = {SIGGRAPH '21}
}

@proceedings{10.1145/3450569,
title = {SACMAT '21: Proceedings of the 26th ACM Symposium on Access Control Models and Technologies},
year = {2021},
isbn = {9781450383653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the ACM Symposium on Access Control Models and Technologies (SACMAT 2021). This year's symposium continues its tradition of being the premier forum for the presentation of research results and experience reports on leading-edge issues of access control, including models, systems, applications, and theory, while also embracing a renovated focus on the general area of security.The aim of the symposium is to share novel access control and security solutions that fulfill the needs of heterogeneous applications and environments, and to identify new directions for future research and development.SACMAT provides researchers and practitioners with a unique opportunity to share their perspectives with others interested in the various aspects of access control and security.},
location = {Virtual Event, Spain}
}

@article{10.1145/3450626.3459762,
author = {Yaldiz, Mustafa B. and Meuleman, Andreas and Jang, Hyeonjoong and Ha, Hyunho and Kim, Min H.},
title = {DeepFormableTag: end-to-end generation and recognition of deformable fiducial markers},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3450626.3459762},
doi = {10.1145/3450626.3459762},
abstract = {Fiducial markers have been broadly used to identify objects or embed messages that can be detected by a camera. Primarily, existing detection methods assume that markers are printed on ideally planar surfaces. The size of a message or identification code is limited by the spatial resolution of binary patterns in a marker. Markers often fail to be recognized due to various imaging artifacts of optical/perspective distortion and motion blur. To overcome these limitations, we propose a novel deformable fiducial marker system that consists of three main parts: First, a fiducial marker generator creates a set of free-form color patterns to encode significantly large-scale information in unique visual codes. Second, a differentiable image simulator creates a training dataset of photorealistic scene images with the deformed markers, being rendered during optimization in a differentiable manner. The rendered images include realistic shading with specular reflection, optical distortion, defocus and motion blur, color alteration, imaging noise, and shape deformation of markers. Lastly, a trained marker detector seeks the regions of interest and recognizes multiple marker patterns simultaneously via inverse deformation transformation. The deformable marker creator and detector networks are jointly optimized via the differentiable photorealistic renderer in an end-to-end manner, allowing us to robustly recognize a wide range of deformable markers with high accuracy. Our deformable marker system is capable of decoding 36-bit messages successfully at ~29 fps with severe shape deformation. Results validate that our system significantly outperforms the traditional and data-driven marker methods. Our learning-based marker system opens up new interesting applications of fiducial markers, including cost-effective motion capture of the human body, active 3D scanning using our fiducial markers' array as structured light patterns, and robust augmented reality rendering of virtual objects on dynamic surfaces.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {67},
numpages = {14},
keywords = {deep learning, fiducial marker system, object detection, tracking}
}

@article{10.1145/3450626.3459954,
author = {H\"{a}drich, Torsten and Banuti, Daniel T. and Pa\l{}ubicki, Wojtek and Pirk, S\"{o}ren and Michels, Dominik L.},
title = {Fire in paradise: mesoscale simulation of wildfires},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3450626.3459954},
doi = {10.1145/3450626.3459954},
abstract = {Resulting from changing climatic conditions, wildfires have become an existential threat across various countries around the world. The complex dynamics paired with their often rapid progression renders wildfires an often disastrous natural phenomenon that is difficult to predict and to counteract. In this paper we present a novel method for simulating wildfires with the goal to realistically capture the combustion process of individual trees and the resulting propagation of fires at the scale of forests. We rely on a state-of-the-art modeling approach for large-scale ecosystems that enables us to represent each plant as a detailed 3D geometric model. We introduce a novel mathematical formulation for the combustion process of plants - also considering effects such as heat transfer, char insulation, and mass loss - as well as for the propagation of fire through the entire ecosystem. Compared to other wildfire simulations which employ geometric representations of plants such as cones or cylinders, our detailed 3D tree models enable us to simulate the interplay of geometric variations of branching structures and the dynamics of fire and wood combustion. Our simulation runs at interactive rates and thereby provides a convenient way to explore different conditions that affect wildfires, ranging from terrain elevation profiles and ecosystem compositions to various measures against wildfires, such as cutting down trees as firebreaks, the application of fire retardant, or the simulation of rain.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {163},
numpages = {15},
keywords = {combustion, fire, fluid dynamics, level of detail, numerical simulation, physics-based modeling, wildfires}
}

@book{10.1145/3453538,
author = {ACM Data Science Task Force},
title = {Computing competencies for undergraduate data science curricula},
year = {2021},
isbn = {9781450390606},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@article{10.1145/3457142,
author = {Karolus, Jakob and Kiss, Francisco and Eckerth, Caroline and Viot, Nicolas and Bachmann, Felix and Schmidt, Albrecht and Wozniak, Pawel W.},
title = {EMBody: A Data-Centric Toolkit for EMG-Based Interface Prototyping and Experimentation},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {EICS},
url = {https://doi.org/10.1145/3457142},
doi = {10.1145/3457142},
abstract = {Body movements, from a short smile to a marathon run, are driven by muscle activity. Despite the fact that measuring muscle activity with electromyography (EMG) is technically well established, it is highly complex and its use in interfaces has been limited. Easy access to muscle sensing can offer new opportunities to Human-Computer Interaction (HCI) research. Off-the-shelf sensors often only provide low-level access, hence requiring expertise in signal processing and widening the gulf of execution for users without engineering skills. To address this challenge, we introduce EMBody, a data-centric toolkit for EMG-based interface prototyping and experimentation. EMBody offers multiple levels of prototyping fidelity for EMG sensing, signal processing, and data interpretation. Our data-centric toolkit encapsulates the different data representation stages, offering a wide range of customization opportunities to experts while also allowing non-technical designers to focus on creating new interaction techniques. EMBody features a lightweight form factor and wireless connectivity. Additionally, the system leverages an exploration-centered workflow by allowing rapid access to measurement data via the accompanying software. Users define a set of motions to be recognized and interactively provide example data points. The toolkit then handles signal processing and classification. The recognized movements are streamed on the local network, ready to be used by interactive applications. This paper reports on how to use EMBody and its implementation. We iteratively developed the toolkit in a series of workshops and example applications. Users who had none or very limited knowledge of EMG could rapidly create engaging functional prototypes, while experts appreciated the modularity of the software component allowing for a high degree of customization. We contribute the software and hardware components of EMBody as a tool for the research community to stimulate creative exploration of EMG systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {195},
numpages = {29},
keywords = {electromyography, embodied interaction, prototyping, toolkit}
}

@inproceedings{10.1145/3459637.3482379,
author = {Zhou, Yujia and Dou, Zhicheng and Zhu, Yutao and Wen, Ji-Rong},
title = {PSSL: Self-supervised Learning for Personalized Search with Contrastive Sampling},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482379},
doi = {10.1145/3459637.3482379},
abstract = {Personalized search plays a crucial role in improving user search experience owing to its ability to build user profiles based on historical behaviors. Previous studies have made great progress in extracting personal signals from the query log and learning user representations. However, neural personalized search is extremely dependent on sufficient data to train the user model. Data sparsity is an inevitable challenge for existing methods to learn high-quality user representations. Moreover, the overemphasis on final ranking quality leads to rough data representations and impairs the generalizability of the model. To tackle these issues, we propose a Personalized Search framework with Self-supervised Learning (PSSL) to enhance data representations. Specifically, we adopt a contrastive sampling method to extract paired self-supervised information from sequences of user behaviors in query logs. Four auxiliary tasks are designed to pre-train the sentence encoder and the sequence encoder used in the ranking model. They are optimized by contrastive loss which aims to close the distance between similar user sequences, queries, and documents. Experimental results on two datasets demonstrate that our proposed model PSSL achieves state-of-the-art performance compared with existing baselines.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {2749–2758},
numpages = {10},
keywords = {contrastive learning, personalized search, self-supervised learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3459637.3482380,
author = {Gowda, Sindhu C. M. and Joshi, Shalmali and Zhang, Haoran and Ghassemi, Marzyeh},
title = {Pulling Up by the Causal Bootstraps: Causal Data Augmentation for Pre-training Debiasing},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482380},
doi = {10.1145/3459637.3482380},
abstract = {Machine learning models achieve state-of-the-art performance on many supervised learning tasks. However, prior evidence suggests that these models may learn to rely on "shortcut" biases or spurious correlations (intuitively, correlations that do not hold in the test as they hold in train) for good predictive performance. Such models cannot be trusted in deployment environments to provide accurate predictions. While viewing the problem from a causal lens is known to be useful, the seamless integration of causation techniques into machine learning pipelines remains cumbersome and expensive. In this work, we study and extend a causal pre-training debiasing technique called causal bootstrapping (CB) under five practical confounded-data generation-acquisition scenarios (with known and unknown confounding). Under these settings, we systematically investigate the effect of confounding bias on deep learning model performance, demonstrating their propensity to rely on shortcut biases when these biases are not properly accounted for. We demonstrate that such a causal pre-training technique can significantly outperform existing base practices to mitigate confounding bias on real-world domain generalization benchmarking tasks. This systematic investigation underlines the importance of accounting for the underlying data-generating mechanisms and fortifying data-preprocessing pipelines with a causal framework to develop methods robust to confounding biases.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {606–616},
numpages = {11},
keywords = {causal graphs, confounding bias, debiasing, pre-training, re-sampling},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3459990.3460694,
author = {Tseng, Tiffany and Murai, Yumiko and Freed, Natalie and Gelosi, Deanna and Ta, Tung D. and Kawahara, Yoshihiro},
title = {PlushPal: Storytelling with Interactive Plush Toys and Machine Learning},
year = {2021},
isbn = {9781450384520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459990.3460694},
doi = {10.1145/3459990.3460694},
abstract = {This paper presents PlushPal, a web-based design tool for children to make plush toys interactive with machine learning (ML). With PlushPal, children attach micro:bit hardware to stuffed animals, design custom gestures for their toy, and build gesture-recognition ML models to trigger their own sounds. We describe how, in the context of storytelling, PlushPal introduces core concepts in ML including data sampling and model evaluation. We conducted online workshops and in-person play sessions with 11 children between 8-14 years old building interactive stuffed animals with PlushPal. In these play sessions, we observed how children imagined bringing their toys to life using ML, as well as how children’s data literacy changed as a result of experimenting with sensors, data sampling, and building their own ML models. Our work contributes a novel design space for children to express their ideas using gesture, as well as a description of observed debugging practices, building on efforts to support children using ML to enhance creative play.},
booktitle = {Proceedings of the 20th Annual ACM Interaction Design and Children Conference},
pages = {236–245},
numpages = {10},
keywords = {children, machine learning, physical computing, play},
location = {Athens, Greece},
series = {IDC '21}
}

@inproceedings{10.1145/3460120.3484777,
author = {Bahramali, Alireza and Nasr, Milad and Houmansadr, Amir and Goeckel, Dennis and Towsley, Don},
title = {Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3484777},
doi = {10.1145/3460120.3484777},
abstract = {There is significant enthusiasm for the employment of Deep Neural Networks (DNNs) for important tasks in major wireless communication systems: channel estimation and decoding in orthogonal frequency division multiplexing (OFDM) systems, end-to-end autoencoder system design, radio signal classification, and signal authentication. Unfortunately, DNNs can be susceptible to adversarial examples, potentially making such wireless systems fragile and vulnerable to attack. In this work, by designing robust adversarial examples that meet key criteria, we perform a comprehensive study of the threats facing DNN-based wireless systems. We model the problem of adversarial wireless perturbations as an optimization problem that incorporates domain constraints specific to different wireless systems. This allows us to generate wireless adversarial perturbations that can be applied to wireless signals on-the-fly (i.e., with no need to know the target signals a priori), are undetectable from natural wireless noise, and are robust against removal. We show that even in the presence of significant defense mechanisms deployed by the communicating parties, our attack performs significantly better compared to existing attacks against DNN-based wireless systems. In particular, the results demonstrate that even when employing well-considered defenses, DNN-based wireless communication systems are vulnerable to adversarial attacks and call into question the employment of DNNs for a number of tasks in robust wireless communication.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {126–140},
numpages = {15},
keywords = {adversarial examples, deep neural networks, universal perturbations, wireless communication systems},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/3460120.3485259,
author = {Pasquini, Dario and Ateniese, Giuseppe and Bernaschi, Massimo},
title = {Unleashing the Tiger: Inference Attacks on Split Learning},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3485259},
doi = {10.1145/3460120.3485259},
abstract = {We investigate the security of split learning---a novel collaborative machine learning framework that enables peak performance by requiring minimal resource consumption. In the present paper, we expose vulnerabilities of the protocol and demonstrate its inherent insecurity by introducing general attack strategies targeting the reconstruction of clients' private training sets. More prominently, we show that a malicious server can actively hijack the learning process of the distributed model and bring it into an insecure state that enables inference attacks on clients' data. We implement different adaptations of the attack and test them on various datasets as well as within realistic threat scenarios. We demonstrate that our attack can overcome recently proposed defensive techniques aimed at enhancing the security of the split learning protocol. Finally, we also illustrate the protocol's insecurity against malicious clients by extending previously devised attacks for Federated Learning.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2113–2129},
numpages = {17},
keywords = {ML security, collaborative learning, deep learning},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/3460319.3464801,
author = {Dunn, Isaac and Pouget, Hadrien and Kroening, Daniel and Melham, Tom},
title = {Exposing previously undetectable faults in deep neural networks},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464801},
doi = {10.1145/3460319.3464801},
abstract = {Existing methods for testing DNNs solve the oracle problem by constraining the raw features (e.g. image pixel values) to be within a small distance of a dataset example for which the desired DNN output is known. But this limits the kinds of faults these approaches are able to detect. In this paper, we introduce a novel DNN testing method that is able to find faults in DNNs that other methods cannot. The crux is that, by leveraging generative machine learning, we can generate fresh test inputs that vary in their high-level features (for images, these include object shape, location, texture, and colour). We demonstrate that our approach is capable of detecting deliberately injected faults as well as new faults in state-of-the-art DNNs, and that in both cases, existing methods are unable to find these faults.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {56–66},
numpages = {11},
keywords = {Deep Learning, Generative Adversarial Networks, Robustness},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3461702.3462626,
author = {Slavkovik, Marija and Stachl, Clemens and Pitman, Caroline and Askonas, Jonathan},
title = {Digital Voodoo Dolls},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462626},
doi = {10.1145/3461702.3462626},
abstract = {An institution, be it a body of government, commercial enterprise, or a service, cannot interact directly with a person. Instead, a model is created to represent us. We argue the existence of a new high-fidelity type of person model which we call a digital voodoo doll. We conceptualize it and compare its features with existing models of persons. Digital voodoo dolls are distinguished by existing completely beyond the influence and control of the person they represent. We discuss the ethical issues that such a lack of accountability creates and argue how these concerns can be mitigated.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {967–977},
numpages = {11},
keywords = {impact of AI on society, power and inequality, rights and representation, value alignment},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{10.1145/3461778.3462021,
author = {Dagan, Ella and Isbister, Katherine},
title = {Synergistic Social Technology: Designing Systems with ‘Needs’ that Encourage and Support Social Interaction},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462021},
doi = {10.1145/3461778.3462021},
abstract = {In this paper, we propose a strong concept for interaction design: Synergistic Social Technology (SST). This concept describes systems in which technology is designed with its own ‘need’ for interaction. As a result of responding to these needs, people who use the system may benefit from social interaction with others who use the system. This concept arose through the design, prototyping, and study of a social wearable design that we called Robo-Shoe-Flies. We articulate the core principles of the SST concept. We also describe the Research-through-Design process that inspired its development and associated design-focused observations. This work may inspire those in the IxD or HCI communities focused on the design and development of technology intended to support social interaction, particularly in the sense of encouraging collective, mutually-beneficial action.},
booktitle = {Proceedings of the 2021 ACM Designing Interactive Systems Conference},
pages = {1419–1432},
numpages = {14},
keywords = {Research through Design, co-located, social, social wearables, strong concept, synergistic},
location = {Virtual Event, USA},
series = {DIS '21}
}

@inproceedings{10.1145/3461778.3462065,
author = {Bennett, Cynthia and Ackerman, Emily and Fan, Bonnie and Bigham, Jeffrey and Carrington, Patrick and Fox, Sarah},
title = {Accessibility and The Crowded Sidewalk: Micromobility's Impact on Public Space},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462065},
doi = {10.1145/3461778.3462065},
abstract = {Over the past several years, micromobility devices—small-scale, networked vehicles used to travel short distances—have begun to pervade cities, bringing promises of sustainable transportation and decreased congestion. Though proponents herald their role in offering lightweight solutions to disconnected transit, smart scooters and autonomous delivery robots increasingly occupy pedestrian pathways, reanimating tensions around the right to public space. Drawing on interviews with disabled activists, government officials, and commercial representatives, we chart how devices and policies co-evolve to fulfill municipal sustainability goals, while creating obstacles for people with disabilities whose activism has long resisted inaccessible infrastructure. We reflect on efforts to redistribute space, institute tech governance, and offer accountability to those who involuntarily encounter interventions on the ground. In studying micromobility within spatial and political context, we call for the HCI community to consider how innovation transforms as it moves out from centers of development toward peripheries of design consideration.},
booktitle = {Proceedings of the 2021 ACM Designing Interactive Systems Conference},
pages = {365–380},
numpages = {16},
keywords = {Accessibility, Activism, Governance, Micromobility, Public space},
location = {Virtual Event, USA},
series = {DIS '21}
}

@inproceedings{10.1145/3462172.3462199,
author = {Vie\ss{}mann, Hans-Nikolai and Scholz, Sven-Bodo},
title = {Effective Host-GPU Memory Management Through Code Generation},
year = {2021},
isbn = {9781450389631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462172.3462199},
doi = {10.1145/3462172.3462199},
abstract = {NVIDIA’s CUDA provides several options to orchestrate the management of host and device memory as well as the communication between them. In this paper we look at these choices, identify the program changes required when switching between them, and we observe their effect on application performance. We present code generation schemes that translate resource-agnostic program specifications, i.e., programs without any explicit notion of memory or GPU kernels, into five CUDA versions that differ in the use of the memory and communication API of CUDA only. An implementation of these code generators within the compiler of the functional programming language Single-Assignment C (SaC) shows performance differences between the variants by up to a factor of 3. Performance analyses reveal that the preferred choices depend on a combination of several factors, including the actual hardware being used, and several aspects of the application itself. A clear choice, therefore, cannot be made a priori. Instead, it seems essential that different variants can be generated from a single source for achieving performance portability across GPU devices.},
booktitle = {Proceedings of the 32nd Symposium on Implementation and Application of Functional Languages},
pages = {138–149},
numpages = {12},
keywords = {CUDA, GPU, SaC, code generation, communication models, memory management, transfer bandwidth},
location = {Canterbury, United Kingdom},
series = {IFL '20}
}

@inproceedings{10.1145/3462244.3479914,
author = {Wang, Siyang and Alexanderson, Simon and Gustafson, Joakim and Beskow, Jonas and Henter, Gustav Eje and Sz\'{e}kely, \'{E}va},
title = {Integrated Speech and Gesture Synthesis},
year = {2021},
isbn = {9781450384810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462244.3479914},
doi = {10.1145/3462244.3479914},
abstract = {Text-to-speech and co-speech gesture synthesis have until now been treated as separate areas by two different research communities, and applications merely stack the two technologies using a simple system-level pipeline. This can lead to modeling inefficiencies and may introduce inconsistencies that limit the achievable naturalness. We propose to instead synthesize the two modalities in a single model, a new problem we call integrated speech and gesture synthesis (ISG). We also propose a set of models modified from state-of-the-art neural speech-synthesis engines to achieve this goal. We evaluate the models in three carefully-designed user studies, two of which evaluate the synthesized speech and gesture in isolation, plus a combined study that evaluates the models like they will be used in real-world applications – speech and gesture presented together. The results show that participants rate one of the proposed integrated synthesis models as being as good as the state-of-the-art pipeline system we compare against, in all three tests. The model is able to achieve this with faster synthesis time and greatly reduced parameter count compared to the pipeline system, illustrating some of the potential benefits of treating speech and gesture synthesis together as a single, unified problem.},
booktitle = {Proceedings of the 2021 International Conference on Multimodal Interaction},
pages = {177–185},
numpages = {9},
keywords = {gesture generation, neural networks, speech synthesis},
location = {Montr\'{e}al, QC, Canada},
series = {ICMI '21}
}

@inproceedings{10.1145/3465481.3465766,
author = {Arenas, Monica and Demirci, Huseyin and Lenzini, Gabriele},
title = {Cholesteric Spherical Reflectors as Physical Unclonable Identifiers in Anti-counterfeiting},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3465766},
doi = {10.1145/3465481.3465766},
abstract = {Cholesteric Spherical Reflectors (CSRs) are made of droplets of cholesteric liquid crystals (the same material under the screen of our mobile phones) but molded in a spherical shape and hardened into a solid. CSRs have a peculiar behavior when illuminated: they reflect light and produce unique optical patterns whose full display is hardly predictable. They have been argued to behave like an optical Physical Unclonable Function (PUF), therefore finding application in anti-counterfeiting, in particular for object authentication. However, a fundamental challenge remains open: to understand what makes each optical response unique and how to extract this identifying information reliably and repeatedly. We study the problem, and we design and discuss two pivotal procedures to build authentication protocols for objects coated with CSRs. We test the quality of our procedures against large data sets of pattern images: images from CSRs are used to calculate inter- and intra-distance; simulated patterns created artificially are used to measure security in terms of false positive ratio. Our procedures successfully cluster images coming from the same CSR, distinguishing them from images of different CSRs and decoys. Our work is one of the few that has studied procedures of information extraction for materials derived from CSRs. It advances the state of the art in this area, closing the gap between the research on optical PUFs and practical applications.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {2},
numpages = {11},
keywords = {Physical unclonable function, anti-counterfeiting Cholesteric Spherical Reflectors, authentication},
location = {Vienna, Austria},
series = {ARES '21}
}

@proceedings{10.1145/3468013,
title = {APCORISE '21: Proceedings of the 4th Asia Pacific Conference on Research in Industrial and Systems Engineering},
year = {2021},
isbn = {9781450390385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Depok, Indonesia}
}

@inproceedings{10.1145/3468264.3468588,
author = {Roy, Devjeet and Fakhoury, Sarah and Arnaoudova, Venera},
title = {Reassessing automatic evaluation metrics for code summarization tasks},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468588},
doi = {10.1145/3468264.3468588},
abstract = {In recent years, research in the domain of source code summarization has adopted data-driven techniques pioneered in machine translation (MT). Automatic evaluation metrics such as BLEU, METEOR, and ROUGE, are fundamental to the evaluation of MT systems and have been adopted as proxies of human evaluation in the code summarization domain. However, the extent to which automatic metrics agree with the gold standard of human evaluation has not been evaluated on code summarization tasks. Despite this, marginal improvements in metric scores are often used to discriminate between the performance of competing summarization models. In this paper, we present a critical exploration of the applicability and interpretation of automatic metrics as evaluation techniques for code summarization tasks. We conduct an empirical study with 226 human annotators to assess the degree to which automatic metrics reflect human evaluation. Results indicate that metric improvements of less than 2 points do not guarantee systematic improvements in summarization quality, and are unreliable as proxies of human evaluation. When the difference between metric scores for two summarization approaches increases but remains within 5 points, some metrics such as METEOR and chrF become highly reliable proxies, whereas others, such as corpus BLEU, remain unreliable. Based on these findings, we make several recommendations for the use of automatic metrics to discriminate model performance in code summarization.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1105–1116},
numpages = {12},
keywords = {automatic evaluation metrics, code summarization, machine translation},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1145/3469440,
author = {Gheibi, Omid and Weyns, Danny and Quin, Federico},
title = {Applying Machine Learning in Self-adaptive Systems: A Systematic Literature Review},
year = {2021},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3469440},
doi = {10.1145/3469440},
abstract = {Recently, we have been witnessing a rapid increase in the use of machine learning techniques in self-adaptive systems. Machine learning has been used for a variety of reasons, ranging from learning a model of the environment of a system during operation to filtering large sets of possible configurations before analyzing them. While a body of work on the use of machine learning in self-adaptive systems exists, there is currently no systematic overview of this area. Such an overview is important for researchers to understand the state of the art and direct future research efforts. This article reports the results of a systematic literature review that aims at providing such an overview. We focus on self-adaptive systems that are based on a traditional Monitor-Analyze-Plan-Execute (MAPE)-based feedback loop. The research questions are centered on the problems that motivate the use of machine learning in self-adaptive systems, the key engineering aspects of learning in self-adaptation, and open challenges in this area. The search resulted in 6,709 papers, of which 109 were retained for data collection. Analysis of the collected data shows that machine learning is mostly used for updating adaptation rules and policies to improve system qualities, and managing resources to better balance qualities and resources. These problems are primarily solved using supervised and interactive learning with classification, regression, and reinforcement learning as the dominant methods. Surprisingly, unsupervised learning that naturally fits automation is only applied in a small number of studies. Key open challenges in this area include the performance of learning, managing the effects of learning, and dealing with more complex types of goals. From the insights derived from this systematic literature review, we outline an initial design process for applying machine learning in self-adaptive systems that are based on MAPE feedback loops.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = aug,
articleno = {9},
numpages = {37},
keywords = {MAPE-K, Self-adaptation, feedback loops}
}

@inproceedings{10.1145/3472538.3472540,
author = {L\"{o}we, Mathias and Villareale, Jennifer and Freed, Evan and Sladek, Aleksanteri and Zhu, Jichen and Risi, Sebastian},
title = {Dealing with Adversarial Player Strategies in the Neural Network Game iNNk through Ensemble Learning},
year = {2021},
isbn = {9781450384223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472538.3472540},
doi = {10.1145/3472538.3472540},
abstract = {Applying neural network (NN) methods in games can lead to various new and exciting game dynamics not previously possible. However, they also lead to new challenges such as the lack of large, clean datasets, varying player skill levels, and changing gameplay strategies. In this paper, we focus on the adversarial player strategy aspect in the game iNNk, in which players try to communicate secret code words through drawings with the goal of not being deciphered by a NN. Some strategies exploit weaknesses in the NN that consistently trick it into making incorrect classifications, leading to unbalanced gameplay. We present a method that combines transfer learning and ensemble methods to obtain a data-efficient adaptation to these strategies. This combination significantly outperforms the baseline NN across all adversarial player strategies despite only being trained on a limited set of adversarial examples. We expect the methods developed in this paper to be useful for the rapidly growing field of NN-based games, which will require new approaches to deal with unforeseen player creativity.},
booktitle = {Proceedings of the 16th International Conference on the Foundations of Digital Games},
articleno = {1},
numpages = {10},
keywords = {adversarial attacks, ensemble methods, games, neural networks, transfer learning},
location = {Montreal, QC, Canada},
series = {FDG '21}
}

@inproceedings{10.1145/3472538.3472575,
author = {Johansen, Mads and Pichlmair, Martin and Risi, Sebastian},
title = {Squeezer - A Mixed-Initiative Tool for Designing Juice Effects},
year = {2021},
isbn = {9781450384223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472538.3472575},
doi = {10.1145/3472538.3472575},
abstract = {This paper presents a Mixed-Initiative version of Squeezer, a tool for designing juice effects in the Unity game engine. Drawing upon sound synthesizers and game description languages, Squeezer can synthesize common types of juice effects by combining simple building blocks into sequences. Additionally, Squeezer offers effect generation based on predefined recipes as well as an interface for interactively evolving effect sequences. We conducted a user study with five experts to verify the functionality and interest among game designers. By applying generative and evolutionary strategies to juice effect design, Squeezer allows game designers and researchers using games in their work to explore adding juice effects to their games and frameworks. Squeezer is available at: https://github.com/pyjamads/Squeezer0},
booktitle = {Proceedings of the 16th International Conference on the Foundations of Digital Games},
articleno = {37},
numpages = {11},
keywords = {Game Design, Game Development, Generator, Interaction Feedback, Interactive Evolution, Juice Effects, Mixed-Initiative, Prototyping, Toolkit},
location = {Montreal, QC, Canada},
series = {FDG '21}
}

@inproceedings{10.1145/3472538.3472576,
author = {Mawhorter, Peter},
title = {Fractal Coordinates for Incremental Procedural Content Generation},
year = {2021},
isbn = {9781450384223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472538.3472576},
doi = {10.1145/3472538.3472576},
abstract = {Incremental procedural content generation (IPCG) has been used in games such as Minecraft to provide a unique flavor of gameplay, but requires that parts of the world can be generated independently of one another in any order and will always fit together in the end. Noise functions such as simplex noise are a very popular building block for IPCG systems, because they support this property, but noise functions alone have their limitations, one of which being an inability to create non-local structures or continuity. To combat this, noise functions are usually applied at several scales to provide fractal continuity and recognizable features similar to real world geography’s fractal complexity. This paper describes a system of fractal coordinates suitable for use with IPCG that generalize this idea of multiple layers of structure at different scales, and demonstrates how it can be used to achieve some interesting effects.},
booktitle = {Proceedings of the 16th International Conference on the Foundations of Digital Games},
articleno = {35},
numpages = {10},
keywords = {coordinate systems, fractals, incremental procedural content generation, procedural content generation},
location = {Montreal, QC, Canada},
series = {FDG '21}
}

@inproceedings{10.1145/3472538.3472587,
author = {Maurer, Thomas and Guzdial, Matthew},
title = {Adversarial Random Forest Classifier for Automated Game Design},
year = {2021},
isbn = {9781450384223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472538.3472587},
doi = {10.1145/3472538.3472587},
abstract = {Autonomous game design, generating games algorithmically, has been a longtime goal within the technical games research field. However, existing autonomous game design systems have relied in large part on human-authoring for game design knowledge, such as fitness functions in search-based methods. In this paper, we describe an experiment to attempt to learn a human-like fitness function for autonomous game design in an adversarial manner. While our experimental work did not meet our expectations, we present an analysis of our system and results that we hope will be informative to future autonomous game design research.},
booktitle = {Proceedings of the 16th International Conference on the Foundations of Digital Games},
articleno = {48},
numpages = {6},
keywords = {autonomous game design, random forests, search-based procedural content generation},
location = {Montreal, QC, Canada},
series = {FDG '21}
}

@inproceedings{10.1145/3472749.3474758,
author = {Zhu, Junyi and Snowden, Jackson C and Verdejo, Joshua and Chen, Emily and Zhang, Paul and Ghaednia, Hamid and Schwab, Joseph H and Mueller, Stefanie},
title = {EIT-kit: An Electrical Impedance Tomography Toolkit for Health and Motion Sensing},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474758},
doi = {10.1145/3472749.3474758},
abstract = {In this paper, we propose EIT-kit, an electrical impedance tomography toolkit for designing and fabricating health and motion sensing devices. EIT-kit contains (1)&nbsp;an extension to a 3D editor for personalizing the form factor of electrode arrays and electrode distribution, (2)&nbsp;a customized EIT sensing motherboard for performing the measurements, (3)&nbsp;a microcontroller library that automates signal calibration and facilitates data collection, and (4)&nbsp;an image reconstruction library for mobile devices for interpolating and visualizing the measured data. Together, these EIT-kit components allow for applications that require 2- or 4-terminal setups, up to 64 electrodes, and single or multiple (up to four) electrode arrays&nbsp;simultaneously. We motivate the design of each component of EIT-kit with a formative study, and conduct a technical evaluation of the data fidelity of our EIT measurements. We demonstrate the design space that EIT-kit enables by showing various applications in health as well as motion sensing and control.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {400–413},
numpages = {14},
keywords = {electrical impedance tomography, electronic prototyping, health sensing, personal fabrication.},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474791,
author = {Luo, Zhongjin and Zhou, Jie and Zhu, Heming and Du, Dong and Han, Xiaoguang and Fu, Hongbo},
title = {SimpModeling: Sketching Implicit Field to Guide Mesh Modeling for 3D Animalmorphic Head Design},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474791},
doi = {10.1145/3472749.3474791},
abstract = {Head shapes play an important role in 3D character design. In this work, we propose SimpModeling, a novel sketch-based system for helping users, especially amateur users, easily model 3D animalmorphic heads - a prevalent kind of heads in character design. Although sketching provides an easy way to depict desired shapes, it is challenging to infer dense geometric information from sparse line drawings. Recently, deepnet-based approaches have been taken to address this challenge and try to produce rich geometric details from very few strokes. However, while such methods reduce users’ workload, they would cause less controllability of target shapes. This is mainly due to the uncertainty of the neural prediction. Our system tackles this issue and provides good controllability from three aspects: 1) we separate coarse shape design and geometric detail specification into two stages and respectively provide different sketching means; 2) in coarse shape designing, sketches are used for both shape inference and geometric constraints to determine global geometry, and in geometric detail crafting, sketches are used for carving surface details; 3) in both stages, we use the advanced implicit-based shape inference methods, which have strong ability to handle the domain gap between freehand sketches and synthetic ones used for training. Experimental results confirm the effectiveness of our method and the usability of our interactive system. We also contribute to a dataset of high-quality 3D animal heads, which are manually created by artists.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {854–863},
numpages = {10},
keywords = {3D modeling interface, datasets, implicit fields, neural networks},
location = {Virtual Event, USA},
series = {UIST '21}
}

@article{10.1145/3473337,
author = {Pan, Yaoxin and Liang, Shangsong and Ren, Jiaxin and Meng, Zaiqiao and Zhang, Qiang},
title = {Personalized, Sequential, Attentive, Metric-Aware Product Search},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3473337},
doi = {10.1145/3473337},
abstract = {The task of personalized product search aims at retrieving a ranked list of products given a user’s input query and his/her purchase history. To address this task, we propose the PSAM model, a Personalized, Sequential, Attentive and Metric-aware (PSAM) model, that learns the semantic representations of three different categories of entities, i.e., users, queries, and products, based on user sequential purchase historical data and the corresponding sequential queries. Specifically, a query-based attentive LSTM (QA-LSTM) model and an attention mechanism are designed to infer users dynamic embeddings, which is able to capture their short-term and long-term preferences. To obtain more fine-grained embeddings of the three categories of entities, a metric-aware objective is deployed in our model to force the inferred embeddings subject to the triangle inequality, which is a more realistic distance measurement for product search. Experiments conducted on four benchmark datasets show that our PSAM model significantly outperforms the state-of-the-art product search baselines in terms of effectiveness by up to 50.9\% improvement under NDCG@20. Our visualization experiments further illustrate that the learned product embeddings are able to distinguish different types of products.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {36},
numpages = {29},
keywords = {Product search, personalized web search, neural networks, LSTM, metric learning}
}

@inproceedings{10.1145/3474085.3475225,
author = {Wen, Qi and Li, Shuang and Han, Bingfeng and Yuan, Yi},
title = {ZiGAN: Fine-grained Chinese Calligraphy Font Generation via a Few-shot Style Transfer Approach},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475225},
doi = {10.1145/3474085.3475225},
abstract = {Chinese character style transfer is a very challenging problem because of the complexity of the glyph shapes or underlying structures and large numbers of existed characters, when comparing with English letters. Moreover, the handwriting of calligraphy masters has a more irregular stroke and is difficult to obtain in real-world scenarios. Recently, several GAN-based methods have been proposed for font synthesis, but some of them require numerous reference data and the other part of them have cumbersome preprocessing steps to divide the character into different parts to be learned and transferred separately. In this paper, we propose a simple but powerful end-to-end Chinese calligraphy font generation framework ZiGAN, which does not require any manual operation or redundant preprocessing to generate fine-grained target style characters with few-shot references. To be specific, a few paired samples from different character styles are leveraged to attain fine-grained correlation between structures underlying different glyphs. To capture valuable style knowledge in target and strengthen the coarse-grained understanding of character content, we utilize multiple unpaired samples to align the feature distributions belonging to different character styles. By doing so, only a few target Chinese calligraphy characters are needed to generated expected style transferred characters. Experiments demonstrate that our method has a state-of-the-art generalization ability in few-shot Chinese character style transfer.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {621–629},
numpages = {9},
keywords = {GANs, font generation, image-to-image translation},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3474085.3475520,
author = {Ying, Qichao and Qian, Zhenxing and Zhou, Hang and Xu, Haisheng and Zhang, Xinpeng and Li, Siyi},
title = {From Image to Imuge: Immunized Image Generation},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475520},
doi = {10.1145/3474085.3475520},
abstract = {We introduce Imuge, an image tamper resilient generative scheme for image self-recovery. The traditional manner of concealing image content within the image are inflexible and fragile to diverse digital attack, i.e. image cropping and JPEG compression. To address this issue, we jointly train a U-Net backboned encoder, a tamper localization network and a decoder for image recovery. Given an original image, the encoder produces a visually indistinguishable immunized image. At the recipient's side, the verifying network localizes the malicious modifications, and the original content can be approximately recovered by the decoder, despite the presence of the attacks. Several strategies are proposed to boost the training efficiency. We demonstrate that our method can recover the details of the tampered regions with a high quality despite the presence of various kinds of attacks. Comprehensive ablation studies are conducted to validate our network designs.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {3565–3573},
numpages = {9},
keywords = {image restoration, image trust, tamper localization},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3474085.3475688,
author = {Khan, Md Fahim Faysal and Troncoso Aldas, Nelson Daniel and Kumar, Abhishek and Advani, Siddharth and Narayanan, Vijaykrishnan},
title = {Sparse to Dense Depth Completion using a Generative Adversarial Network with Intelligent Sampling Strategies},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475688},
doi = {10.1145/3474085.3475688},
abstract = {Predicting dense depth accurately is essential for 3D scene understanding applications such as autonomous driving and robotics. However, the depth obtained from commercially available LiDAR and Time-of-Flight sensors is very sparse. With RGB color guidance, modern convolutional neural network (CNN) based approaches can recover the missing depth information. However, there could be scenarios such as low-light environments where it might be difficult to get an associated RGB image with the sparse depth. In this work, we propose a Generative Adversarial Network (GAN) that can accurately predict the dense depth using only sparse samples without any RGB inputs. Generally, the sparsity in the depth samples is uniformly distributed and cannot guarantee capturing all intricate details. In this study, we also explore different variants of sparse sampling strategies from uniform to feature based directed sampling. We find that feature based intelligent sampling enjoys better compression ratio without sacrificing intricate details, saving data communication bandwidth. Compared to uniform sampling, depending on how aggressively the directed sampling is done, we observe about 3\% to 25\% reduction in size. We can easily reduce the size by 8\% with directed sampling without sacrificing the reconstruction accuracy. Although such directed sampling strategies are not readily available with commercially viable depth sensors, we believe that our study paves the way for future intelligent sensing and sampling strategies. To further investigate data reduction and reconstruction accuracy trade-offs we deploy our GAN to generate higher resolution dense depth from 4 times smaller sparse samples. With slight decrease in accuracy, our GAN is able to recover the depth successfully which shows great promise in edge Internet of Things (IoT) applications where we have very tight constraint on data transmission bandwidth. Our source code along with examples is available at: https://github.com/kocchop/depth-completion-gan},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {5528–5536},
numpages = {9},
keywords = {depth completion, gan, image compression, sensor sampling},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3474717.3483651,
author = {Bastani, Favyen and He, Songtao and Jagwani, Satvat and Alizadeh, Mohammad and Balakrishnan, Hari and Chawla, Sanjay and Madden, Sam and Sadeghi, Mohammad Amin},
title = {Updating Street Maps using Changes Detected in Satellite Imagery},
year = {2021},
isbn = {9781450386647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474717.3483651},
doi = {10.1145/3474717.3483651},
abstract = {Accurately maintaining digital street maps is labor-intensive. To address this challenge, much work has studied automatically processing geospatial data sources such as GPS trajectories and satellite images to reduce the cost of maintaining digital maps. An end-to-end map update system would first process geospatial data sources to extract insights, and second leverage those insights to update and improve the map. However, prior work largely focuses on the first step of this pipeline: these map extraction methods infer road networks from scratch given geospatial data sources (in effect creating entirely new maps), but do not address the second step of leveraging this extracted information to update the existing map data. In this paper, we first explain why current map extraction techniques yield low accuracy when extended to update existing maps. We then propose a novel method that leverages the progression of satellite imagery over time to substantially improve accuracy. Our approach first compares satellite images captured at different times to identify portions of the physical road network that have visibly changed, and then updates the existing map accordingly. We show that our change-based approach reduces error rates four-fold.},
booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
pages = {53–56},
numpages = {4},
keywords = {automatic map update, machine learning},
location = {Beijing, China},
series = {SIGSPATIAL '21}
}

@article{10.1145/3476058,
author = {Scheuerman, Morgan Klaus and Hanna, Alex and Denton, Emily},
title = {Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476058},
doi = {10.1145/3476058},
abstract = {Data is a crucial component of machine learning. The field is reliant on data to train, validate, and test models. With increased technical capabilities, machine learning research has boomed in both academic and industry settings, and one major focus has been on computer vision. Computer vision is a popular domain of machine learning increasingly pertinent to real-world applications, from facial recognition in policing to object detection for autonomous vehicles. Given computer vision's propensity to shape machine learning research and impact human life, we seek to understand disciplinary practices around dataset documentation - how data is collected, curated, annotated, and packaged into datasets for computer vision researchers and practitioners to use for model tuning and development. Specifically, we examine what dataset documentation communicates about the underlying values of vision data and the larger practices and goals of computer vision as a field. To conduct this study, we collected a corpus of about 500 computer vision datasets, from which we sampled 114 dataset publications across different vision tasks. Through both a structured and thematic content analysis, we document a number of values around accepted data practices, what makes desirable data, and the treatment of humans in the dataset construction process. We discuss how computer vision datasets authors value efficiency at the expense of care; universality at the expense of contextuality; impartiality at the expense of positionality; and model work at the expense of data work. Many of the silenced values we identify sit in opposition with social computing practices. We conclude with suggestions on how to better incorporate silenced values into the dataset creation and curation process.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {317},
numpages = {37},
keywords = {computer vision, datasets, machine learning, values in design, work practice}
}

@article{10.1145/3476059,
author = {Vaccaro, Kristen and Xiao, Ziang and Hamilton, Kevin and Karahalios, Karrie},
title = {Contestability For Content Moderation},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476059},
doi = {10.1145/3476059},
abstract = {Content moderation systems for social media have had numerous issues of bias, in terms of race, gender, and ability among many others. One proposal for addressing such issues in automated decision making is by designing for contestability, whereby users can shape and influence how decisions are made. In this study, we conduct a series of participatory design workshops with participants from communities that have experienced problems with social media content moderation in the past. Together with participants, we explore the idea of designing for contestability in content moderation and find that users' designs suggest three fruitful, practical avenues: adding representation, improving communication, and designing with compassion. We conclude with design recommendations drawn from participants' proposals, and reflect on the challenges that remain.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {318},
numpages = {28},
keywords = {algorithmic experience, content moderation, contestability, participatory design}
}

@book{10.1145/3477322,
editor = {Lugrin, Birgit and Pelachaud, Catherine and Traum, David},
title = {The Handbook on Socially Interactive Agents: 20 years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 1: Methods, Behavior, Cognition},
year = {2021},
isbn = {9781450387200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {37},
abstract = {The Handbook on Socially Interactive Agents provides a comprehensive overview of the research fields of Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics. Socially Interactive Agents (SIAs), whether virtually or physically embodied, are autonomous agents that are able to perceive an environment including people or other agents, reason, decide how to interact, and express attitudes such as emotions, engagement, or empathy. They are capable of interacting with people and one another in a socially intelligent manner using multimodal communicative behaviors, with the goal to support humans in various domains.Written by international experts in their respective fields, the book summarizes research in the many important research communities pertinent for SIAs, while discussing current challenges and future directions. The handbook provides easy access to modeling and studying SIAs for researchers and students, and aims at further bridging the gap between the research communities involved.In two volumes, the book clearly structures the vast body of research. The first volume starts by introducing what is involved in SIAs research, in particular research methodologies and ethical implications of developing SIAs. It further examines research on appearance and behavior, focusing on multimodality. Finally, social cognition for SIAs is investigated using different theoretical models and phenomena such as theory of mind or pro-sociality. The second volume starts with perspectives on interaction, examined from different angles such as interaction in social space, group interaction, or long-term interaction. It also includes an extensive overview summarizing research and systems of human–agent platforms and of some of the major application areas of SIAs such as education, aging support, autism, and games.}
}

@inproceedings{10.1145/3477495.3531944,
author = {Chatterjee, Shubham and Dietz, Laura},
title = {BERT-ER: Query-specific BERT Entity Representations for Entity Ranking},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531944},
doi = {10.1145/3477495.3531944},
abstract = {Entity-oriented search systems often learn vector representations of entities via the introductory paragraph from the Wikipedia page of the entity. As such representations are the same for every query, our hypothesis is that the representations are not ideal for IR tasks. In this work, we present BERT Entity Representations (BERT-ER) which are query-specific vector representations of entities obtained from text that describes how an entity is relevant for a query. Using BERT-ER in a downstream entity ranking system, we achieve a performance improvement of 13-42\% (Mean Average Precision) over a system that uses the BERT embedding of the introductory paragraph from Wikipedia on two large-scale test collections. Our approach also outperforms entity ranking systems using entity embeddings from Wikipedia2Vec, ERNIE, and E-BERT. We show that our entity ranking system using BERT-ER can increase precision at the top of the ranking by promoting relevant entities to the top. With this work, we release our BERT models and query-specific entity embeddings fine-tuned for the entity ranking task.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1466–1477},
numpages = {12},
keywords = {bert, entity ranking, query-specific entity representations},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3477495.3531994,
author = {Zhuang, Shengyao and Li, Hang and Zuccon, Guido},
title = {Implicit Feedback for Dense Passage Retrieval: A Counterfactual Approach},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531994},
doi = {10.1145/3477495.3531994},
abstract = {In this paper we study how to effectively exploit implicit feedback in Dense Retrievers (DRs). We consider the specific case in which click data from a historic click log is available as implicit feedback. We then exploit such historic implicit interactions to improve the effectiveness of a DR. A key challenge that we study is the effect that biases in the click signal, such as position bias, have on the DRs. To overcome the problems associated with the presence of such bias, we propose the Counterfactual Rocchio (CoRocchio) algorithm for exploiting implicit feedback in Dense Retrievers. We demonstrate both theoretically and empirically that dense query representations learnt with CoRocchio are unbiased with respect to position bias and lead to higher retrieval effectiveness. We make available the implementations of the proposed methods and the experimental framework, along with all results at https://github.com/ielab/Counterfactual-DR.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {18–28},
numpages = {11},
keywords = {counterfactual learning, dense passage retrieval, implicit feedback},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{10.1145/3478513.3480509,
author = {Chandran, Prashanth and Winberg, Sebastian and Zoss, Gaspard and Riviere, J\'{e}r\'{e}my and Gross, Markus and Gotardo, Paulo and Bradley, Derek},
title = {Rendering with style: combining traditional and neural approaches for high-quality face rendering},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3478513.3480509},
doi = {10.1145/3478513.3480509},
abstract = {For several decades, researchers have been advancing techniques for creating and rendering 3D digital faces, where a lot of the effort has gone into geometry and appearance capture, modeling and rendering techniques. This body of research work has largely focused on facial skin, with much less attention devoted to peripheral components like hair, eyes and the interior of the mouth. As a result, even with the best technology for facial capture and rendering, in most high-end productions a lot of artist time is still spent modeling the missing components and fine-tuning the rendering parameters to combine everything into photo-real digital renders. In this work we propose to combine incomplete, high-quality renderings showing only facial skin with recent methods for neural rendering of faces, in order to automatically and seamlessly create photo-realistic full-head portrait renders from captured data without the need for artist intervention. Our method begins with traditional face rendering, where the skin is rendered with the desired appearance, expression, viewpoint, and illumination. These skin renders are then projected into the latent space of a pre-trained neural network that can generate arbitrary photo-real face images (StyleGAN2). The result is a sequence of realistic face images that match the identity and appearance of the 3D character at the skin level, but is completed naturally with synthesized hair, eyes, inner mouth and surroundings. Notably, we present the first method for multi-frame consistent projection into this latent space, allowing photo-realistic rendering and preservation of the identity of the digital human over an animated performance sequence, which can depict different expressions, lighting conditions and viewpoints. Our method can be used in new face rendering pipelines and, importantly, in other deep learning applications that require large amounts of realistic training data with ground-truth 3D geometry, appearance maps, lighting, and viewpoint.},
journal = {ACM Trans. Graph.},
month = dec,
articleno = {223},
numpages = {14},
keywords = {GAN inversion, face rendering, neural rendering}
}

@article{10.1145/3478513.3480521,
author = {Yu, Chris and Brakensiek, Caleb and Schumacher, Henrik and Crane, Keenan},
title = {Repulsive surfaces},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3478513.3480521},
doi = {10.1145/3478513.3480521},
abstract = {Functionals that penalize bending or stretching of a surface play a key role in geometric and scientific computing, but to date have ignored a very basic requirement: in many situations, surfaces must not pass through themselves or each other. This paper develops a numerical framework for optimization of surface geometry while avoiding (self-)collision. The starting point is the tangent-point energy, which effectively pushes apart pairs of points that are close in space but distant along the surface. We develop a discretization of this energy for triangle meshes, and introduce a novel acceleration scheme based on a fractional Sobolev inner product. In contrast to similar schemes developed for curves, we avoid the complexity of building a multiresolution mesh hierarchy by decomposing our preconditioner into two ordinary Poisson equations, plus forward application of a fractional differential operator. We further accelerate this scheme via hierarchical approximation, and describe how to incorporate a variety of constraints (on area, volume, etc.). Finally, we explore how this machinery might be applied to problems in mathematical visualization, geometric modeling, and geometry processing.},
journal = {ACM Trans. Graph.},
month = dec,
articleno = {268},
numpages = {19},
keywords = {computational design, shape optimization, surfaces}
}

@article{10.1145/3478513.3480545,
author = {Xiang, Donglai and Prada, Fabian and Bagautdinov, Timur and Xu, Weipeng and Dong, Yuan and Wen, He and Hodgins, Jessica and Wu, Chenglei},
title = {Modeling clothing as a separate layer for an animatable human avatar},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3478513.3480545},
doi = {10.1145/3478513.3480545},
abstract = {We have recently seen great progress in building photorealistic animatable full-body codec avatars, but generating high-fidelity animation of clothing is still difficult. To address these difficulties, we propose a method to build an animatable clothed body avatar with an explicit representation of the clothing on the upper body from multi-view captured videos. We use a two-layer mesh representation to register each 3D scan separately with the body and clothing templates. In order to improve the photometric correspondence across different frames, texture alignment is then performed through inverse rendering of the clothing geometry and texture predicted by a variational autoencoder. We then train a new two-layer codec avatar with separate modeling of the upper clothing and the inner body layer. To learn the interaction between the body dynamics and clothing states, we use a temporal convolution network to predict the clothing latent code based on a sequence of input skeletal poses. We show photorealistic animation output for three different actors, and demonstrate the advantage of our clothed-body avatars over the single-layer avatars used in previous work. We also show the benefit of an explicit clothing model that allows the clothing texture to be edited in the animation output.},
journal = {ACM Trans. Graph.},
month = dec,
articleno = {199},
numpages = {15},
keywords = {clothing animation, codec avatar}
}

@article{10.1145/3478513.3480570,
author = {Valle-P\'{e}rez, Guillermo and Henter, Gustav Eje and Beskow, Jonas and Holzapfel, Andre and Oudeyer, Pierre-Yves and Alexanderson, Simon},
title = {Transflower: probabilistic autoregressive dance generation with multimodal attention},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3478513.3480570},
doi = {10.1145/3478513.3480570},
abstract = {Dance requires skillful composition of complex movements that follow rhythmic, tonal and timbral features of music. Formally, generating dance conditioned on a piece of music can be expressed as a problem of modelling a high-dimensional continuous motion signal, conditioned on an audio signal. In this work we make two contributions to tackle this problem. First, we present a novel probabilistic autoregressive architecture that models the distribution over future poses with a normalizing flow conditioned on previous poses as well as music context, using a multimodal transformer encoder. Second, we introduce the currently largest 3D dance-motion dataset, obtained with a variety of motion-capture technologies, and including both professional and casual dancers. Using this dataset, we compare our new model against two baselines, via objective metrics and a user study, and show that both the ability to model a probability distribution, as well as being able to attend over a large motion and music context are necessary to produce interesting, diverse, and realistic dance that matches the music.},
journal = {ACM Trans. Graph.},
month = dec,
articleno = {195},
numpages = {14},
keywords = {dance, generative models, glow, machine learning, normalising flows, transformers}
}

@article{10.1145/3479561,
author = {Smith, C. Estelle and Lane, William and Miller Hillberg, Hannah and Kluver, Daniel and Terveen, Loren and Yarosh, Svetlana},
title = {Effective Strategies for Crowd-Powered Cognitive Reappraisal Systems: A Field Deployment of the Flip*Doubt Web Application for Mental Health},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479561},
doi = {10.1145/3479561},
abstract = {Online technologies offer great promise to expand models of delivery for therapeutic interventions to help users cope with increasingly common mental illnesses like anxiety and depression. For example, "cognitive reappraisal" is a skill that involves changing one's perspective on negative thoughts in order to improve one's emotional state. In this work, we present Flip*Doubt, a novel crowd-powered web application that provides users with cognitive reappraisals ("reframes") of negative thoughts. A one-month field deployment of Flip*Doubt with 13 graduate students yielded a data set of negative thoughts paired with positive reframes, as well as rich interview data about how participants interacted with the system. Through this deployment, our work contributes: (1) an in-depth qualitative understanding of how participants used a crowd-powered cognitive reappraisal system in the wild; and (2) detailed codebooks that capture informative context about negative input thoughts and reframes. Our results surface data-derived hypotheses that may help to explain what types of reframes are helpful for users, while also providing guidance to future researchers and developers interested in building collaborative systems for mental health. In our discussion, we outline implications for systems research to leverage peer training and support, as well as opportunities to integrate AI/ML-based algorithms to support the cognitive reappraisal task. (Note: This paper includes potentially triggering mentions of mental health issues and suicide.)},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {417},
numpages = {37},
keywords = {amazon mechanical turk, cognitive reappraisal, crowdsourcing, human-centered machine learning, mental health, online health communities, peer support, social support}
}

@inproceedings{10.1145/3485441.3485650,
author = {Black, Alexander and Bui, Tu and Jenni, Simon and Swaminathan, Viswanathan (Vishy) and Collomosse, John},
title = {VPN: Video Provenance Network for Robust Content Attribution},
year = {2021},
isbn = {9781450390941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485441.3485650},
doi = {10.1145/3485441.3485650},
abstract = {We present VPN - a content attribution method for recovering provenance information from videos shared online. Platforms, and users, often transform video into different quality, codecs, sizes, shapes, etc. or slightly edit its content such as adding text or emoji, as they are redistributed online. We learn a robust search embedding for matching such video, invariant to these transformations, using full-length or truncated video queries. Once matched against a trusted database of video clips, associated information on the provenance of the clip is presented to the user. We use an inverted index to match temporal chunks of video using late-fusion to combine both visual and audio features. In both cases, features are extracted via a deep neural network trained using contrastive learning on a dataset of original and augmented video clips. We demonstrate high accuracy recall over a corpus of 100,000 videos.},
booktitle = {Proceedings of the 18th ACM SIGGRAPH European Conference on Visual Media Production},
articleno = {4},
numpages = {10},
keywords = {Video retrieval, attribution, content authenticity., media provenance},
location = {London, United Kingdom},
series = {CVMP '21}
}

@inproceedings{10.1145/3485447.3511945,
author = {Liu, Xiao and Hong, Haoyun and Wang, Xinghao and Chen, Zeyi and Kharlamov, Evgeny and Dong, Yuxiao and Tang, Jie},
title = {SelfKG: Self-Supervised Entity Alignment in Knowledge Graphs},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511945},
doi = {10.1145/3485447.3511945},
abstract = {Entity alignment, aiming to identify equivalent entities across different knowledge graphs (KGs), is a fundamental problem for constructing Web-scale KGs. Over the course of its development, the label supervision has been considered necessary for accurate alignments. Inspired by the recent progress of self-supervised learning, we explore the extent to which we can get rid of supervision for entity alignment. Commonly, the label information (positive entity pairs) is used to supervise the process of pulling the aligned entities in each positive pair closer. However, our theoretical analysis suggests that the learning of entity alignment can actually benefit more from pushing unlabeled negative pairs far away from each other than pulling labeled positive pairs close. By leveraging this discovery, we develop the self-supervised learning objective for entity alignment. We present SelfKG with efficient strategies to optimize this objective for aligning entities without label supervision. Extensive experiments on benchmark datasets demonstrate that SelfKG &nbsp;without supervision can match or achieve comparable results with state-of-the-art supervised baselines. The performance of SelfKG suggests that self-supervised learning offers great potential for entity alignment in KGs. The code and data are available at https://github.com/THUDM/SelfKG.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {860–870},
numpages = {11},
keywords = {Entity Alignment, Knowledge Graphs, Self-Supervised Learning},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3485447.3512004,
author = {Cho, JinUk and Jeong, MinSu and Bak, JinYeong and Cheong, Yun-Gyung},
title = {Genre-Controllable Story Generation via Supervised Contrastive Learning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512004},
doi = {10.1145/3485447.3512004},
abstract = {While controllable text generation has received attention due to the recent advances in large-scale pre-trained language models, there is a lack of research that focuses on story-specific controllability. To address this, we present Story Control via Supervised Contrastive learning model (SCSC), to create a story conditioned on genre. For this, we design a supervised contrastive objective combined with log-likelihood objective, to capture the intrinsic differences among the stories in different genres. The results of our automated evaluation and user study demonstrate that the proposed method is effective in genre-controlled story generation.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2839–2849},
numpages = {11},
keywords = {automated story generation, contrastive learning, controllable text generation, natural language generation},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3485447.3512084,
author = {Gourru, Antoine and Velcin, Julien and Gravier, Christophe and Jacques, Julien},
title = {Dynamic Gaussian Embedding of Authors},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512084},
doi = {10.1145/3485447.3512084},
abstract = {Authors publish documents in a dynamic manner. Their topic of interest and writing style might shift over time. Tasks such as author classification, author identification or link prediction are difficult to solve in such complex data settings. We propose a new representation learning model, DGEA (for Dynamic Gaussian Embedding of Authors), that is more suited to solve these tasks by capturing this temporal evolution. We formulate a general embedding framework: author representation at time t is a Gaussian distribution that leverages pre-trained document vectors, and that depends on the publications observed until t. The representations should retain some form of multi-topic information and temporal smoothness. We propose two models that fit into this framework. The first one, K-DGEA, uses a first order Markov model optimized with an Expectation Maximization Algorithm with Kalman Equations. The second, R-DGEA, makes use of a Recurrent Neural Network to model the time dependence. We evaluate our method on several quantitative tasks: author identification, classification, and co-authorship prediction, on two datasets written in English. In addition, our model is language agnostic since it only requires pre-trained document embeddings. It outperforms existing baselines by up to 18\% on an author classification task on a news articles dataset.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2109–2119},
numpages = {11},
keywords = {Author Embedding, Document Embedding, Dynamic Gaussian Embedding, Representation Learning},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3485447.3512285,
author = {Wang, Lingzhi and Li, Jing and Zeng, Xingshan and Wong, Kam-Fai},
title = {Successful New-entry Prediction for Multi-Party Online Conversations via Latent Topics and Discourse Modeling},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512285},
doi = {10.1145/3485447.3512285},
abstract = {With the increasing popularity of social media, online interpersonal communication now plays an essential role in people’s everyday information exchange. Whether and how a newcomer can better engage in the community has attracted great interest due to its application in many scenarios. Although some prior works that explore early socialization have obtained salient achievements, they are focusing on sociological surveys based on the small group. To help individuals get through the early socialization period and engage well in online conversations, we study a novel task to foresee whether a newcomer’s message will be responded to by other participants in a multi-party conversation (henceforth Successful New-entry Prediction)1. The task would be an important part of the research in online assistants and social media. To further investigate the key factors indicating such engagement success, we employ an unsupervised neural network, Variational Auto-Encoder (VAE), to examine the topic content and discourse behavior from newcomer’s chatting history and conversation’s ongoing context. Furthermore, two large-scale datasets, from Reddit and Twitter, are collected to support further research on new-entries. Extensive experiments on both Twitter and Reddit datasets show that our model significantly outperforms all the baselines and popular neural models. Additional explainable and visual analyses on new-entry behavior shed light on how to better join in others’ discussions.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {1663–1672},
numpages = {10},
keywords = {latent variable learning, multi-party conversation, newcomer socialization, response prediction},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1145/3485527,
author = {Pelenitsyn, Artem and Belyakova, Julia and Chung, Benjamin and Tate, Ross and Vitek, Jan},
title = {Type stability in Julia: avoiding performance pathologies in JIT compilation},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {OOPSLA},
url = {https://doi.org/10.1145/3485527},
doi = {10.1145/3485527},
abstract = {As a scientific programming language, Julia strives for performance but also provides high-level productivity features. To avoid performance pathologies, Julia users are expected to adhere to a coding discipline that enables so-called type stability. Informally, a function is type stable if the type of the output depends only on the types of the inputs, not their values. This paper provides a formal definition of type stability as well as a stronger property of type groundedness, shows that groundedness enables compiler optimizations, and proves the compiler correct. We also perform a corpus analysis to uncover how these type-related properties manifest in practice.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {150},
numpages = {26},
keywords = {compilation, dynamic languages, method dispatch, type inference}
}

@inproceedings{10.1145/3485832.3485838,
author = {Hu, Hailong and Pang, Jun},
title = {Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3485838},
doi = {10.1145/3485832.3485838},
abstract = {Model extraction attacks aim to duplicate a machine learning model through query access to a target model. Early studies mainly focus on discriminative models. Despite the success, model extraction attacks against generative models are less well explored. In this paper, we systematically study the feasibility of model extraction attacks against generative adversarial networks&nbsp;(GANs). Specifically, we first define fidelity and accuracy on model extraction attacks against GANs. Then we study model extraction attacks against GANs from the perspective of fidelity extraction and accuracy extraction, according to the adversary’s goals and background knowledge. We further conduct a case study where the adversary can transfer knowledge of the extracted model which steals a state-of-the-art GAN trained with more than 3 million images to new domains to broaden the scope of applications of model extraction attacks. Finally, we propose effective defense techniques to safeguard GANs, considering a trade-off between the utility and security of GAN models.},
booktitle = {Proceedings of the 37th Annual Computer Security Applications Conference},
pages = {1–16},
numpages = {16},
keywords = {Generative adversarial networks, Model extraction, Perturbation-based defenses, Transfer learning},
location = {Virtual Event, USA},
series = {ACSAC '21}
}

@inproceedings{10.1145/3486609.3487196,
author = {Bragan\c{c}a, Alexandre and Azevedo, Isabel and Bettencourt, Nuno and Morais, Carlos and Teixeira, Diogo and Caetano, David},
title = {Towards supporting SPL engineering in low-code platforms using a DSL approach},
year = {2021},
isbn = {9781450391122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486609.3487196},
doi = {10.1145/3486609.3487196},
abstract = {Low-code application platforms enable citizen developers to autonomously build complete applications, such as web applications or mobile applications. Some of these platforms also offer support for reuse to facilitate the development of similar applications. The offered mechanisms are usually elementary, they allow module reuse or building a new application from a template. However, they are insufficient to achieve the industrial level reuse necessary for software product lines (SPL). In fact, these platforms were conceived to help build standalone applications, not software families and even fewer software product lines. In this paper, we argue that the major limitation is that these platforms seldom provide access to their metamodel, the access to applications’ models and code is also limited and, therefore, makes it harder to analyze commonality and variability and construct models based on it. An approach is proposed to surpass these limitations: firstly, a metamodel of the applications built with the platform is obtained, and then, based on the metamodel, a domain-specific language (DSL) that can express the models of the applications, including variability, is constructed. With this DSL, users can combine and reuse models from different applications to explore and build similar applications. The solution is illustrated with an industrial case study. A discussion of the results is presented as well as its limitations and related work. The authors hope that this work provides inspiration and some ideas that the community can explore to facilitate the adoption and implementation of SPLs in the context, and supported by, low-code platforms.},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {16–28},
numpages = {13},
keywords = {domain specific languages, low-code platforms, software product line engineering},
location = {Chicago, IL, USA},
series = {GPCE 2021}
}

@inproceedings{10.1145/3486609.3487197,
author = {Ataei, Parisa and Khan, Fariba and Walkingshaw, Eric},
title = {A variational database management system},
year = {2021},
isbn = {9781450391122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486609.3487197},
doi = {10.1145/3486609.3487197},
abstract = {Many problems require working with data that varies in its structure and content. Current approaches, such as schema evolution or data integration tools, are highly tailored to specific kinds of variation in databases. While these approaches work well in their roles, they do not address all kinds of variation and do address the interaction of different kinds of variation in databases. In this paper, we define a framework for capturing variation as a generic and orthogonal con- cern in relational databases. We define variational schemas, variational databases, and variational queries for capturing variation in the structure, content, and information needs of relational databases, respectively. We define a type system that ensures variational queries are consistent with respect to a variational schema. Finally, we design and implement a variational database management system as an abstraction layer over a traditional relational database management system. Using previously developed use cases, we show the feasibility of our framework and demonstrate the performance of different approaches used in our system},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {29–42},
numpages = {14},
keywords = {choice calculus, relational databases, software product lines, type systems, variation, variational data},
location = {Chicago, IL, USA},
series = {GPCE 2021}
}

@inproceedings{10.1145/3486609.3487200,
author = {Scaletta, Marco and H\"{a}hnle, Reiner and Steinh\"{o}fel, Dominic and Bubel, Richard},
title = {Delta-based verification of software product families},
year = {2021},
isbn = {9781450391122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486609.3487200},
doi = {10.1145/3486609.3487200},
abstract = {The quest for feature- and family-oriented deductive verification of software product lines resulted in several proposals. In this paper we look at delta-oriented modeling of product lines and combine two new ideas: first, we extend H\"{a}hnle \&amp; Schaefer’s delta-oriented version of Liskov’s substitution principle for behavioral subtyping to work also for overridden behavior in benign cases. For this to succeed, programs need to be in a certain normal form. The required normal form turns out to be achievable in many cases by a set of program transformations, whose correctness is ensured by the recent technique of abstract execution. This is a generalization of symbolic execution that permits reasoning about abstract code elements. It is needed, because code deltas contain partially unknown code contexts in terms of “original” calls. Second, we devise a modular verification procedure for deltas based on abstract execution, representing deltas as abstract programs calling into unknown contexts. The result is a “delta-based” verification approach, where each modification of a method in a code delta is verified in isolation, but which overcomes the strict limitations of behavioral subtyping and works for many practical programs. The latter claim is substantiated with case studies and benchmarks.},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {69–82},
numpages = {14},
keywords = {Software product lines, abstract execution, behavioral subtyping, deductive verification, delta-oriented programming, program transformation},
location = {Chicago, IL, USA},
series = {GPCE 2021}
}

@proceedings{10.1145/3487553,
title = {WWW '22: Companion Proceedings of the Web Conference 2022},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Lyon, France}
}

@inproceedings{10.1145/3488560.3498492,
author = {Sikdar, Satyaki and Shah, Neil and Weninger, Tim},
title = {Attributed Graph Modeling with Vertex Replacement Grammars},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498492},
doi = {10.1145/3488560.3498492},
abstract = {Recent work at the intersection of formal language theory and graph theory has explored graph grammars for graph modeling. However, existing models and formalisms can only operate on homogeneous (i.e., untyped or unattributed) graphs. We relax this restriction and introduce the Attributed Vertex Replacement Grammar (AVRG), which can be efficiently extracted from heterogeneous (i.e., typed, colored, or attributed) graphs. Unlike current state-of-the-art methods, which train enormous models over complicated deep neural architectures, the AVRG model is unsupervised and interpretable. It is based on context-free string grammars and works by encoding graph rewriting rules into a graph grammar containing graphlets and instructions on how they fit together. We show that the AVRG can encode succinct models of input graphs yet faithfully preserve their structure and assortativity properties. Experiments on large real-world datasets show that graphs generated from the AVRG model exhibit substructures and attribute configurations that match those found in the input networks.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {928–936},
numpages = {9},
keywords = {assortativity, attributed graphs, graph generation, graph models},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3488932.3527282,
author = {Charmet, Fabien and Tanuwidjaja, Harry C. and Morikawa, Tomohiro and Takahashi, Takeshi},
title = {Towards Polyvalent Adversarial Attacks on URL Classification Engines},
year = {2022},
isbn = {9781450391405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488932.3527282},
doi = {10.1145/3488932.3527282},
abstract = {Clicking on the wrong link and downloading a malware or leaking confidential information has been a common threat to everyone. Novel defense mechanisms against web threats have emerged, from URL parsing via Machine Learning to JavaScript deep graph analysis. In this study, we consider the problem of generating Adversarial Examples to evade URL classifiers. We propose a neural network architecture that will be able to generate synthetic URLs for a variety of attacks that will evade existing security classifiers. The main focus of the literature is on classifiers for phishing URLs, but in this work we aim to extend it to other attacks like Drive-by-Download. Our system splits the URL at the word level before substituting words with symbols. This approach contrasts with the existing literature that works on the character level without semantic considerations. We also envision a combination of the word level generation with a character level analysis by a Convolutional Neural Network to improve the evasion capacity of the synthetic samples.},
booktitle = {Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security},
pages = {1246–1248},
numpages = {3},
keywords = {adversarial examples, data generation, machine learning},
location = {Nagasaki, Japan},
series = {ASIA CCS '22}
}

@inproceedings{10.1145/3490354.3494373,
author = {Schreyer, Marco and Sattarov, Timur and Borth, Damian},
title = {Multi-view contrastive self-supervised learning of accounting data representations for downstream audit tasks},
year = {2022},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494373},
doi = {10.1145/3490354.3494373},
abstract = {International audit standards require the direct assessment of a financial statement's underlying accounting transactions, referred to as journal entries. Recently, driven by the advances in artificial intelligence, deep learning inspired audit techniques have emerged in the field of auditing vast quantities of journal entry data. Nowadays, the majority of such methods rely on a set of specialized models, each trained for a particular audit task. At the same time, when conducting a financial statement audit, audit teams are confronted with (i) challenging time-budget constraints, (ii) extensive documentation obligations, and (iii) strict model interpretability requirements. As a result, auditors prefer to harness only a single preferably 'multi-purpose' model throughout an audit engagement. We propose a contrastive self-supervised learning framework designed to learn audit task invariant accounting data representations to meet this requirement. The framework encompasses deliberate interacting data augmentation policies that utilize the attribute characteristics of journal entry data. We evaluate the framework on two real-world datasets of city payments and transfer the learned representations to three downstream audit tasks: anomaly detection, audit sampling, and audit documentation. Our experimental results provide empirical evidence that the proposed framework offers the ability to increase the efficiency of audits by learning rich and interpretable 'multi-task' representations.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {8},
numpages = {8},
keywords = {accounting information systems, audit, computer assisted audit techniques, enterprise resource planning systems, multi-task learning, representation learning, self-supervised learning},
location = {Virtual Event},
series = {ICAIF '21}
}

@inproceedings{10.1145/3491101.3503549,
author = {Hwang, Angel Hsing-Chi},
title = {Too Late to be Creative? AI-Empowered Tools in Creative Processes},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3503549},
doi = {10.1145/3491101.3503549},
abstract = {The present case study examines the product landscape of current AI-empowered co-creative tools. Specifically, I review literature in both creativity and HCI research and investigate how these tools support different stages in humans’ creative processes and how common challenges in human-AI interaction (HAII) are addressed. I find these AI-driven tools mostly support the generation and execution of ideas and are less involved in the early stages of co-creation. Moreover, HAII challenges identified in other fields receive little attention in the creative domain. Based on a synthetic analysis, I elaborate on how future tools can leverage the ”non-human” quality of AI to achieve innovation through a more human-centered, collaborative journey.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {38},
numpages = {9},
keywords = {creativity, creativity support tool, human-AI interaction},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/3491101.3519665,
author = {Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L.},
title = {Expectation vs.&nbsp;Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519665},
doi = {10.1145/3491101.3519665},
abstract = {Recent advances in Large Language Models (LLM) have made automatic code generation possible for real-world programming tasks in general-purpose programming languages such as Python. However, there are few human studies on the usability of these tools and how they fit the programming workflow. In this work, we conducted a within-subjects user study with 24 participants to understand how programmers use and perceive Copilot, a LLM-based code generation tool. We found that, while Copilot did not necessarily improve the task completion time or success rate, most participants preferred to use Copilot in daily programming tasks, since Copilot often provided a useful starting point and saved the effort of searching online. However, participants did face difficulties in understanding, editing, and debugging code snippets generated by Copilot, which significantly hindered their task-solving effectiveness. Finally, we highlighted several promising directions for improving the design of Copilot based on our observations and participants’ feedback.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {332},
numpages = {7},
keywords = {github copilot, large language model},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/3491102.3501819,
author = {Chung, John Joon Young and Kim, Wooseok and Yoo, Kang Min and Lee, Hwaran and Adar, Eytan and Chang, Minsuk},
title = {TaleBrush: Sketching Stories with Generative Pretrained Language Models},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501819},
doi = {10.1145/3491102.3501819},
abstract = {While advanced text generation algorithms (e.g., GPT-3) have enabled writers to co-create stories with an AI, guiding the narrative remains a challenge. Existing systems often leverage simple turn-taking between the writer and the AI in story development. However, writers remain unsupported in intuitively understanding the AI’s actions or steering the iterative generation. We introduce TaleBrush, a generative story ideation tool that uses line sketching interactions with a GPT-based language model for control and sensemaking of a protagonist’s fortune in co-created stories. Our empirical evaluation found our pipeline reliably controls story generation while maintaining the novelty of generated sentences. In a user study with 14 participants with diverse writing experiences, we found participants successfully leveraged sketching to iteratively explore and write stories according to their intentions about the character’s fortune while taking inspiration from generated stories. We conclude with a reflection on how sketching interactions can facilitate the iterative human-AI co-creation process.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {209},
numpages = {19},
keywords = {controlled generation, creativity support tool, sketching, story generation, story writing},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3491102.3501939,
author = {Bae, S. Sandra and Zheng, Clement and West, Mary Etta and Do, Ellen Yi-Luen and Huron, Samuel and Szafir, Danielle Albers},
title = {Making Data Tangible: A Cross-disciplinary Design Space for Data Physicalization},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501939},
doi = {10.1145/3491102.3501939},
abstract = {Designing a data physicalization requires a myriad of different considerations. Despite the cross-disciplinary nature of these considerations, research currently lacks a synthesis across the different communities data physicalization sits upon, including their approaches, theories, and even terminologies. To bridge these communities synergistically, we present a design space that describes and analyzes physicalizations according to three facets: context (end-user considerations), structure (the physical structure of the artifact), and interactions (interactions with both the artifact and data). We construct this design space through a systematic review of 47 physicalizations and analyze the interrelationships of key factors when designing a physicalization. This design space cross-pollinates knowledge from relevant HCI communities, providing a cohesive overview of what designers should consider when creating a data physicalization while suggesting new design possibilities. We analyze the design decisions present in current physicalizations, discuss emerging trends, and identify underlying open challenges.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {81},
numpages = {18},
keywords = {data physicalization, data visualization, design, design space, tangible user interface},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3491102.3501973,
author = {Elsden, Chris and Chatting, David and Duggan, Michael and Dwyer, Andrew Carl and Thornton, Pip},
title = {Zoom Obscura: Counterfunctional Design for Video-Conferencing},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501973},
doi = {10.1145/3491102.3501973},
abstract = {This paper reports on Zoom Obscura – an artist-based design research project, responding to the ubiquity of video-conferencing as a technical and cultural phenomenon throughout the Covid-19 pandemic. As enterprise software, such as Zoom, rapidly came to mediate even the most personal and intimate interactions, we supported and collaborated with seven independent artists to explore technical and creative interventions in video-conferencing. Our call for participation sought critical interventions that would help users counter, and regain agency in regard to the various ways in which personal data is captured, transmitted and processed in video-conferencing tools. In this design study, we analyse post-hoc how each of the seven projects employed aspects of counterfunctional design to achieve these aims. Each project reveals different avenues and strategies for counterfunctionality in video-conferencing software, as well as opportunities to design critically towards interactions and experiences that challenge existing norms and expectations around these platforms.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {143},
numpages = {17},
keywords = {Counterfunctional Design, Covid-19, Design Research, Surveillance, Video Conferencing, Zoom},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3491102.3501991,
author = {Cho, Janghee and Xu, Tian and Zimmermann-Niefield, Abigail and Voida, Stephen},
title = {Reflection in Theory and Reflection in Practice: An Exploration of the Gaps in Reflection Support among Personal Informatics Apps},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501991},
doi = {10.1145/3491102.3501991},
abstract = {Personal informatics (PI) systems have been developed to support reflection. While reflection is considered an indispensable activity in PI use, how and when reflection occurs is still under-studied. In this paper, we present an analysis of the interactive features of 123 commercial PI apps, revealing that reflective practices are unevenly supported. The lack of features that encourage user-driven reflection, scaffolding for setting goals and configuring data collection and presentation, and consideration of wider implications stand to limit meaning-making and frustrate nuanced insight generation based on lived experiences. Based on our findings, we discuss how reflection is currently misrepresented in personal informatics tools, identify and characterize the gaps between theoretical research on reflection and interface features in current apps, and offer suggestions about how reflection could be better supported.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {142},
numpages = {23},
keywords = {agency, mobile apps, personal informatics, reflection, self-tracking},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3491102.3502141,
author = {Dang, Hai and Mecke, Lukas and Buschek, Daniel},
title = {GANSlider: How Users Control Generative Models for Images using Multiple Sliders with and without Feedforward Information},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502141},
doi = {10.1145/3491102.3502141},
abstract = {We investigate how multiple sliders with and without feedforward visualizations influence users’ control of generative models. In an online study (N=138), we collected a dataset of people interacting with a generative adversarial network (StyleGAN2) in an image reconstruction task. We found that more control dimensions (sliders) significantly increase task difficulty and user actions. Visual feedforward partly mitigates this by enabling more goal-directed interaction. However, we found no evidence of faster or more accurate task performance. This indicates a tradeoff between feedforward detail and implied cognitive costs, such as attention. Moreover, we found that visualizations alone are not always sufficient for users to understand individual control dimensions. Our study quantifies fundamental UI design factors and resulting interaction behavior in this context, revealing opportunities for improvement in the UI design for interactive applications of generative models. We close by discussing design directions and further aspects.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {569},
numpages = {15},
keywords = {dataset, generative adversarial network, image manipulation, interactive AI, user study},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3491102.3517446,
author = {Gamage, Dilrukshi and Ghasiya, Piyush and Bonagiri, Vamshi and Whiting, Mark E. and Sasahara, Kazutoshi},
title = {Are Deepfakes Concerning? Analyzing Conversations of Deepfakes on Reddit and Exploring Societal Implications},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517446},
doi = {10.1145/3491102.3517446},
abstract = {Deepfakes are synthetic content generated using advanced deep learning and AI technologies. The advancement of technology has created opportunities for anyone to create and share deepfakes much easier. This may lead to societal concerns based on how communities engage with it. However, there is limited research available to understand how communities perceive deepfakes. We examined deepfake conversations on Reddit from 2018 to 2021—including major topics and their temporal changes as well as implications of these conversations. Using a mixed-method approach—topic modeling and qualitative coding, we found 6,638 posts and 86,425 comments discussing concerns of the believable nature of deepfakes and how platforms moderate them. We also found Reddit conversations to be pro-deepfake and building a community that supports creating and sharing deepfake artifacts and building a marketplace regardless of the consequences. Possible implications derived from qualitative codes indicate that deepfake conversations raise societal concerns. We propose that there are implications for Human Computer Interaction (HCI) to mitigate the harm created from deepfakes.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {103},
numpages = {19},
keywords = {content analysis, deepfake, societal implication, topic modeling},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3491102.3517582,
author = {Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
title = {AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517582},
doi = {10.1145/3491102.3517582},
abstract = {Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by “unit-testing” sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {385},
numpages = {22},
keywords = {Human-AI Interaction, Large Language Models, Natural Language Processing},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3491102.3517612,
author = {Zhang, Yu and Wang, Yun and Zhang, Haidong and Zhu, Bin and Chen, Siming and Zhang, Dongmei},
title = {OneLabeler: A Flexible System for Building Data Labeling Tools},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517612},
doi = {10.1145/3491102.3517612},
abstract = {Labeled datasets are essential for supervised machine learning. Various data labeling tools have been built to collect labels in different usage scenarios. However, developing labeling tools is time-consuming, costly, and expertise-demanding on software development. In this paper, we propose a conceptual framework for data labeling and OneLabeler based on the conceptual framework to support easy building of labeling tools for diverse usage scenarios. The framework consists of common modules and states in labeling tools summarized through coding of existing tools. OneLabeler supports configuration and composition of common software modules through visual programming to build data labeling tools. A module can be a human, machine, or mixed computation procedure in data labeling. We demonstrate the expressiveness and utility of the system through ten example labeling tools built with OneLabeler. A user study with developers provides evidence that OneLabeler supports efficient building of diverse data labeling tools.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {93},
numpages = {22},
keywords = {data labeling, framework, interactive machine learning, toolkit, visual programming},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3491102.3517621,
author = {Harrington, Christina and Martin-Hammond, Aqueasha and Bray, Kirsten E},
title = {Examining Identity as a Variable of Health Technology Research for Older Adults: A Systematic Review},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517621},
doi = {10.1145/3491102.3517621},
abstract = {Innovations in HCI research of health-related pervasive and ubiquitous technologies can potentially improve older adults’ access to healthcare resources and support long-term independence in the home. Despite efforts to include their voices in technology research and design, many older adults have yet to actualize these health benefits, with barriers of access and proficiency actually widening the gap of health inequities. We reviewed 174 HCI publications through a systematic review to examine who is engaged in the design of health technologies for older adults, methods used to engage them, and how different types of participation might impact design directions. Findings highlight that thus far, many identity dimensions have not been explored in HCI aging research. We identify research gaps and implications to promote expanding research engagement with these dimensions as a way to support the design of health technologies that see better adoption among marginalized populations.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {265},
numpages = {24},
keywords = {cultural diversity, diversity dimensions, health, older adults, pervasive and ubiquitous technologies, study participants, systematic review},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3491371.3491385,
author = {Li, Zitao and Dang, Trung and Wang, Tianhao and Li, Ninghui},
title = {MGD: A Utility Metric for Private Data Publication},
year = {2021},
isbn = {9781450387378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491371.3491385},
doi = {10.1145/3491371.3491385},
abstract = {Differential privacy has been accepted as one of the most popular techniques to protect user data privacy. A common way for utilizing private data under DP is to take an input dataset and synthesize a new dataset that preserves features of the input dataset while satisfying DP. A trade-off always exists between the strength of privacy protection and the utility of the final output: stronger privacy protection requires larger randomness, so the outputs usually have a larger variance and can be far from optimal. In this paper, we summarize our proposed metric for the NIST “A Better Meter Stick for Differential Privacy” competition&nbsp;[26], MarGinal Difference (MGD), for measuring the utility of a synthesized dataset. Our metric is based on earth mover distance. We introduce new features in our metric so that it is not affected by some small random noise that is unavoidable in the DP context but focuses more on the significant difference. We show that our metric can reflect the range query error better compared with other existing metrics. We introduce an efficient computation method based on the min-cost flow to alleviate the high computation cost of the earth mover’s distance.},
booktitle = {Proceedings of the 8th International Conference on Networking, Systems and Security},
pages = {106–119},
numpages = {14},
keywords = {Privacy, data synthesis, metric},
location = {Cox's Bazar, Bangladesh},
series = {NSysS '21}
}

@inproceedings{10.1145/3492321.3519591,
author = {Schumilo, Sergej and Aschermann, Cornelius and Jemmett, Andrea and Abbasi, Ali and Holz, Thorsten},
title = {Nyx-net: network fuzzing with incremental snapshots},
year = {2022},
isbn = {9781450391627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492321.3519591},
doi = {10.1145/3492321.3519591},
abstract = {Coverage-guided fuzz testing ("fuzzing") has become mainstream and we have observed lots of progress in this research area recently. However, it is still challenging to efficiently test network services with existing coverage-guided fuzzing methods. In this paper, we introduce the design and implementation of Nyx-Net, a novel snapshot-based fuzzing approach that can successfully fuzz a wide range of targets spanning servers, clients, games, and even Firefox's Inter-Process Communication (IPC) interface. Compared to state-of-the-art methods, Nyx-Net improves test throughput by up to 300x and coverage found by up to 70\%. Additionally, Nyx-Net is able to find crashes in two of ProFuzzBench's targets that no other fuzzer found previously. When using Nyx-Net to play the game Super Mario, Nyx-Net shows speedups of 10--30x compared to existing work. Moreover, Nyx-Net is able to find previously unknown bugs in servers such as Lighttpd, clients such as MySQL client, and even Firefox's IPC mechanism---demonstrating the strength and versatility of the proposed approach. Lastly, our prototype implementation was awarded a $20.000 bug bounty for enabling fuzzing on previously unfuzzable code in Firefox and solving a long-standing problem at Mozilla.},
booktitle = {Proceedings of the Seventeenth European Conference on Computer Systems},
pages = {166–180},
numpages = {15},
keywords = {fuzzing, software security, testing},
location = {Rennes, France},
series = {EuroSys '22}
}

@article{10.1145/3498686,
author = {Wang, Yuting and Zhang, Ling and Shao, Zhong and Koenig, J\'{e}r\'{e}mie},
title = {Verified compilation of C programs with a nominal memory model},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {POPL},
url = {https://doi.org/10.1145/3498686},
doi = {10.1145/3498686},
abstract = {Memory models play an important role in verified compilation of imperative programming languages. A representative one is the block-based memory model of CompCert---the state-of-the-art verified C compiler. Despite its success, the abstraction over memory space provided by CompCert's memory model is still primitive and inflexible. In essence, it uses a fixed representation for identifying memory blocks in a global memory space and uses a globally shared state for distinguishing between used and unused blocks. Therefore, any reasoning about memory must work uniformly for the global memory; it is impossible to individually reason about different sub-regions of memory (i.e., the stack and global definitions). This not only incurs unnecessary complexity in compiler verification, but also poses significant difficulty for supporting verified compilation of open or concurrent programs which need to work with contextual memory, as manifested in many previous extensions of CompCert.  To remove the above limitations, we propose an enhancement to the block-based memory model based on nominal techniques; we call it the nominal memory model. By adopting the key concepts of nominal techniques such as atomic names and supports to model the memory space, we are able to 1) generalize the representation of memory blocks to any types satisfying the properties of atomic names and 2) remove the global constraints for managing memory blocks, enabling flexible memory structures for open and concurrent programs.  To demonstrate the effectiveness of the nominal memory model, we develop a series of extensions of CompCert based on it. These extensions show that the nominal memory model 1) supports a general framework for verified compilation of C programs, 2) enables intuitive reasoning of compiler transformations on partial memory; and 3) enables modular reasoning about programs working with contextual memory. We also demonstrate that these extensions require limited changes to the original CompCert, making the verification techniques based on the nominal memory model easy to adopt.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {25},
numpages = {31},
keywords = {Memory Models, Nominal Techniques, Verified Compilation}
}

@proceedings{10.1145/3500868,
title = {CSCW'22 Companion: Companion Publication of the 2022 Conference on Computer Supported Cooperative Work and Social Computing},
year = {2022},
isbn = {9781450391900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Taiwan}
}

@inproceedings{10.1145/3501385.3543957,
author = {Sarsa, Sami and Denny, Paul and Hellas, Arto and Leinonen, Juho},
title = {Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models},
year = {2022},
isbn = {9781450391948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501385.3543957},
doi = {10.1145/3501385.3543957},
abstract = {This article explores the natural language generation capabilities of large language models with application to the production of two types of learning resources common in programming courses. Using OpenAI Codex as the large language model, we create programming exercises (including sample solutions and test cases) and code explanations, assessing these qualitatively and quantitatively. Our results suggest that the majority of the automatically generated content is both novel and sensible, and in some cases ready to use as is. When creating exercises we find that it is remarkably easy to influence both the programming concepts and the contextual themes they contain, simply by supplying keywords as input to the model. Our analysis suggests that there is significant value in massive generative machine learning models as a tool for instructors, although there remains a need for some oversight to ensure the quality of the generated content before it is delivered to students. We further discuss the implications of OpenAI Codex and similar tools for introductory programming education and highlight future research streams that have the potential to improve the quality of the educational experience for both teachers and students alike.},
booktitle = {Proceedings of the 2022 ACM Conference on International Computing Education Research - Volume 1},
pages = {27–43},
numpages = {17},
keywords = {Automated feedback, CS1, Code explanations, Exercise generation, GPT-3, Large language models, Natural language generation, OpenAI Codex, Programming exercises, Resource generation, Robosourcing},
location = {Lugano and Virtual Event, Switzerland},
series = {ICER '22}
}

@inproceedings{10.1145/3501712.3529725,
author = {Brady, Corey and Jen, Tessaly and Vogelstein, Lauren and Dim, Efrat},
title = {Designing with Feeling: How Students Constructed Embodied Participatory Simulations for Groups of Younger Learners to Understand and Care About Sustainability in Ecosystems},
year = {2022},
isbn = {9781450391979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501712.3529725},
doi = {10.1145/3501712.3529725},
abstract = {Technological supports for collective, embodied ideation that were only dimly envisioned by 20th century futurists are now solid fact. For instance, advances in location tracking, mixed reality, and agent-based modeling have converged, enabling groups to collectively construct and animate shared, dynamic representations in real time. Pre-existing genres of activity may support activity design here, including participatory simulations; interactive theater; embodied play; or games. However, if we want emerging tools for collective ideation to be expressive for all groups, we need to attend carefully to how participants themselves conceptualize their potential. This paper describes a project that engaged high-school students in designing embodied participatory activities for groups of younger students, about a topic they selected as important: sustainability in social-ecological systems. Students’ design work was diverse and generative. We focus here on one emergent theme: design decisions that aimed to evoke particular feelings in participants, to foster learning and reflection on actions.},
booktitle = {Proceedings of the 21st Annual ACM Interaction Design and Children Conference},
pages = {315–326},
numpages = {12},
keywords = {Affect, Collaborative Learning, Computational Modeling, Embodied Cognition, Participatory Simulations},
location = {Braga, Portugal},
series = {IDC '22}
}

@inproceedings{10.1145/3501712.3529735,
author = {McDermott, Tiarnach and Robson, James and Winters, Niall and Malmberg, Lars-Erik},
title = {Mapping the Changing Landscape of Child-Computer Interaction Research Through Correlated Topic Modelling},
year = {2022},
isbn = {9781450391979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501712.3529735},
doi = {10.1145/3501712.3529735},
abstract = {As the field of child-computer interaction (CCI) develops and forms an increasingly distinct identity, there is a need for reflection upon the state of the field, and its development thus far. This paper provides an overview of the thematic structure of the CCI field in order to support such reflection, expanding upon previous reviews through implementation of a correlated topic model, an automated, inductive content analysis method, in analysing 4,771 CCI research papers published between 2003 and 2021. Prominence of research topics, and their evolution, are explored. Results portray CCI as a vibrant and varied research landscape which has evolved dynamically over time, exhibiting increasing specialisation and emergence of distinct subfields, and progressing from a technology- to needs-driven agenda. This analysis contributes an extensive empirical mapping of the CCI research landscape, facilitating reflection upon the field and its development, and revealing gaps in extant literature and opportunities for future research.},
booktitle = {Proceedings of the 21st Annual ACM Interaction Design and Children Conference},
pages = {82–97},
numpages = {16},
keywords = {CCI Research, Literature Review, Correlated Topic Model, Automated Text Analysis, Child-Computer Interaction},
location = {Braga, Portugal},
series = {IDC '22}
}

@inproceedings{10.1145/3501712.3529736,
author = {Dietz, Griffin and King Chen, Jennifer and Beason, Jazbo and Tarrow, Matthew and Hilliard, Adriana and Shapiro, R. Benjamin},
title = {ARtonomous: Introducing Middle School Students to Reinforcement Learning Through Virtual Robotics},
year = {2022},
isbn = {9781450391979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501712.3529736},
doi = {10.1145/3501712.3529736},
abstract = {Typical educational robotics approaches rely on imperative programming for robot navigation. However, with the increasing presence of AI in everyday life, these approaches miss an opportunity to introduce machine learning (ML) techniques grounded in an authentic and engaging learning context. Furthermore, the needs for costly specialized equipment and ample physical space are barriers that limit access to robotics experiences for all learners. We propose ARtonomous, a relatively low-cost, virtual alternative to physical, programming-only robotics kits. With ARtonomous, students employ reinforcement learning (RL) alongside code to train and customize virtual autonomous robotic vehicles. Through a study evaluating ARtonomous, we found that middle-school students developed an understanding of RL, reported high levels of engagement, and demonstrated curiosity for learning more about ML. This research demonstrates the feasibility of an approach like ARtonomous for 1) eliminating barriers to robotics education and 2) promoting student learning and interest in RL and ML.},
booktitle = {Proceedings of the 21st Annual ACM Interaction Design and Children Conference},
pages = {430–441},
numpages = {12},
keywords = {AI, education, middle school, reinforcement learning, robotics},
location = {Braga, Portugal},
series = {IDC '22}
}

@book{10.1145/3501714,
editor = {Geffner, Hector and Dechter, Rina and Halpern, Joseph Y.},
title = {Probabilistic and Causal Inference: The Works of Judea Pearl},
year = {2022},
isbn = {9781450395861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {36},
abstract = {Professor Judea Pearl won the 2011 Turing Award “for fundamental contributions to artificial intelligence through the development of a calculus for probabilistic and causal reasoning.” This book contains the original articles that led to the award, as well as other seminal works, divided into four parts: heuristic search, probabilistic reasoning, causality, first period (1988–2001), and causality, recent period (2002–2020). Each of these parts starts with an introduction written by Judea Pearl. The volume also contains original, contributed articles by leading researchers that analyze, extend, or assess the influence of Pearl’s work in different fields: from AI, Machine Learning, and Statistics to Cognitive Science, Philosophy, and the Social Sciences. The first part of the volume includes a biography, a transcript of his Turing Award Lecture, two interviews, and a selected bibliography annotated by him.}
}

@book{10.1145/3502398,
author = {Tian, Leimin and Oviatt, Sharon and Muszynski, Michal and Chamberlain, Brent C. and Healey, Jennifer and Sano, Akane},
title = {Applied Affective Computing},
year = {2022},
isbn = {9781450395908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {41},
abstract = {Affective computing is a nascent field situated at the intersection of artificial intelligence with social and behavioral science. It studies how human emotions are perceived and expressed, which then informs the design of intelligent agents and systems that can either mimic this behavior to improve their intelligence or incorporate such knowledge to effectively understand and communicate with their human collaborators. Affective computing research has recently seen significant advances and is making a critical transformation from exploratory studies to real-world applications in the emerging research area known as applied affective computing.This book offers readers an overview of the state-of-the-art and emerging themes in affective computing, including a comprehensive review of the existing approaches to affective computing systems and social signal processing. It provides in-depth case studies of applied affective computing in various domains, such as social robotics and mental well-being. It also addresses ethical concerns related to affective computing and how to prevent misuse of the technology in research and applications. Further, this book identifies future directions for the field and summarizes a set of guidelines for developing next-generation affective computing systems that are effective, safe, and human-centered.For researchers and practitioners new to affective computing, this book will serve as an introduction to the field to help them in identifying new research topics or developing novel applications. For more experienced researchers and practitioners, the discussions in this book provide guidance for adopting a human-centered design and development approach to advance affective computing}
}

@inproceedings{10.1145/3502871.3502884,
author = {Zhu, Kecheng},
title = {Active Learning for Microarray based Leukemia Classification},
year = {2022},
isbn = {9781450385077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502871.3502884},
doi = {10.1145/3502871.3502884},
abstract = {In machine learning, data labeling is assumed to be easy and cheap. However, in real word cases especially clinical field, data sets are rare and expensive to obtain. Active learning is an approach that can query the most informative data for the training. This leads to an alternative to deal with the concern mentioned above. The Sampling method is one of the key parts in active learning because it minimizes the training cost of the classifier. By different query method, models with considerable difference could be produced. The difference in model could lead to significant difference in training cost and final accuracy outcome. The approaches that were used to in this experiment is uncertainty sampling, diversity sampling and query by committee. In the experiment, active learning is applied on the microarray data with improving results. The classification on two types leukemia (acute myeloid leukemia and acute lymophoblastic leukemia) indicates a boost in accuracy with the same number of samples compared to passive machine learning. The experiments leads to the conclusion that with small number of samples with randomness in the field of leukemia classification, active learning produce an more active model. Additionally, active learning with query by committee finds the most informative sample with fewest trials.},
booktitle = {Proceedings of the 2021 8th International Conference on Biomedical and Bioinformatics Engineering},
pages = {77–81},
numpages = {5},
keywords = {Active learning, Pool-based, Query Method, Query by committee, Uncertainty sampling},
location = {Kyoto, Japan},
series = {ICBBE '21}
}

@inproceedings{10.1145/3503161.3548270,
author = {Hou, Xingzhong and Liu, Boxiao and Zhang, Shuai and Shi, Lulin and Jiang, Zite and You, Haihang},
title = {Dynamic Weighted Semantic Correspondence for Few-Shot Image Generative Adaptation},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548270},
doi = {10.1145/3503161.3548270},
abstract = {Few-shot image generative adaptation, which finetunes well-trained generative models on limited examples, is of practical importance. The main challenge is that the few-shot model easily becomes overfitting. It can be attributed to two aspects: the lack of sample diversity for the generator and the failure of fidelity discrimination for the discriminator. In this paper, we introduce two novel methods to solve the diversity and fidelity respectively. Concretely, we propose dynamic weighted semantic correspondence to keep the diversity for the generator, which benefits from the richness of samples generated by source models. To prevent discriminator overfitting, we propose coupled training paradigm across the source and target domains to keep the feature extraction capability of the discriminator backbone. Extensive experiments show that our method outperforms previous methods both on image quality and diversity significantly.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {1214–1222},
numpages = {9},
keywords = {few-shot adaptation, gan, image generation},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3503161.3548414,
author = {Zhang, Yufan and Man, Junkai and Sun, Peng},
title = {MF-Net: A Novel Few-shot Stylized Multilingual Font Generation Method},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548414},
doi = {10.1145/3503161.3548414},
abstract = {Creating a complete stylized font library that helps the audience to perceive information from the text often requires years of study and proficiency in the use of many professional tools. Accordingly, automatic stylized font generation in a deep learning-based fashion is a desirable but challenging task that has attracted a lot of attention in recent years. This paper revisits the state-of-the-art methods for stylized font generation and presents a taxonomy of the deep learning-based stylized font generation. Despite the notable performance of the existing models, stylized multilingual font generation, the task of applying specific font style to diverse characters in multiple languages has never been reported to be addressed. An efficient and economical method for stylized multilingual font generation is essential in numerous application scenarios that require communication with international audiences. We propose a solution for few-shot multilingual stylized font generation by a fast feed-forward network, Multilingual Font Generation Network (MF-Net), which can transfer previously unseen font styles from a few samples to characters from previously unseen languages. Following the Generative Adversarial Network (GAN) framework, MF-Net adopts two separate encoders in the generator to decouple a font image's content and style information. We adopt an attention module in the style encoder to extract both shallow and deep style features. Moreover, we also design a novel language complexity-aware skip connection to adaptive adjust the structural information to be preserved. With an effective loss function to improve the visual quality of the generated font images, we show the effectiveness of the proposed MF-Net based on quantitative and subjective visual evaluation, and compare it with the existing models in the scenario of stylized multilingual font generation. The source code is available on https://github.com/iamyufan/MF-Net.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {2088–2096},
numpages = {9},
keywords = {few-shot learning, font design, image synthesis, style transfer},
location = {Lisboa, Portugal},
series = {MM '22}
}

@proceedings{10.1145/3505270,
title = {CHI PLAY '22: Extended Abstracts of the 2022 Annual Symposium on Computer-Human Interaction in Play},
year = {2022},
isbn = {9781450392112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bremen, Germany}
}

@proceedings{10.1145/3508352,
title = {ICCAD '22: Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
year = {2022},
isbn = {9781450392174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Jointly sponsored by ACM and IEEE, ICCAD is the premier forum to explore new challenges, present leading-edge innovative solutions, and identify emerging technologies in the Electronic Design Automation (EDA) research areas. ICCAD covers the full range of Computer-Aided Design (CAD) topics - from device and circuit-level up through system-level, as well as post-CMOS design.},
location = {San Diego, California}
}

@proceedings{10.1145/3508397,
title = {MEDES '22: Proceedings of the 14th International Conference on Management of Digital EcoSystems},
year = {2022},
isbn = {9781450392198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {During the past years, the International Conference on ManagEment of Digital EcoSystems (MEDES) has become one of the most important international scientific events bringing together researchers, developers, and practitioners to discuss latest research issues and experiences in developing advanced solutions that will help to design, deploy, exploit and tune emerging ecosystems.},
location = {Venice, Italy}
}

@inproceedings{10.1145/3510003.3510153,
author = {Kharkar, Anant and Moghaddam, Roshanak Zilouchian and Jin, Matthew and Liu, Xiaoyu and Shi, Xin and Clement, Colin and Sundaresan, Neel},
title = {Learning to reduce false positives in analytic bug detectors},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510153},
doi = {10.1145/3510003.3510153},
abstract = {Due to increasingly complex software design and rapid iterative development, code defects and security vulnerabilities are prevalent in modern software. In response, programmers rely on static analysis tools to regularly scan their codebases and find potential bugs. In order to maximize coverage, however, these tools generally tend to report a significant number of false positives, requiring developers to manually verify each warning. To address this problem, we propose a Transformer-based learning approach to identify false positive bug warnings. We demonstrate that our models can improve the precision of static analysis by 17.5\%. In addition, we validated the generalizability of this approach across two major bug types: null dereference and resource leak.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1307–1316},
numpages = {10},
keywords = {datasets, gaze detection, neural networks, text tagging},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3510003.3510160,
author = {Sun, Zhensu and Li, Li and Liu, Yan and Du, Xiaoning and Li, Li},
title = {On the importance of building high-quality training datasets for neural code search},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510160},
doi = {10.1145/3510003.3510160},
abstract = {The performance of neural code search is significantly influenced by the quality of the training data from which the neural models are derived. A large corpus of high-quality query and code pairs is demanded to establish a precise mapping from the natural language to the programming language. Due to the limited availability, most widely-used code search datasets are established with compromise, such as using code comments as a replacement of queries. Our empirical study on a famous code search dataset reveals that over one-third of its queries contain noises that make them deviate from natural user queries. Models trained through noisy data are faced with severe performance degradation when applied in real-world scenarios. To improve the dataset quality and make the queries of its samples semantically identical to real user queries is critical for the practical usability of neural code search. In this paper, we propose a data cleaning framework consisting of two subsequent filters: a rule-based syntactic filter and a model-based semantic filter. This is the first framework that applies semantic query cleaning to code search datasets. Experimentally, we evaluated the effectiveness of our framework on two widely-used code search models and three manually-annotated code retrieval benchmarks. Training the popular DeepCS model with the filtered dataset from our framework improves its performance by 19.2\% MRR and 21.3\% Answer@1, on average with the three validation benchmarks.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1609–1620},
numpages = {12},
keywords = {code search, data cleaning, dataset, deep learning},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3510003.3510172,
author = {Izadi, Maliheh and Gismondi, Roberta and Gousios, Georgios},
title = {CodeFill: multi-token code completion by jointly learning from structure and naming sequences},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510172},
doi = {10.1145/3510003.3510172},
abstract = {Code completion is an essential feature of IDEs, yet current auto-completers are restricted to either grammar-based or NLP-based single token completions. Both approaches have significant drawbacks: grammar-based autocompletion is restricted in dynamically-typed language environments, whereas NLP-based autocompleters struggle to understand the semantics of the programming language and the developer's code context.In this work, we present CodeFill, a language model for autocompletion that combines learned structure and naming information. Using a parallel Transformer architecture and multi-task learning, CodeFill consumes sequences of source code token names and their equivalent AST token types. Uniquely, CodeFill is trained both for single-token and multi-token (statement) prediction, which enables it to learn long-range dependencies among grammatical and naming elements. We train CodeFill on two datasets, consisting of 29M and 425M lines of code, respectively. To make the evaluation more realistic, we develop a method to automatically infer points in the source code at which completion matters. We compare CodeFill against four baselines and two state-of-the-art models, GPT-C and TravTrans+. CodeFill surpasses all baselines in single token prediction (MRR: 70.9\% vs. 66.2\% and 67.8\%) and outperforms the state of the art for multi-token prediction (ROUGE-L: 63.7\% vs. 52.4\% and 59.2\%, for n = 4 tokens). We publicly release our source code and datasets.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {401–412},
numpages = {12},
keywords = {automatic code completion, dynamically-typed languages, multi-task learning, transformers, types},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@proceedings{10.1145/3510606,
title = {SIGMIS-CPR '22: Proceedings of the 2022 Computers and People Research Conference},
year = {2022},
isbn = {9781450392310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ACM SIGMIS CPR, for the 59th edition, continues to bring together the academic and practitioner communities to discuss issues at the intersection of society, organizations, individuals and information technology (IT).},
location = {Atlanta, Georgia}
}

@inproceedings{10.1145/3511808.3557310,
author = {Chen, Haonan and Dou, Zhicheng and Zhu, Yutao and Cao, Zhao and Cheng, Xiaohua and Wen, Ji-Rong},
title = {Enhancing User Behavior Sequence Modeling by Generative Tasks for Session Search},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557310},
doi = {10.1145/3511808.3557310},
abstract = {Users' search tasks have become increasingly complicated, requiring multiple queries and interactions with the results. Recent studies have demonstrated that modeling the historical user behaviors in a session can help understand the current search intent. Existing context-aware ranking models primarily encode the current session sequence (from the first behavior to the current query) and compute the ranking score using the high-level representations. However, there is usually some noise in the current session sequence (useless behaviors for inferring the search intent) that may affect the quality of the encoded representations. To help the encoding of the current user behavior sequence, we propose to use a decoder and the information of future sequences and a supplemental query. Specifically, we design three generative tasks that can help the encoder to infer the actual search intent: (1) predicting future queries, (2) predicting future clicked documents, and (3) predicting a supplemental query. We jointly learn the ranking task with these generative tasks using an encoder-decoder structured approach. Extensive experiments on two public search logs demonstrate that our model outperforms all existing baselines, and the designed generative tasks can actually help the ranking task. Besides, additional experiments also show that our approach can be easily applied to various Transformer-based encoder-decoder models and improve their performance.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {180–190},
numpages = {11},
keywords = {auto-session-encoder, document ranking, session search},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3511808.3557474,
author = {Zha, Daochen and Lai, Kwei-Herng and Tan, Qiaoyu and Ding, Sirui and Zou, Na and Hu, Xia Ben},
title = {Towards Automated Imbalanced Learning with Deep Hierarchical Reinforcement Learning},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557474},
doi = {10.1145/3511808.3557474},
abstract = {Imbalanced learning is a fundamental challenge in data mining, where there is a disproportionate ratio of training samples in each class. Over-sampling is an effective technique to tackle imbalanced learning through generating synthetic samples for the minority class. While numerous over-sampling algorithms have been proposed, they heavily rely on heuristics, which could be sub-optimal since we may need different sampling strategies for different datasets and base classifiers, and they cannot directly optimize the performance metric. Motivated by this, we investigate developing a learning-based over-sampling algorithm to optimize the classification performance, which is a challenging task because of the huge and hierarchical decision space. At the high level, we need to decide how many synthetic samples to generate. At the low level, we need to determine where the synthetic samples should be located, which depends on the high-level decision since the optimal locations of the samples may differ for different numbers of samples. To address the challenges, we propose AutoSMOTE, an automated over-sampling algorithm that can jointly optimize different levels of decisions. Motivated by the success of SMOTE and its extensions, we formulate the generation process as a Markov decision process (MDP) consisting of three levels of policies to generate synthetic samples within the SMOTE search space. Then we leverage deep hierarchical reinforcement learning to optimize the performance metric on the validation data. Extensive experiments on six real-world datasets demonstrate that AutoSMOTE significantly outperforms the state-of-the-art resampling algorithms. The code is at https://github.com/daochenzha/autosmote},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {2476–2485},
numpages = {10},
keywords = {automated machine learning, classification, imbalanced learning, reinforcement learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1145/3512919,
author = {Hamidi, Foad and Owuor, Patrick Mbullo and Hynie, Michaela and Baljko, Melanie},
title = {"Knowledge Comes Through Participation": Understanding Disability through the Lens of DIY Assistive Technology in Western Kenya},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512919},
doi = {10.1145/3512919},
abstract = {People with disabilities in Low- and Middle-Income Countries (LMICs) have limited access to digital assistive technologies (ATs). Most ATs in LMICs are manufactured elsewhere and are expensive and difficult to maintain. Do-It-Yourself Assistive Technologies (DIY-ATs) designed, customized, and repaired by non-technical users offer exciting directions in these contexts. We have been exploring the possibilities and challenges of DIY-ATs in Western Kenya, using community-engaged workshops in rural and urban special education schools for the past three years. We present findings from a concluding-stage research activity: a multiple stakeholder focus group where teachers, disability advocates, and representatives from the local government and technology innovation hubs, discussed the possibilities and challenges of addressing disability issues through DIY-ATs in this context. Participants identified opportunities for DIY-ATs for social inclusion, disability assessment, and inclusive education, and shared concerns about their sustainability, safety, and contextual relevance.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {72},
numpages = {25},
keywords = {DIY assistive technologies, Kenya, community engagement, disability, participatory design}
}

@proceedings{10.1145/3513130,
title = {SIGDOC '22: Proceedings of the 40th ACM International Conference on Design of Communication},
year = {2022},
isbn = {9781450392464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Boston, MA, USA}
}

@inproceedings{10.1145/3514094.3534136,
author = {Wolfe, Robert and Caliskan, Aylin},
title = {American == White in Multimodal Language-and-Image AI},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534136},
doi = {10.1145/3514094.3534136},
abstract = {Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP, are evaluated for evidence of a bias previously observed in social and experimental psychology: equating American identity with being White. Embedding association tests (EATs) using standardized images of self-identified Asian, Black, Latina/o, and White individuals from the Chicago Face Database (CFD) reveal that White individuals are more associated with collective in-group words than are Asian, Black, or Latina/o individuals, with effect sizes &gt;.4 for White vs. Asian comparisons across all models. In assessments of three core aspects of American identity reported by social psychologists, single-category EATs reveal that images of White individuals are more associated with patriotism and with being born in America, but that, consistent with prior findings in psychology, White individuals are associated with being less likely to treat people of all races and backgrounds equally. Additional tests reveal that the number of images of Black individuals returned by an image ranking task is more strongly correlated with state-level implicit bias scores for White individuals (Pearson's ρ=.63 in CLIP, ρ=.69 in BLIP) than are state demographics (ρ=.60), suggesting a relationship between regional prototypicality and implicit bias. Three downstream machine learning tasks demonstrate biases associating American with White. In a visual question answering task using BLIP, 97\% of White individuals are identified as American, compared to only 3\% of Asian individuals. When asked in what state the individual depicted lives in, the model responds China 53\% of the time for Asian individuals, but always with an American state for White individuals. In an image captioning task, BLIP remarks upon the race of Asian individuals as much as 36\% of the time, and the race of Black individuals as much as 18\% of the time, but never remarks upon race for White individuals. Finally, when provided with an initialization image of individuals from the CFD and the text "an American person," a synthetic image generator (VQGAN) using the text-based guidance of CLIP consistently lightens the skin tone of individuals of all races (by 35\% for Black individuals, based on mean pixel brightness), and generates output images of White individuals with blonde hair. The results indicate that societal biases equating American identity with being White are learned by multimodal language-and-image AI, and that these biases propagate to downstream applications of such models.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {800–812},
numpages = {13},
keywords = {bias in ai, multimodal models, racial bias, visual semantics},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534155,
author = {Liu, David and Nanayakkara, Priyanka and Sakha, Sarah Ariyan and Abuhamad, Grace and Blodgett, Su Lin and Diakopoulos, Nicholas and Hullman, Jessica R. and Eliassi-Rad, Tina},
title = {Examining Responsibility and Deliberation in AI Impact Statements and Ethics Reviews},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534155},
doi = {10.1145/3514094.3534155},
abstract = {The artificial intelligence research community is continuing to grapple with the ethics of its work by encouraging researchers to discuss potential positive and negative consequences. Neural Information Processing Systems (NeurIPS), a top-tier conference for machine learning and artificial intelligence research, first required a statement of broader impact in 2020. In 2021, NeurIPS updated their call for papers such that 1) the impact statement focused on negative societal impacts and was not required but encouraged, 2) a paper checklist and ethics guidelines were provided to authors, and 3) papers underwent ethics reviews and could be rejected on ethical grounds. In light of these changes, we contribute a qualitative analysis of 231 impact statements and all publicly-available ethics reviews. We describe themes arising around the ways in which authors express agency (or lack thereof) in identifying or mitigating negative consequences and assign responsibility for mitigating negative societal impacts. We also characterize ethics reviews in terms of the types of issues raised by ethics reviewers (falling into categories of policy-oriented and non-policy-oriented), recommendations ethics reviewers make to authors (e.g., in terms of adding or removing content), and interaction between authors, ethics reviewers, and original reviewers (e.g., consistency between issues flagged by original reviewers and those discussed by ethics reviewers). Finally, based on our analysis we make recommendations for how authors can be further supported in engaging with the ethical implications of their work.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {424–435},
numpages = {12},
keywords = {ai ethics, broader impact, ethics review, impact statements},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514221.3517841,
author = {Islam, Maliha Tashfia and Fariha, Anna and Meliou, Alexandra and Salimi, Babak},
title = {Through the Data Management Lens: Experimental Analysis and Evaluation of Fair Classification},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3517841},
doi = {10.1145/3514221.3517841},
abstract = {Classification, a heavily-studied data-driven machine learning task, drives an increasing number of prediction systems involving critical human decisions such as loan approval and criminal risk assessment. However, classifiers often demonstrate discriminatory behavior, especially when presented with biased data. Consequently, fairness in classification has emerged as a high-priority research area. Data management research is showing an increasing presence and interest in topics related to data and algorithmic fairness, including the topic of fair classification. The interdisciplinary efforts in fair classification, with machine learning research having the largest presence, have resulted in a large number of fairness notions and a wide range of approaches that have not been systematically evaluated and compared. In this paper, we contribute a broad analysis of 13 fair classification approaches and additional variants, over their correctness, fairness, efficiency, scalability, robustness to data errors, sensitivity to underlying ML model, data efficiency, and stability using a variety of metrics and real-world datasets. Our analysis highlights novel insights on the impact of different metrics and high-level approach characteristics on different aspects of performance. We also discuss general principles for choosing approaches suitable for different practical settings, and identify areas where data-management-centric solutions are likely to have the most impact.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {232–246},
numpages = {15},
keywords = {algorithmic fairness, classifiers, empirical study},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.1145/3514221.3517867,
author = {Kornaropoulos, Evgenios M. and Ren, Silei and Tamassia, Roberto},
title = {The Price of Tailoring the Index to Your Data: Poisoning Attacks on Learned Index Structures},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3517867},
doi = {10.1145/3514221.3517867},
abstract = {The concept of learned index structures relies on the idea that the input-output functionality of a database index can be viewed as a prediction task and, thus, implemented using a machine learning model instead of traditional algorithmic techniques. This novel angle for a decades-old problem has inspired exciting results at the intersection of machine learning and data structures. However, the advantage of learned index structures, i.e., the ability to adjust to the data at hand via the underlying ML-model, can become a disadvantage from a security perspective as it could be exploited.In this work, we present the first study of data poisoning attacks on learned index structures. Our poisoning approach is different from all previous works since the model under attack is trained on a cumulative distribution function (CDF) and, thus, every injection on the training set has a cascading impact on multiple data values. We formulate the first poisoning attacks on linear regression models trained on a CDF, which is a basic building block of the proposed learned index structures. We generalize our poisoning techniques to attack the advanced two-stage design of learned index structures called recursive model index (RMI), which has been shown to outperform traditional B-Trees. We evaluate our attacks under a variety of parameterizations of the model and show that the error of the RMI increases up to 300x and the error of its second-stage models increases up to 3000x.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {1331–1344},
numpages = {14},
keywords = {attacks, data poisoning, indexing, learned systems},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.1145/3514221.3517886,
author = {Pradhan, Romila and Zhu, Jiongli and Glavic, Boris and Salimi, Babak},
title = {Interpretable Data-Based Explanations for Fairness Debugging},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3517886},
doi = {10.1145/3514221.3517886},
abstract = {A wide variety of fairness metrics and eXplainable Artificial Intelligence (XAI) approaches have been proposed in the literature to identify bias in machine learning models that are used in critical real-life contexts. However, merely reporting on a model's bias or generating explanations using existing XAI techniques is insufficient to locate and eventually mitigate sources of bias. We introduce Gopher, a system that produces compact, interpretable, and causal explanations for bias or unexpected model behavior by identifying coherent subsets of the training data that are root-causes for this behavior. Specifically, we introduce the concept of causal responsibility that quantifies the extent to which intervening on training data by removing or updating subsets of it can resolve the bias. Building on this concept, we develop an efficient approach for generating the top-k patterns that explain model bias by utilizing techniques from the machine learning (ML) community to approximate causal responsibility, and using pruning rules to manage the large search space for patterns. Our experimental evaluation demonstrates the effectiveness of Gopher in generating interpretable explanations for identifying and debugging sources of bias.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {247–261},
numpages = {15},
keywords = {data debugging, explanations, fairness, interpretability},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.1145/3514221.3526168,
author = {Yang, Jingyi and Wu, Peizhi and Cong, Gao and Zhang, Tieying and He, Xiao},
title = {SAM: Database Generation from Query Workloads with Supervised Autoregressive Models},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3526168},
doi = {10.1145/3514221.3526168},
abstract = {With the prevalence of cloud databases, database users are increasingly reliant on the cloud database providers to manage their data. It becomes a challenge for cloud providers to benchmark different DBMS for a specific database instance without having access to the underlying data. One viable solution is to leverage a query workload, which contains a set of queries and the corresponding cardinalities, to generate a synthetic database with similar query performance. Existing methods for database generation with cardinality constraints, however, can only handle very small query workloads due to their high complexity and encounter challenges when handling join queries. In this work, we propose SAM, a supervised deep autoregressive model-based method for database generation from query workloads. First, SAM is able to process large-scale query workloads efficiently as its complexity is linear in the size of the query workload, the number of attributes and the attribute domain size. Second, we develop algorithms to obtain unbiased samples of base relations from the deep autoregressive model and assign join keys in a way that accurately recovers the full outer join of the target database. Comprehensive experiments on real-world datasets demonstrate that SAM is able to efficiently generate a high-fidelity database that not only satisfies the input cardinality constraints, but also is close to the target database.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {1542–1555},
numpages = {14},
keywords = {database generation, supervised autoregressive models},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@article{10.1145/3517189,
author = {Suhail, Sabah and Hussain, Rasheed and Jurdak, Raja and Oracevic, Alma and Salah, Khaled and Hong, Choong Seon and Matulevi\v{c}ius, Raimundas},
title = {Blockchain-Based Digital Twins: Research Trends, Issues, and Future Challenges},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {11s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3517189},
doi = {10.1145/3517189},
abstract = {Industrial processes rely on sensory data for decision-making processes, risk assessment, and performance evaluation. Extracting actionable insights from the collected data calls for an infrastructure that can ensure the dissemination of trustworthy data. For the physical data to be trustworthy, it needs to be cross validated through multiple sensor sources with overlapping fields of view. Cross-validated data can then be stored on the blockchain, to maintain its integrity and trustworthiness. Once trustworthy data is recorded on the blockchain, product lifecycle events can be fed into data-driven systems for process monitoring, diagnostics, and optimized control. In this regard, digital twins (DTs) can be leveraged to draw intelligent conclusions from data by identifying the faults and recommending precautionary measures ahead of critical events. Empowering DTs with blockchain in industrial use cases targets key challenges of disparate data repositories, untrustworthy data dissemination, and the need for predictive maintenance. In this survey, while highlighting the key benefits of using blockchain-based DTs, we present a comprehensive review of the state-of-the-art research results for blockchain-based DTs. Based on the current research trends, we discuss a trustworthy blockchain-based DTs framework. We also highlight the role of artificial intelligence in blockchain-based DTs. Furthermore, we discuss the current and future research and deployment challenges of blockchain-supported DTs that require further investigation.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {240},
numpages = {34},
keywords = {Artificial intelligence (AI), blockchain, cyber-physical systems (CPSs), digital twins (DTs), industrial control systems (ICSs), Internet of Things (IoT), Industry 4.0}
}

@proceedings{10.1145/3517428,
title = {ASSETS '22: Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
year = {2022},
isbn = {9781450392587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@inproceedings{10.1145/3517428.3544819,
author = {Goodman, Steven M. and Buehler, Erin and Clary, Patrick and Coenen, Andy and Donsbach, Aaron and Horne, Tiffanie N. and Lahav, Michal and MacDonald, Robert and Michaels, Rain Breaw and Narayanan, Ajit and Pushkarna, Mahima and Riley, Joel and Santana, Alex and Shi, Lei and Sweeney, Rachel and Weaver, Phil and Yuan, Ann and Morris, Meredith Ringel},
title = {LaMPost: Design and Evaluation of an AI-assisted Email Writing Prototype for Adults with Dyslexia},
year = {2022},
isbn = {9781450392587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517428.3544819},
doi = {10.1145/3517428.3544819},
abstract = {Prior work has explored the writing challenges experienced by people with dyslexia, and the potential for new spelling, grammar, and word retrieval technologies to address these challenges. However, the capabilities for natural language generation demonstrated by the latest class of large language models (LLMs) highlight an opportunity to explore new forms of human-AI writing support tools. In this paper, we introduce LaMPost, a prototype email-writing interface that explores the potential for LLMs to power writing support tools that address the varied needs of people with dyslexia. LaMPost draws from our understanding of these needs and introduces novel AI-powered features for email-writing, including: outlining main ideas, generating a subject line, suggesting changes, rewriting a selection. We evaluated LaMPost with 19 adults with dyslexia, identifying many promising routes for further exploration (including the popularity of the “rewrite” and “subject line” features), but also finding that the current generation of LLMs may not surpass the accuracy and quality thresholds required to meet the needs of writers with dyslexia. Surprisingly, we found that participants’ awareness of the AI had no effect on their perception of the system, nor on their feelings of autonomy, expression, and self-efficacy when writing emails. Our findings yield further insight into the benefits and drawbacks of using LLMs as writing support for adults with dyslexia and provide a foundation to build upon in future research.},
booktitle = {Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {24},
numpages = {18},
keywords = {dyslexia, large language models, writing},
location = {Athens, Greece},
series = {ASSETS '22}
}

@proceedings{10.1145/3520495,
title = {OzCHI '21: Proceedings of the 33rd Australian Conference on Human-Computer Interaction},
year = {2021},
isbn = {9781450395984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@proceedings{10.1145/3523227,
title = {RecSys '22: Proceedings of the 16th ACM Conference on Recommender Systems},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@inproceedings{10.1145/3523227.3546755,
author = {He, Zhankui and Zhao, Handong and Yu, Tong and Kim, Sungchul and Du, Fan and McAuley, Julian},
title = {Bundle MCR: Towards Conversational Bundle Recommendation},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523227.3546755},
doi = {10.1145/3523227.3546755},
abstract = {Bundle recommender systems recommend sets of items (e.g.,&nbsp;pants, shirt, and shoes) to users, but they often suffer from two issues: significant interaction sparsity and a large output space. In this work, we extend multi-round conversational recommendation (MCR) to alleviate these issues. MCR—which uses a conversational paradigm to elicit user interests by asking user preferences on tags (e.g.,&nbsp;categories or attributes) and handling user feedback across multiple rounds—is an emerging recommendation setting to acquire user feedback and narrow down the output space, but has not been explored in the context of bundle recommendation. In this work, we propose a novel recommendation task named Bundle MCR. Unlike traditional bundle recommendation (a bundle-aware user model and bundle generation), Bundle MCR studies how to encode user feedback as conversation states and how to post questions to users. Unlike existing MCR in which agents recommend individual items only, Bundle MCR handles more complicated user feedback on multiple items and related tags. To support this, we first propose a new framework to formulate Bundle MCR as Markov Decision Processes (MDPs) with multiple agents, for user modeling, consultation and feedback handling in bundle contexts. Under this framework, we propose a model architecture, called Bundle Bert (Bunt) to (1)&nbsp;recommend items, (2)&nbsp;post questions and (3) manage conversations based on bundle-aware conversation states. Moreover, to train Bunt effectively, we propose a two-stage training strategy. In an offline pre-training stage, Bunt is trained using multiple cloze tasks to mimic bundle interactions in conversations. Then in an online fine-tuning stage, Bunt agents are enhanced by user interactions. Our experiments on multiple offline datasets as well as the human evaluation show the value of extending MCR frameworks to bundle settings and the effectiveness of our Bunt design.},
booktitle = {Proceedings of the 16th ACM Conference on Recommender Systems},
pages = {288–298},
numpages = {11},
keywords = {bundle recommendation, conversational recommendation, recommender systems},
location = {Seattle, WA, USA},
series = {RecSys '22}
}

@article{10.1145/3523699,
author = {Zhou, Yang and Wang, Fang and Feng, Dan},
title = {A Disk Failure Prediction Method Based on Active Semi-supervised Learning},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1553-3077},
url = {https://doi.org/10.1145/3523699},
doi = {10.1145/3523699},
abstract = {Disk failure has always been a major problem for data centers, leading to data loss. Current disk failure prediction approaches are mostly offline and assume that the disk labels required for training learning models are available and accurate. However, these offline methods are no longer suitable for disk failure prediction tasks in large-scale data centers. Behind this explosive amount of data, most methods do not consider whether it is not easy to get the label values during the training or the obtained label values are not completely accurate. These problems further restrict the development of supervised learning and offline modeling in disk failure prediction. In this article, Active Semi-supervised Learning Disk-failure Prediction (ASLDP), a novel disk failure prediction method is proposed, which uses active learning and semi-supervised learning. According to the characteristics of data in the disk lifecycle, ASLDP carries out active learning for those clear labeled samples, which selects valuable samples with the most significant probability uncertainty and eliminates redundancy. For those samples that are unclearly labeled or unlabeled, ASLDP uses semi-supervised learning for pre-labeled by calculating the conditional values of the samples and enhances the generalization ability by active learning. Compared with several state-of-the-art offline and online learning approaches, the results on four realistic datasets from Backblaze and Baidu demonstrate that ASLDP achieves stable failure detection rates of 80–85\% with low false alarm rates. In addition, we use a dataset from Alibaba to evaluate the generality of ASLDP. Furthermore, ASLDP can overcome the problem of missing sample labels and data redundancy in large data centers, which are not considered and implemented in all offline learning methods for disk failure prediction to the best of our knowledge. Finally, ASLDP can predict the disk failure 4.9 days in advance with lower overhead and latency.},
journal = {ACM Trans. Storage},
month = nov,
articleno = {35},
numpages = {33},
keywords = {Disk failure prediction, semi-supervised learning, active learning, machine learning}
}

@inproceedings{10.1145/3524059.3532374,
author = {Alharthi, Khalid Ayedh and Jhumka, Arshad and Di, Sheng and Cappello, Franck},
title = {Clairvoyant: a log-based transformer-decoder for failure prediction in large-scale systems},
year = {2022},
isbn = {9781450392815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524059.3532374},
doi = {10.1145/3524059.3532374},
abstract = {System failures are expected to be frequent in the exascale era such as current Petascale systems. The health of such systems is usually determined from challenging analysis of large amounts of unstructured \&amp; redundant log data. In this paper, we leverage log data and propose Clairvoyant, a novel self-supervised (i.e., no labels needed) model to predict node failures in HPC systems based on a recent deep learning approach called transformer-decoder and the self-attention mechanism. Clairvoyant predicts node failures by (i) predicting a sequence of log events and then (ii) identifying if a failure is a part of that sequence. We carefully evaluate Clairvoyant and another state-of-the-art failure prediction approach - Desh, based on two real-world system log datasets. Experiments show that Clairvoyant is significantly better: e.g., it can predict node failures with an average Bleu, Rouge, and MCC scores of 0.90, 0.78, and 0.65 respectively while Desh scores only 0.58, 0.58, and 0.25. More importantly, this improvement is achieved with faster training and prediction time, with Clairvoyant being about 25X and 15X faster than Desh respectively.},
booktitle = {Proceedings of the 36th ACM International Conference on Supercomputing},
articleno = {35},
numpages = {14},
keywords = {HPC systems, LSTM, deep learning, failure prediction, logs, transformer-decoder},
location = {Virtual Event},
series = {ICS '22}
}

@proceedings{10.1145/3524458,
title = {GoodIT '22: Proceedings of the 2022 ACM Conference on Information Technology for Social Good},
year = {2022},
isbn = {9781450392846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Limassol, Cyprus}
}

@proceedings{10.1145/3524494,
title = {GAS '22: Proceedings of the 6th International ICSE Workshop on Games and Software Engineering: Engineering Fun, Inspiration, and Motivation},
year = {2022},
isbn = {9781450392938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GAS explores how the growing adoption of gameful elements in various contexts can make the design and development of new technology increasingly complex, and provides a forum to explore these issues that crosscut the software engineering and games development communities. The goal of this one day workshop is to bring together interdisciplinary researchers and practitioners to discuss emerging and new research trends, challenges, costs, and benefits for entertainment games, serious games, and the gamification of traditional (non-game) applications and activities.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3524610.3527893,
author = {Haryono, Stefanus A. and Kang, Hong Jin and Sharma, Abhishek and Sharma, Asankhaya and Santosa, Andrew and Yi, Ang Ming and Lo, David},
title = {Automated identification of libraries from vulnerability data: can we do better?},
year = {2022},
isbn = {9781450392983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524610.3527893},
doi = {10.1145/3524610.3527893},
abstract = {Software engineers depend heavily on software libraries and have to update their dependencies once vulnerabilities are found in them. Software Composition Analysis (SCA) helps developers identify vulnerable libraries used by an application. A key challenge is the identification of libraries related to a given reported vulnerability in the National Vulnerability Database (NVD), which may not explicitly indicate the affected libraries. Recently, researchers have tried to address the problem of identifying the libraries from an NVD report by treating it as an extreme multi-label learning (XML) problem, characterized by its large number of possible labels and severe data sparsity. As input, the NVD report is provided, and as output, a set of relevant libraries is returned.In this work, we evaluated multiple XML techniques. While previous work only evaluated a traditional XML technique, FastXML, we trained four other traditional XML models (DiSMEC, Parabel, Bonsai, ExtremeText) as well as two deep learning-based models (XML-CNN and LightXML). We compared both their effectiveness and the time cost of training and using the models for predictions. We find that other than DiSMEC and XML-CNN, recent XML models outperform the FastXML model by 3\%--10\% in terms of F1-scores on Top-k (k=1,2,3) predictions. Furthermore, we observe significant improvements in both the training and prediction time of these XML models, with Bonsai and Parabel model achieving 627x and 589x faster training time and 12x faster prediction time from the FastXML baseline. We discuss the implications of our experimental results and highlight limitations for future work to address.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension},
pages = {178–189},
numpages = {12},
keywords = {machine learning, multi-label classification, vulnerability report},
location = {Virtual Event},
series = {ICPC '22}
}

@inproceedings{10.1145/3524610.3527897,
author = {He, Junda and Xu, Bowen and Yang, Zhou and Han, DongGyun and Yang, Chengran and Lo, David},
title = {PTM4Tag: sharpening tag recommendation of stack overflow posts with pre-trained models},
year = {2022},
isbn = {9781450392983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524610.3527897},
doi = {10.1145/3524610.3527897},
abstract = {Stack Overflow is often viewed as one of the most influential Software Question \&amp; Answer (SQA) websites, containing millions of programming-related questions and answers. Tags play a critical role in efficiently structuring the contents in Stack Overflow and are vital to support a range of site operations, e.g., querying relevant contents. Poorly selected tags often introduce extra noise and redundancy, which raises problems like tag synonym and tag explosion. Thus, an automated tag recommendation technique that can accurately recommend high-quality tags is desired to alleviate the problems mentioned above.Inspired by the recent success of pre-trained language models (PTMs) in natural language processing (NLP), we present PTM4Tag, a tag recommendation framework for Stack Overflow posts that utilize PTMs with a triplet architecture, which models the components of a post, i.e., Title, Description, and Code with independent language models. To the best of our knowledge, this is the first work that leverages PTMs in the tag recommendation task of SQA sites. We comparatively evaluate the performance of PTM4Tag based on five popular pre-trained models: BERT, RoBERTa, ALBERT, CodeBERT, and BERTOverflow. Our results show that leveraging CodeBERT, a software engineering (SE) domain-specific PTM in PTM4Tag achieves the best performance among the five considered PTMs and outperforms the state-of-the-art Convolutional Neural Network-based approach by a large margin in terms of average Precision@k, Recall@k, and F1-score@k. We conduct an ablation study to quantify the contribution of a post's constituent components (Title, Description, and Code Snippets) to the performance of PTM4Tag. Our results show that Title is the most important in predicting the most relevant tags, and utilizing all the components achieves the best performance.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension},
pages = {1–11},
numpages = {11},
keywords = {pre-trained models, tag recommendation, transformer},
location = {Virtual Event},
series = {ICPC '22}
}

@proceedings{10.1145/3524846,
title = {MET '22: Proceedings of the 7th International Workshop on Metamorphic Testing},
year = {2022},
isbn = {9781450393072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MET series has provided a good forum for discussing novel ideas, new perspectives, new applications, and the state of research, related to or inspired by Metamorphic Testing. This workshop aims to bring together researchers and practitioners in both academia and industry to discuss their research results, experiences and insights into Metamorphic Testing. In this year, we are facing the similar situation of the pandemic as the previous MET in 2020 and 2021, and hence we will run this workshop virtually again.},
location = {Pittsburgh, Pennsylvania}
}

@proceedings{10.1145/3526072,
title = {SBST '22: Proceedings of the 15th Workshop on Search-Based Software Testing},
year = {2022},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Pittsburgh, Pennsylvania}
}

@proceedings{10.1145/3526114,
title = {UIST '22 Adjunct: Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bend, OR, USA}
}

@proceedings{10.1145/3527188,
title = {HAI '22: Proceedings of the 10th International Conference on Human-Agent Interaction},
year = {2022},
isbn = {9781450393232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Christchurch, New Zealand}
}

@inproceedings{10.1145/3527927.3532870,
author = {Nicholas, Molly Jane and Sterman, Sarah and Paulos, Eric},
title = {Creative and Motivational Strategies Used by Expert Creative Practitioners},
year = {2022},
isbn = {9781450393270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527927.3532870},
doi = {10.1145/3527927.3532870},
abstract = {Creative practice often requires persevering through moments of ambiguity, where the outcome of a process is unclear. Creative practitioners intentionally manage this process, for example by developing strategies to break out of creative ruts, or stay motivated through uncertainty. Understanding the way experts engage with and manage these creativity-relevant processes represents a rich source of foundational knowledge for designers of Creativity Support Tools. These strategies represent an opportunity for CST research: to create CSTs that embody emotional and process-focused strategies and techniques. Through interviews with expert practitioners in diverse domains including performance, craft, engineering, and design, we identify four strategies for managing process: Strategic Forgetting, Mode Switching, Embodying Process, and Aestheticizing. Understanding tool- and domain-agnostic creative strategies used by experts to manage their own creative process can inform the design of future CSTs that amplify the benefits of successful strategies and scaffold new techniques.},
booktitle = {Proceedings of the 14th Conference on Creativity and Cognition},
pages = {323–335},
numpages = {13},
keywords = {Creativity Support Tools, Process, Qualitative Methods},
location = {Venice, Italy},
series = {C&amp;C '22}
}

@article{10.1145/3528223.3530133,
author = {Li, Changjian and Pan, Hao and Bousseau, Adrien and Mitra, Niloy J.},
title = {Free2CAD: parsing freehand drawings into CAD commands},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3528223.3530133},
doi = {10.1145/3528223.3530133},
abstract = {CAD modeling, despite being the industry-standard, remains restricted to usage by skilled practitioners due to two key barriers. First, the user must be able to mentally parse a final shape into a valid sequence of supported CAD commands; and second, the user must be sufficiently conversant with CAD software packages to be able to execute the corresponding CAD commands. As a step towards addressing both these challenges, we present Free2CAD wherein the user can simply sketch the final shape and our system parses the input strokes into a sequence of commands expressed in a simplified CAD language. When executed, these commands reproduce the sketched object. Technically, we cast sketch-based CAD modeling as a sequence-to-sequence translation problem, for which we leverage the powerful Transformers neural network architecture. Given the sequence of pen strokes as input, we introduce the new task of grouping strokes that correspond to individual CAD operations. We combine stroke grouping with geometric fitting of the operation parameters, such that intermediate groups are geometrically corrected before being reused, as context, for subsequent steps in the sequence inference. Although trained on synthetically-generated data, we demonstrate that Free2CAD generalizes to sketches created from real-world CAD models as well as to sketches drawn by novice users.Code and data are at https://github.com/Enigma-li/Free2CAD.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {93},
numpages = {16},
keywords = {CAD modeling, procedural modeling, sketch, transformer}
}

@article{10.1145/3528223.3530185,
author = {Shi, Zheng and Bahat, Yuval and Baek, Seung-Hwan and Fu, Qiang and Amata, Hadi and Li, Xiao and Chakravarthula, Praneeth and Heidrich, Wolfgang and Heide, Felix},
title = {Seeing through obstructions with diffractive cloaking},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3528223.3530185},
doi = {10.1145/3528223.3530185},
abstract = {Unwanted camera obstruction can severely degrade captured images, including both scene occluders near the camera and partial occlusions of the camera cover glass. Such occlusions can cause catastrophic failures for various scene understanding tasks such as semantic segmentation, object detection, and depth estimation. Existing camera arrays capture multiple redundant views of a scene to see around thin occlusions. Such multi-camera systems effectively form a large synthetic aperture, which can suppress nearby occluders with a large defocus blur, but significantly increase the overall form factor of the imaging setup. In this work, we propose a monocular single-shot imaging approach that optically cloaks obstructions by emulating a large array. Instead of relying on different camera views, we learn a diffractive optical element (DOE) that performs depth-dependent optical encoding, scattering nearby occlusions while allowing paraxial wavefronts to be focused. We computationally reconstruct unobstructed images from these superposed measurements with a neural network that is trained jointly with the optical layer of the proposed imaging system. We assess the proposed method in simulation and with an experimental prototype, validating that the proposed computational camera is capable of recovering occluded scene information in the presence of severe camera obstruction.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {37},
numpages = {15},
keywords = {computational imaging, end-to-end optics design}
}

@proceedings{10.1145/3528226,
title = {WETSEB '22: Proceedings of the 5th International Workshop on Emerging Trends in Software Engineering for Blockchain},
year = {2022},
isbn = {9781450393317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 5th Workshop on Emerging Trends in Software Engineering for Blockchain gathers the interests of researchers and practitioneers, from both academia and industry, as well as Ph.D. students working in the field of Blockchain technology, to investigate on and to and tackle the new challenges defined by BOSE. The Workshop's goal is to discuss the progresses made by software engineering on the research and on the practical applications of blockchain technologies and smart contracts, focusing on software engineering principles and practices adopted to deal with such new software technology, and for the technologies relying on it.},
location = {Pittsburgh, Pennsylvania}
}

@proceedings{10.1145/3528229,
title = {SESoS '22: Proceedings of the 10th IEEE/ACM International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
year = {2022},
isbn = {9781450393348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SESoS 2022 provides researchers and practitioners with a forum to exchange ideas and experiences, analyze research and development issues, discuss promising solutions, and propose theoretical foundations for development and evolution of complex software-intensive systems, inspiring visions for the future of Software Engineering for Systems-of-Systems (SoS) and Software Ecosystems (SECO), as well as paving the way for a more structured community effort.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3528233.3530747,
author = {Abdal, Rameen and Zhu, Peihao and Femiani, John and Mitra, Niloy and Wonka, Peter},
title = {CLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions},
year = {2022},
isbn = {9781450393379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528233.3530747},
doi = {10.1145/3528233.3530747},
abstract = {The success of StyleGAN has enabled unprecedented semantic editing capabilities, on both synthesized and real images. However, such editing operations are either trained with semantic supervision or annotated manually by users. In another development, the CLIP architecture has been trained with internet-scale loose image and text pairings, and has been shown to be useful in several zero-shot learning settings. In this work, we investigate how to effectively link the pretrained latent spaces of StyleGAN and CLIP, which in turn allows us to automatically extract semantically-labeled edit directions from StyleGAN, finding and naming meaningful edit operations, in a fully unsupervised setup, without additional human guidance. Technically, we propose two novel building blocks; one for discovering interesting CLIP directions and one for semantically labeling arbitrary directions in CLIP latent space. The setup does not assume any pre-determined labels and hence we do not require any additional supervised text/attributes to build the editing framework. We evaluate the effectiveness of the proposed method and demonstrate that extraction of disentangled labeled StyleGAN edit directions is indeed possible, revealing interesting and non-trivial edit directions.},
booktitle = {ACM SIGGRAPH 2022 Conference Proceedings},
articleno = {48},
numpages = {9},
keywords = {Generative Adversarial Networks, Unsupervised Image Editing},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH '22}
}

@inproceedings{10.1145/3531146.3533086,
author = {Luccioni, Alexandra Sasha and Corry, Frances and Sridharan, Hamsini and Ananny, Mike and Schultz, Jason and Crawford, Kate},
title = {A Framework for Deprecating Datasets: Standardizing Documentation, Identification, and Communication},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533086},
doi = {10.1145/3531146.3533086},
abstract = {Datasets are central to training machine learning (ML) models. The ML community has recently made significant improvements to data stewardship and documentation practices across the model development life cycle. However, the act of deprecating, or deleting, datasets has been largely overlooked, and there are currently no standardized approaches for structuring this stage of the dataset life cycle. In this paper, we study the practice of dataset deprecation in ML, identify several cases of datasets that continued to circulate despite having been deprecated, and describe the different technical, legal, ethical, and organizational issues raised by such continuations. We then propose a Dataset Deprecation Framework that includes considerations of risk, mitigation of impact, appeal mechanisms, timeline, post-deprecation protocols, and publication checks that can be adapted and implemented by the ML community. Finally, we propose creating a centralized, sustainable repository system for archiving datasets, tracking dataset modifications or deprecations, and facilitating practices of care and stewardship that can be integrated into research and publication processes.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {199–212},
numpages = {14},
keywords = {data stewardship data management dataset deprecation, datasets},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3531146.3533128,
author = {Ghosh, Avijit and Jagielski, Matthew and Wilson, Christo},
title = {Subverting Fair Image Search with Generative Adversarial Perturbations},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533128},
doi = {10.1145/3531146.3533128},
abstract = {In this work we explore the intersection fairness and robustness in the context of ranking: when a ranking model has been calibrated to achieve some definition of fairness, is it possible for an external adversary to make the ranking model behave unfairly without having access to the model or training data? To investigate this question, we present a case study in which we develop and then attack a state-of-the-art, fairness-aware image search engine using images that have been maliciously modified using a Generative Adversarial Perturbation (GAP) model&nbsp;[75]. These perturbations attempt to cause the fair re-ranking algorithm to unfairly boost the rank of images containing people from an adversary-selected subpopulation. We present results from extensive experiments demonstrating that our attacks can successfully confer significant unfair advantage to people from the majority class relative to fairly-ranked baseline search results. We demonstrate that our attacks are robust across a number of variables, that they have close to zero impact on the relevance of search results, and that they succeed under a strict threat model. Our findings highlight the danger of deploying fair machine learning algorithms in-the-wild when (1) the data necessary to achieve fairness may be adversarially manipulated, and (2) the models themselves are not robust against attacks.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {637–650},
numpages = {14},
keywords = {Adversarial Machine Learning, Demographic Inference, Fair Ranking, Information Retrieval},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3531146.3533183,
author = {Wolfe, Robert and Caliskan, Aylin},
title = {Markedness in Visual Semantic AI},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533183},
doi = {10.1145/3531146.3533183},
abstract = {We evaluate the state-of-the-art multimodal ”visual semantic” model CLIP (”Contrastive Language Image Pretraining”) for biases related to the marking of age, gender, and race or ethnicity. Given the option to label an image as ”a photo of a person” or to select a label denoting race or ethnicity, CLIP chooses the ”person” label 47.9\% of the time for White individuals, compared with 5.0\% or less for individuals who are Black, East Asian, Southeast Asian, Indian, or Latino or Hispanic. The model is also more likely to rank the unmarked ”person” label higher than labels denoting gender for Male individuals (26.7\% of the time) vs. Female individuals (15.2\% of the time). Age also affects whether an individual is marked by the model: Female individuals under the age of 20 are more likely than Male individuals to be marked with a gender label, but less likely to be marked with an age label, while Female individuals over the age of 40 are more likely to be marked based on age than Male individuals. We trace our results back to the CLIP embedding space by examining the self-similarity (mean pairwise cosine similarity) for each social group, where higher self-similarity denotes greater attention directed by CLIP to the shared characteristics (i.e., age, race, or gender) of the social group. The results indicate that, as age increases, the self-similarity of representations of Female individuals increases at a higher rate than for Male individuals, with the disparity most pronounced at the ”more than 70” age range. Six of the ten least self-similar social groups are individuals who are White and Male, while all ten of the most self-similar social groups are individuals under the age of 10 or over the age of 70, and six of the ten are Female individuals. Our results yield evidence that bias in CLIP is intersectional: existing biases of self-similarity and markedness between Male and Female gender groups are further exacerbated when the groups compared are individuals who are White and Male and individuals who are Black and Female. CLIP is an English-language model trained on internet content gathered based on a query list generated from an American website (Wikipedia), and results indicate that CLIP reflects the biases of the language and society which produced this training data.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1269–1279},
numpages = {11},
keywords = {age bias, bias in AI, language-and-vision AI, markedness, multimodal, visual semantics},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3531146.3533185,
author = {Wolfe, Robert and Banaji, Mahzarin R. and Caliskan, Aylin},
title = {Evidence for Hypodescent in Visual Semantic AI},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533185},
doi = {10.1145/3531146.3533185},
abstract = {We examine the state-of-the-art multimodal ”visual semantic” model CLIP (”Contrastive Language Image Pretraining”) for the rule of hypodescent, or one-drop rule, whereby multiracial people are more likely to be assigned a racial or ethnic label corresponding to a minority or disadvantaged racial or ethnic group than to the equivalent majority or advantaged group. A face morphing experiment grounded in psychological research demonstrating hypodescent indicates that, at the midway point of 1,000 series of morphed images, CLIP associates 69.7\% of Black-White female images with a Black text label over a White text label, and similarly prefers Latina (75.8\%) and Asian (89.1\%) text labels at the midway point for Latina-White female and Asian-White female morphs, reflecting hypodescent. Additionally, assessment of the underlying cosine similarities in the model reveals that association with White is correlated with association with ”person,” with Pearson’s ρ as high as 0.82, p &lt; 10− 90 over a 21,000-image morph series, indicating that a White person corresponds to the default representation of a person in CLIP. Finally, we show that the stereotype-congruent pleasantness association of an image correlates with association with the Black text label in CLIP, with Pearson’s ρ = 0.48, p &lt; 10− 90 for 21,000 Black-White multiracial male images, and ρ = 0.41, p &lt; 10− 90 for Black-White multiracial female images. CLIP is trained on English-language text gathered using data collected from an American website (Wikipedia), and our findings demonstrate that CLIP embeds the values of American racial hierarchy, reflecting the implicit and explicit beliefs that are present in human minds. We contextualize these findings within the history of and psychology of hypodescent. Overall, the data suggests that AI supervised using natural language will, unless checked, learn biases that reflect racial hierarchies.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1293–1304},
numpages = {12},
keywords = {bias in AI, hypodescent, language-image models, multimodal, racial bias, visual semantics},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3531146.3533192,
author = {Schramowski, Patrick and Tauchmann, Christopher and Kersting, Kristian},
title = {Can Machines Help Us Answering Question 16 in Datasheets, and In Turn Reflecting on Inappropriate Content?},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533192},
doi = {10.1145/3531146.3533192},
abstract = {This paper contains images and descriptions that are offensive in nature.Large datasets underlying much of current machine learning raise serious issues concerning inappropriate content such as offensive, insulting, threatening, or might otherwise cause anxiety. This calls for increased dataset documentation, e.g., using datasheets. They, among other topics, encourage to reflect on the composition of the datasets. So far, this documentation, however, is done manually and therefore can be tedious and error-prone, especially for large image datasets. Here we ask the arguably “circular” question of whether a machine can help us reflect on inappropriate content, answering Question 16 in Datasheets. To this end, we propose to use the information stored in pre-trained transformer models to assist us in the documentation process. Specifically, prompt-tuning based on a dataset of socio-moral values steers CLIP to identify potentially inappropriate content, therefore reducing human labor. We then document the inappropriate images found using word clouds, based on captions generated using a vision-language model. The documentations of two popular, large-scale computer vision datasets—ImageNet and OpenImages—produced this way suggest that machines can indeed help dataset creators to answer Question 16 on inappropriate image content.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1350–1361},
numpages = {12},
keywords = {Dataset curation, Dataset documentation, Datasets, Datasheets},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3531146.3534637,
author = {Jernite, Yacine and Nguyen, Huu and Biderman, Stella and Rogers, Anna and Masoud, Maraim and Danchev, Valentin and Tan, Samson and Luccioni, Alexandra Sasha and Subramani, Nishant and Johnson, Isaac and Dupont, Gerard and Dodge, Jesse and Lo, Kyle and Talat, Zeerak and Radev, Dragomir and Gokaslan, Aaron and Nikpoor, Somaieh and Henderson, Peter and Bommasani, Rishi and Mitchell, Margaret},
title = {Data Governance in the Age of Large-Scale Data-Driven Language Technology},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534637},
doi = {10.1145/3531146.3534637},
abstract = {The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2206–2222},
numpages = {17},
keywords = {data rights, datasets, language data, technology governance},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3531146.3534642,
author = {Brown, Hannah and Lee, Katherine and Mireshghallah, Fatemehsadat and Shokri, Reza and Tram\`{e}r, Florian},
title = {What Does it Mean for a Language Model to Preserve Privacy?},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534642},
doi = {10.1145/3531146.3534642},
abstract = {Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2280–2292},
numpages = {13},
keywords = {Data Sanitization, Differential Privacy, Natural Language Processing, Privacy},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3532106.3533449,
author = {Benabdallah, Gabrielle and Alexander, Ashten and Ghosh, Sourojit and Glogovac-Smith, Chariell and Jacoby, Lacey and Lustig, Caitlin and Nguyen, Anh and Parkhurst, Anna and Reyes, Kathryn and Tan, Neilly H. and Wolcher, Edward and Psarra, Afroditi and Rosner, Daniela},
title = {Slanted Speculations: Material Encounters with Algorithmic Bias},
year = {2022},
isbn = {9781450393584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532106.3533449},
doi = {10.1145/3532106.3533449},
abstract = {Over the past few years, AI bias has become a central concern within design and computing fields. But as the concept of bias has grown in visibility, its meaning and form have become harder to grasp. To help designers realize bias, we take inspiration from textile bias (the skew of woven material) and examine the topic across its myriad forms: visual, textual, and tactile. By introducing a slanted experience of material and therefore of reality, we explore the translation of fraught machine learning algorithms into personal and probing artifacts. In this pictorial, we present nine pieces that materialize complex relationships with machine learning; ground these relationships in the present and the personal; and point to generative ways of engaging with biased systems around us.},
booktitle = {Proceedings of the 2022 ACM Designing Interactive Systems Conference},
pages = {85–99},
numpages = {15},
keywords = {Algorithmic bias, arts, design practice, machine learning, materiality, speculative design},
location = {Virtual Event, Australia},
series = {DIS '22}
}

@proceedings{10.1145/3533271,
title = {ICAIF '22: Proceedings of the Third ACM International Conference on AI in Finance},
year = {2022},
isbn = {9781450393768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New York, NY, USA}
}

@inproceedings{10.1145/3534678.3539145,
author = {Nigenda, David and Karnin, Zohar and Zafar, Muhammad Bilal and Ramesha, Raghu and Tan, Alan and Donini, Michele and Kenthapadi, Krishnaram},
title = {Amazon SageMaker Model Monitor: A System for Real-Time Insights into Deployed Machine Learning Models},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539145},
doi = {10.1145/3534678.3539145},
abstract = {With the increasing adoption of machine learning (ML) models and systems in high-stakes settings across different industries, guaranteeing a model's performance after deployment has become crucial. Monitoring models in production is a critical aspect of ensuring their continued performance and reliability. We present Amazon SageMaker Model Monitor, a fully managed service that continuously monitors the quality of machine learning models hosted on Amazon SageMaker. Our system automatically detects data, concept, bias, and feature attribution drift in models in real-time and provides alerts so that model owners can take corrective actions and thereby maintain high quality models. We describe the key requirements obtained from customers, system design and architecture, and methodology for detecting different types of drift. Further, we provide quantitative evaluations followed by use cases, insights, and lessons learned from more than two years of production deployment.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3671–3681},
numpages = {11},
keywords = {MLOps, amazon sagemaker, bias \&amp; fairness in ML, drift detection, feature attribution, real-time model monitoring},
location = {Washington DC, USA},
series = {KDD '22}
}

@proceedings{10.1145/3536220,
title = {ICMI '22 Companion: Companion Publication of the 2022 International Conference on Multimodal Interaction},
year = {2022},
isbn = {9781450393898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bengaluru, India}
}

@proceedings{10.1145/3536221,
title = {ICMI '22: Proceedings of the 2022 International Conference on Multimodal Interaction},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bengaluru, India}
}

@inproceedings{10.1145/3536221.3558058,
author = {Yoon, Youngwoo and Wolfert, Pieter and Kucherenko, Taras and Viegas, Carla and Nikolov, Teodor and Tsakov, Mihail and Henter, Gustav Eje},
title = {The GENEA Challenge 2022: A large evaluation of data-driven co-speech gesture generation},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536221.3558058},
doi = {10.1145/3536221.3558058},
abstract = {This paper reports on the second GENEA Challenge to benchmark data-driven automatic co-speech gesture generation. Participating teams used the same speech and motion dataset to build gesture-generation systems. Motion generated by all these systems was rendered to video using a standardised visualisation pipeline and evaluated in several large, crowdsourced user studies. Unlike when comparing different research papers, differences in results are here only due to differences between methods, enabling direct comparison between systems. This year’s dataset was based on 18 hours of full-body motion capture, including fingers, of different persons engaging in dyadic conversation. Ten teams participated in the challenge across two tiers: full-body and upper-body gesticulation. For each tier we evaluated both the human-likeness of the gesture motion and its appropriateness for the specific speech signal. Our evaluations decouple human-likeness from gesture appropriateness, which previously was a major challenge in the field. The evaluation results are a revolution, and a revelation. Some synthetic conditions are rated as significantly more human-like than human motion capture. To the best of our knowledge, this has never been shown before on a high-fidelity avatar. On the other hand, all synthetic motion is found to be vastly less appropriate for the speech than the original motion-capture recordings.},
booktitle = {Proceedings of the 2022 International Conference on Multimodal Interaction},
pages = {736–747},
numpages = {12},
keywords = {embodied conversational agents, evaluation paradigms, gesture generation},
location = {Bengaluru, India},
series = {ICMI '22}
}

@article{10.1145/3538707,
author = {Wang, Zhibo and Ma, Jingjing and Wang, Xue and Hu, Jiahui and Qin, Zhan and Ren, Kui},
title = {Threats to Training: A Survey of Poisoning Attacks and Defenses on Machine Learning Systems},
year = {2022},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3538707},
doi = {10.1145/3538707},
abstract = {Machine learning (ML) has been universally adopted for automated decisions in a variety of fields, including recognition and classification applications, recommendation systems, natural language processing, and so on. However, in light of high expenses on training data and computing resources, recent years have witnessed a rapid increase in outsourced ML training, either partially or completely, which provides vulnerabilities for adversaries to exploit. A prime threat in training phase is called poisoning attack, where adversaries strive to subvert the behavior of machine learning systems by poisoning training data or other means of interference. Although a growing number of relevant studies have been proposed, the research among poisoning attack is still overly scattered, with each paper focusing on a particular task in a specific domain. In this survey, we summarize and categorize existing attack methods and corresponding defenses, as well as demonstrate compelling application scenarios, thus providing a unified framework to analyze poisoning attacks. Besides, we also discuss the main limitations of current works, along with the corresponding future directions to facilitate further researches. Our ultimate motivation is to provide a comprehensive and self-contained survey of this growing field of research and lay the foundation for a more standardized approach to reproducible studies.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {134},
numpages = {36},
keywords = {Poisoning attacks, adversarial machine learning, AI security}
}

@proceedings{10.1145/3538712,
title = {SSDBM '22: Proceedings of the 34th International Conference on Scientific and Statistical Database Management},
year = {2022},
isbn = {9781450396677},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Copenhagen, Denmark}
}

@proceedings{10.1145/3538969,
title = {ARES '22: Proceedings of the 17th International Conference on Availability, Reliability and Security},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@inproceedings{10.1145/3539597.3570397,
author = {Ge, Suyu and Huang, Jiaxin and Meng, Yu and Han, Jiawei},
title = {FineSum: Target-Oriented, Fine-Grained Opinion Summarization},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3570397},
doi = {10.1145/3539597.3570397},
abstract = {Target-oriented opinion summarization is to profile a target by extracting user opinions from multiple related documents. Instead of simply mining opinion ratings on a target (e.g., a restaurant) or on multiple aspects (e.g., food, service) of a target, it is desirable to go deeper, to mine opinion on fine-grained sub-aspects (e.g., fish). However, it is expensive to obtain high-quality annotations at such fine-grained scale. This leads to our proposal of a new framework, FineSum, which advances the frontier of opinion analysis in three aspects: (1) minimal supervision, where no document-summary pairs are provided, only aspect names and a few aspect/sentiment keywords are available; (2) fine-grained opinion analysis, where sentiment analysis drills down to a specific subject or characteristic within each general aspect; and (3) phrase-based summarization, where short phrases are taken as basic units for summarization, and semantically coherent phrases are gathered to improve the consistency and comprehensiveness of summary. Given a large corpus with no annotation, FineSum first automatically identifies potential spans of opinion phrases, and further reduces the noise in identification results using aspect and sentiment classifiers. It then constructs multiple fine-grained opinion clusters under each aspect and sentiment. Each cluster expresses uniform opinions towards certain sub-aspects (e.g., "fish" in "food" aspect) or characteristics (e.g., "Mexican" in "food" aspect). To accomplish this, we train a spherical word embedding space to explicitly represent different aspects and sentiments. We then distill the knowledge from embedding to a contextualized phrase classifier, and perform clustering using the contextualized opinion-aware phrase embedding. Both automatic evaluations on the benchmark and quantitative human evaluation validate the effectiveness of our approach.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {1093–1101},
numpages = {9},
keywords = {aspect extraction, opinion summarization, sentiment analysis},
location = {Singapore, Singapore},
series = {WSDM '23}
}

@proceedings{10.1145/3539637,
title = {WebMedia '22: Proceedings of the Brazilian Symposium on Multimedia and the Web},
year = {2022},
isbn = {9781450394093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Curitiba, Brazil}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3540250.3549128,
author = {Nong, Yu and Ou, Yuzhe and Pradel, Michael and Chen, Feng and Cai, Haipeng},
title = {Generating realistic vulnerabilities via neural code editing: an empirical study},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549128},
doi = {10.1145/3540250.3549128},
abstract = {The availability of large-scale, realistic vulnerability datasets is essential both for benchmarking existing techniques and for developing effective new data-driven approaches for software security. Yet such datasets are critically lacking. A promising solution is to generate such datasets by injecting vulnerabilities into real-world programs, which are richly available. Thus, in this paper, we explore the feasibility of vulnerability injection through neural code editing. With a synthetic dataset and a real-world one, we investigate the potential and gaps of three state-of-the-art neural code editors for vulnerability injection. We find that the studied editors have critical limitations on the real-world dataset, where the best accuracy is only 10.03\%, versus 79.40\% on the synthetic dataset. While the graph-based editors are more effective (successfully injecting vulnerabilities in up to 34.93\% of real-world testing samples) than the sequence-based one (0 success), they still suffer from complex code structures and fall short for long edits due to their insufficient designs of the preprocessing and deep learning (DL) models. We reveal the promise of neural code editing for generating realistic vulnerable samples, as they help boost the effectiveness of DL-based vulnerability detectors by up to 49.51\% in terms of F1 score. We also provide insights into the gaps in current editors (e.g., they are good at deleting but not at replacing code) and actionable suggestions for addressing them (e.g., designing effective editing primitives).},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1097–1109},
numpages = {13},
keywords = {benchmarking, data augmentation, data generation, datasets, deep learning, software vulnerability, vulnerability detection},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@proceedings{10.1145/3542929,
title = {SoCC '22: Proceedings of the 13th Symposium on Cloud Computing},
year = {2022},
isbn = {9781450394147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SoCC 2022 is the thirteenth annual ACMSymposium on Cloud Computing, the premier conference on cloud computing. It brings together researchers, software developers, end-users, and practitioners interested in wide-ranging aspects of cloud computing, and it is the only conference co-sponsored by the ACM Special Interest Groups on Management of Data (SIGMOD) and on Operating Systems (SIGOPS).},
location = {San Francisco, California}
}

@proceedings{10.1145/3543758,
title = {MuC '22: Proceedings of Mensch und Computer 2022},
year = {2022},
isbn = {9781450396905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Darmstadt, Germany}
}

@proceedings{10.1145/3543829,
title = {CUI '22: Proceedings of the 4th Conference on Conversational User Interfaces},
year = {2022},
isbn = {9781450397391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Glasgow, United Kingdom}
}

@proceedings{10.1145/3544902,
title = {ESEM '22: Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Helsinki, Finland}
}

@proceedings{10.1145/3545008,
title = {ICPP '22: Proceedings of the 51st International Conference on Parallel Processing},
year = {2022},
isbn = {9781450397339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bordeaux, France}
}

@proceedings{10.1145/3545258,
title = {Internetware '22: Proceedings of the 13th Asia-Pacific Symposium on Internetware},
year = {2022},
isbn = {9781450397803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hohhot, China}
}

@proceedings{10.1145/3545822,
title = {ICMSSP '22: Proceedings of the 2022 7th International Conference on Multimedia Systems and Signal Processing},
year = {2022},
isbn = {9781450396424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shenzhen, China}
}

@proceedings{10.1145/3546000,
title = {HP3C '22: Proceedings of the 6th International Conference on High Performance Compilation, Computing and Communications},
year = {2022},
isbn = {9781450396295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Jilin, China}
}

@proceedings{10.1145/3546155,
title = {NordiCHI '22: Nordic Human-Computer Interaction Conference},
year = {2022},
isbn = {9781450396998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Aarhus, Denmark}
}

@inproceedings{10.1145/3546155.3546643,
author = {H\r{a}kansson, Maria and Renstr\"{o}m, Sara and L\"{o}\"{o}f, Jenny and Sall Vessel\'{e}nyi, L\'{a}szl\'{o} and Jonasson Tolv, Julia},
title = {”Do they pass the woman test?”: Navigating and negotiating the gendering of residential solar panels},
year = {2022},
isbn = {9781450396998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546155.3546643},
doi = {10.1145/3546155.3546643},
abstract = {Residential solar panels are increasingly popular, yet women are largely invisible as customers and users. This creates barriers for reaching gender equality and climate goals where increased renewable energy is key. We present results from a norm-critical study drawing on 10 interviews with solar industry representatives and focus groups with 28 women, either owning solar panels or in the process of buying. The study aims to critically analyze current gender norms related to technology, market, and use, as well as to identify difficulties for women's solar panel engagement. The study shows how women at different touchpoints in the process of buying and having solar panels both navigate and negotiate an ongoing gendering of this technology, despite the industry attempts to present solar panels as gender neutral. While the study focuses on residential solar panels, the contribution is relevant for wider HCI, e.g. work related to smart home technologies.},
booktitle = {Nordic Human-Computer Interaction Conference},
articleno = {47},
numpages = {12},
keywords = {Feminist HCI, Gender, Norm-critical design, Norms, Solar energy, Sustainability},
location = {Aarhus, Denmark},
series = {NordiCHI '22}
}

@proceedings{10.1145/3546157,
title = {ICISDM '22: Proceedings of the 6th International Conference on Information System and Data Mining},
year = {2022},
isbn = {9781450396257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Silicon Valley, CA, USA}
}

@proceedings{10.1145/3546607,
title = {ICVARS '22: Proceedings of the 2022 6th International Conference on Virtual and Augmented Reality Simulations},
year = {2022},
isbn = {9781450387330},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Brisbane, QLD, Australia}
}

@proceedings{10.1145/3546790,
title = {ICONS '22: Proceedings of the International Conference on Neuromorphic Systems 2022},
year = {2022},
isbn = {9781450397896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Knoxville, TN, USA}
}

@article{10.1145/3546872,
author = {Liu, Haochen and Wang, Yiqi and Fan, Wenqi and Liu, Xiaorui and Li, Yaxin and Jain, Shaili and Liu, Yunhao and Jain, Anil and Tang, Jiliang},
title = {Trustworthy AI: A Computational Perspective},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3546872},
doi = {10.1145/3546872},
abstract = {In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone’s daily life and profoundly altering the course of human society. The intention behind developing AI was and is to benefit humans by reducing labor, increasing everyday conveniences, and promoting social good. However, recent research and AI applications indicate that AI can cause unintentional harm to humans by, for example, making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against a group or groups. Consequently, trustworthy AI has recently garnered increased attention regarding the need to avoid the adverse effects that AI could bring to people, so people can fully trust and live in harmony with AI technologies. A tremendous amount of research on trustworthy AI has been conducted and witnessed in recent years. In this survey, we present a comprehensive appraisal of trustworthy AI from a computational perspective to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex subject, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety \&amp; Robustness, (ii) Nondiscrimination \&amp; Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability \&amp; Auditability, and (vi) Environmental Well-being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {4},
numpages = {59},
keywords = {Artificial intelligence, Robustness, Fairness, Explainability, Privacy, Accountability, Environmental Well-being}
}

@proceedings{10.1145/3547522,
title = {NordiCHI '22 Adjunct: Adjunct Proceedings of the 2022 Nordic Human-Computer Interaction Conference},
year = {2022},
isbn = {9781450394482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Aarhus, Denmark}
}

@inproceedings{10.1145/3548606.3560581,
author = {Cretu, Ana-Maria and Houssiau, Florimond and Cully, Antoine and de Montjoye, Yves-Alexandre},
title = {QuerySnout: Automating the Discovery of Attribute Inference Attacks against Query-Based Systems},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3560581},
doi = {10.1145/3548606.3560581},
abstract = {Although query-based systems (QBS) have become one of the main solutions to share data anonymously, building QBSes that robustly protect the privacy of individuals contributing to the dataset is a hard problem. Theoretical solutions relying on differential privacy guarantees are difficult to implement correctly with reasonable accuracy, while ad-hoc solutions might contain unknown vulnerabilities. Evaluating the privacy provided by QBSes must thus be done by evaluating the accuracy of a wide range of privacy attacks. However, existing attacks against QBSes require time and expertise to develop, need to be manually tailored to the specific systems attacked, and are limited in scope. In this paper, we develop QuerySnout, the first method to automatically discover vulnerabilities in query-based systems. QuerySnout takes as input a target record and the QBS as a black box, analyzes its behavior on one or more datasets, and outputs a multiset of queries together with a rule to combine answers to them in order to reveal the sensitive attribute of the target record. QuerySnout uses evolutionary search techniques based on a novel mutation operator to find a multiset of queries susceptible to lead to an attack, and a machine learning classifier to infer the sensitive attribute from answers to the queries selected. We showcase the versatility of QuerySnout by applying it to two attack scenarios (assuming access to either the private dataset or to a different dataset from the same distribution), three real-world datasets, and a variety of protection mechanisms. We show the attacks found by QuerySnout to consistently equate or outperform, sometimes by a large margin, the best attacks from the literature. We finally show how QuerySnout can be extended to QBSes that require a budget, and apply QuerySnout to a simple QBS based on the Laplace mechanism. Taken together, our results show how powerful and accurate attacks against QBSes can already be found by an automated system, allowing for highly complex QBSes to be automatically tested "at the pressing of a button". We believe this line of research to be crucial to improve the robustness of systems providing privacy-preserving access to personal data in theory and in practice.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {623–637},
numpages = {15},
keywords = {anonymization, privacy attacks, query-based systems},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@proceedings{10.1145/3548608,
title = {ICCIR '22: Proceedings of the 2022 2nd International Conference on Control and Intelligent Robotics},
year = {2022},
isbn = {9781450397179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanjing, China}
}

@proceedings{10.1145/3548659,
title = {A-TEST 2022: Proceedings of the 13th International Workshop on Automating Test Case Design, Selection and Evaluation},
year = {2022},
isbn = {9781450394529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 13th edition of the International Workshop on Automating Test Case Design, Selection and Evaluation (A-TEST 2022), co-located with and organized at ESEC/FSE 2022 during two days November 17-18, 2022 in Singapore. The A-TEST workshop aims to provide a venue for researchers and industry members alike to exchange and discuss trending views, ideas, state of the art, work in progress, and scientific results on automated testing.},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3548785,
title = {IDEAS '22: Proceedings of the 26th International Database Engineered Applications Symposium},
year = {2022},
isbn = {9781450397094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Budapest, Hungary}
}

@proceedings{10.1145/3548814,
title = {SAP '22: ACM Symposium on Applied Perception 2022},
year = {2022},
isbn = {9781450394550},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {TBC, USA}
}

@proceedings{10.1145/3549034,
title = {MaLTeSQuE 2022: Proceedings of the 6th International Workshop on Machine Learning Techniques for Software Quality Evaluation},
year = {2022},
isbn = {9781450394567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 6th edition of the workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE 2022), held in Singapore, on November 18th, 2022, co-located with the 30th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). MaLTeSQuE received a total of six submissions from all over the world, from which five papers were included in the program. The program also features two keynotes, by Yuriy Brun and Mike Papadakis, on the promises, dangers, and best practices of working at the intersection of machine learning and software engineering.},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3549035,
title = {MSR4P&amp;S 2022: Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security},
year = {2022},
isbn = {9781450394574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the Program Committee, we are pleased to present the proceedings of the 1st International Workshop on Mining Software Repositories for Privacy and Security (MSR4P&amp;S 2022). MSR4P&amp;S is co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE). This year, because of the Covid-19 pandemic, MSR4P&amp;S (as part of ESEC/FSE) is held virtually with an adapted program that will bring together international researchers to exchange ideas, share experiences, investigate problems, and propose promising solutions concerning the application of Mining Software Repositories (MSR) to investigate the different stages of privacy and security. The workshop topics cover a wide range of MSR applications for cybersecurity research, including empirical and mixed-method approaches, as well as datasets and tools.},
location = {Singapore, Singapore}
}

@article{10.1145/3549493,
author = {Volkmar, Georg and Alexandrovsky, Dmitry and Eilks, Asmus Eike and Queck, Dirk and Herrlich, Marc and Malaka, Rainer},
title = {Effects of PCG on Creativity in Playful City-Building Environments in VR},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3549493},
doi = {10.1145/3549493},
abstract = {The use of procedural content generation (PCG) in the context of video games has increased over the years as it provides an economical way to generate game content whilst enhancing their variety and replayability. For city-building games, this approach is often utilized to predefine map layouts, terrains, or cityscapes for the player. One core aspect of facilitating enjoyment in these games comes from creative expressivity. PCG, in this context, may support creativity by lowering the technical complexity for content creation, or it may hinder creativity by taking away control and freedom from the user. To examine these potential effects, this paper investigates if PCG has an impact on players' creativity in the context of VR city-building games. We present a VR prototype that provides varying degrees of procedural content: No PCG, terrain generation, city generation, and full (city + terrain) generation. In a remote user study, these conditions were compared regarding their capability to support creativity. Statistical tests for equivalence revealed that the presence of PCG did not affect creativity in any way. Our work suggests that PCG can be a useful integration into city-building games without notably decreasing players' ability to express themselves creatively.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {230},
numpages = {20},
keywords = {city-building games, creativity, procedural content generation, virtual reality}
}

@article{10.1145/3549497,
author = {Xiao, Ruowei and Jung, Sangwon and Buruk, Oguz 'Oz' and Hamari, Juho},
title = {Exploring the Player Experiences of Wearable Gaming Interfaces: A User Elicitation Study},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3549497},
doi = {10.1145/3549497},
abstract = {The design and development of playful wearable devices is a challenging and complicated problem. It entails not only multidisciplinary expertise but also a comprehensive understanding of player experience. There is a scarcity of evidence-based studies in current state-of-art literature that investigate general design practices and provide pragmatic design implications and suggestions based on solid user-centered research. To bridge the gap, we developed five experience prototypes based on the speculative design concepts from previous studies, and a Wizard of Oz experiment was conducted to elicit end users' feedback regarding general gaming experience as well as specific design themes in different gaming scenarios. The user experiment results were analyzed qualitatively following a rigorous thematic analysis, generating five major design implications as output. We believe this study will offer forward-looking insights to designers, developers and the research community, facilitating future work in this field.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {234},
numpages = {26},
keywords = {Wizard-Of-Oz, experience prototyping, game controllers, game design, gaming devices, thematic analysis, user-centered design, wearable}
}

@proceedings{10.1145/3549555,
title = {CBMI '22: Proceedings of the 19th International Conference on Content-based Multimedia Indexing},
year = {2022},
isbn = {9781450397209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Graz, Austria}
}

@proceedings{10.1145/3549737,
title = {SETN '22: Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Corfu, Greece}
}

@proceedings{10.1145/3549865,
title = {Interacci\'{o}n '22: Proceedings of the XXII International Conference on Human Computer Interaction},
year = {2022},
isbn = {9781450397025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Teruel, Spain}
}

@proceedings{10.1145/3550082,
title = {SA '22: SIGGRAPH Asia 2022 Posters},
year = {2022},
isbn = {9781450394628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Daegu, Republic of Korea}
}

@proceedings{10.1145/3550340,
title = {SA '22: SIGGRAPH Asia 2022 Technical Communications},
year = {2022},
isbn = {9781450394659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Daegu, Republic of Korea}
}

@article{10.1145/3550454.3555488,
author = {H\"{a}hnlein, Felix and Li, Changjian and Mitra, Niloy J. and Bousseau, Adrien},
title = {CAD2Sketch: Generating Concept Sketches from CAD Sequences},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3550454.3555488},
doi = {10.1145/3550454.3555488},
abstract = {Concept sketches are ubiquitous in industrial design, as they allow designers to quickly depict imaginary 3D objects. To construct their sketches with accurate perspective, designers rely on longstanding drawing techniques, including the use of auxiliary construction lines to identify midpoints of perspective planes, to align points vertically and horizontally, and to project planar curves from one perspective plane to another. We present a method to synthesize such construction lines from CAD sequences. Importantly, our method balances the presence of construction lines with overall clutter, such that the resulting sketch is both well-constructed and readable, as professional designers are trained to do. In addition to generating sketches that are visually similar to real ones, we apply our method to synthesize a large quantity of paired sketches and normal maps, and show that the resulting dataset can be used to train a neural network to infer normals from concept sketches.1},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {279},
numpages = {18},
keywords = {computer-aided design, industrial design, line drawing, non-photorealistic rendering, sketch-based modeling, sketching}
}

@inproceedings{10.1145/3550469.3555424,
author = {Lambourne, Joseph George and Willis, Karl and Jayaraman, Pradeep Kumar and Zhang, Longfei and Sanghi, Aditya and Malekshan, Kamal Rahimi},
title = {Reconstructing editable prismatic CAD from rounded voxel models},
year = {2022},
isbn = {9781450394703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550469.3555424},
doi = {10.1145/3550469.3555424},
abstract = {Reverse Engineering a CAD shape from other representations is an important geometric processing step for many downstream applications. In this work, we introduce a novel neural network architecture to solve this challenging task and approximate a smoothed signed distance function with an editable, constrained, prismatic CAD model. During training, our method reconstructs the input geometry in the voxel space by decomposing the shape into a series of 2D profile images and 1D envelope functions. These can then be recombined in a differentiable way allowing a geometric loss function to be defined. During inference, we obtain the CAD data by first searching a database of 2D constrained sketches to find curves which approximate the profile images, then extrude them and use Boolean operations to build the final CAD model. Our method approximates the target shape more closely than other methods and outputs highly editable constrained parametric sketches which are compatible with existing CAD software.},
booktitle = {SIGGRAPH Asia 2022 Conference Papers},
articleno = {53},
numpages = {9},
keywords = {CAD, Computer aided design, reconstruction, reverse engineering, voxel},
location = {Daegu, Republic of Korea},
series = {SA '22}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rochester, MI, USA}
}

@inproceedings{10.1145/3551349.3556925,
author = {Imran, Mia Mohammad and Jain, Yashasvi and Chatterjee, Preetha and Damevski, Kostadin},
title = {Data Augmentation for Improving Emotion Recognition in Software Engineering Communication},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556925},
doi = {10.1145/3551349.3556925},
abstract = {Emotions (e.g., Joy, Anger) are prevalent in daily software engineering (SE) activities, and are known to be significant indicators of work productivity (e.g., bug fixing efficiency). Recent studies have shown that directly applying general purpose emotion classification tools to SE corpora is not effective. Even within the SE domain, tool performance degrades significantly when trained on one communication channel and evaluated on another (e.g, StackOverflow vs. GitHub comments). Retraining a tool with channel-specific data takes significant effort since manually annotating a large dataset of ground truth data is expensive. In this paper, we address this data scarcity problem by automatically creating new training data using a data augmentation technique. Based on an analysis of the types of errors made by popular SE-specific emotion recognition tools, we specifically target our data augmentation strategy in order to improve the performance of emotion recognition. Our results show an average improvement of 9.3\% in micro F1-Score for three existing emotion classification tools (ESEM-E, EMTk, SEntiMoji) when trained with our best augmentation strategy.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {29},
numpages = {13},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3551349.3560422,
author = {Li, Xueyang and Liu, Shangqing and Feng, Ruitao and Meng, Guozhu and Xie, Xiaofei and Chen, Kai and Liu, Yang},
title = {TransRepair: Context-aware Program Repair for Compilation Errors},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560422},
doi = {10.1145/3551349.3560422},
abstract = {Automatically fixing compilation errors can greatly raise the productivity of software development, by guiding the novice or AI programmers to write and debug code. Recently, learning-based program repair has gained extensive attention and became the state-of-the-art in practice. But it still leaves plenty of space for improvement. In this paper, we propose an end-to-end solution&nbsp;TransRepair to locate the error lines and create the correct substitute for a C program simultaneously. Superior to the counterpart, our approach takes into account the context of erroneous code and diagnostic compilation feedback. Then we devise a Transformer-based neural network to learn the ways of repair from the erroneous code as well as its context and the diagnostic feedback. To increase the effectiveness of TransRepair, we summarize 5 types and 74 fine-grained sub-types of compilations errors from two real-world program datasets and the Internet. Then a program corruption technique is developed to synthesize a large dataset with 1,821,275 erroneous C programs. Through the extensive experiments, we demonstrate that TransRepair outperforms the state-of-the-art in both single repair accuracy and full repair accuracy. Further analysis sheds light on the strengths and weaknesses in the contemporary solutions for future improvement.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {108},
numpages = {13},
keywords = {Program repair, compilation error, context-aware, deep learning},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@proceedings{10.1145/3551624,
title = {EAAMO '22: Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Arlington, VA, USA}
}

@proceedings{10.1145/3551626,
title = {MMAsia '22: Proceedings of the 4th ACM International Conference on Multimedia in Asia},
year = {2022},
isbn = {9781450394789},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The ACM Multimedia Asia conference series was established in 2019 by putting together the long-lasting experience of former PCM and ICIMCS, which both have good history as well as attending experiences. Officially sponsored by ACM SIGMM, MM Asia is a newly established international conference to showcase the scientific achievements and industrial innovations in the multimedia field. Its mission is to illuminate the state of the art in multimedia computing by bringing together researchers and practitioners in this field.},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3552327,
title = {ECCE '22: Proceedings of the 33rd European Conference on Cognitive Ergonomics},
year = {2022},
isbn = {9781450398084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kaiserslautern, Germany}
}

@proceedings{10.1145/3554944,
title = {VINCI '22: Proceedings of the 15th International Symposium on Visual Information Communication and Interaction},
year = {2022},
isbn = {9781450398060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chur, Switzerland}
}

@proceedings{10.1145/3555050,
title = {CoNEXT '22: Proceedings of the 18th International Conference on emerging Networking EXperiments and Technologies},
year = {2022},
isbn = {9781450395083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {CoNEXT is a premier and highly selective venue in computer networking. This year's exciting technical program helps us better understand and improve the performance, reliability and security of networks in all layers.},
location = {Roma, Italy}
}

@article{10.1145/3555097,
author = {Pinter, Anthony T. and Brubaker, Jed R.},
title = {Behold the Once and Future Me: Online Identity After the End of a Romantic Relationship},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555097},
doi = {10.1145/3555097},
abstract = {After a romantic relationship ends, individuals are left to deal with the digital remnants of the relationship. These possessions and connections pose difficulties for users -- they are identity markers of an identity that one may no longer want legible to their online audiences. Further, they can cause upsetting moments that might impede moving on from the break-up.Through interviews with 11 women who had had a recent break-up, this empirical study examined how people managed their online identity after their break-up. We found that people took different actions towards their possessions and connections in service of creating a post-break-up identity. Using Brubaker \&amp; Hayes's (2011) representational framework, we find users attempting to deal with connections as if they were possessions, creating tensions that our current systems are ill-suited to address. Turning to Hogan's (2010) framework of the online identity 'exhibition', we see users creating exhibitions for an audience of one -- themselves -- while also making decisions about who is allowed to see their public-facing exhibitions. We conclude by arguing that existing tools are ill-designed to support competing desires to present authentic past and future online identities and offer design suggestions for consideration.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {372},
numpages = {35},
keywords = {digital identity, empirical work, life transitions, relationship dissolution, social media}
}

@article{10.1145/3555116,
author = {Rigaud, Clara and Bailly, Gilles and Avellino, Ignacio and Jansen, Yvonne},
title = {Exploring Capturing Approaches in Shared Fabrication Workshops: Current Practice and Opportunities},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555116},
doi = {10.1145/3555116},
abstract = {Capturing content of fabrication activities is the first step for producing knowledge resources and an integral part of maker culture. As a secondary task, it conflicts though with the fabrication activity, and thus it is often forgotten and knowledge resources end up incomplete. In this article, we investigate different dimensions of content capture for knowledge resources in fabrication workshops. Based on past work in this area, we first propose a framework through which we identify two research directions to investigate. From these, we derive three dimensions to explore in more depth: The number of capturing devices, their feature variety and the degree of automation of each feature. We then explore the design space resulting from these three dimensions with the help of a design concept and an online survey study (N=66). Results show (1) a variety of needs and preferences justifying feature variety and multiplicity, (2) challenges in defining the right degree of manual and automatic control, and (3) the socio-technical impact of cameras in a shared space regarding privacy and ethics. We conclude with discussions on the benefits and vulnerabilities of equipping fabrication workshop with distributed camera-based capturing devices and offer opportunities for design.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {391},
numpages = {33},
keywords = {capture, fabrication workshop, knowledge resources, makerspace}
}

@article{10.1145/3555126,
author = {Walker, Dawn and Ishikawa Sutton, Mai and Vira, Udit and Lau, Benedict},
title = {Commoning for Fun and Profit: Experimental Publishing on the Decentralized Web},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555126},
doi = {10.1145/3555126},
abstract = {The World Wide Web is dominated by big tech and seemingly endless scandals after a decade of growing distrust about the role technology and the Internet play in our society. As a result, there are calls for the creation of alternatives to the existing platforms and infrastructures. One such alternative is a decentralized web (DWeb) where users have control of their data and decisions. This paper presents a collectively-produced organizational autoethnography of the development of an emerging tool for publishing on the decentralized web and the magazine using it to contribute to the digital commons. Three key themes emerged: 1) how a commons-based understanding of boundaries supports participation in a broader ecosystem; 2) the ways commoning as a frame deepens engagement as opposed to a passive model of a digital commons platform; finally 3) the need to re-assess how a cohort lab model that structured the work feeds back into larger goals. From these findings, we reflect on how this project fits into a maturing DWeb ecosystem and what possibilities for social transformation are present in transitional forms of commons. We discuss the pressing need for CSCW and adjacent research communities to participate in the design of, and debates over, the new computing paradigms developing out of this wave of decentralized technologies.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {401},
numpages = {21},
keywords = {commoning, decentralization, decentralized web, transitions}
}

@article{10.1145/3555542,
author = {Nishal, Sachita and Diakopoulos, Nicholas},
title = {From Crowd Ratings to Predictive Models of Newsworthiness to Support Science Journalism},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555542},
doi = {10.1145/3555542},
abstract = {The scale of scientific publishing continues to grow, creating overload on science journalists who are inundated with choices for what would be most interesting, important, and newsworthy to cover in their reporting. Our work addresses this problem by considering the viability of creating a predictive model of newsworthiness of scientific articles that is trained using crowdsourced evaluations of newsworthiness. We proceed by first evaluating the potential of crowd-sourced evaluations of newsworthiness by assessing their alignment with expert ratings of newsworthiness, analyzing both quantitative correlations and qualitative rating rationale to understand limitations. We then demonstrate and evaluate a predictive model trained on these crowd ratings together with arXiv article metadata, text, and other computed features. Based on the crowdsourcing protocol we developed, we find that while crowdsourced ratings of newsworthiness often align moderately with expert ratings, there are also notable differences and divergences which limit the approach. Yet despite these limitations we also find that the predictive model we built provides a reasonably precise set of rankings when validated against expert evaluations (P@10 = 0.8, P@15 = 0.67), suggesting that a viable signal can be learned from crowdsourced evaluations of newsworthiness. Based on these findings we discuss opportunities for future work to leverage crowdsourcing and predictive approaches to support journalistic work in discovering and filtering newsworthy information.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {441},
numpages = {28},
keywords = {crowdsourcing, news values, newsworthiness, science journalism}
}

@article{10.1145/3555803,
author = {Li, Bo and Qi, Peng and Liu, Bo and Di, Shuai and Liu, Jingen and Pei, Jiquan and Yi, Jinfeng and Zhou, Bowen},
title = {Trustworthy AI: From Principles to Practices},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3555803},
doi = {10.1145/3555803},
abstract = {The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people’s trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {177},
numpages = {46},
keywords = {Trustworthy AI, robustness, generalization, explainability, transparency, reproducibility, fairness, privacy protection, accountability}
}

@proceedings{10.1145/3555858,
title = {FDG '22: Proceedings of the 17th International Conference on the Foundations of Digital Games},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@inproceedings{10.1145/3555858.3555867,
author = {Clark, Lynda and Sood, Divij},
title = {Working Backwards: Creating a Character Backstory Generation System Using Idealized Creative Writing Outputs: Creating a Character Backstory Generation System Using Idealized Creative Writing Outputs},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3555867},
doi = {10.1145/3555858.3555867},
abstract = {This paper is a reflection on the process of designing and developing a character backstory generation system prototype following literary analysis of idealized textual outputs. An overview of previous generative systems (both academic and commercial) and some of the design priorities associated with these systems is described in order to set a context for the project. The design process is then described, with particular focus on the creation of the idealized outputs and their purpose. Finally, the learning outcomes following initial generative texts created by the prototype engine are shared, weighing the pros and cons of both the design approach, the resulting generator and its outputs.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {29},
numpages = {9},
keywords = {Text generation, creative writing, literary analysis, narrative design},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3555858.3555895,
author = {Madkour, Abdelrahman and Marsella, Stacy and Harteveld, Casper},
title = {Towards Non-Technical Designer Control over PCG Systems: Investigating an Example-Based Mechanism for Controlling Graph Grammars},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3555895},
doi = {10.1145/3555858.3555895},
abstract = {Increasingly, PCG systems are developed to help game designers create content for their games. However, game designers have limited control over the content current PCG systems generate. We investigate an interaction mechanism non-technical users can use to control generative grammars without the need for understanding the grammar’s rules. To demonstrate this control mechanism, we present a system, built using a probabilistic graph grammar, that allows designers to specify their desired generative space by defining a region on an expressive range plot. We ran a user study with game design students to assess its viability. Our findings suggest that designers have an easier time controlling the grammar using this mechanism over manually interacting with grammars rules.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {39},
numpages = {12},
keywords = {PCG, designer control, expressive range, game AI, graph grammar},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3555858.3555940,
author = {Salge, Christoph and Aranha, Claus and Brightmoore, Adrian and Butler, Sean and De Moura Canaan, Rodrigo and Cook, Michael and Green, Michael and Fischer, Hagen and Guckelsberger, Christian and Hadley, Jupiter and Herve, Jean-Baptiste and Johnson, Mark and Kybartas, Quinn and Mason, David and Preuss, Mike and Smith, Tristan and Thawonmas, Ruck and Togelius, Julian},
title = {Impressions of the GDMC AI Settlement Generation Challenge in Minecraft},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3555940},
doi = {10.1145/3555858.3555940},
abstract = {The GDMC AI settlement generation challenge is a procedural content generation (PCG) competition about producing an algorithm that can create a settlement in the game Minecraft. In contrast to the majority of AI competitions, the GDMC entries are evaluated by human experts on several criteria such as adaptability, functionality, evocative narrative, and visual aesthetics – all of which represent challenges to state-of-the-art PCG systems. This paper contains a collection of written experiences with this competition, by participants, judges, organizers and advisors. We asked people to reflect both on the artifacts themselves, and on the competition in general. The aim of this paper is to offer a shareable and edited collection of experiences and qualitative feedback which have the potential to push forward PCG and computational creativity, but would be lost once the individual assessments are compressed to scalar ratings. We reflect upon organizational issues for AI competitions, and discuss the future of the GDMC competition.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {45},
numpages = {16},
keywords = {Artificial Intelligence, Competition, Computational Creativity, Experience Survey, Minecraft, Procedural Content Generation},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3555858.3563262,
author = {Sfikas, Konstantinos and Liapis, Antonios and Yannakakis, Georgios N.},
title = {A General-Purpose Expressive Algorithm for Room-Based Environments},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3563262},
doi = {10.1145/3555858.3563262},
abstract = {This paper presents a generative architecture for general-purpose room layouts that can be treated as geometric definitions of dungeons, mansions, shooter levels and more. The motivation behind this work is to provide a design tool for virtual environments that combines aspects of controllability, expressivity and generality. Towards that end, a two-tier level representation is realized, with a graph-based design specification constraining and guiding the generated geometries, facilitated by constrained evolutionary search. Expressivity is secured through quality-diversity search which can provide the designer with a broad variety of level layouts to choose from. Finally, the generator is general-purpose as it can produce layouts based on different types of static grid structures or as free-form, curved structures through an adaptive Voronoi diagram that is evolved along with the level itself. The method is tested on a variety of design specifications and grid types, and results show that even with complex design constraints or malleable grids the algorithm can produce a broad variety of levels.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {64},
numpages = {9},
keywords = {constrained optimization, controllability, evolutionary algorithms, level generation, quality diversity search},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3555858.3563272,
author = {Acevedo, Pedro and Choi, Minsoo and Liu, Huimin and Kao, Dominic and Mousas, Christos},
title = {Procedural Game Level Design to Trigger Spatial Exploration},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3563272},
doi = {10.1145/3555858.3563272},
abstract = {Synthesizing game levels that evoke players’ curiosity, driving them to explore different level parts, is time-consuming and tedious. Typically, game level designers manually perform this synthesis using trial and error. In this paper, we propose a method with which to replace this manual, time-consuming process. We benefited from recent work that had proposed game level design patterns to evoke curiosity, and we propose an approach to automatically synthesizing game levels in order to encourage players to pursue designer-specified exploration goals. We started by creating a dataset of level assets, based on the four design patterns that evoke curiosity-driven exploration in games (reaching extreme points, resolving visual obstructions, out-of-place objects, and understanding spatial connections). We annotated the assets in our dataset with spatial exploration measurements (the time players took to explore an asset over their total time spent in the game level). We then formulated game level design as an optimization problem, encoding both spatial exploration (mean spatial exploration, spatial exploration variance, and spatial exploration distribution) and game level design (occupied area, adjacent penalty, and height distribution) decisions. Then, we solved this problem by implementing a reversible-jump Markov chain Monte Carlo method. We demonstrate our method’s ability to synthesize game level variations with different spatial exploration and level design decisions. Finally, a user study showed that our approach can automatically synthesize game levels, encouraging a certain amount of spatial exploration by players.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {70},
numpages = {11},
keywords = {curiosity, game level, level design, procedural content generation, spatial exploration},
location = {Athens, Greece},
series = {FDG '22}
}

@proceedings{10.1145/3555962,
title = {ICCBDC '22: Proceedings of the 2022 6th International Conference on Cloud and Big Data Computing},
year = {2022},
isbn = {9781450396578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Birmingham, United Kingdom}
}

@proceedings{10.1145/3556055,
title = {ICNSER '22: Proceedings of the 3rd International Conference on Industrial Control Network and System Engineering Research},
year = {2022},
isbn = {9781450384964},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanjing, China}
}

@proceedings{10.1145/3556223,
title = {ICCCM '22: Proceedings of the 10th International Conference on Computer and Communications Management},
year = {2022},
isbn = {9781450396349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Okayama, Japan}
}

@proceedings{10.1145/3556384,
title = {SPML '22: Proceedings of the 2022 5th International Conference on Signal Processing and Machine Learning},
year = {2022},
isbn = {9781450396912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Dalian, China}
}

@proceedings{10.1145/3556557,
title = {FedEdge '22: Proceedings of the 1st ACM Workshop on Data Privacy and Federated Learning Technologies for Mobile Edge Network},
year = {2022},
isbn = {9781450395212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, New South Wales, Australia}
}

@proceedings{10.1145/3556677,
title = {ICDLT '22: Proceedings of the 2022 6th International Conference on Deep Learning Technologies},
year = {2022},
isbn = {9781450396936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xi'an, China}
}

@proceedings{10.1145/3557915,
title = {SIGSPATIAL '22: Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
year = {2022},
isbn = {9781450395298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The conference started as a series of workshops and symposia back in 1993 with the aim of promoting interdisciplinary discussions among researchers, developers, users, and practitioners and fostering research in all aspects of geographic information systems, especially in relation to novel systems based on geospatial data and knowledge. It continues to provide a forum for original research contributions covering all conceptual, design and implementation aspects of geospatial data ranging from applications, user interfaces and visualization, to data storage, query processing, indexing, machine learning and data mining. The conference is the premier annual event of the ACM Special Interest Group on Spatial Information (ACM SIGSPATIAL).},
location = {Seattle, Washington}
}

@proceedings{10.1145/3558100,
title = {DocEng '22: Proceedings of the 22nd ACM Symposium on Document Engineering},
year = {2022},
isbn = {9781450395441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The symposium brings together experts in all areas of document engineering, across academia and industry, with the intention of presenting and discussing the most recent advances in the field of Document Engineering.},
location = {San Jose, California}
}

@proceedings{10.1145/3558489,
title = {PROMISE 2022: Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
year = {2022},
isbn = {9781450398602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome you to the 18th ACM International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE 2022), to be held in hybrid mode (physically and virtually) on November 18th, 2022, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). PROMISE is an annual forum for researchers and practitioners to present, discuss and exchange ideas, results, expertise and experiences in the construction and/or application of predictive models and data analytics in software engineering. Such models and analyses could be targeted at planning, design, implementation, testing, maintenance, quality assurance, evaluation, process improvement, management, decision making, and risk assessment in software and systems development. This year PROMISE received a total of 18 paper submissions. The review process was double blind and each paper was reviewed by at least three members of the program committee. An online discussion was also held for 8 days. Based on this procedure, we accepted a total of 10 full papers, which will be presented in 3 technical sessions. The acceptance criteria were entirely based on the quality of the papers, without imposing any constraint on the number of papers to be accepted.  

We are delighted to announce an outstanding keynote: Release Engineering in the AI World: How can Analytics Help? By Prof. Bram Adams, Queen’s University, Canada  

We would like to thank all authors for submitting high quality papers, and program committee members for their timely and accurate reviewing activity. Last, but not least, we would like to thank the FSE 2022 organizers for hosting PROMISE 2022 as a co-located event and for their logistic support in the organization of the conference.  

We hope you will enjoy PROMISE 2022.  
We certainly will!  

Many thanks from  
Shane McIntosh (General Chair),  
Gema Rodriguez-Perez and Weiyi Shang (Program Chairs).},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3558884,
title = {iWOAR '22: Proceedings of the 7th International Workshop on Sensor-based Activity Recognition and Artificial Intelligence},
year = {2022},
isbn = {9781450396240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rostock, Germany}
}

@proceedings{10.1145/3559009,
title = {PACT '22: Proceedings of the International Conference on Parallel Architectures and Compilation Techniques},
year = {2022},
isbn = {9781450398688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {PACT is a long-running and unique conference at the intersection of parallel architectures and compilers that brings together researchers from industry, academia, and national laboratories to present and discuss their latest research results. How applications serve as a driver for innovations in architecture and compilers is an important theme of the conference.},
location = {Chicago, Illinois}
}

@proceedings{10.1145/3560107,
title = {ICEGOV '22: Proceedings of the 15th International Conference on Theory and Practice of Electronic Governance},
year = {2022},
isbn = {9781450396356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guimar\~{a}es, Portugal}
}

@proceedings{10.1145/3560442,
title = {HPCCT '22: Proceedings of the 2022 6th High Performance Computing and Cluster Technologies Conference},
year = {2022},
isbn = {9781450396646},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Fuzhou, China}
}

@proceedings{10.1145/3560905,
title = {SenSys '22: Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
year = {2022},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM SenSys 2022, the 20th ACM Conference on Embedded Networked Sensor Systems, the premier computer systems conference focused on networked sensing systems and applications.},
location = {Boston, Massachusetts}
}

@proceedings{10.1145/3561212,
title = {AM '22: Proceedings of the 17th International Audio Mostly Conference},
year = {2022},
isbn = {9781450397018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {St. P\"{o}lten, Austria}
}

@proceedings{10.1145/3561518,
title = {ICGSP '22: Proceedings of the 6th International Conference on Graphics and Signal Processing},
year = {2022},
isbn = {9781450396370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chiba, Japan}
}

@proceedings{10.1145/3561613,
title = {ICCCV '22: Proceedings of the 5th International Conference on Control and Computer Vision},
year = {2022},
isbn = {9781450397315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3561801,
title = {BDIOT '22: Proceedings of the 2022 5th International Conference on Big Data and Internet of Things},
year = {2022},
isbn = {9781450390361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chongqing, China}
}

@proceedings{10.1145/3561833,
title = {COMPUTE '22: Proceedings of the 15th Annual ACM India Compute Conference},
year = {2022},
isbn = {9781450397759},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Jaipur, India}
}

@proceedings{10.1145/3561877,
title = {ICISS '22: Proceedings of the 5th International Conference on Information Science and Systems},
year = {2022},
isbn = {9781450396837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@proceedings{10.1145/3561975,
title = {MIG '22: Proceedings of the 15th ACM SIGGRAPH Conference on Motion, Interaction and Games},
year = {2022},
isbn = {9781450398886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guanajuato, Mexico}
}

@proceedings{10.1145/3562007,
title = {CCRIS '22: Proceedings of the 2022 3rd International Conference on Control, Robotics and Intelligent System},
year = {2022},
isbn = {9781450396851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, China}
}

@proceedings{10.1145/3562939,
title = {VRST '22: Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tsukuba, Japan}
}

@article{10.1145/3563302,
author = {Zhang, Yuhao and Bajpai, Yasharth and Gupta, Priyanshu and Ketkar, Ameya and Allamanis, Miltiadis and Barik, Titus and Gulwani, Sumit and Radhakrishna, Arjun and Raza, Mohammad and Soares, Gustavo and Tiwari, Ashish},
title = {Overwatch: learning patterns in code edit sequences},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563302},
doi = {10.1145/3563302},
abstract = {Integrated Development Environments (IDEs) provide tool support to automate many source code editing tasks. Traditionally, IDEs use only the spatial context, i.e., the location where the developer is editing, to generate candidate edit recommendations. However, spatial context alone is often not sufficient to confidently predict the developer’s next edit, and thus IDEs generate many suggestions at a location. Therefore, IDEs generally do not actively offer suggestions and instead, the developer is usually required to click on a specific  
icon or menu and then select from a large list of potential suggestions. As a consequence, developers often miss the opportunity to use the tool support because they are not aware it exists or forget to use it.  
To better understand common patterns in developer behavior and produce better edit recommendations, we can additionally use the temporal context, i.e., the edits that a developer was recently performing. To enable edit recommendations based on temporal context, we present Overwatch, a novel technique for learning edit sequence patterns from traces of developers’ edits performed in an IDE. Our experiments show that Overwatch has 78\% precision and that Overwatch not only completed edits when developers missed the  
opportunity to use the IDE tool support but also predicted new edits that have no tool support in the IDE.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {139},
numpages = {29},
keywords = {Artificial Intelligence, Program Generation, Program Synthesis}
}

@article{10.1145/3563304,
author = {Parreaux, Lionel and Chau, Chun Yin},
title = {MLstruct: principal type inference in a Boolean algebra of structural types},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563304},
doi = {10.1145/3563304},
abstract = {Intersection and union types are becoming more popular by the day, entering the mainstream in programming languages like TypeScript and Scala 3. Yet, no language so far has managed to combine these powerful types with principal polymorphic type inference. We present a solution to this problem in MLstruct, a language with subtyped records, equirecursive types, first-class unions and intersections, class-based instance matching, and ML-style principal type inference. While MLstruct is mostly structurally typed, it contains a healthy sprinkle of nominality for classes, which gives it desirable semantics, enabling the expression of a powerful form of extensible variants that does not need row variables. Technically, we define the constructs of our language using conjunction, disjunction, and negation connectives, making sure they form a Boolean algebra, and we show that the addition of a few nonstandard but sound subtyping rules gives us enough structure to derive a sound and complete type inference algorithm. With this work, we hope to foster the development of better type inference for present and future programming languages with expressive subtyping systems.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {141},
numpages = {30},
keywords = {principal type inference, structural typing, union and intersection types}
}

@article{10.1145/3563327,
author = {Bavishi, Rohan and Joshi, Harshit and Cambronero, Jos\'{e} and Fariha, Anna and Gulwani, Sumit and Le, Vu and Radi\v{c}ek, Ivan and Tiwari, Ashish},
title = {Neurosymbolic repair for low-code formula languages},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563327},
doi = {10.1145/3563327},
abstract = {Most users of low-code platforms, such as Excel and PowerApps, write programs in domain-specific formula languages to carry out nontrivial tasks. Often users can write most of the program they want, but introduce small mistakes that yield broken formulas. These mistakes, which can be both syntactic and semantic, are hard for low-code users to identify and fix, even though they can be resolved with just a few edits. We formalize the problem of producing such edits as the last-mile repair problem. To address this problem, we developed LaMirage, a LAst-MIle RepAir-engine GEnerator that combines symbolic and neural techniques to perform last-mile repair in low-code formula languages. LaMirage takes a grammar and a set of domain-specific constraints/rules, which jointly approximate the target language, and uses these to generate a repair engine that can fix formulas in that language. To tackle the challenges of localizing errors and ranking candidate repairs, LaMirage leverages neural techniques, whereas it relies on symbolic methods to generate candidate edits. This combination allows LaMirage to find repairs that satisfy the provided grammar and constraints, and then pick the most natural repair. We compare LaMirage to state-of-the-art neural and symbolic approaches on 400 real Excel and Power Fx formulas, where LaMirage outperforms all baselines. We release these benchmarks to encourage subsequent work in low-code domains.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {164},
numpages = {30},
keywords = {Low-Code, Neurosymbolic, Program Repair}
}

@proceedings{10.1145/3563357,
title = {BuildSys '22: Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
year = {2022},
isbn = {9781450398909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Over the past thirteen years, BuildSys has been an interdisciplinary conference that brings together various stakeholders, including researchers, practitioners, and policymakers from different disciplines, including civil engineering, mechanical engineering, environmental science, electrical and computer engineering, computer science, system management and control, and many others. This year is no exception, with papers and attendees from all these disciplines and regions worldwide. The conference's focus extends beyond building systems to the built environment more generally.},
location = {Boston, Massachusetts}
}

@article{10.1145/3563391,
author = {Koblah, David and Acharya, Rabin and Capecci, Daniel and Dizon-Paradis, Olivia and Tajik, Shahin and Ganji, Fatemeh and Woodard, Damon and Forte, Domenic},
title = {A Survey and Perspective on Artificial Intelligence for Security-Aware Electronic Design Automation},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/3563391},
doi = {10.1145/3563391},
abstract = {Artificial intelligence (AI) and machine learning (ML) techniques have been increasingly used in several fields to improve performance and the level of automation. In recent years, this use has exponentially increased due to the advancement of high-performance computing and the ever increasing size of data. One of such fields is that of hardware design—specifically the design of digital and analog integrated circuits, where AI/ ML techniques have been extensively used to address ever-increasing design complexity, aggressive time to market, and the growing number of ubiquitous interconnected devices. However, the security concerns and issues related to integrated circuit design have been highly overlooked. In this article, we summarize the state-of-the-art in AI/ML for circuit design/optimization, security and engineering challenges, research in security-aware computer-aided design/electronic design automation, and future research directions and needs for using AI/ML for security-aware circuit design.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = mar,
articleno = {16},
numpages = {57},
keywords = {Integrated circuit, deep learning, reinforcement learning, security primitive}
}

@book{10.1145/3563659,
editor = {Lugrin, Birgit and Pelachaud, Catherine and Traum, David},
title = {The Handbook on Socially Interactive Agents: 20 years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 2: Interactivity, Platforms, Application},
year = {2022},
isbn = {9781450398961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {48},
abstract = {The Handbook on Socially Interactive Agents provides a comprehensive overview of the research fields of Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics. Socially Interactive Agents (SIAs), whether virtually or physically embodied, are autonomous agents that are able to perceive an environment including people or other agents, reason, and decide how to interact, and express attitudes such as emotions, engagement, or empathy. They are capable of interacting with people and each other in a socially intelligent manner using multimodal communicative behaviors with the goal to support humans in various domains.Written by international experts in their respective fields, the book summarizes research in the many important research communities pertinent for SIAs, while discussing current challenges and future directions. The handbook provides easy access to modeling and studying SIAs for researchers and students and aims at further bridging the gap between the research communities involved.In two volumes, the book clearly structures the vast body of research. The first volume starts by introducing what is involved in SIAs research, in particular research methodologies and ethical implications of developing SIAs. It further examines research on appearance and behavior, focusing on multimodality. Finally, social cognition for SIAs is investigated by different theoretical models and phenomena such as theory of mind or pro-sociality. The second volume starts with perspectives on interaction, examined from different angles such as interaction in social space, group interaction, or long-term interaction. It also includes an extensive overview summarizing research and systems of human-agent platforms and of some of the major application areas of SIAs such as education, aging support, autism or games.}
}

@proceedings{10.1145/3563737,
title = {ICBIP '22: Proceedings of the 7th International Conference on Biomedical Signal and Image Processing},
year = {2022},
isbn = {9781450396691},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Suzhou, China}
}

@proceedings{10.1145/3563767,
title = {SPLASH-E 2022: Proceedings of the 2022 ACM SIGPLAN International Symposium on SPLASH-E},
year = {2022},
isbn = {9781450399005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The SPLASH-E symposium is a forum for researchers and educators to discuss  
the intersection of education and the core SPLASH research areas: systems, pro-  
gramming languages, and their applications. We investigate how to deliver systems  
and programming languages concepts to students, how systems and languages can  
aid in education broadly, and how to prepare students to apply these concepts to  
their later work in industry or academia.  

This year's SPLASH-E program includes 8 papers (five long, three  
short). This year's program covers a wide gamut of SPLASH-E topics:  
various techniques for improving introductory learning; instructor  
support for course management; experience reports; general computer  
science education; and education in specific languages and programming  
language concepts.},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3563768,
title = {SPLASH Companion 2022: Companion Proceedings of the 2022 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
year = {2022},
isbn = {9781450399012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the SPLASH 2022! After two years of virtual only (SPLASH 2020), closed borders USA only (SPLASH 2021), we finally feel the reopening and going back to the pre-Covid in person vibe of the 37th OOPSLA/SPLASH. I am especially proud of having SPLASH outside of the USA/Canada region for the 3rd time in its history and the first time it is held in the Asia Pacific. We invited the Asian Symposium on Programming Languages and Systems (APLAS) to co-locate with us for the 3rd year in a row to celebrate this occasion.},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3563835,
title = {Onward! 2022: Proceedings of the 2022 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2022},
isbn = {9781450399098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to Onward! 2022. Onward! is a premier multidisciplinary conference focused on everything to do with programming and software, including processes, methods, languages, communities, and applications. Onward! is more radical, more visionary, and more open than other conferences to ideas that are well-argued but not yet proven. We welcome different ways of thinking about, approaching, and reporting on programming language and software engineering research. Onward! 2022 is part of SPLASH 2022, taking place from Monday 5th to Saturday 10th December 2022 in Auckland, New Zealand.},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3563836,
title = {PAINT 2022: Proceedings of the 1st ACM SIGPLAN International Workshop on Programming Abstractions and Interactive Notations, Tools, and Environments},
year = {2022},
isbn = {9781450399104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Programming environments that integrate tools, notations, and abstractions into a holistic user experience can provide programmers with better support for what they want to achieve. These programming environments can create an engaging place to do new forms of informational work—resulting in enjoyable, creative, and productive experiences with programming. 

In the workshop on Programming Abstractions and Interactive Notations, Tools, and Environments (PAINT), we want to discuss programming environments that support users in working with and creating notations and abstractions that matter to them. We are interested in the relationship between people centric notations and general-purpose programming languages and environments. How do we reflect the various experiences, needs, and priorities of the many people involved in programming—whether they call it that or not?},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3564533,
title = {Web3D '22: Proceedings of the 27th International Conference on 3D Web Technology},
year = {2022},
isbn = {9781450399142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Evry-Courcouronnes, France}
}

@inproceedings{10.1145/3564533.3564572,
author = {Durchon, Hugo and Preda, Marius and Zaharia, Titus and Grall, Yannick},
title = {Challenges in Applying Deep Learning to Augmented Reality for Manufacturing},
year = {2022},
isbn = {9781450399142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564533.3564572},
doi = {10.1145/3564533.3564572},
abstract = {Augmented Reality (AR) for industry has become a significant research area because of its potential benefits for operators and factories. AR tools could help to collect data, create standardized representations of industrial procedures, guide operators in real-time during operations, assess factory efficiency, and elaborate personalized training and coaching systems. However, AR is not yet widely deployed in industries, and this is due to several factors: hardware, software, user acceptance, and companies’ constraints. One of the causes we have identified in our factory is the poor user experience when using AR assistance software. We argue that adding computer vision and deep learning (DL) algorithms into AR assistance software could improve the quality of interactions with the user, handle dynamic environments, and facilitate AR adoption. We conduct a preliminary experiment aiming to perform 3D pose estimation of a boiler with MobileNetv2 in an uncontrolled industrial environment. This experiment produces insufficient results that cannot be directly used but allow us to establish a list of challenges and perspectives for future work.},
booktitle = {Proceedings of the 27th International Conference on 3D Web Technology},
articleno = {13},
numpages = {4},
keywords = {3D object pose estimation, AR registration in dynamic environments, Deep learning, Industrial manufacturing},
location = {Evry-Courcouronnes, France},
series = {Web3D '22}
}

@proceedings{10.1145/3564625,
title = {ACSAC '22: Proceedings of the 38th Annual Computer Security Applications Conference},
year = {2022},
isbn = {9781450397599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@inproceedings{10.1145/3564625.3567985,
author = {Thapa, Chandra and Jang, Seung Ick and Ahmed, Muhammad Ejaz and Camtepe, Seyit and Pieprzyk, Josef and Nepal, Surya},
title = {Transformer-Based Language Models for Software Vulnerability Detection},
year = {2022},
isbn = {9781450397599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564625.3567985},
doi = {10.1145/3564625.3567985},
abstract = {The large transformer-based language models demonstrate excellent performance in natural language processing. By considering the transferability of the knowledge gained by these models in one domain to other related domains, and the closeness of natural languages to high-level programming languages, such as C/C++, this work studies how to leverage (large) transformer-based language models in detecting software vulnerabilities and how good are these models for vulnerability detection tasks. In this regard, firstly, we present a systematic (cohesive) framework that details source code translation, model preparation, and inference. Then, we perform an empirical analysis of software vulnerability datasets of C/C++ source codes having multiple vulnerabilities corresponding to the library function call, pointer usage, array usage, and arithmetic expression. Our empirical results demonstrate the good performance of the language models in vulnerability detection. Moreover, these language models have better performance metrics, such as F1-score, than the contemporary models, namely bidirectional long short term memory and bidirectional gated recurrent unit. Experimenting with the language models is always challenging due to the requirement of computing resources, platforms, libraries, and dependencies. Thus, this paper also analyses the popular platforms to efficiently fine-tune these models and present recommendations while choosing the platforms for our framework.},
booktitle = {Proceedings of the 38th Annual Computer Security Applications Conference},
pages = {481–496},
numpages = {16},
keywords = {BERT, GPT-2, Software vulnerability detection, transformer-based models},
location = {Austin, TX, USA},
series = {ACSAC '22}
}

@proceedings{10.1145/3564719,
title = {GPCE 2022: Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2022},
isbn = {9781450399203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 21st ACM SIGPLAN International Conference on Generative Programming: Concept \&amp; Experiences (GPCE 2022) held on December 6th and 7th, 2022 in Auckland, New Zealand. GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation, domain-specific languages, and component deployment to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between software engineering and programming language.},
location = {Auckland, New Zealand}
}

@inproceedings{10.1145/3564719.3568686,
author = {Pacak, Andr\'{e} and Szab\'{o}, Tam\'{a}s and Erdweg, Sebastian},
title = {Incremental Processing of Structured Data in Datalog},
year = {2022},
isbn = {9781450399203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564719.3568686},
doi = {10.1145/3564719.3568686},
abstract = {Incremental computations react to input changes by updating their outputs. Compared to a non-incremental rerun, incremental computations can provide order-of-magnitude speedups, since often small input changes trigger small output changes. One popular means for implementing incremental computations is to encode the computation in Datalog, for which efficient incremental solvers exist. However, Datalog is very restrictive in terms of the data types it can process: Atomic data organized in relations. While structured tree and graph-shaped data can be encoded in relations, a naive encoding inhibits incrementality. In this paper, we present an encoding of structured data in Datalog that supports efficient incrementality such that small input changes are expressible. We explain how to efficiently implement and integrate this encoding into an existing incremental Datalog engine, and we show how tree diffing algorithms can be used to change the encoded data.},
booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {20–32},
numpages = {13},
keywords = {Datalog, incremental computing, relational encoding, structured data},
location = {Auckland, New Zealand},
series = {GPCE 2022}
}

@inproceedings{10.1145/3564719.3568698,
author = {Shaikhha, Amir},
title = {Deep Fusion for Efficient Nested Recursive Computations},
year = {2022},
isbn = {9781450399203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564719.3568698},
doi = {10.1145/3564719.3568698},
abstract = {One of the performance bottlenecks of nested recursive computations is the intermediate collections created at different levels of recursion. The existing techniques such as vertical and horizontal loop fusion do not remove such intermediate allocations. This paper proposes deep fusion, a technique for the efficient compilation of nested recursive computation over collections. The input to our compilation framework is a high-level functional program that can represent computations on flat and nested collections such as lists, sets, bags, and maps. The intermediate collections are removed in three levels. First, the immutable collections are translated into mutable ones by leveraging in-place updates using the destination-passing style technique. Second, deep fusion enables the inner level of recursion to reuse the destinations of the outer levels for in-place updates. Third, deep fusion removes the need to allocate tiny intermediate collections at different depths of recursion. Our experiments show that deep fusion can improve the performance of nested recursion over nested lists and maps.},
booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {33–44},
numpages = {12},
keywords = {Collection Programming, Deep Fusion, Destination-Passing Style, In-Place Update, Loop Fusion, Monoid},
location = {Auckland, New Zealand},
series = {GPCE 2022}
}

@proceedings{10.1145/3564721,
title = {Koli Calling '22: Proceedings of the 22nd Koli Calling International Conference on Computing Education Research},
year = {2022},
isbn = {9781450396165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Koli, Finland}
}

@proceedings{10.1145/3565011,
title = {DE '22: Proceedings of the 1st International Workshop on Data Economy},
year = {2022},
isbn = {9781450399234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Data-driven decision making powered by Machine Learning (ML) algorithms is changing how the society and the economy work and is having a profound positive impact on our daily life. With the exception of very large companies that have both the data and the skills to develop powerful ML-driven services, the large majority of provably possible ML services, from e-health, to transportation and predictive maintenance, to name just a few, still remain at the idea or prototype level for the simple reason that data, the skills to manipulate them, and the business models to bring them to market, seldom co-exist under the same roof.Data must somehow meet with the ML and business skills that can unleash its full power for the society and economy. This has given rise to a highly dynamic sector around the Data Economy, involving Data Providers/Controllers, data Intermediaries, often-times in the form of Data Marketplaces or Personal Information Management Systems for end-users to control and even monetise their personal data.},
location = {Rome, Italy}
}

@proceedings{10.1145/3565383,
title = {DICG '22: Proceedings of the 3rd International Workshop on Distributed Infrastructure for the Common Good},
year = {2022},
isbn = {9781450399289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Quebec, Quebec City, Canada}
}

@proceedings{10.1145/3565384,
title = {WoC '22: Proceedings of the Eighth International Workshop on Container Technologies and Container Clouds},
year = {2022},
isbn = {9781450399296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Quebec, Quebec City, Canada}
}

@proceedings{10.1145/3565387,
title = {CSAE '22: Proceedings of the 6th International Conference on Computer Science and Application Engineering},
year = {2022},
isbn = {9781450396004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, China}
}

@proceedings{10.1145/3565516,
title = {CVMP '22: Proceedings of the 19th ACM SIGGRAPH European Conference on Visual Media Production},
year = {2022},
isbn = {9781450399395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {London, United Kingdom}
}

@article{10.1145/3565971,
author = {Di Grazia, Luca and Pradel, Michael},
title = {Code Search: A Survey of Techniques for Finding Code},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3565971},
doi = {10.1145/3565971},
abstract = {The immense amounts of source code provide ample challenges and opportunities during software development. To handle the size of code bases, developers commonly search for code, e.g., when trying to find where a particular feature is implemented or when looking for code examples to reuse. To support developers in finding relevant code, various code search engines have been proposed. This article surveys 30 years of research on code search, giving a comprehensive overview of challenges and techniques that address them. We discuss the kinds of queries that code search engines support, how to preprocess and expand queries, different techniques for indexing and retrieving code, and ways to rank and prune search results. Moreover, we describe empirical studies of code search in practice. Based on the discussion of prior work, we conclude the article with an outline of challenges and opportunities to be addressed in the future.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {220},
numpages = {31},
keywords = {Code search, code retrieval, API, learning, survey}
}

@proceedings{10.1145/3566097,
title = {ASPDAC '23: Proceedings of the 28th Asia and South Pacific Design Automation Conference},
year = {2023},
isbn = {9781450397834},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ASP-DAC is a high-quality and premium conference on Electronic Design Automation (EDA) like other sister conferences such as Design Automation Conference (DAC), Design, Automation \&amp; Test in Europe (DATE), International Conference on Computer Aided Design (ICCAD), and Embedded SystemsWeek (ESWEEK). ASP-DAC started in 1995 and has continuously offered opportunities to learn about the advancements on LSI design and design automation fields, as well as to communicate with each other for researchers and designers around Asia and South Pacific regions.},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3567445,
title = {IoT '22: Proceedings of the 12th International Conference on the Internet of Things},
year = {2022},
isbn = {9781450396653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Delft, Netherlands}
}

@proceedings{10.1145/3567512,
title = {SLE 2022: Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2022},
isbn = {9781450399197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 15th ACM SIGPLAN International Conference on Software Language Engineering (SLE), co-located with the ACM SIGPLAN conference on Systems, Programming, Languages, and Applications (SPLASH) in Auckland, a vibrant port city in northern New Zealand, from December 5th to December 10th 2022. Like its predecessors, the this edition of the SLE conference, SLE 2022, is devoted to the principles of software languages: their design, their implementation, and their evolution. As such, SLE brings together researchers united by their common interest in the creation, capture, and tooling of software languages.},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3567600,
title = {WUWNet '22: Proceedings of the 16th International Conference on Underwater Networks \&amp; Systems},
year = {2022},
isbn = {9781450399524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Boston, MA, USA}
}

@inproceedings{10.1145/3568162.3576967,
author = {Kamino, Waki and Sabanovic, Selma},
title = {Coffee, Tea, Robots? The Performative Staging of Service Robots in 'Robot Cafes' in Japan},
year = {2023},
isbn = {9781450399647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568162.3576967},
doi = {10.1145/3568162.3576967},
abstract = {We present an ethnographic observational study of six robot cafes in Japan to understand how service robots are performatively staged and presented to the public. We particularly attend to the diverse ways in which the physical setting and ambience of the cafes, the verbal characterization of and staff behaviors toward robots, explicit and implicit instructions on appropriate interactions with robots, and handling of robot malfunctions constitute robots as socially acceptable and useful in daily life. Such scaffolding enables robots to provide material and affective services to cafe visitors, and visitors to explore various interaction possibilities with robots. Our work contributes to the critical study of the ongoing construction of "robot cultures" in Japan, and calls attention to public interactions with robots and the importance of contextual staging beyond individual robot features in human-robot interaction design.},
booktitle = {Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {183–191},
numpages = {9},
keywords = {ethnography, human-robot interaction, japan, robot cafes, robot culture, robots in public, service robot, social robot, sociality},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@proceedings{10.1145/3568199,
title = {MLMI '22: Proceedings of the 2022 5th International Conference on Machine Learning and Machine Intelligence},
year = {2022},
isbn = {9781450397551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@proceedings{10.1145/3568231,
title = {SIET '22: Proceedings of the 7th International Conference on Sustainable Information Engineering and Technology},
year = {2022},
isbn = {9781450397117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Malang, Indonesia}
}

@proceedings{10.1145/3568364,
title = {WSSE '22: Proceedings of the 4th World Symposium on Software Engineering},
year = {2022},
isbn = {9781450396950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3568444,
title = {MUM '22: Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia},
year = {2022},
isbn = {9781450398206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3568562,
title = {SoICT '22: Proceedings of the 11th International Symposium on Information and Communication Technology},
year = {2022},
isbn = {9781450397254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hanoi, Vietnam}
}

@proceedings{10.1145/3568739,
title = {ICDTE '22: Proceedings of the 6th International Conference on Digital Technology in Education},
year = {2022},
isbn = {9781450398091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@proceedings{10.1145/3568834,
title = {ICIBE '22: Proceedings of the 8th International Conference on Industrial and Business Engineering},
year = {2022},
isbn = {9781450397582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macau, China}
}

@proceedings{10.1145/3568923,
title = {ICIST '22: Proceedings of the 4th International Conference on Intelligent Science and Technology},
year = {2022},
isbn = {9781450397230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Harbin, China}
}

@proceedings{10.1145/3569009,
title = {TEI '23: Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction},
year = {2023},
isbn = {9781450399777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Warsaw, Poland}
}

@proceedings{10.1145/3569192,
title = {ICBRA '22: Proceedings of the 9th International Conference on Bioinformatics Research and Applications},
year = {2022},
isbn = {9781450396868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Berlin, Germany}
}

@proceedings{10.1145/3569219,
title = {Academic Mindtrek '22: Proceedings of the 25th International Academic Mindtrek Conference},
year = {2022},
isbn = {9781450399555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tampere, Finland}
}

@proceedings{10.1145/3569551,
title = {NSysS '22: Proceedings of the 9th International Conference on Networking, Systems and Security},
year = {2022},
isbn = {9781450399036},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cox's Bazar, Bangladesh}
}

@proceedings{10.1145/3569966,
title = {CSSE '22: Proceedings of the 5th International Conference on Computer Science and Software Engineering},
year = {2022},
isbn = {9781450397780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guilin, China}
}

@proceedings{10.1145/3570236,
title = {ICIIP '22: Proceedings of the 7th International Conference on Intelligent Information Processing},
year = {2022},
isbn = {9781450396714},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bucharest, Romania}
}

@proceedings{10.1145/3570773,
title = {ISAIMS '22: Proceedings of the 3rd International Symposium on Artificial Intelligence for Medicine Sciences},
year = {2022},
isbn = {9781450398442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@proceedings{10.1145/3570991,
title = {CODS-COMAD '23: Proceedings of the 6th Joint International Conference on Data Science \&amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)},
year = {2023},
isbn = {9781450397971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Mumbai, India}
}

@article{10.1145/3571200,
author = {Bembenek, Aaron and Greenberg, Michael and Chong, Stephen},
title = {From SMT to ASP: Solver-Based Approaches to Solving Datalog Synthesis-as-Rule-Selection Problems},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {POPL},
url = {https://doi.org/10.1145/3571200},
doi = {10.1145/3571200},
abstract = {Given a set of candidate Datalog rules, the Datalog synthesis-as-rule-selection problem chooses a subset of these rules that satisfies a specification (such as an input-output example). Building off prior work using counterexample-guided inductive synthesis, we present a progression of three solver-based approaches for solving Datalog synthesis-as-rule-selection problems. Two of our approaches offer some advantages over existing approaches, and can be used more generally to solve arbitrary SMT formulas containing Datalog predicates; the third—an encoding into standard, off-the-shelf answer set programming (ASP)—leads to significant speedups (∼ 9\texttimes{} geomean) over the state of the art while synthesizing higher quality programs.  

Our progression of solutions explores the space of interactions between SAT/SMT and Datalog, identifying ASP as a promising tool for working with and reasoning about Datalog. Along the way, we identify Datalog programs as monotonic SMT theories, which enjoy particularly efficient interactions in SMT; our plugins for popular SMT solvers make it easy to load an arbitrary Datalog program into the SMT solver as a custom monotonic theory. Finally, we evaluate our approaches using multiple underlying solvers to provide a more thorough and nuanced comparison against the current state of the art.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {7},
numpages = {33},
keywords = {Datalog, inductive logic programming, program synthesis, satisfiability}
}

@article{10.1145/3571234,
author = {Bowers, Matthew and Olausson, Theo X. and Wong, Lionel and Grand, Gabriel and Tenenbaum, Joshua B. and Ellis, Kevin and Solar-Lezama, Armando},
title = {Top-Down Synthesis for Library Learning},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {POPL},
url = {https://doi.org/10.1145/3571234},
doi = {10.1145/3571234},
abstract = {This paper introduces corpus-guided top-down synthesis as a mechanism for synthesizing library functions that capture common functionality from a corpus of programs in a domain specific language (DSL). The algorithm builds abstractions directly from initial DSL primitives, using syntactic pattern matching of intermediate abstractions to intelligently prune the search space and guide the algorithm towards abstractions that maximally capture shared structures in the corpus. We present an implementation of the approach in a tool called Stitch and evaluate it against the state-of-the-art deductive library learning algorithm from DreamCoder. Our evaluation shows that Stitch is 3-4 orders of magnitude faster and uses 2 orders of magnitude less memory while maintaining comparable or better library quality (as measured by compressivity). We also demonstrate Stitch’s scalability on corpora containing hundreds of complex programs that are intractable with prior deductive approaches and show empirically that it is robust to terminating the search procedure early—further allowing it to scale to challenging datasets by means of early stopping.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {41},
numpages = {32},
keywords = {Abstraction Learning, Library Learning, Program Synthesis}
}

@proceedings{10.1145/3571306,
title = {ICDCN '23: Proceedings of the 24th International Conference on Distributed Computing and Networking},
year = {2023},
isbn = {9781450397964},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kharagpur, India}
}

@proceedings{10.1145/3571560,
title = {ICAAI '22: Proceedings of the 6th International Conference on Advances in Artificial Intelligence},
year = {2022},
isbn = {9781450396943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Birmingham, United Kingdom}
}

@proceedings{10.1145/3571662,
title = {ICCIP '22: Proceedings of the 8th International Conference on Communication and Information Processing},
year = {2022},
isbn = {9781450397100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@proceedings{10.1145/3571697,
title = {ESSE '22: Proceedings of the 2022 European Symposium on Software Engineering},
year = {2022},
isbn = {9781450397308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rome, Italy}
}

@proceedings{10.1145/3571786,
title = {PEPM 2023: Proceedings of the 2023 ACM SIGPLAN International Workshop on Partial Evaluation and Program Manipulation},
year = {2023},
isbn = {9798400700118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to present the proceedings of the 2023 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation (PEPM 2023), held January 16-17th, 2023 in Boston, in affiliation with the annual Symposium on Principles of Programming Languages (POPL 2023). PEPM has a history going back to 1991 and originated with the discoveries of useful automated techniques for evaluating programs with only partial input. Over the years, the scope of PEPM has expanded to include a variety of research areas centered around the theme of semantics-based program manipulation - the systematic exploitation of treating programs not only as subjects to black-box execution but also as data structures that can be generated, analysed, and transformed while establishing or maintaining important semantic properties.},
location = {Boston, MA, USA}
}

@proceedings{10.1145/3571788,
title = {VaMoS '23: Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Odense, Denmark}
}

@proceedings{10.1145/3572864,
title = {HotMobile '23: Proceedings of the 24th International Workshop on Mobile Computing Systems and Applications},
year = {2023},
isbn = {9798400700170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Newport Beach, California}
}

@proceedings{10.1145/3572921,
title = {OzCHI '22: Proceedings of the 34th Australian Conference on Human-Computer Interaction},
year = {2022},
isbn = {9798400700248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Canberra, ACT, Australia}
}

@proceedings{10.1145/3573428,
title = {EITCE '22: Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
year = {2022},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3573834,
title = {AISS '22: Proceedings of the 4th International Conference on Advanced Information Science and System},
year = {2022},
isbn = {9781450397933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@proceedings{10.1145/3574131,
title = {VRCAI '22: Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry},
year = {2022},
isbn = {9798400700316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guangzhou, China}
}

@proceedings{10.1145/3574198,
title = {ICBBE '22: Proceedings of the 2022 9th International Conference on Biomedical and Bioinformatics Engineering},
year = {2022},
isbn = {9781450397223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kyoto, Japan}
}

@proceedings{10.1145/3575693,
title = {ASPLOS 2023: Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to introduce Volume II of ASPLOS ’23. For the first time, ASPLOS has embarked on a new multi-deadline review model. ASPLOS ’23 features 3 deadlines spaced throughout the year and papers will be published in three volumes. Multiple deadlines are meant to encourage authors to submit their papers when ready and to facilitate the selection of some papers for revision. For this volume of ASPLOS ’23, we discontinued the use of the 2-page extended abstract submissions that were used in ASPLOS ’21 and ASPLOS ’22. We found the extended abstract offered limited filtering and moved to a more traditional two phase review process. Each paper received 3 reviews in phase 1 and papers with positive scores advanced to the second round and received up to 2 more reviews. In our preface to Volume III, we will give a more detailed rundown of how the process worked.},
location = {Vancouver, BC, Canada}
}

@proceedings{10.1145/3575879,
title = {PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics},
year = {2022},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3575882,
title = {IC3INA '22: Proceedings of the 2022 International Conference on Computer, Control, Informatics and Its Applications},
year = {2022},
isbn = {9781450397902},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Indonesia}
}

@proceedings{10.1145/3576123,
title = {ACE '23: Proceedings of the 25th Australasian Computing Education Conference},
year = {2023},
isbn = {9781450399418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3576840.3578327,
author = {Deckers, Niklas and Fr\"{o}be, Maik and Kiesel, Johannes and Pandolfo, Gianluca and Schr\"{o}der, Christopher and Stein, Benno and Potthast, Martin},
title = {The Infinite Index: Information Retrieval on Generative Text-To-Image Models},
year = {2023},
isbn = {9798400700354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576840.3578327},
doi = {10.1145/3576840.3578327},
abstract = {Conditional generative models such as DALL-E and Stable Diffusion generate images based on a user-defined text, the prompt. Finding and refining prompts that produce a desired image has become the art of prompt engineering. Generative models do not provide a built-in retrieval model for a user’s information need expressed through prompts. In light of an extensive literature review, we reframe prompt engineering for generative models as interactive text-based retrieval on a novel kind of “infinite index”. We apply these insights for the first time in a case study on image generation for game design with an expert. Finally, we envision how active learning may help to guide the retrieval of generated images.},
booktitle = {Proceedings of the 2023 Conference on Human Information Interaction and Retrieval},
pages = {172–186},
numpages = {15},
keywords = {case study, evaluation, generative models, image retrieval},
location = {Austin, TX, USA},
series = {CHIIR '23}
}

@proceedings{10.1145/3577117,
title = {ICAIP '22: Proceedings of the 6th International Conference on Advances in Image Processing},
year = {2022},
isbn = {9781450397155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Zhanjiang, China}
}

@proceedings{10.1145/3577164,
title = {VSIP '22: Proceedings of the 2022 4th International Conference on Video, Signal and Image Processing},
year = {2022},
isbn = {9781450397810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shanghai, China}
}

@proceedings{10.1145/3577530,
title = {CSAI '22: Proceedings of the 2022 6th International Conference on Computer Science and Artificial Intelligence},
year = {2022},
isbn = {9781450397773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@proceedings{10.1145/3578360,
title = {CC 2023: Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction},
year = {2023},
isbn = {9798400700880},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 32nd ACM SIGPLAN International Conference on Compiler Construction (CC 2023), held in Montr\'{e}al, Qu\'{e}bec, Canada over February 25–26, 2023. As in the previous eight years, CC is held jointly with the International Symposium on Code Generation and Optimization (CGO), the Symposium on Principles and Practice of Parallel Programming (PPoPP), and the International Symposium on High-Performance Computer Architecture (HPCA). Colocation of these four conferences creates an exciting opportunity for a broad range of researchers in the areas of compilation, optimization, parallelism, and computer architecture to interact and explore collaborative research opportunities.},
location = {Montr\'{e}al, QC, Canada}
}

@proceedings{10.1145/3578527,
title = {ISEC '23: Proceedings of the 16th Innovations in Software Engineering Conference},
year = {2023},
isbn = {9798400700644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Allahabad, India}
}

@proceedings{10.1145/3578741,
title = {MLNLP '22: Proceedings of the 2022 5th International Conference on Machine Learning and Natural Language Processing},
year = {2022},
isbn = {9781450399067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@proceedings{10.1145/3579051,
title = {IJCKG '22: Proceedings of the 11th International Joint Conference on Knowledge Graphs},
year = {2022},
isbn = {9781450399876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@proceedings{10.1145/3579109,
title = {ICVIP '22: Proceedings of the 2022 6th International Conference on Video and Image Processing},
year = {2022},
isbn = {9781450397568},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shanghai, China}
}

@article{10.1145/3579334,
author = {Hsu, Amanda and Li, Frank and Pearce, Paul},
title = {Fiat Lux: Illuminating IPv6 Apportionment with Different Datasets},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/3579334},
doi = {10.1145/3579334},
abstract = {IPv6 adoption continues to grow, making up more than 40\% of client traffic to Google globally. While the ubiquity of the IPv4 address space makes it comparably easier to understand, the vast and less studied IPv6 address space motivates a variety of works detailing methodology to collect and analyze IPv6 properties, many of which use knowledge from specific data sources as a lens for answering research questions. Despite such work, questions remain on basic properties such as the appropriate prefix size for different research tasks.Our work fills this knowledge gap by presenting an analysis of the apportionment of the IPv6 address space from the ground-up, using data and knowledge from numerous data sources simultaneously, aimed at identifying how to leverage IPv6 address information for a variety of research tasks. Utilizing WHOIS data from RIRs, routing data, and hitlists, we highlight fundamental differences in apportionment sizes and structural properties depending on data source and examination method. We focus on the different perspectives each dataset offers and the disjoint, heterogeneous nature of these datasets when taken together. We additionally leverage a graph-based analysis method for these datasets that allows us to draw conclusions regarding when and how to intersect the datasets and their utility. The differences in each dataset's perspective is not due to dataset problems but rather stems from a variety of differing structural and deployment behaviors across RIRs and IPv6 providers alike. In light of these inconsistencies, we discuss network address partitioning, best practices, and considerations for future IPv6 measurement and analysis projects.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = mar,
articleno = {21},
numpages = {24},
keywords = {IP allocation, IP apportionment, IPv6, network measurement}
}

@proceedings{10.1145/3579375,
title = {ACSW '23: Proceedings of the 2023 Australasian Computer Science Week},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3579375.3579385,
author = {Nguyen, Minh Hoang and Huynh, Phuong Duy and Dau, Son Hoang and Li, Xiaodong},
title = {Rug-pull malicious token detection on blockchain using supervised learning with feature engineering},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579375.3579385},
doi = {10.1145/3579375.3579385},
abstract = {The rapid development of blockchain and cryptocurrency in the past decade has created a huge demand for digital trading platforms. Popular decentralised exchanges (DEXs) such as Uniswap and PancakeSwap were created to address this market gap, facilitating cryptocurrency exchange without intermediaries and hence eliminating security and privacy issues associated with traditional centralised platforms. This, however, due to lack of regulation, results in the emergence of a host of damaging investment fraudulent schemes, including Ponzi, honey pot, pump-and-dump, and rug-pull.In this study, we aim to investigate the problem of detecting rug-pull on Uniswap using supervised learning. We aggregate a list of 23 features and propose the use of a hybrid feature selection technique to find the most relevant features for rug-pull. The classifier, using this refined set of features, outperforms the classifier in the previous studies and achieves an f1-score of 99\%, a precision of 97\% on non-malicious tokens, and a recall of 99\% on malicious tokens. Additionally, we show that the XGBoost classifier, built using these proposed features, can distinguish scam tokens and newly listed tokens, which are often harder to differentiate as they have similar characteristics, and also propose a validation method.},
booktitle = {Proceedings of the 2023 Australasian Computer Science Week},
pages = {72–81},
numpages = {10},
keywords = {Blockchain, cryptocurrency, feature selection, rug-pull, scam detection, supervised learning},
location = {Melbourne, VIC, Australia},
series = {ACSW '23}
}

@inproceedings{10.1145/3579375.3579391,
author = {Rani, Nanda and Saha, Bikash and Maurya, Vikas and Shukla, Sandeep Kumar},
title = {TTPHunter: Automated Extraction of Actionable Intelligence as TTPs from Narrative Threat Reports},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579375.3579391},
doi = {10.1145/3579375.3579391},
abstract = {With the proliferation of attacks from various Advanced Persistent Threats (APT) groups, it is essential to comprehend the threat actor’s attack patterns to accelerate threat detection and response. The MITRE ATT&amp;CK framework’s Tactics, Techniques, and Procedures (TTPs) help to decipher attack patterns. The APT reports, published by security firms, contain rich information on tools and techniques used by threat actors. These reports are available in unstructured and natural language texts. There is a need for an automated tool to extract TTPs present in natural language text. However, there are few tools available in the literature, but their performance is not very satisfactory. In this work, we propose TTPHunter, to extract TTPs from APT reports by mapping sentence context to relevant TTPs. We fine-tune linear classifiers, which take input as BERT (Bidirectional Encoder Representations from Transformers) embeddings of sentences. We create two datasets: sentence-based (8,387 sentence samples) and document-based (50 threat reports) to validate TTPHunter. TTPHunter achieves the F1-score of 88\% and 75\% for both datasets, respectively. We compare the TTPHunter with rcATT and AttacKG baseline models, and it outperforms both baselines.},
booktitle = {Proceedings of the 2023 Australasian Computer Science Week},
pages = {126–134},
numpages = {9},
keywords = {Cybersecurity, MITRE ATT&amp;CK, Natural Language Processing, TTP Extraction, Threat Intelligence},
location = {Melbourne, VIC, Australia},
series = {ACSW '23}
}

@article{10.1145/3579485,
author = {Watkins, Elizabeth Anne},
title = {Face Work: A Human-Centered Investigation into Facial Verification in Gig Work},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579485},
doi = {10.1145/3579485},
abstract = {Through intensive research on datasets, benchmarks, and models, the computer-vision community has taken great strides to identify the societal biases intrinsic to these technologies. Less is known about the last mile of the computer-vision machine-learning pipeline: on-the-ground integration into the real world. In this paper, I&nbsp;analyze facial verification technology (FVT) through its use as account verification in ride-hail work. Using a sociotechnical framework combined with empirical qualitative research methods, including interviews and analysis of an online community of workers, this research is a deep dive into recognition technologies at the level of local practice.&nbsp;Findings reveal the high-stakes articulation labor demanded of workers to be recognized by these systems, including maintaining multiple mobile devices, repeatedly uploading requisite images, spending time and resources visiting customer-service centers, and making physical changes to their bodies and environments. These strategies constitute repairs to the failures of computer vision in dynamic environments and are required to successfully engage in the sociotechnical interaction protocols demanded by FVT. Drawing on Erving Goffman's terminology around social interaction rituals, I term these cognitive and behavioral negotiations "face work." Drivers' dynamic, responsive, and ad-hoc attempts to become machine-readable have significant implications for relations between identity and power in spaces of security and work, as well as for the integration of machine-learning systems into safety-critical infrastructures. Ultimately, this research emphasizes the crucial role of end users who create and maintain the conditions required for computer vision to produce judgment.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {52},
numpages = {24},
keywords = {articulation, computer vision, facial recognition, identity, repair, work}
}

@article{10.1145/3579607,
author = {Lin, Cindy Kaiying and Jackson, Steven J.},
title = {From Bias to Repair: Error as a Site of Collaboration and Negotiation in Applied Data Science Work},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579607},
doi = {10.1145/3579607},
abstract = {Managing error has become an increasingly central and contested arena within data science work. While recent scholarship in artificial intelligence and machine learning has focused on limiting and eliminating error, practitioners have long used error as a site of collaboration and learning vis-\`{a}-vis labelers, domain experts, and the worlds data scientists seek to model and understand. Drawing from work in CSCW, STS, HCML, and repair studies, as well as from multi-sited ethnographic fieldwork within a government institution and a non-profit organization, we move beyond the notion of error as an edge case or anomaly to make three basic arguments. First, error discloses or calls to attention existing structures of collaboration unseen or underappreciated under 'working' systems. Second, error calls into being new forms and sites of collaboration (including, sometimes, new actors). Third, error redeploys old sites and actors in new ways, often through restructuring relations of hierarchy and expertise which recenter or devalue the position of different actors. We conclude by discussing how an artful living with error can better support the creative strategies of negotiation and adjustment which data scientists and their collaborators engage in when faced with disruption, breakdown, and friction in their work.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {131},
numpages = {32},
keywords = {AI ethics, critical data studies, data science, error, machine learning, repair}
}

@proceedings{10.1145/3579654,
title = {ACAI '22: Proceedings of the 2022 5th International Conference on Algorithms, Computing and Artificial Intelligence},
year = {2022},
isbn = {9781450398336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@proceedings{10.1145/3579895,
title = {ICNCC '22: Proceedings of the 2022 11th International Conference on Networks, Communication and Computing},
year = {2022},
isbn = {9781450398039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@proceedings{10.1145/3579990,
title = {CGO '23: Proceedings of the 21st ACM/IEEE International Symposium on Code Generation and Optimization},
year = {2023},
isbn = {9798400701016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 21st ACM/IEEE International Symposium on Code Generation and Optimization (CGO ’23), and to Montreal. After two years of virtual CGO, we are finally back in person! 

CGO provides a premier venue to bring together researchers and practitioners working at the interface of hardware and software on a wide range of optimization and code generation techniques and related issues. The conference spans the spectrum from purely static to fully dynamic approaches, and from pure software-based methods to specific architectural features and support for code generation and optimization.},
location = {Montr\'{e}al, QC, Canada}
}

@proceedings{10.1145/3580219,
title = {CCEAI '23: Proceedings of the 7th International Conference on Control Engineering and Artificial Intelligence},
year = {2023},
isbn = {9781450397513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@article{10.1145/3580872,
author = {Deng, Kaikai and Zhao, Dong and Han, Qiaoyue and Zhang, Zihan and Wang, Shuyue and Zhou, Anfu and Ma, Huadong},
title = {Midas: Generating mmWave Radar Data from Videos for Training Pervasive and Privacy-preserving Human Sensing Tasks},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/3580872},
doi = {10.1145/3580872},
abstract = {Millimeter wave radar is a promising sensing modality for enabling pervasive and privacy-preserving human sensing. However, the lack of large-scale radar datasets limits the potential of training deep learning models to achieve generalization and robustness. To close this gap, we resort to designing a software pipeline that leverages wealthy video repositories to generate synthetic radar data, but it confronts key challenges including i) multipath reflection and attenuation of radar signals among multiple humans, ii) unconvertible generated data leading to poor generality for various applications, and iii) the class-imbalance issue of videos leading to low model stability. To this end, we design Midas to generate realistic, convertible radar data from videos via two components: (i) a data generation network (DG-Net) combines several key modules, depth prediction, human mesh fitting and multi-human reflection model, to simulate the multipath reflection and attenuation of radar signals to output convertible coarse radar data, followed by a Transformer model to generate realistic radar data; (ii) a variant Siamese network (VS-Net) selects key video clips to eliminate data redundancy for addressing the class-imbalance issue. We implement and evaluate Midas with video data from various external data sources and real-world radar data, demonstrating its great advantages over the state-of-the-art approach for both activity recognition and object detection tasks.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {9},
numpages = {26},
keywords = {cross domain translation, data generation, human activity recognition, radar sensing}
}

@proceedings{10.1145/3581641,
title = {IUI '23: Proceedings of the 28th International Conference on Intelligent User Interfaces},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3581641.3584037,
author = {Ross, Steven I. and Martinez, Fernando and Houde, Stephanie and Muller, Michael and Weisz, Justin D.},
title = {The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584037},
doi = {10.1145/3581641.3584037},
abstract = {Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model’s responses. We developed a prototype system – the Programmer’s Assistant – in order to explore the utility of conversational interactions grounded in code, as well as software engineers’ receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant’s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {491–514},
numpages = {24},
keywords = {code-fluent large language models, conversational interaction, foundation models, human-centered AI},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@inproceedings{10.1145/3581641.3584041,
author = {Bako, Hannah K. and Varma, Alisha and Faboro, Anuoluwapo and Haider, Mahreen and Nerrise, Favour and Kenah, Bissaka and Dickerson, John P and Battle, Leilani},
title = {User-Driven Support for Visualization Prototyping in D3},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584041},
doi = {10.1145/3581641.3584041},
abstract = {Templates have emerged as an effective approach to simplifying the visualization design and programming process. For example, they enable users to quickly generate multiple visualization designs even when using complex toolkits like D3. However, these templates are often treated as rigid artifacts that respond poorly to changes made outside of the template’s established parameters, limiting user creativity. Preserving the user’s creative flow requires a more dynamic approach to template-based visualization design, where tools can respond gracefully to users’ edits when they modify templates in unexpected ways. In this paper, we leverage the structural similarities revealed by templates to design resilient support features for prototyping D3 visualizations: recommendations to suggest complementary interactions for a users’ D3 program; and code augmentation to implement recommended interactions with a single click, even when users deviate from pre-defined templates. We demonstrate the utility of these features in Mirny, a design-focused prototyping environment for D3. In a user study with 20 D3 users, we find that these automated features enable participants to prototype their design ideas with significantly fewer programming iterations. We also characterize key modification strategies used by participants to customize D3 templates. Informed by our findings and participants’ feedback, we discuss the key implications of the use of templates for interleaving visualization programming and design.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {958–972},
numpages = {15},
keywords = {programming, templates, visualization design, visualization prototyping, visualization tool},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@inproceedings{10.1145/3581641.3584059,
author = {Wu, Sherry and Shen, Hua and Weld, Daniel S and Heer, Jeffrey and Ribeiro, Marco Tulio},
title = {ScatterShot: Interactive In-context Example Curation for Text Transformation},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584059},
doi = {10.1145/3581641.3584059},
abstract = {The in-context learning capabilities of LLMs like GPT-3 allow annotators to customize an LLM to their specific tasks with a small number of examples. However, users tend to include only the most obvious patterns when crafting examples, resulting in underspecified in-context functions that fall short on unseen cases. Further, it is hard to know when “enough” examples have been included even for known patterns. In this work, we present ScatterShot, an interactive system for building high-quality demonstration sets for in-context learning. ScatterShot iteratively slices unlabeled data into task-specific patterns, samples informative inputs from underexplored or not-yet-saturated slices in an active learning manner, and helps users label more efficiently with the help of an LLM and the current example set. In simulation studies on two text perturbation scenarios, ScatterShot sampling improves the resulting few-shot functions by 4-5 percentage points over random sampling, with less variance as more examples are added. In a user study, ScatterShot greatly helps users in covering different patterns in the input space and labeling in-context examples more efficiently, resulting in better in-context learning and less user effort.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {353–367},
numpages = {15},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@inproceedings{10.1145/3581641.3584088,
author = {Brachman, Michelle and Pan, Qian and Do, Hyo Jin and Dugan, Casey and Chaudhary, Arunima and Johnson, James M. and Rai, Priyanshu and Chakraborti, Tathagata and Gschwind, Thomas and Laredo, Jim A and Miksovic, Christoph and Scotton, Paolo and Talamadupula, Kartik and Thomas, Gegi},
title = {Follow the Successful Herd: Towards Explanations for Improved Use and Mental Models of Natural Language Systems},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584088},
doi = {10.1145/3581641.3584088},
abstract = {While natural language systems continue improving, they are still imperfect. If a user has a better understanding of how a system works, they may be able to better accomplish their goals even in imperfect systems. We explored whether explanations can support effective authoring of natural language utterances and how those explanations impact users’ mental models in the context of a natural language system that generates small programs. Through an online study (n=252), we compared two main types of explanations: 1) system-focused, which provide information about how the system processes utterances and matches terms to a knowledge base, and 2) social, which provide information about how other users have successfully interacted with the system. Our results indicate that providing social suggestions of terms to add to an utterance helped users to repair and generate correct flows more than system-focused explanations or social recommendations of words to modify. We also found that participants commonly understood some mechanisms of the natural language system, such as the matching of terms to a knowledge base, but they often lacked other critical knowledge, such as how the system handled structuring and ordering. Based on these findings, we make design recommendations for supporting interactions with and understanding of natural language systems.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {220–239},
numpages = {20},
keywords = {AI explainability, mental models, natural language interaction},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@proceedings{10.1145/3581754,
title = {IUI '23 Companion: Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
year = {2023},
isbn = {9798400701078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3581971,
title = {ICBTA '22: Proceedings of the 2022 5th International Conference on Blockchain Technology and Applications},
year = {2022},
isbn = {9781450397575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xi'an, China}
}

@proceedings{10.1145/3582084,
title = {ICSED '22: Proceedings of the 2022 4th International Conference on Software Engineering and Development},
year = {2022},
isbn = {9781450397940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3582197,
title = {ICIT '22: Proceedings of the 2022 10th International Conference on Information Technology: IoT and Smart City},
year = {2022},
isbn = {9781450397438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shanghai, China}
}

@proceedings{10.1145/3582437,
title = {FDG '23: Proceedings of the 18th International Conference on the Foundations of Digital Games},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3582437.3582451,
author = {Villareale, Jennifer and Maram, Sai Siddartha and Seif El-Nasr, Magy and Zhu, Jichen},
title = {Integrating Players’ Perspectives in AI-Based Games: Case Studies of Player-AI Interaction Design},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3582451},
doi = {10.1145/3582437.3582451},
abstract = {The game design community has a long history of adapting different forms of AI techniques to produce new playable experiences. However, current AI-based game design literature focuses primarily on designers’ intent and expression. This paper argues that engaging with players’ perspectives on AI during development is an essential but often overlooked piece in existing AI-based game design processes. By integrating this perspective, game designers can better outline how players may experience AI in the context of games and tailor design decisions to the intended experience. This paper offers three case studies that incorporate the player perspective into the design process and discusses design implications.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {6},
numpages = {9},
keywords = {AI-based Game Design, Game Design, Player Experience, Player-AI Interaction},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3582437.3582467,
author = {Julia, Clement and van Rozen, Riemer},
title = {ScriptButler serves an Empirical Study of PuzzleScript: Analyzing the Expressive Power of a Game DSL through Source Code Analysis},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3582467},
doi = {10.1145/3582437.3582467},
abstract = {Automated Game Design (AGD) empowers game designers with languages and tools that automate game design processes. Domain-Specific Languages (DSLs) promise to deliver an expressive means for rapidly prototyping and fine-tuning interaction mechanisms that support rich emergent player experiences. However, despite the growing number of studies that center around languages for games and play, few prototypes are ever thoroughly validated and evaluated in practice. As a result, it is not yet well understood what the costs, benefits and limitations of DSL formalisms are. To find out, we investigate to what extent rules, affordances and play can be related by means of source code analysis. We study PuzzleScript, a language and online game engine with an active user community. We reverse engineer PuzzleScript’s design and propose ScriptButler, a novel tool prototype and engine for its analysis. To validate our approach, we conduct an empirical study on the quality of the source code by performing an analysis on a curated collection of 95 games. Our results show that ScriptButler can identify bugs and helps relate PuzzleScript rules to game qualities.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {1},
numpages = {11},
keywords = {PuzzleScript, automated game design, domain-specific languages, game design tools, reverse engineering, source code analysis},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3582437.3582484,
author = {Rabii, Youn\`{e}s and Cook, Michael},
title = {Why Oatmeal is Cheap: Kolmogorov Complexity and Procedural Generation},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3582484},
doi = {10.1145/3582437.3582484},
abstract = {Although procedural generation is popular among game developers, academic research on the topic has primarily focused on new applications, with some research into empirical analysis. In this paper we relate theoretical work in information theory to the generation of content for games. We prove that there is a relationship between the Kolomogorov complexity of the most complex artifact a generator can produce, and the size of that generator’s possibility space. In doing so, we identify the limiting relationship between the knowledge encoded in a generator, the density of its output space, and the intricacy of the artifacts it produces. We relate our result to the experience of expert procedural generator designers, and illustrate it with some examples.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {25},
numpages = {7},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@proceedings{10.1145/3582649,
title = {ICIGP '23: Proceedings of the 2023 6th International Conference on Image and Graphics Processing},
year = {2023},
isbn = {9781450398572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chongqing, China}
}

@proceedings{10.1145/3582700,
title = {AHs '23: Proceedings of the Augmented Humans International Conference 2023},
year = {2023},
isbn = {9781450399845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Glasgow, United Kingdom}
}

@proceedings{10.1145/3582935,
title = {ICITEE '22: Proceedings of the 5th International Conference on Information Technologies and Electrical Engineering},
year = {2022},
isbn = {9781450396806},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Changsha, China}
}

@article{10.1145/3586030,
author = {Barke, Shraddha and James, Michael B. and Polikarpova, Nadia},
title = {Grounded Copilot: How Programmers Interact with Code-Generating Models},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586030},
doi = {10.1145/3586030},
abstract = {Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants—with a range of prior experience using the assistant—as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {78},
numpages = {27},
keywords = {AI Assistants, Grounded Theory, Program Synthesis}
}

@article{10.14778/3231751.3231757,
author = {Park, Noseong and Mohammadi, Mahmoud and Gorde, Kshitij and Jajodia, Sushil and Park, Hongkyu and Kim, Youngmin},
title = {Data synthesis based on generative adversarial networks},
year = {2018},
issue_date = {June 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3231751.3231757},
doi = {10.14778/3231751.3231757},
abstract = {Privacy is an important concern for our society where sharing data with partners or releasing data to the public is a frequent occurrence. Some of the techniques that are being used to achieve privacy are to remove identifiers, alter quasi-identifiers, and perturb values. Unfortunately, these approaches suffer from two limitations. First, it has been shown that private information can still be leaked if attackers possess some background knowledge or other information sources. Second, they do not take into account the adverse impact these methods will have on the utility of the released data. In this paper, we propose a method that meets both requirements. Our method, called table-GAN, uses generative adversarial networks (GANs) to synthesize fake tables that are statistically similar to the original table yet do not incur information leakage. We show that the machine learning models trained using our synthetic tables exhibit performance that is similar to that of models trained using the original table for unknown testing cases. We call this property model compatibility. We believe that anonymization/perturbation/synthesis methods without model compatibility are of little value. We used four real-world datasets from four different domains for our experiments and conducted indepth comparisons with state-of-the-art anonymization, perturbation, and generation techniques. Throughout our experiments, only our method consistently shows balance between privacy level and model compatibility.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {1071–1083},
numpages = {13}
}

@article{10.14778/3275536.3275541,
author = {Yang, Jingru and Fan, Ju and Wei, Zhewei and Li, Guoliang and Liu, Tongyu and Du, Xiaoyong},
title = {Cost-effective data annotation using game-based crowdsourcing},
year = {2018},
issue_date = {September 2018},
publisher = {VLDB Endowment},
volume = {12},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/3275536.3275541},
doi = {10.14778/3275536.3275541},
abstract = {Large-scale data annotation is indispensable for many applications, such as machine learning and data integration. However, existing annotation solutions either incur expensive cost for large datasets or produce noisy results. This paper introduces a cost-effective annotation approach, and focuses on the labeling rule generation problem that aims to generate high-quality rules to largely reduce the labeling cost while preserving quality. To address the problem, we first generate candidate rules, and then devise a game-based crowdsourcing approach CROWDGAME to select high-quality rules by considering coverage and precision. CROWDGAME employs two groups of crowd workers: one group answers rule validation tasks (whether a rule is valid) to play a role of rule generator, while the other group answers tuple checking tasks (whether the annotated label of a data tuple is correct) to play a role of rule refuter. We let the two groups play a two-player game: rule generator identifies high-quality rules with large coverage and precision, while rule refuter tries to refute its opponent rule generator by checking some tuples that provide enough evidence to reject rules covering the tuples. This paper studies the challenges in CROWDGAME. The first is to balance the trade-off between coverage and precision. We define the loss of a rule by considering the two factors. The second is rule precision estimation. We utilize Bayesian estimation to combine both rule validation and tuple checking tasks. The third is to select crowdsourcing tasks to fulfill the game-based framework for minimizing the loss. We introduce a minimax strategy and develop efficient task selection algorithms. We conduct experiments on entity matching and relation extraction, and the results show that our method outperforms state-of-the-art solutions.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {57–70},
numpages = {14}
}

@article{10.14778/3377369.3377376,
author = {Yang, Renchi and Shi, Jieming and Xiao, Xiaokui and Yang, Yin and Bhowmick, Sourav S.},
title = {Homogeneous network embedding for massive graphs via reweighted personalized PageRank},
year = {2020},
issue_date = {January 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3377369.3377376},
doi = {10.14778/3377369.3377376},
abstract = {Given an input graph G and a node v ∈ G, homogeneous network embedding (HNE) maps the graph structure in the vicinity of v to a compact, fixed-dimensional feature vector. This paper focuses on HNE for massive graphs, e.g., with billions of edges. On this scale, most existing approaches fail, as they incur either prohibitively high costs, or severely compromised result utility.Our proposed solution, called Node-Reweighted PageRank (NRP), is based on a classic idea of deriving embedding vectors from pairwise personalized PageRank (PPR) values. Our contributions are twofold: first, we design a simple and efficient baseline HNE method based on PPR that is capable of handling billion-edge graphs on commodity hardware; second and more importantly, we identify an inherent drawback of vanilla PPR, and address it in our main proposal NRP. Specifically, PPR was designed for a very different purpose, i.e., ranking nodes in G based on their relative importance from a source node's perspective. In contrast, HNE aims to build node embeddings considering the whole graph. Consequently, node embeddings derived directly from PPR are of suboptimal utility.The proposed NRP approach overcomes the above deficiency through an effective and efficient node reweighting algorithm, which augments PPR values with node degree information, and iteratively adjusts embedding vectors accordingly. Overall, NRP takes O(mlogn) time and O(m) space to compute all node embeddings for a graph with m edges and n nodes. Our extensive experiments that compare NRP against 18 existing solutions over 7 real graphs demonstrate that NRP achieves higher result utility than all the solutions for link prediction, graph reconstruction and node classification, while being up to orders of magnitude faster. In particular, on a billion-edge Twitter graph, NRP terminates within 4 hours, using a single CPU core.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {670–683},
numpages = {14}
}

@article{10.14778/3384345.3384349,
author = {Hilprecht, Benjamin and Schmidt, Andreas and Kulessa, Moritz and Molina, Alejandro and Kersting, Kristian and Binnig, Carsten},
title = {DeepDB: learn from data, not from queries!},
year = {2020},
issue_date = {March 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3384345.3384349},
doi = {10.14778/3384345.3384349},
abstract = {The typical approach for learned DBMS components is to capture the behavior by running a representative set of queries and use the observations to train a machine learning model. This workload-driven approach, however, has two major downsides. First, collecting the training data can be very expensive, since all queries need to be executed on potentially large databases. Second, training data has to be recollected when the workload or the database changes. To overcome these limitations, we take a different route and propose a new data-driven approach for learned DBMS components which directly supports changes of the workload and data without the need of retraining. Indeed, one may now expect that this comes at a price of lower accuracy since workload-driven approaches can make use of more information. However, this is not the case. The results of our empirical evaluation demonstrate that our data-driven approach not only provides better accuracy than state-ofthe- art learned components but also generalizes better to unseen queries.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {992–1005},
numpages = {14}
}

@article{10.14778/3415478.3415556,
author = {Rupprecht, Lukas and Davis, James C. and Arnold, Constantine and Gur, Yaniv and Bhagwat, Deepavali},
title = {Improving reproducibility of data science pipelines through transparent provenance capture},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415556},
doi = {10.14778/3415478.3415556},
abstract = {Data science has become prevalent in a large variety of domains. Inherent in its practice is an exploratory, probing, and fact finding journey, which consists of the assembly, adaptation, and execution of complex data science pipelines. The trustworthiness of the results of such pipelines rests entirely on their ability to be reproduced with fidelity, which is difficult if pipelines are not documented or recorded minutely and consistently. This difficulty has led to a reproducibility crisis and presents a major obstacle to the safe adoption of the pipeline results in production environments. The crisis can be resolved if the provenance for each data science pipeline is captured transparently as pipelines are executed. However, due to the complexity of modern data science pipelines, transparently capturing sufficient provenance to allow for reproducibility is challenging. As a result, most existing systems require users to augment their code or use specific tools to capture provenance, which hinders productivity and results in a lack of adoption.In this paper, we present Ursprung,1 a transparent provenance collection system designed for data science environments.2 The Ursprung philosophy is to capture provenance and build lineage by integrating with the execution environment to automatically track static and runtime configuration parameters of data science pipelines. Rather than requiring data scientists to make changes to their code, Ursprung records basic provenance information from system-level sources and combines it with provenance from application-level sources (e.g., log files, stdout), which can be accessed and recorded through a domain-specific language. In our evaluation, we show that Ursprung is able to capture sufficient provenance for a variety of use cases and only adds an overhead of up to 4\%.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3354–3368},
numpages = {15}
}

@article{10.14778/3415478.3415557,
author = {Quamar, Abdul and \"{O}zcan, Fatma and Miller, Dorian and Moore, Robert J and Niehus, Rebecca and Kreulen, Jeffrey},
title = {Conversational BI: an ontology-driven conversation system for business intelligence applications},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415557},
doi = {10.14778/3415478.3415557},
abstract = {Business intelligence (BI) applications play an important role in the enterprise to make critical business decisions. Conversational interfaces enable non-technical enterprise users to explore their data, democratizing access to data significantly. In this paper, we describe an ontology-based framework for creating a conversation system for BI applications termed as Conversational BI. We create an ontology from a business model underlying the BI application, and use this ontology to automatically generate various artifacts of the conversation system. These include the intents, entities, as well as the training samples for each intent. Our approach builds upon our earlier work, and exploits common BI access patterns to generate intents, their training examples and adapt the dialog structure to support typical BI operations. We have implemented our techniques in Health Insights (HI), an IBM Watson Healthcare offering, providing analysis over insurance data on claims. Our user study demonstrates that our system is quite intuitive for gaining business insights from data. We also show that our approach not only captures the analysis available in the fixed application dashboards, but also enables new queries and explorations.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3369–3381},
numpages = {13}
}

@article{10.14778/3430915.3430918,
author = {Kingsbury, Kyle and Alvaro, Peter},
title = {Elle: inferring isolation anomalies from experimental observations},
year = {2020},
issue_date = {November 2020},
publisher = {VLDB Endowment},
volume = {14},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3430915.3430918},
doi = {10.14778/3430915.3430918},
abstract = {Users who care about their data store it in databases, which (at least in principle) guarantee some form of transactional isolation. However, experience shows that many databases do not provide the isolation guarantees they claim. With the recent proliferation of new distributed databases, demand has grown for checkers that can, by generating client workloads and injecting faults, produce anomalies that witness a violation of a stated guarantee. An ideal checker would be sound (no false positives), efficient (polynomial in history length and concurrency), effective (finding violations in real databases), general (analyzing many patterns of transactions), and informative (justifying the presence of an anomaly with understandable counterexamples). Sadly, we are aware of no checkers that satisfy these goals.We present Elle: a novel checker which infers an Adya-style dependency graph between client-observed transactions. It does so by carefully selecting database objects and operations when generating histories, so as to ensure that the results of database reads reveal information about their version history. Elle can detect every anomaly in Adya et al's formalism (except for predicates), discriminate between them, and provide concise explanations of each. This paper makes the following contributions: we present Elle, demonstrate its soundness over specific datatypes, measure its efficiency against the current state of the art, and give evidence of its effectiveness via a case study of four real databases.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {268–280},
numpages = {13}
}

@article{10.14778/3430915.3430921,
author = {Deng, Xiang and Sun, Huan and Lees, Alyssa and Wu, You and Yu, Cong},
title = {TURL: table understanding through representation learning},
year = {2020},
issue_date = {November 2020},
publisher = {VLDB Endowment},
volume = {14},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3430915.3430921},
doi = {10.14778/3430915.3430921},
abstract = {Relational tables on the Web store a vast amount of knowledge. Owing to the wealth of such tables, there has been tremendous progress on a variety of tasks in the area of table understanding. However, existing work generally relies on heavily-engineered task-specific features and model architectures. In this paper, we present TURL, a novel framework that introduces the pre-training/fine-tuning paradigm to relational Web tables. During pre-training, our framework learns deep contextualized representations on relational tables in an unsupervised manner. Its universal model design with pre-trained representations can be applied to a wide range of tasks with minimal task-specific fine-tuning.Specifically, we propose a structure-aware Transformer encoder to model the row-column structure of relational tables, and present a new Masked Entity Recovery (MER) objective for pre-training to capture the semantics and knowledge in large-scale unlabeled data. We systematically evaluate TURL with a benchmark consisting of 6 different tasks for table understanding (e.g., relation extraction, cell filling). We show that TURL generalizes well to all tasks and substantially outperforms existing methods in almost all instances.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {307–319},
numpages = {13}
}

@article{10.14778/3476249.3476272,
author = {Cai, Kuntai and Lei, Xiaoyu and Wei, Jianxin and Xiao, Xiaokui},
title = {Data synthesis via differentially private markov random fields},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476272},
doi = {10.14778/3476249.3476272},
abstract = {This paper studies the synthesis of high-dimensional datasets with differential privacy (DP). The state-of-the-art solution addresses this problem by first generating a set M of noisy low-dimensional marginals of the input data D, and then use them to approximate the data distribution in D for synthetic data generation. However, it imposes several constraints on M that considerably limits the choices of marginals. This makes it difficult to capture all important correlations among attributes, which in turn degrades the quality of the resulting synthetic data.To address the above deficiency, we propose PrivMRF, a method that (i) also utilizes a set M of low-dimensional marginals for synthesizing high-dimensional data with DP, but (ii) provides a high degree of flexibility in the choices of marginals. The key idea of PrivMRF is to select an appropriate M to construct a Markov random field (MRF) that models the correlations among the attributes in the input data, and then use the MRF for data synthesis. Experimental results on four benchmark datasets show that PrivMRF consistently outperforms the state of the art in terms of the accuracy of counting queries and classification tasks conducted on the synthetic data generated.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2190–2202},
numpages = {13}
}

@article{10.14778/3476249.3476304,
author = {Lockhart, Brandon and Peng, Jinglin and Wu, Weiyuan and Wang, Jiannan and Wu, Eugene},
title = {Explaining inference queries with bayesian optimization},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476304},
doi = {10.14778/3476249.3476304},
abstract = {Obtaining an explanation for an SQL query result can enrich the analysis experience, reveal data errors, and provide deeper insight into the data. Inference query explanation seeks to explain unexpected aggregate query results on inference data; such queries are challenging to explain because an explanation may need to be derived from the source, training, or inference data in an ML pipeline. In this paper, we model an objective function as a black-box function and propose BOExplain, a novel framework for explaining inference queries using Bayesian optimization (BO). An explanation is a predicate defining the input tuples that should be removed so that the query result of interest is significantly affected. BO --- a technique for finding the global optimum of a black-box function --- is used to find the best predicate. We develop two new techniques (individual contribution encoding and warm start) to handle categorical variables. We perform experiments showing that the predicates found by BOExplain have a higher degree of explanation compared to those found by the state-of-the-art query explanation engines. We also show that BOExplain is effective at deriving explanations for inference queries from source and training data on a variety of real-world datasets. BOExplain is open-sourced as a Python package at https://github.com/sfu-db/BOExplain.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2576–2585},
numpages = {10}
}

@article{10.14778/3523210.3523226,
author = {Simonini, Giovanni and Zecchini, Luca and Bergamaschi, Sonia and Naumann, Felix},
title = {Entity resolution on-demand},
year = {2022},
issue_date = {March 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3523210.3523226},
doi = {10.14778/3523210.3523226},
abstract = {Entity Resolution (ER) aims to identify and merge records that refer to the same real-world entity. ER is typically employed as an expensive cleaning step on the entire data before consuming it. Yet, determining which entities are useful once cleaned depends solely on the user's application, which may need only a fraction of them. For instance, when dealing with Web data, we would like to be able to filter the entities of interest gathered from multiple sources without cleaning the entire, continuously-growing data. Similarly, when querying data lakes, we want to transform data on-demand and return the results in a timely manner---a fundamental requirement of ELT (Extract-Load-Transform) pipelines.We propose BrewER, a framework to evaluate SQL SP queries on dirty data while progressively returning results as if they were issued on cleaned data. BrewER tries to focus the cleaning effort on one entity at a time, following an ORDER BY predicate. Thus, it inherently supports top-k and stop-and-resume execution. For a wide range of applications, a significant amount of resources can be saved. We exhaustively evaluate and show the efficacy of BrewER on four real-world datasets.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {1506–1518},
numpages = {13}
}

@article{10.14778/3538598.3538604,
author = {Zhang, Xinyi and Chang, Zhuo and Li, Yang and Wu, Hong and Tan, Jian and Li, Feifei and Cui, Bin},
title = {Facilitating database tuning with hyper-parameter optimization: a comprehensive experimental evaluation},
year = {2022},
issue_date = {May 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3538598.3538604},
doi = {10.14778/3538598.3538604},
abstract = {Recently, using automatic configuration tuning to improve the performance of modern database management systems (DBMSs) has attracted increasing interest from the database community. This is embodied with a number of systems featuring advanced tuning capabilities being developed. However, it remains a challenge to select the best solution for database configuration tuning, considering the large body of algorithm choices. In addition, beyond the applications on database systems, we could find more potential algorithms designed for configuration tuning. To this end, this paper provides a comprehensive evaluation of configuration tuning techniques from a broader perspective, hoping to better benefit the database community. In particular, we summarize three key modules of database configuration tuning systems and conduct extensive ablation studies using various challenging cases. Our evaluation demonstrates that the hyper-parameter optimization algorithms can be borrowed to further enhance the database configuration tuning. Moreover, we identify the best algorithm choices for different modules. Beyond the comprehensive evaluations, we offer an efficient and unified database configuration tuning benchmark via surrogates that reduces the evaluation cost to a minimum, allowing for extensive runs and analysis of new techniques.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1808–1821},
numpages = {14}
}

@article{10.14778/3538598.3538608,
author = {Fan, Wenfei and Jin, Ruochun and Lu, Ping and Tian, Chao and Xu, Ruiqi},
title = {Towards event prediction in temporal graphs},
year = {2022},
issue_date = {May 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3538598.3538608},
doi = {10.14778/3538598.3538608},
abstract = {This paper proposes a class of temporal association rules, denoted by TACOs, for event prediction. As opposed to previous graph rules, TACOs monitor updates to graphs, and can be used to capture temporal interests in recommendation and catch frauds in response to behavior changes, among other things. TACOs are defined on temporal graphs in terms of change patterns and (temporal) conditions, and may carry machine learning (ML) predicates for temporal event prediction. We settle the complexity of reasoning about TACOs, including their satisfiability, implication and prediction problems. We develop a system, referred to as TASTE. TASTE discovers TACOs by iteratively training a rule creator based on generative ML models in a creator-critic framework. Moreover, it predicts events by applying the discovered TACOs. Using real-life and synthetic datasets, we experimentally verify that TASTE is on average 31.4 times faster than conventional data mining methods in TACO discovery, and it improves the accuracy of state-of-the-art event prediction models by 23.4\%.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1861–1874},
numpages = {14}
}

@article{10.14778/3551793.3551797,
author = {Denham, Benjamin and Lai, Edmund M-K. and Sinha, Roopak and Naeem, M. Asif},
title = {Witan: unsupervised labelling function generation for assisted data programming},
year = {2022},
issue_date = {July 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3551793.3551797},
doi = {10.14778/3551793.3551797},
abstract = {Effective supervised training of modern machine learning models often requires large labelled training datasets, which could be prohibitively costly to acquire for many practical applications. Research addressing this problem has sought ways to leverage weak supervision sources, such as the user-defined heuristic labelling functions used in the data programming paradigm, which are cheaper and easier to acquire. Automatic generation of these functions can make data programming even more efficient and effective. However, existing approaches rely on initial supervision in the form of small labelled datasets or interactive user feedback. In this paper, we propose Witan, an algorithm for generating labelling functions without any initial supervision. This flexibility affords many interaction modes, including unsupervised dataset exploration before the user even defines a set of classes. Experiments in binary and multi-class classification demonstrate the efficiency and classification accuracy of Witan compared to alternative labelling approaches.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2334–2347},
numpages = {14}
}

@article{10.14778/3551793.3551846,
author = {Trummer, Immanuel},
title = {BABOONS: black-box optimization of data summaries in natural language},
year = {2022},
issue_date = {July 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3551793.3551846},
doi = {10.14778/3551793.3551846},
abstract = {BABOONS (BlAck BOx Optimization of Natural language data Summaries) optimizes text data summaries for an arbitrary, user-defined utility function. Primarily, it targets scenarios in which utility is evaluated via large language models. Users describe their utility function in natural language or provide a model, trained to score text summaries in a specific domain.BABOONS uses reinforcement learning to explore the space of possible descriptions. In each iteration, BABOONS generates summaries and evaluates their utility. To reduce data processing overheads during summary generation, BABOONS uses a proactive processing strategy that dynamically merges current with likely future queries for efficient processing. Also, BABOONS supports scenario-specific sampling and batch processing strategies. These mechanisms allow to scale processing to large data and item sets. The experiments show that BABOONS scales significantly better than baselines. Also, they show that summaries generated by BABOONS receive higher average grades from users in a large survey.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2980–2993},
numpages = {14}
}

@article{10.14778/3570690.3570694,
author = {Peng, Jinfeng and Shen, Derong and Tang, Nan and Liu, Tieying and Kou, Yue and Nie, Tiezheng and Cui, Hang and Yu, Ge},
title = {Self-Supervised and Interpretable Data Cleaning with Sequence Generative Adversarial Networks},
year = {2022},
issue_date = {November 2022},
publisher = {VLDB Endowment},
volume = {16},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3570690.3570694},
doi = {10.14778/3570690.3570694},
abstract = {We study the problem of self-supervised and interpretable data cleaning, which automatically extracts interpretable data repair rules from dirty data. In this paper, we propose a novel framework, namely Garf, based on sequence generative adversarial networks (SeqGAN). One key information Garf tries to capture is data repair rules (for example, if the city is "Dothan", then the county should be "Houston"). Garf employs a SeqGAN consisting of a generator G and a discriminator D that trains G to learn the dependency relationships (e.g., given a city value "Dothan" as input, the county can be determined as "Houston"). After training, the generator G can be used to generate data repair rules, but may contain both trusted and untrusted rules, especially when learning from dirty data. To mitigate this problem, Garf further updates the learned relationships with another discriminator D' to iteratively improve the quality of both rules and data. Garf takes advantages of both logical and learning-based methods, which allow cleaning dirty data with high interpretability and have no requirements for prior knowledge and training data. Extensive experiments on real-world and synthetic datasets demonstrate the effectiveness of Garf. Garf achieves new state-of-the-art data cleaning result with high accuracy, through learning from dirty datasets without human supervision.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {433–446},
numpages = {14}
}

@article{10.14778/3574245.3574258,
author = {Narayan, Avanika and Chami, Ines and Orr, Laurel and R\'{e}, Christopher},
title = {Can Foundation Models Wrangle Your Data?},
year = {2022},
issue_date = {December 2022},
publisher = {VLDB Endowment},
volume = {16},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3574245.3574258},
doi = {10.14778/3574245.3574258},
abstract = {Foundation Models (FMs) are models trained on large corpora of data that, at very large scale, can generalize to new tasks without any task-specific finetuning. As these models continue to grow in size, innovations continue to push the boundaries of what these models can do on language and image tasks. This paper aims to understand an underexplored area of FMs: classical data tasks like cleaning and integration. As a proof-of-concept, we cast five data cleaning and integration tasks as prompting tasks and evaluate the performance of FMs on these tasks. We find that large FMs generalize and achieve SoTA performance on data cleaning and integration tasks, even though they are not trained for these data tasks. We identify specific research challenges and opportunities that these models present, including challenges with private and domain specific data, and opportunities to make data management systems more accessible to non-experts. We make our code and experiments publicly available at: https://github.com/HazyResearch/fm_data_tasks.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {738–746},
numpages = {9}
}

@article{10.14778/3583140.3583148,
author = {F\"{u}rst, Jonathan and Argerich, Mauricio Fadel and Cheng, Bin},
title = {VersaMatch: Ontology Matching with Weak Supervision},
year = {2023},
issue_date = {February 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3583140.3583148},
doi = {10.14778/3583140.3583148},
abstract = {Ontology matching is crucial to data integration for across-silo data sharing and has been mainly addressed with heuristic and machine learning (ML) methods. While heuristic methods are often inflexible and hard to extend to new domains, ML methods rely on substantial and hard to obtain amounts of labeled training data. To overcome these limitations, we propose VersaMatch, a flexible, weakly-supervised ontology matching system. VersaMatch employs various weak supervision sources, such as heuristic rules, pattern matching, and external knowledge bases, to produce labels from a large amount of unlabeled data for training a discriminative ML model. For prediction, VersaMatch develops a novel ensemble model combining the weak supervision sources with the discriminative model to support generalization while retaining a high precision. Our ensemble method boosts end model performance by 4 points compared to a traditional weak-supervision baseline. In addition, compared to state-of-the-art ontology matchers, VersaMatch achieves an overall 4-point performance improvement in F1 score across 26 ontology combinations from different domains. For recently released, in-the-wild datasets, VersaMatch beats the next best matchers by 9 points in F1. Furthermore, its core weak-supervision logic can easily be improved by adding more knowledge sources and collecting more unlabeled data for training.},
journal = {Proc. VLDB Endow.},
month = feb,
pages = {1305–1318},
numpages = {14}
}

@article{10.14778/3587136.3587146,
author = {Fan, Grace and Wang, Jin and Li, Yuliang and Zhang, Dan and Miller, Ren\'{e}e J.},
title = {Semantics-Aware Dataset Discovery from Data Lakes with Contextualized Column-Based Representation Learning},
year = {2023},
issue_date = {March 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3587136.3587146},
doi = {10.14778/3587136.3587146},
abstract = {Dataset discovery from data lakes is essential in many real application scenarios. In this paper, we propose Starmie, an end-to-end framework for dataset discovery from data lakes (with table union search as the main use case). Our proposed framework features a contrastive learning method to train column encoders from pre-trained language models in a fully unsupervised manner. The column encoder of Starmie captures the rich contextual semantic information within tables by leveraging a contrastive multi-column pre-training strategy. We utilize the cosine similarity between column embedding vectors as the column unionability score and propose a filter-and-verification framework that allows exploring a variety of design choices to compute the unionability score between two tables accordingly. Empirical results on real table benchmarks show that Starmie outperforms the best-known solutions in the effectiveness of table union search by 6.8 in MAP and recall. Moreover, Starmie is the first to employ the HNSW (Hierarchical Navigable Small World) index to accelerate query processing of table union search which provides a 3,000X performance gain over the linear scan baseline and a 400X performance gain over an LSH index (the state-of-the-art solution for data lake indexing).},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {1726–1739},
numpages = {14}
}

@article{10.1613/jair.1.12590,
author = {Kiritchenko, Svetlana and Nejadgholi, Isar and Fraser, Kathleen C.},
title = {Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12590},
doi = {10.1613/jair.1.12590},
abstract = {The pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. Significant effort in Natural Language Processing (NLP) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. Although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this technology can cause unintended harms, such as the silencing of under-represented groups. We review a large body of NLP research on automatic abuse detection with a new focus on ethical challenges, organized around eight established ethical principles: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. In many cases, these principles relate not only to situational ethical codes, which may be context-dependent, but are in fact connected to universal human rights, such as the right to privacy, freedom from discrimination, and freedom of expression. We highlight the need to examine the broad social impacts of this technology, and to bring ethical and human rights considerations to every stage of the application life-cycle, from task formulation and dataset design, to model training and evaluation, to application deployment. Guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including ‘nudging’, ‘quarantining’, value sensitive design, counter-narratives, style transfer, and AI-driven public education applications.evaluation, to application deployment. Guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including 'nudging', 'quarantining', value sensitive design, counter-narratives, style transfer, and AI-driven public education applications.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {431–478},
numpages = {48},
keywords = {natural language}
}

@proceedings{10.5555/3571885,
title = {SC '22: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
year = {2022},
isbn = {9784665454445},
publisher = {IEEE Press},
abstract = {This volume, containing the accepted technical papers and ACM Gordon Bell prize finalists, captures the best current research in all aspects of High Performance Computing (HPC). The SC22 Archive at the conference web site sc22.supercomputing.org complements this volume by collecting other high quality, peer-reviewed material including research posters, the visualization \&amp; data analytics showcase, panels, birds of a feather, workshops, and tutorials.},
location = {Dallas, Texas}
}

@article{10.5555/3586589.3586678,
author = {Chami, Ines and Abu-El-Haija, Sami and Perozzi, Bryan and R\'{e}, Christopher and Murphy, Kevin},
title = {Machine learning on graphs: a model and comprehensive taxonomy},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {There has been a surge of recent interest in graph representation learning (GRL). GRL methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding, focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between network embedding, graph regularization and graph neural networks. We propose a comprehensive taxonomy of GRL methods, aiming to unify several disparate bodies of work. Specifically, we propose the GRAPHEDM framework, which generalizes popular algorithms for semi-supervised learning (e.g. GraphSage, GCN, GAT), and unsupervised learning (e.g. DeepWalk, node2vec) of graph representations into a single consistent approach. To illustrate the generality of GRAPHEDM, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {89},
numpages = {64},
keywords = {network embedding, graph neural networks, geometric deep learning, manifold learning, relational learning}
}

@article{10.5555/3648699.3648859,
author = {Masud, Shoaib Bin and Werenski, Matthew and Murphy, James M. and Aeron, Shuchin},
title = {Multivariate soft rank via entropy-regularized optimal transport: sample efficiency and generative modeling},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {The framework of optimal transport has been leveraged to extend the notion of rank to the multivariate setting as corresponding to an optimal transport map, while preserving desirable properties of the resulting goodness-of-fit (GoF) statistics. In particular, the rank energy (RE) and rank maximum mean discrepancy (RMMD) are distribution-free under the null, exhibit high power in statistical testing, and are robust to outliers. In this paper, we point to and alleviate some of the shortcomings of these GoF statistics that are of practical significance, namely high computational cost, curse of dimensionality in statistical sample complexity, and lack of differentiability with respect to the data. We show that all these issues are addressed by defining multivariate rank as an entropic transport map derived from the entropic regularization of the optimal transport problem, which we refer to as the soft rank. We consequently propose two new statistics, the soft rank energy (sRE) and soft rank maximum mean discrepancy (sRMMD). Given n sample data points, we provide non-asymptotic convergence rates for the sample estimate of the entropic transport map to its population version that are essentially of the order n-1/2 when the source measure is subgaussian and the target measure has compact support. This result is novel compared to existing results which achieve a rate of n-1 but crucially rely on both measures having compact support. In contrast, the corresponding convergence rate of estimating an optimal transport map, and hence the rank map, is exponential in the data dimension. We leverage these fast convergence rates to show that the sample estimates of sRE and sRMMD converge rapidly to their population versions. Combined with the computational efficiency of methods in solving the entropy-regularized optimal transport problem, these results enable efficient rank-based GoF statistical computation, even in high dimensions. Furthermore, the sample estimates of sRE and sRMMD are differentiable with respect to the data and amenable to popular machine learning frameworks that rely on gradient methods. We leverage these properties towards showcasing their utility for generative modeling on two important problems: image generation and generating valid knockoffs for controlled feature selection.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {160},
numpages = {65},
keywords = {optimal transport, multivariate rank, high-dimensional statistics, goodness-of-fit testing, generative modeling, knockoff filtering}
}

@article{10.5555/3648699.3648939,
author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sashank and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
title = {PaLM: scaling language modeling with pathways},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540- billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {240},
numpages = {113},
keywords = {large language models, few-shot learning, natural language processing, scalable deep learning}
}

