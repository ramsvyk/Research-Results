@proceedings{10.1109/3686225,
title = {JCDL '21: Proceedings of the 2021 ACM/IEEE Joint Conference on Digital Libraries},
year = {2021},
isbn = {9781665417709},
publisher = {IEEE Press},
location = {Virtual Event}
}

@inproceedings{10.1109/ASE56229.2023.00099,
author = {Xiong, Jiaqi and Chen, Guoqiang and Chen, Kejiang and Gao, Han and Cheng, Shaoyin and Zhang, Weiming},
title = {HexT5: Unified Pre-Training for Stripped Binary Code Information Inference},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00099},
doi = {10.1109/ASE56229.2023.00099},
abstract = {Decompilation is a widely used process for reverse engineers to significantly enhance code readability by lifting assembly code to a higher-level C-like language, pseudo-code. Nevertheless, the process of compilation and stripping irreversibly discards high-level semantic information that is crucial to code comprehension, such as comments, identifier names, and types. Existing approaches typically recover only one type of information, making them suboptimal for semantic inference. In this paper, we treat pseudo-code as a special programming language, then present a unified pre-trained model, HexT5, that is trained on vast amounts of natural language comments, source identifiers, and pseudo-code using novel pseudo-code-based pretraining objectives. We fine-tune HexT5 on various downstream tasks, including code summarization, variable name recovery, function name recovery, and similarity detection. Comprehensive experiments show that HexT5 achieves state-of-the-art performance on four downstream tasks, and it demonstrates the robust effectiveness and generalizability of HexT5 for binary-related tasks.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {774–786},
numpages = {13},
keywords = {reverse engineering, deep learning, binary diffing, information inference, programming language model},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1109/ASE56229.2023.00150,
author = {Li, Linyu and Xu, Sihan and Liu, Yang and Gao, Ya and Cai, Xiangrui and Wu, Jiarun and Song, Wenli and Liu, Zheli},
title = {LiSum: Open Source Software License Summarization with Multi-Task Learning},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00150},
doi = {10.1109/ASE56229.2023.00150},
abstract = {Open source software (OSS) licenses regulate the conditions under which users can reuse, modify, and distribute the software legally. However, there exist various OSS licenses in the community, written in a formal language, which are typically long and complicated to understand. In this paper, we conducted a 661-participants online survey to investigate the perspectives and practices of developers towards OSS licenses. The user study revealed an indeed need for an automated tool to facilitate license understanding. Motivated by the user study and the fast growth of licenses in the community, we propose the first study towards automated license summarization. Specifically, we released the first high quality text summarization dataset and designed two tasks, i.e., license text summarization (LTS), aiming at generating a relatively short summary for an arbitrary license, and license term classification (LTC), focusing on the attitude inference towards a predefined set of key license terms (e.g., Distribute). Aiming at the two tasks, we present LiSum, a multi-task learning method to help developers overcome the obstacles of understanding OSS licenses. Comprehensive experiments demonstrated that the proposed jointly training objective boosted the performance on both tasks, surpassing state-of-the-art baselines with gains of at least 5 points w.r.t. F1 scores of four summarization metrics and achieving 95.13\% micro average F1 score for classification simultaneously. We released all the datasets, the replication package, and the questionnaires for the community.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {787–799},
numpages = {13},
keywords = {open source software licenses, multi-task learning, license comprehension},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@proceedings{10.1145/3565698,
title = {Chinese CHI '22: Proceedings of the Tenth International Symposium of Chinese CHI},
year = {2022},
isbn = {9781450398695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guangzhou, China and Online, China}
}

@proceedings{10.1145/3583740,
title = {SEC '23: Proceedings of the Eighth ACM/IEEE Symposium on Edge Computing},
year = {2023},
isbn = {9798400701238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SEC is a premier forum for top researchers and practitioners to come together to discuss the opportunities and challenges of edge computing.},
location = {Wilmington, DE, USA}
}

@inproceedings{10.1145/3589335.3665993,
author = {Lim, Ying Ying and Hee, Ming Shan and Yee, Xun Wei and Yau, Weng Kuan and Sim, Xinming and Tay, Wesley and Ng, Wee Siong and Ng, See-Kiong and Lee, Roy Ka-Wei},
title = {AISG's Online Safety Prize Challenge: Detecting Harmful Social Bias in Multimodal Memes},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3665993},
doi = {10.1145/3589335.3665993},
abstract = {Identifying internet memes that perpetuate harmful social biases is a significant challenge due to the memes' associated cultural references and multilingualism. This challenge is particularly apparent in Singapore, where multiple languages and diverse cultural backgrounds can make it more difficult to detect and address these biases. To better address this issue, the Online Safety Prize Challenge (OSPC) was held over ten weeks, focusing on the zero-shot detection of multilingual memes with harmful social bias within the Singaporean context. The OSPC featured an evaluation dataset of 1,629 memes, covering Singapore's four official languages of English, Chinese, Tamil and Malay. As Singlish is an informal, colloquial form of English that is widely used in Singapore, this challenge also included Singlish in the English dataset. The 10-week challenge attracted more than 310 participants from 34 countries, forming 135 teams. This challenge report contains the details for constructing the evaluation dataset and an overview of the systems proposed across various languages and social biases.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1884–1891},
numpages = {8},
keywords = {harmful meme, multilingual, social bias},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3597503.3608130,
author = {Arteaga Garcia, Emily Judith and Nicolaci Pimentel, Jo\~{a}o Felipe and Feng, Zixuan and Gerosa, Marco and Steinmacher, Igor and Sarma, Anita},
title = {How to Support ML End-User Programmers through a Conversational Agent},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608130},
doi = {10.1145/3597503.3608130},
abstract = {Machine Learning (ML) is increasingly gaining significance for enduser programmer (EUP) applications. However, machine learning end-user programmers (ML-EUPs) without the right background face a daunting learning curve and a heightened risk of mistakes and flaws in their models. In this work, we designed a conversational agent named "Newton" as an expert to support ML-EUPs. Newton's design was shaped by a comprehensive review of existing literature, from which we identified six primary challenges faced by ML-EUPs and five strategies to assist them. To evaluate the efficacy of Newton's design, we conducted a Wizard of Oz within-subjects study with 12 ML-EUPs. Our findings indicate that Newton effectively assisted ML-EUPs, addressing the challenges highlighted in the literature. We also proposed six design guidelines for future conversational agents, which can help other EUP applications and software engineering activities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {53},
numpages = {12},
keywords = {end-user programming, conversational agent, wizard of Oz},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3623298,
author = {Huang, Yuchao and Wang, Junjie and Liu, Zhe and Wang, Yawen and Wang, Song and Chen, Chunyang and Hu, Yuanzhe and Wang, Qing},
title = {CrashTranslator: Automatically Reproducing Mobile Application Crashes Directly from Stack Trace},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623298},
doi = {10.1145/3597503.3623298},
abstract = {Crash reports are vital for software maintenance since they allow the developers to be informed of the problems encountered in the mobile application. Before fixing, developers need to reproduce the crash, which is an extremely time-consuming and tedious task. Existing studies conducted the automatic crash reproduction with the natural language described reproducing steps. Yet we find a non-neglectable portion of crash reports only contain the stack trace when the crash occurs. Such stack-trace-only crashes merely reveal the last GUI page when the crash occurs, and lack step-by-step guidance. Developers tend to spend more effort in understanding the problem and reproducing the crash, and existing techniques cannot work on this, thus calling for a greater need for automatic support. This paper proposes an approach named CrashTranslator to automatically reproduce mobile application crashes directly from the stack trace. It accomplishes this by leveraging a pre-trained Large Language Model to predict the exploration steps for triggering the crash, and designing a reinforcement learning based technique to mitigate the inaccurate prediction and guide the search holistically. We evaluate CrashTranslator on 75 crash reports involving 58 popular Android apps, and it successfully reproduces 61.3\% of the crashes, outperforming the state-of-the-art baselines by 109\% to 206\%. Besides, the average reproducing time is 68.7 seconds, outperforming the baselines by 302\% to 1611\%. We also evaluate the usefulness of CrashTranslator with promising results.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {18},
numpages = {13},
keywords = {bug reproduction, stack trace, mobile application testing},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3623314,
author = {Neelofar, Neelofar and Aleti, Aldeida},
title = {Towards Reliable AI: Adequacy Metrics for Ensuring the Quality of System-level Testing of Autonomous Vehicles},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623314},
doi = {10.1145/3597503.3623314},
abstract = {AI-powered systems have gained widespread popularity in various domains, including Autonomous Vehicles (AVs). However, ensuring their reliability and safety is challenging due to their complex nature. Conventional test adequacy metrics, designed to evaluate the effectiveness of traditional software testing, are often insufficient or impractical for these systems. White-box metrics, which are specifically designed for these systems, leverage neuron coverage information. These coverage metrics necessitate access to the underlying AI model and training data, which may not always be available. Furthermore, the existing adequacy metrics exhibit weak correlations with the ability to detect faults in the generated test suite, creating a gap that we aim to bridge in this study.In this paper, we introduce a set of black-box test adequacy metrics called "Test suite Instance Space Adequacy" (TISA) metrics, which can be used to gauge the effectiveness of a test suite. The TISA metrics offer a way to assess both the diversity and coverage of the test suite and the range of bugs detected during testing. Additionally, we introduce a framework that permits testers to visualise the diversity and coverage of the test suite in a two-dimensional space, facilitating the identification of areas that require improvement.We evaluate the efficacy of the TISA metrics by examining their correlation with the number of bugs detected in system-level simulation testing of AVs. A strong correlation, coupled with the short computation time, indicates their effectiveness and efficiency in estimating the adequacy of testing AVs.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {68},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3623322,
author = {Zhang, Yakun and Zhang, Wenjie and Ran, Dezhi and Zhu, Qihao and Dou, Chengfeng and Hao, Dan and Xie, Tao and Zhang, Lu},
title = {Learning-based Widget Matching for Migrating GUI Test Cases},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623322},
doi = {10.1145/3597503.3623322},
abstract = {GUI test case migration is to migrate GUI test cases from a source app to a target app. The key of test case migration is widget matching. Recently, researchers have proposed various approaches by formulating widget matching as a matching task. However, since these matching approaches depend on static word embeddings without using contextual information to represent widgets and manually formulated matching functions, there are main limitations of these matching approaches when handling complex matching relations in apps. To address the limitations, we propose the first learning-based widget matching approach named TEMdroid (TEst Migration) for test case migration. Unlike the existing approaches, TEMdroid uses BERT to capture contextual information and learns a matching model to match widgets. Additionally, to balance the significant imbalance between positive and negative samples in apps, we design a two-stage training strategy where we first train a hard-negative sample miner to mine hard-negative samples, and further train a matching model using positive samples and mined hard-negative samples. Our evaluation on 34 apps shows that TEM-droid is effective in event matching (i.e., widget matching and target event synthesis) and test case migration. For event matching, TEM-droid's Top1 accuracy is 76\%, improving over 17\% compared to baselines. For test case migration, TEMdroid's F1 score is 89\%, also 7\% improvement compared to the baseline approach.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {69},
numpages = {13},
keywords = {test migration, GUI testing, deep learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@proceedings{10.1145/3605507,
title = {WCAE '23: Proceedings of the Workshop on Computer Architecture Education},
year = {2023},
isbn = {9798400702532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Orlando, FL, USA}
}

@proceedings{10.1145/3615895,
title = {IWCTS '23: Proceedings of the 16th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
year = {2023},
isbn = {9798400703577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 16th International Workshop on Computational Transportation Science (IWCTS 2023) - Smart Mobility track is particularly timely given the prominence of human mobility data, such as probe data from cell phones and connected automated vehicles, volunteered geographic information, and other sensing and simulation data. This unprecedented access to sensing data of mobility and integration of this analytics into smart cities and mobility management has led to innovations in intelligent transportation systems, building information management, human dynamics modeling, and urban planning. Due to the scale of these data and simulations, these developments are deeply computational.},
location = {Hamburg, Germany}
}

@proceedings{10.1145/3615979,
title = {SIGSIM-PADS '24: Proceedings of the 38th ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
year = {2024},
isbn = {9798400703638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Atlanta, GA, USA}
}

@proceedings{10.1145/3616855,
title = {WSDM '24: Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 17th ACM International Conference on Web Search and Data Mining - WSDM 2024. WSDM is one of the premier conferences in the fields of web search and data mining, with a dynamic and growing community from academia and industry. After two years of virtual conferences and in-person conferences in Singapore, the 2024 edition is an in-person conference with virtual elements. We hope you enjoy the conference at the "Centro Internacional de Congresos de Yucatan (CIC)" in Merida from March 4 to March 8, 2024.We are excited to kick off the program with a dynamic mix of Tutorials and Industry Day. Our seven tutorials will cover a broad range of search and data mining topics. Industry Day will provide valuable insights from leaders at major technology companies. The core technical program continues WSDM's tradition of a single-track format, featuring 109 thought-provoking papers from both academic and industry experts. We're honored to have inspiring keynote speakers each day: Nicolas Christin (CMU), Elizabeth Reid (Google), and Saiph Savage (Civic A.I. Lab). Additionally, 17 interactive demonstrations will showcase the latest prototypes and systems. The final day offers a stimulating Doctoral Consortium and six engaging workshops on topics including integrity in social networks, large language model for society, psychology-informed information access system, interactive and scalable information retrieval system and machine learning on graphs. WSDM 2024 proudly presents WSDM day on information retrieval and Web in the region. WSDM Cup Day highlights finalists' presentations addressing challenges in Conversational Multi-Doc QA. This diverse and stimulating program promises to be an enriching experience for all!.},
location = {Merida, Mexico}
}

@inproceedings{10.1145/3616855.3635779,
author = {Lin, Ziqian and Ding, Hao and Hoang, Nghia Trong and Kveton, Branislav and Deoras, Anoop and Wang, Hao},
title = {Pre-trained Recommender Systems: A Causal Debiasing Perspective},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635779},
doi = {10.1145/3616855.3635779},
abstract = {Recent studies on pre-trained vision/language models have demonstrated the practical benefit of a new, promising solution-building paradigm in AI where models can be pre-trained on broad data describing a generic task space and then adapted successfully to solve a wide range of downstream tasks, even when training data is severely limited (e.g., in zero- or few-shot learning scenarios). Inspired by such progress, we investigate in this paper the possibilities and challenges of adapting such a paradigm to the context of recommender systems, which is less investigated from the perspective of pre-trained model. In particular, we propose to develop a generic recommender that captures universal interaction patterns by training on generic user-item interaction data extracted from different domains, which can then be fast adapted to improve few-shot learning performance in unseen new domains (with limited data).  However, unlike vision/language data which share strong conformity in the semantic space, universal patterns underlying recommendation data collected across different domains (e.g., different countries or different E-commerce platforms) are often occluded by both in-domain and cross-domain biases implicitly imposed by the cultural differences in their user and item bases, as well as their uses of different e-commerce platforms. As shown in our experiments, such heterogeneous biases in the data tend to hinder the effectiveness of the pre-trained model. To address this challenge, we further introduce and formalize a causal debiasing perspective, which is substantiated via a hierarchical Bayesian deep learning model, named model. Our empirical studies on real-world data show that the proposed model could significantly improve the recommendation performance in zero- and few-shot learning settings under both cross-market and cross-platform scenarios.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {424–433},
numpages = {10},
keywords = {bayesian inference, causality, pre-trained models, probabilistic methods, recommender systems},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3616855.3635795,
author = {Deldari, Shohreh and Spathis, Dimitris and Malekzadeh, Mohammad and Kawsar, Fahim and Salim, Flora D. and Mathur, Akhil},
title = {CroSSL: Cross-modal Self-Supervised Learning for Time-series through Latent Masking},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635795},
doi = {10.1145/3616855.3635795},
abstract = {Limited availability of labeled data for machine learning on multimodal time-series extensively hampers progress in the field. Self-supervised learning (SSL) is a promising approach to learn data representations without relying on labels. However, existing SSL methods require expensive computations of negative pairs and are typically designed for single modalities, which limits their versatility. We introduce CroSSL (Cross-modal SSL), which puts forward two novel concepts: masking intermediate embeddings produced by modality-specific encoders, and their aggregation into a global embedding through a cross-modal aggregator CroSSL allows for handling missing modalities and end-to-end cross-modal earning without requiring prior data preprocessing for handling missing inputs or negative-pair sampling for contrastive learning. We evaluate our method on a wide range of data, including motion sensors such as accelerometers or gyroscopes and biosignals (heart rate, electroencephalograms, electromyograms, electrooculograms, and electrodermal). Overall, CroSSL outperforms previous SSL and supervised benchmarks using minimal labeled data, and also sheds light on how latent masking can improve cross-modal learning.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {152–160},
numpages = {9},
keywords = {cross-modal time-series., masking, self-supervised learning},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3616855.3635811,
author = {Yang, Mingdai and Liu, Zhiwei and Yang, Liangwei and Liu, Xiaolong and Wang, Chen and Peng, Hao and Yu, Philip S.},
title = {Unified Pretraining for Recommendation via Task Hypergraphs},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635811},
doi = {10.1145/3616855.3635811},
abstract = {Although pretraining has garnered significant attention and popularity in recent years, its application in graph-based recommender systems is relatively limited. It is challenging to exploit prior knowledge by pretraining in widely used ID-dependent datasets. On the one hand, user-item interaction history in one dataset can hardly be transferred to other datasets through pretraining, where IDs are different. On the other hand, pretraining and finetuning on the same dataset leads to a high risk of overfitting. In this paper, we propose a novel multitask pretraining framework named Unified Pretraining for Recommendation via Task Hypergraphs. For a unified learning pattern to handle diverse requirements and nuances of various pretext tasks, we design task hypergraphs to generalize pretext tasks to hyperedge prediction. A novel transitional attention layer is devised to discriminatively learn the relevance between each pretext task and recommendation. Experimental results on three benchmark datasets verify the superiority of UPRTH. Additional detailed investigations are conducted to demonstrate the effectiveness of the proposed framework.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {891–900},
numpages = {10},
keywords = {hypergraph learning., multitask pretraining, recommender system},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3616855.3635836,
author = {Chen, Changyu and Li, Yanran and Wei, Chen and Cui, Jianwei and Wang, Bin and Yan, Rui},
title = {Empathetic Response Generation with Relation-aware Commonsense Knowledge},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635836},
doi = {10.1145/3616855.3635836},
abstract = {The development of AI in mental health is a growing field with potential global impact. Machine agents need to perceive users' mental states and respond empathically. Since mental states are often latent and implicit, building such chatbots requires both knowledge learning and knowledge utilization. Our work contributes to this by developing a chatbot that aims to recognize and empathetically respond to users' mental states. We introduce a Conditional Variational Autoencoders (CVAE)-based model that utilizes relation-aware commonsense knowledge to generate responses. This model, while not a replacement for professional mental health support, demonstrates promise in offering informative and empathetic interactions in a controlled environment. On the dataset EmpatheticDialogues, we compare with several SOTA methods and empirically validate the effectiveness of our approach on response informativeness and empathy exhibition. Detailed analysis is also given to demonstrate the learning capability as well as model interpretability. Our code is accessible at http://github.com/ChangyuChen347/COMET-VAE.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {87–95},
numpages = {9},
keywords = {conditional variational autoencoders, dialong system, empathetic response generation},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3616855.3635850,
author = {Jiang, Yangqin and Yang, Yuhao and Xia, Lianghao and Huang, Chao},
title = {DiffKG: Knowledge Graph Diffusion Model for Recommendation},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635850},
doi = {10.1145/3616855.3635850},
abstract = {Knowledge Graphs (KGs) have emerged as invaluable resources for enriching recommendation systems by providing a wealth of factual information and capturing semantic relationships among items. Leveraging KGs can significantly enhance recommendation performance. However, not all relations within a KG are equally relevant or beneficial for the target recommendation task. In fact, certain item-entity connections may introduce noise or lack informative value, thus potentially misleading our understanding of user preferences. To bridge this research gap, we propose a novel knowledge graph diffusion model for recommendation, referred to as DiffKG. Our framework integrates a generative diffusion model with a data augmentation paradigm, enabling robust knowledge graph representation learning. This integration facilitates a better alignment between knowledge-aware item semantics and collaborative relation modeling. Moreover, we introduce a collaborative knowledge graph convolution mechanism that incorporates collaborative signals reflecting user-item interaction patterns, guiding the knowledge graph diffusion process. We conduct extensive experiments on three publicly available datasets, consistently demonstrating the superiority of our DiffKG compared to various competitive baselines. We provide the source code repository of our proposed DiffKG model at the following link: https://github.com/HKUDS/DiffKG},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {313–321},
numpages = {9},
keywords = {diffusion model, knowledge graph learning, recommendation},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3616855.3635851,
author = {Deng, Zhirui and Dou, Zhicheng and Zhu, Yutao and Wen, Ji-Rong},
title = {CL4DIV: A Contrastive Learning Framework for Search Result Diversification},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635851},
doi = {10.1145/3616855.3635851},
abstract = {Search result diversification aims to provide a diversified document ranking list so as to cover as many intents as possible and satisfy the various information needs of different users. Existing approaches usually represented documents by pretrained embeddings (such as doc2vec and Glove). These document representations cannot adequately represent the document's content and are hard to capture the intrinsic user's intent coverage of the given query. Moreover, the limited number of labeled data for search result diversification exacerbates the difficulty of obtaining more efficient document representations. To alleviate these problems and learn more effective document representations, we propose a Contrastive Learning framework for search result DIVersification (CL4DIV). Specifically, we design three contrastive learning tasks from the perspective of subtopics, documents, and candidate document sequences, which correspond to three essential elements in search result diversification. These training tasks are employed to pretrain the document encoder and the document sequence encoder, which are used in the diversified ranking model. Experimental results show that \o{}urs significantly outperforms all existing diversification models. Further analysis demonstrates that our method has wide applicability and can also be used to improve several existing methods.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {171–180},
numpages = {10},
keywords = {contrastive learning, search result diversification, self-supervised learning},
location = {Merida, Mexico},
series = {WSDM '24}
}

@proceedings{10.1145/3616901,
title = {FAIML '23: Proceedings of the 2023 International Conference on Frontiers of Artificial Intelligence and Machine Learning},
year = {2023},
isbn = {9798400707544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@proceedings{10.1145/3623278,
title = {ASPLOS '23: Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
year = {2023},
isbn = {9798400703942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, BC, Canada}
}

@inproceedings{10.1145/3623278.3624758,
author = {Gan, Yu and Liu, Guiyang and Zhang, Xin and Zhou, Qi and Wu, Jiesheng and Jiang, Jiangwei},
title = {Sleuth: A Trace-Based Root Cause Analysis System for Large-Scale Microservices with Graph Neural Networks},
year = {2024},
isbn = {9798400703942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623278.3624758},
doi = {10.1145/3623278.3624758},
abstract = {Cloud microservices are being scaled up due to the rising demand for new features and the convenience of cloud-native technologies. However, the growing scale of microservices complicates the remote procedure call (RPC) dependency graph, exacerbates the tail-of-scale effect, and makes many of the empirical rules for detecting the root cause of end-to-end performance issues unreliable. Additionally, existing open-source microservice benchmarks are too small to evaluate performance debugging algorithms at a production-scale with hundreds or even thousands of services and RPCs.To address these challenges, we present Sleuth, a trace-based root cause analysis (RCA) system for large-scale microservices using un-supervised graph learning. Sleuth leverages a graph neural network to capture the causal impact of each span in a trace, and trace clustering using a trace distance metric to reduce the amount of traces required for root cause localization. A pre-trained Sleuth model can be transferred to different microservice applications without any retraining or with few-shot fine-tuning. To quantitatively evaluate the performance and scalability of Sleuth, we propose a method to generate microservice benchmarks comparable to a production-scale. The experiments on the existing benchmark suites and synthetic large-scale microservices indicate that Sleuth has significantly outperformed the prior work in detection accuracy, performance, and adaptability on a large-scale deployment.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {324–337},
numpages = {14},
location = {Vancouver, BC, Canada},
series = {ASPLOS '23}
}

@proceedings{10.1145/3623509,
title = {TEI '24: Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction},
year = {2024},
isbn = {9798400704024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cork, Ireland}
}

@inproceedings{10.1145/3623509.3633400,
author = {Albaugh, Lea and Gonzalez, Jesse T and Hudson, Scott E},
title = {Tensions and Resolutions in Hybrid Basketry: Joining 3D Printing and Handweaving},
year = {2024},
isbn = {9798400704024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623509.3633400},
doi = {10.1145/3623509.3633400},
abstract = {By documenting and annotating one author's ongoing project combining 3D printing and handweaving to produce computational hybrid baskets, we contribute a framework for understanding hybrid craft. We identify three levels of material practice as observed in the basketry project—physical joinery between rigid printed-plastic parts and soft textiles, seamful multipart fabrication workflows, and aesthetics which negotiate between “basketlike” and “computational” forms—and analyze tensions and possible resolutions at each level.},
booktitle = {Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {52},
numpages = {13},
keywords = {Hybrid fabrication, computational craft, exploratory fabrication, material practice, weaving},
location = {Cork, Ireland},
series = {TEI '24}
}

@proceedings{10.1145/3625223,
title = {RSP '23: Proceedings of the 34th International Workshop on Rapid System Prototyping},
year = {2023},
isbn = {9798400704109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@proceedings{10.1145/3625549,
title = {HPDC '24: Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing},
year = {2024},
isbn = {9798400704130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {HPDC is the premier annual conference for presenting the latest research on the design, implementation, evaluation, and use of parallel and distributed systems for high-end computing. HPDC provides research contributions in all aspects of parallel and distributed computing such as resilience, AI-based systems and applications, data compression, serverless computing, software systems, workflows, performance modeling, hardware accelerators, scientific computing, resource management, security aspects and many others. The scientific contribution of the conference lays its groundwork for the significant endeavor required to implement actual systems and applications, along with the priceless knowledge acquired through active measurement and experimentation in real-world use cases.},
location = {Pisa, Italy}
}

@inproceedings{10.1145/3625549.3658663,
author = {Phung, Thanh Son and Thomas, Colin and Ward, Logan and Chard, Kyle and Thain, Douglas},
title = {Accelerating Function-Centric Applications by Discovering, Distributing, and Retaining Reusable Context in Workflow Systems},
year = {2024},
isbn = {9798400704130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625549.3658663},
doi = {10.1145/3625549.3658663},
abstract = {Workflow systems provide a convenient way for users to write large-scale applications by composing independent tasks into large graphs that can be executed concurrently on high-performance clusters. In many newer workflow systems, tasks are often expressed as a combination of function invocations in a high-level language. Because necessary code and data are not statically known prior to execution, they must be moved into the cluster at runtime. An obvious way of doing this is to translate function invocations into self-contained executable programs and run them as usual, but this brings a hefty performance penalty: a function invocation now needs to piggyback its context with extra code and data to a remote node, and the remote node needs to take extra time to reconstruct the invocation's context before executing it, both detrimental to lightweight short-running functions.A better solution for workflow systems is to treat functions and invocations as first-class abstractions: subsequent invocations of the same function on a worker node should only pay for the cost of context setup once and reuse the context between different invocations. The remaining problems lie in discovering, distributing, and retaining the reusable context among workers. In this paper, we discuss the rationale and design requirement of these mechanisms to support context reuse, and implement them in TaskVine, a data-intensive distributed framework and execution engine. Our results from executing a large-scale neural network inference application and a molecular design application show that treating functions and invocations as first-class abstractions reduces the execution time of the applications by 94.5\% and 26.9\%, respectively.},
booktitle = {Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing},
pages = {122–134},
numpages = {13},
keywords = {workflow systems, serverless computing, distributed storage, burst buffers},
location = {Pisa, Italy},
series = {HPDC '24}
}

@proceedings{10.1145/3626183,
title = {SPAA '24: Proceedings of the 36th ACM Symposium on Parallelism in Algorithms and Architectures},
year = {2024},
isbn = {9798400704161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 36th ACM Symposium on Parallelism in Algorithms and Architectures - SPAA 2024. SPAA aims to develop a deeper understanding of parallel and distributed computing, both in theory and in practice. Topics relevant to SPAA include algorithms, data structures, computational models, complexity theory, architectures, performance engineering, languages, runtime systems, compilers, programming systems, and networking systems. This year, there were 125 submissions to SPAA (117 regular submission and 8 brief announcements). The program committee accepted 35 regular papers and 19 brief announcements.},
location = {Nantes, France}
}

@inproceedings{10.1145/3626183.3659941,
author = {Kim, Jeonghyeon and Jung, Jaehwang and Kang, Jeehoon},
title = {Expediting Hazard Pointers with Bounded RCU Critical Sections},
year = {2024},
isbn = {9798400704161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626183.3659941},
doi = {10.1145/3626183.3659941},
abstract = {Reclamation schemes for concurrent data structures tackle the challenge of synchronizing memory accesses and reclamation. Early schemes faced a tradeoff between robustness and efficiency : hazard pointers (HP) bounds the number of unreclaimed nodes, but it is inefficient due to per-node protection; and RCU sacrifices robustness for efficiency as a single thread may block the entire reclamation. Recent schemes attempt to break the tradeoff by sending signals to blocking threads to abort their operations. However, they are (1)inefficient due to starvation in long-running operations and frequent signals, and (2)inapplicable to a wide class of data structures. We design a novel reclamation scheme that overcomes the above limitations. To address the long-running operations and applicability, we propose HP-RCU, integrating RCU-expedited traversal that alternates between HP and RCU phases. To additionally ensure robustness against stalled threads, we develop HP-BRCU by modularly replacing RCU with bounded RCU (BRCU) that efficiently bounds the duration of RCU phases by rarely sending signals. We show that HP-BRCU is robust, widely applicable, and as efficient as RCU, outperforming robust schemes across various workloads.},
booktitle = {Proceedings of the 36th ACM Symposium on Parallelism in Algorithms and Architectures},
pages = {1–13},
numpages = {13},
keywords = {concurrency, hazard pointers, memory management, read-copy-update},
location = {Nantes, France},
series = {SPAA '24}
}

@proceedings{10.1145/3626203,
title = {PEARC '24: Practice and Experience in Advanced Research Computing 2024: Human Powered Computing},
year = {2024},
isbn = {9798400704192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Providence, RI, USA}
}

@inproceedings{10.1145/3626203.3670586,
author = {Thomas, Mary P and Mehringer, Susan and Cahill, Katharine and Dey, Charlie and Guilfoos, Brian and Joiner, David and Navarro, John-Paul and Powell, Jeaime H. and Knepper, Richard},
title = {Building a Federated Catalog for CyberTraining Materials: The HPC-ED Pilot Project},
year = {2024},
isbn = {9798400704192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626203.3670586},
doi = {10.1145/3626203.3670586},
abstract = {To improve the sharing and discovery of CyberTraining materials, the HPC-ED Pilot project team is building a platform for the community to better share and find training materials through a federated catalog. The platform, currently in early test mode, is focused on a flexible platform, informative metadata, and community participation. By creating a framework for identifying, sharing, and including content broadly, HPC-ED will: allow providers of training materials to reach new groups of learners; extend the breadth and depth of training materials; and enable local sites to add or extend local portals.},
booktitle = {Practice and Experience in Advanced Research Computing 2024: Human Powered Computing},
articleno = {88},
numpages = {5},
keywords = {Community engagement, Cyberinfrastructure, Education, Globus, HPC, Metadata, Training},
location = {Providence, RI, USA},
series = {PEARC '24}
}

@proceedings{10.1145/3626205,
title = {CPSS '24: Proceedings of the 10th ACM Cyber-Physical System Security Workshop},
year = {2024},
isbn = {9798400704208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3626232,
title = {CODASPY '24: Proceedings of the Fourteenth ACM Conference on Data and Application Security and Privacy},
year = {2024},
isbn = {9798400704215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the fourteenth edition of the ACM Conference on Data and Application Security and Privacy (CODASPY 2024), for the first time held outside United States of America. This conference series has been founded to foster novel and exciting research in the data and application security and privacy arena and to help generate new directions for further research and development. The initial concept was established by the two co-founders, Elisa Bertino and Ravi Sandhu, and sharpened by subsequent discussions with several fellow data security and privacy researchers. Their enthusiastic encouragement persuaded the co-founders to move ahead with the always daunting task of creating a high-quality conference. CODASPY has become a leading forum for presentation of research results and experience reports on hardware and software security. The conference gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of data and applications security and privacy.},
location = {Porto, Portugal}
}

@proceedings{10.1145/3626252,
title = {SIGCSE 2024: Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 55th annual SIGCSE Technical Symposium on Computer Science Education (SIGCSE TS 2024)! This year, we have returned to Portland, Oregon. We hope that, like us, you are looking forward to a highly productive and engaging symposium that provides ample opportunity to renew old relationships, build new connections, and learn about the latest advances in our field. While we are sure that there will be a few surprises along the way, we hope and expect that we won't experience anything nearly as disruptive as the opening days of the pandemic, which occurred when we last tried to gather here in 2020.Our theme for this year's symposium is "Blazing New Trails in CS Education." This broad theme captures the exceptional work being performed by this community to enhance our teaching, improve our assessments, attract diverse students, and all of the other laudable projects, initiatives, and undertakings that affect positive change. The breadth of the program is substantial - there truly should be something for everyone. In fact, your biggest challenge may be deciding which session to attend in each time slot because there is so much going on! We know that many of you want to attend as many sessions as possible while you are here in Portland, but we encourage you to also find a little bit of time for yourself so that you leave Portland refreshed, renewed and encouraged, rather than exhausted or burnt out.},
location = {Portland, OR, USA}
}

@inproceedings{10.1145/3626252.3630761,
author = {Mason, Raina and Simon and Becker, Brett A. and Crick, Tom and Davenport, James H.},
title = {A Global Survey of Introductory Programming Courses},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630761},
doi = {10.1145/3626252.3630761},
abstract = {We present results of an in-depth survey of nearly 100 introductory programming (CS1) instructors in 18 countries spanning six continents. Although CS1 is well studied, relatively few broadly-scoped studies have been conducted, and none prior have exceeded regional scale. In addition, CS1 is a notoriously fickle and often changing course, and many might find it beneficial to know what other instructors are doing across the globe; perhaps more so as we continue to understand the impact of the COVID-19 pandemic on computing education and as the effects of Generative AI take hold. Expanding upon several surveys conducted in Australasia, the UK, and Ireland, this survey facilitates a direct comparison of global trends in CS1. The survey goes beyond environmental factors such as languages used, and examines why CS1 instructors teach what they do, in the ways they do. In total the survey spans 84 institutions and 91 courses in which a total of over 40,000 students are enrolled.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {799–805},
numpages = {7},
keywords = {covid-19, cs 1, cs-1, cs1, global, instructors, introductory programming, novice programmers, programming languages, survey, teaching languages},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3626252.3630822,
author = {Taylor, Andrew and Vassar, Alexandra and Renzella, Jake and Pearce, Hammond},
title = {dcc --help: Transforming the Role of the Compiler by Generating Context-Aware Error Explanations with Large Language Models},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630822},
doi = {10.1145/3626252.3630822},
abstract = {In the challenging field of introductory programming, high enrolments and failure rates drive us to explore tools and systems to enhance student outcomes, especially automated tools that scale to large cohorts. This paper presents and evaluates the dcc --help tool, an integration of a Large Language Model (LLM) into the Debugging C Compiler (DCC) to generate unique, novice-focused explanations tailored to each error. dcc --help prompts an LLM with contextual information of compile- and run-time error occurrences, including the source code, error location and standard compiler error message. The LLM is instructed to generate novice-focused, actionable error explanations and guidance, designed to help students understand and resolve problems without providing solutions. dcc --help was deployed to our CS1 and CS2 courses, with 2,565 students using the tool over 64,000 times in ten weeks. We analysed a subset of these error/explanation pairs to evaluate their properties, including conceptual correctness, relevancy, and overall quality. We found that the LLM-generated explanations were conceptually accurate in 90\% of compile-time and 75\% of run-time cases, but often disregarded the instruction not to provide solutions in code. Our findings, observations and reflections following deployment indicate that dcc --help provides novel opportunities for scaffolding students' introduction to programming.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1314–1320},
numpages = {7},
keywords = {ai in cs1, ai in education, compiler error messages, cs1, debugging, error message enhancement, generative ai, large language models, programming error messages},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3626252.3630903,
author = {Smith, Gillian},
title = {Pairing Ungrading with Project-Based Learning in CS1 for Inherently Flexible Course Design},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630903},
doi = {10.1145/3626252.3630903},
abstract = {This experience report details the pedagogical approach and curriculum for an introductory programming course for non-majors that combines creative coding, ungrading, and project-based learning, with typical enrollment between 120-140 students. Through a series of skills labs, a term-long group project, and regular self-evaluation milestones, students both build their confidence and motivation to learn programming, as well as typical introductory programming skills. Key to the course's success is the integration of project-based learning with a self-evaluation approach to ungrading. In this paper, I present the design of the course, the underlying pedagogical approach leading to course design decisions, and offer resources for adopting this approach in similar CS1 courses. The paper closes with discussion reflecting on the experiences observed throughout teaching this course, and suggests that the approach of blending ungrading with project-based learning shows promise as an inherently flexible course design that supports student wellbeing, confidence, and motivation.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1265–1271},
numpages = {7},
keywords = {creative coding, experience report, inclusive pedagogy, project-based learning, ungrading},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@proceedings{10.1145/3626772,
title = {SIGIR '24: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 47th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2024), taking place in Washington D.C., USA, from July 14 to 18, 2024.SIGIR serves as the foremost international forum for the presentation of groundbreaking research findings, the demonstration of innovative systems and techniques, and the exploration of forwardthinking research directions in the field of information retrieval.This year's SIGIR is an in-person conference. We believe that an in-person conference is beneficial for several reasons: it fosters direct engagement and networking opportunities, enhances the exchange of research ideas, contributes to a more dynamic and productive conference experience, and nurtures our research community by welcoming newcomers, providing them with the opportunity to become acquainted with SIGIR traditions. This decision has not been made lightly. We understand the challenges that can pose in the aftermath of a pandemic and amidst the uncertainties of the world around us. To accommodate those who cannot attend, we have implemented a series of measures such as proxy presenters, livestreaming, and recording sessions. These steps are taken to ensure that everyone has access to the valuable content that the conference offers.},
location = {Washington DC, USA}
}

@inproceedings{10.1145/3626772.3657684,
author = {Zhang, Kaike and Cao, Qi and Wu, Yunfan and Sun, Fei and Shen, Huawei and Cheng, Xueqi},
title = {LoRec: Combating Poisons with Large Language Model for Robust Sequential Recommendation},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657684},
doi = {10.1145/3626772.3657684},
abstract = {Sequential recommender systems stand out for their ability to capture users' dynamic interests and the patterns of item transitions. However, the inherent openness of sequential recommender systems renders them vulnerable to poisoning attacks, where fraudsters are injected into the training data to manipulate learned patterns. Traditional defense methods predominantly depend on predefined assumptions or rules extracted from specific known attacks, limiting their generalizability to unknown attacks. To solve the above problems, considering the rich open-world knowledge encapsulated in Large Language Models (LLMs), we attempt to introduce LLMs into defense methods to broaden the knowledge beyond limited known attacks. We propose LoRec, an innovative framework that employs LLM-Enhanced Calibration to strengthen the robustness of sequential Recommender systems against poisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) that refines the training process of sequential recommender systems with knowledge derived from LLMs, applying a user-wise reweighting to diminish the impact of attacks. Incorporating LLMs' open-world knowledge, the LCT effectively converts the limited, specific priors or rules into a more general pattern of fraudsters, offering improved defenses against poisons. Our comprehensive experiments validate that LoRec, as a general framework, significantly strengthens the robustness of sequential recommender systems.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1733–1742},
numpages = {10},
keywords = {large language model, poisoning attack, robust sequential recommendation},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657691,
author = {Faggioli, Guglielmo and Ferro, Nicola and Perego, Raffaele and Tonellotto, Nicola},
title = {Dimension Importance Estimation for Dense Information Retrieval},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657691},
doi = {10.1145/3626772.3657691},
abstract = {Recent advances in Information Retrieval have shown the effectiveness of embedding queries and documents in a latent high-dimensional space to compute their similarity. While operating on such high-dimensional spaces is effective, in this paper, we hypothesize that we can improve the retrieval performance by adequately moving to a query-dependent subspace. More in detail, we formulate the Manifold Clustering (MC) Hypothesis: projecting queries and documents onto a subspace of the original representation space can improve retrieval effectiveness. To empirically validate our hypothesis, we define a novel class of Dimension IMportance Estimators (DIME). Such models aim to determine how much each dimension of a high-dimensional representation contributes to the quality of the final ranking and provide an empirical method to select a subset of dimensions where to project the query and the documents. To support our hypothesis, we propose an oracle DIME, capable of effectively selecting dimensions and almost doubling the retrieval performance. To show the practical applicability of our approach, we then propose a set of DIMEs that do not require any oracular piece of information to estimate the importance of dimensions. These estimators allow us to carry out a dimensionality selection that enables performance improvements of up to +11.5\% (moving from 0.675 to 0.752 nDCG@10) compared to the baseline methods using all dimensions. Finally, we show that, with simple and realistic active feedback, such as the user's interaction with a single relevant document, we can design a highly effective DIME, allowing us to outperform the baseline by up to +0.224 nDCG@10 points (+58.6\%, moving from 0.384 to 0.608).},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1318–1328},
numpages = {11},
keywords = {dense information retrieval, dense representation, dimension importance estimation},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657705,
author = {Wang, Pancheng and Li, Shasha and Li, Dong and Long, Kehan and Tang, Jintao and Wang, Ting},
title = {Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657705},
doi = {10.1145/3626772.3657705},
abstract = {Automatically condensing multiple topic-related scientific papers into a succinct and concise summary is referred to as Multi-Document Scientific Summarization (MDSS). Currently, while commonly used abstractive MDSS methods can generate flexible and coherent summaries, the difficulty in handling global information and the lack of guidance during decoding still make it challenging to generate better summaries. To alleviate these two shortcomings, this paper introduces summary candidates into MDSS, utilizing the global information of the document set and additional guidance from the summary candidates to guide the decoding process. Our insights are twofold: Firstly, summary candidates can provide instructive information from both positive and negative perspectives, and secondly, selecting higher-quality candidates from multiple options contributes to producing better summaries. Drawing on the insights, we propose a summary candidates fusion framework - Disentangling Instructive information from Ranked candidates (DIR) for MDSS. Specifically, DIR first uses a specialized pairwise comparison method towards multiple candidates to pick out those of higher quality. Then DIR disentangles the instructive information of summary candidates into positive and negative latent variables with Conditional Variational Autoencoder. These variables are further incorporated into the decoder to guide generation. We evaluate our approach with three different types of Transformer-based models and three different types of candidates, and consistently observe noticeable performance improvements according to automatic and human evaluation. More analyses further demonstrate the effectiveness of our model in handling global information and enhancing decoding controllability.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2028–2037},
numpages = {10},
keywords = {disentangled representation learning, multi-document scientific summarization, summary candidates, summary ranking},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657750,
author = {Xu, Shicheng and Hou, Danyang and Pang, Liang and Deng, Jingcheng and Xu, Jun and Shen, Huawei and Cheng, Xueqi},
title = {Invisible Relevance Bias: Text-Image Retrieval Models Prefer AI-Generated Images},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657750},
doi = {10.1145/3626772.3657750},
abstract = {With the application of generation models, internet is increasingly inundated with AI-generated content (AIGC), causing both real and AI-generated content indexed in corpus for search. This paper explores the impact of AI-generated images on text-image search in this scenario. Firstly, we construct a benchmark consisting of both real and AI-generated images for this study. In this benchmark, AI-generated images possess visual semantics sufficiently similar to real images. Experiments on this benchmark reveal that text-image retrieval models tend to rank the AI-generated images higher than the real images, even though the AI-generated images do not exhibit more visually relevant semantics to the queries than real images. We call this bias as invisible relevance bias. This bias is detected across retrieval models with different training data and architectures. Further exploration reveals that mixing AI-generated images into the training data of retrieval models exacerbates the invisible relevance bias. These problems cause a vicious cycle in which AI-generated images have a higher chance of exposing from massive data, which makes them more likely to be mixed into the training of retrieval models and such training makes the invisible relevance bias more and more serious. To mitigate this bias and elucidate the potential causes of the bias, firstly, we propose an effective method to alleviate this bias. Subsequently, we apply our proposed debiasing method to retroactively identify the causes of this bias, revealing that the AI-generated images induce the image encoder to embed additional information into their representation. This information makes the retriever estimate a higher relevance score. We conduct experiments to support this assertion.Findings in this paper reveal the potential impact of AI-generated images on retrieval and have implications for further research. Code is released at https://github.com/xsc1234/Invisible-Relevance-Bias.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {208–217},
numpages = {10},
keywords = {aigc, bias and fairness, text-image retrieval},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657807,
author = {Lin, Xinyu and Wang, Wenjie and Li, Yongqi and Yang, Shuo and Feng, Fuli and Wei, Yinwei and Chua, Tat-Seng},
title = {Data-efficient Fine-tuning for LLM-based Recommendation},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657807},
doi = {10.1145/3626772.3657807},
abstract = {Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLM-based recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data.  To tackle these issues, we introduce two primary objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and 2) high efficiency underlines the low costs of the data pruning process. To pursue the two objectives, we propose a novel data pruning method incorporating two scores, namely influence score and effort score, to efficiently identify the influential samples. Particularly, the influence score is introduced to accurately estimate the influence of removing each sample on the overall performance. To achieve low costs of the data pruning process, we employ a small-sized surrogate model to replace LLMs to obtain the influence score. Considering the potential gap between the surrogate model and LLMs, we further propose an effort score to prioritize some hard samples specifically for LLMs. We instantiate the proposed method on two competitive LLM-based recommender models, and empirical results on three real-world datasets validate the effectiveness of our proposed method. In particular, our method uses only 2\% samples to surpass the full data fine-tuning, reducing time costs by 97\%.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {365–374},
numpages = {10},
keywords = {data pruning, efficient fine-tuning, llm-based recommendation},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657808,
author = {Sun, Zhu and Feng, Kaidong and Yang, Jie and Qu, Xinghua and Fang, Hui and Ong, Yew-Soon and Liu, Wenyuan},
title = {Adaptive In-Context Learning with Large Language Models for Bundle Generation},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657808},
doi = {10.1145/3626772.3657808},
abstract = {Most existing bundle generation approaches fall short in generating fixed-size bundles. Furthermore, they often neglect the underlying user intents reflected by the bundles in the generation process, resulting in less intelligible bundles. This paper addresses these limitations through the exploration of two interrelated tasks, i.e., personalized bundle generation and the underlying intent inference, based on different user sessions. Inspired by the reasoning capabilities of large language models (LLMs), we propose an adaptive in-context learning paradigm, which allows LLMs to draw tailored lessons from related sessions as demonstrations, enhancing the performance on target sessions. Specifically, we first employ retrieval augmented generation to identify nearest neighbor sessions, and then carefully design prompts to guide LLMs in executing both tasks on these neighbor sessions. To tackle reliability and hallucination challenges, we further introduce (1) a self-correction strategy promoting mutual improvements of the two tasks without supervision signals and (2) an auto-feedback mechanism for adaptive supervision based on the distinct mistakes made by LLMs on different neighbor sessions. Thereby, the target session can gain customized lessons for improved performance by observing the demonstrations of its neighbor sessions. Experiments on three real-world datasets demonstrate the effectiveness of our proposed method.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {966–976},
numpages = {11},
keywords = {bundle generation, in-context learning, large language models, recommendation, user intent inference},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657840,
author = {Li, Peibo and de Rijke, Maarten and Xue, Hao and Ao, Shuang and Song, Yang and Salim, Flora D.},
title = {Large Language Models for Next Point-of-Interest Recommendation},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657840},
doi = {10.1145/3626772.3657840},
abstract = {The next Point of Interest (POI) recommendation task is to predict users' immediate next POI visit given their historical data. Location-Based Social Network (LBSN) data, which is often used for the next POI recommendation task, comes with challenges. One frequently disregarded challenge is how to effectively use the abundant contextual information present in LBSN data. Previous methods are limited by their numerical nature and fail to address this challenge. In this paper, we propose a framework that uses pretrained Large Language Models (LLMs) to tackle this challenge. Our framework allows us to preserve heterogeneous LBSN data in its original format, hence avoiding the loss of contextual information. Furthermore, our framework is capable of comprehending the inherent meaning of contextual information due to the inclusion of commonsense knowledge. In experiments, we test our framework on three real-world LBSN datasets. Our results show that the proposed framework outperforms the state-of-the-art models in all three datasets. Our analysis demonstrates the effectiveness of the proposed framework in using contextual information as well as alleviating the commonly encountered cold-start and short trajectory problems.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1463–1472},
numpages = {10},
keywords = {large language models, point-of-interest recommendation},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657841,
author = {Chen, Catherine and Merullo, Jack and Eickhoff, Carsten},
title = {Axiomatic Causal Interventions for Reverse Engineering Relevance Computation in Neural Retrieval Models},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657841},
doi = {10.1145/3626772.3657841},
abstract = {Neural models have demonstrated remarkable performance across diverse ranking tasks. However, the processes and internal mechanisms along which they determine relevance are still largely unknown. Existing approaches for analyzing neural ranker behavior with respect to IR properties rely either on assessing overall model behavior or employing probing methods that may offer an incomplete understanding of causal mechanisms. To provide a more granular understanding of internal model decision-making processes, we propose the use of causal interventions to reverse engineer neural rankers, and demonstrate how mechanistic interpretability methods can be used to isolate components satisfying term-frequency axioms within a ranking model. We identify a group of attention heads that detect duplicate tokens in earlier layers of the model, then communicate with downstream heads to compute overall document relevance. More generally, we propose that this style of mechanistic analysis opens up avenues for reverse engineering the processes neural retrieval models use to compute relevance. This work aims to initiate granular interpretability efforts that will not only benefit retrieval model development and training, but ultimately ensure safer deployment of these models.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1401–1410},
numpages = {10},
keywords = {information retrieval axioms, interpretability, neural ranking models, search},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657847,
author = {Frej, Jibril and Dai, Anna and Montariol, Syrielle and Bosselut, Antoine and K\"{a}ser, Tanja},
title = {Course Recommender Systems Need to Consider the Job Market},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657847},
doi = {10.1145/3626772.3657847},
abstract = {Current course recommender systems primarily leverage learner-course interactions, course content, learner preferences, and supplementary course details like instructor, institution, ratings, and reviews, to make their recommendation. However, these systems often overlook a critical aspect: the evolving skill demand of the job market. This paper focuses on the perspective of academic researchers, working in collaboration with the industry, aiming to develop a course recommender system that incorporates job market skill demands. In light of the job market's rapid changes and the current state of research in course recommender systems, we outline essential properties for course recommender systems to address these demands effectively, including explainable, sequential, unsupervised, and aligned with the job market and user's goals. Our discussion extends to the challenges and research questions this objective entails, including unsupervised skill extraction from job listings, course descriptions, and resumes, as well as predicting recommendations that align with learner objectives and the job market and designing metrics to evaluate this alignment. Furthermore, we introduce an initial system that addresses some existing limitations of course recommender systems using large Language Models (LLMs) for skill extraction and Reinforcement Learning (RL) for alignment with the job market. We provide empirical results using open-source data to demonstrate its effectiveness.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {522–532},
numpages = {11},
keywords = {course recommendation, entity linking, recommender system},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657850,
author = {Mansour, Watheq and Zhuang, Shengyao and Zuccon, Guido and Mackenzie, Joel},
title = {Revisiting Document Expansion and Filtering for Effective First-Stage Retrieval},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657850},
doi = {10.1145/3626772.3657850},
abstract = {Document expansion is a technique that aims to reduce the likelihood of term mismatch by augmenting documents with related terms or queries. Doc2Query minus minus (Doc2Query-) represents an extension to the expansion process that uses a neural model to identify and remove expansions that may not be relevant to the given document, thereby increasing the quality of the ranking while simultaneously reducing the amount of augmented data. In this work, we conduct a detailed reproducibility study of Doc2Query- to better understand the trade-offs inherent to document expansion and filtering mechanisms. After successfully reproducing the best-performing method from the Doc2Query- family, we show that filtering actually harms recall-based metrics on various test collections. Next, we explore whether the two-stage "generate-then-filter" process can be replaced with a single generation phase via reinforcement learning. Finally, we extend our experimentation to learned sparse retrieval models and demonstrate that filtering is not helpful when term weights can be learned. Overall, our work provides a deeper understanding of the behaviour and characteristics of common document expansion mechanisms, and paves the way for developing more efficient yet effective augmentation models.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {186–196},
numpages = {11},
keywords = {document expansion, query filtering, reproducibility},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657868,
author = {Yu, Yuanqing and Gao, Chongming and Chen, Jiawei and Tang, Heng and Sun, Yuefeng and Chen, Qian and Ma, Weizhi and Zhang, Min},
title = {EasyRL4Rec: An Easy-to-use Library for Reinforcement Learning Based Recommender Systems},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657868},
doi = {10.1145/3626772.3657868},
abstract = {Reinforcement Learning (RL)-Based Recommender Systems (RSs) have gained rising attention for their potential to enhance long-term user engagement. However, research in this field faces challenges, including the lack of user-friendly frameworks, inconsistent evaluation metrics, and difficulties in reproducing existing studies. To tackle these issues, we introduce EasyRL4Rec, an easy-to-use code library designed specifically for RL-based RSs. This library provides lightweight and diverse RL environments based on five public datasets and includes core modules with rich options, simplifying model development. It provides unified evaluation standards focusing on long-term outcomes and offers tailored designs for state modeling and action representation for recommendation scenarios. Furthermore, we share our findings from insightful experiments with current methods. EasyRL4Rec seeks to facilitate the model development and experimental process in the domain of RL-based RSs. The library is available for public use.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {977–987},
numpages = {11},
keywords = {code library, recommender systems, reinforcement learning},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657882,
author = {Roy, Soumyadeep and Khatua, Aparup and Ghoochani, Fatemeh and Hadler, Uwe and Nejdl, Wolfgang and Ganguly, Niloy},
title = {Beyond Accuracy: Investigating Error Types in GPT-4 Responses to USMLE Questions},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657882},
doi = {10.1145/3626772.3657882},
abstract = {GPT-4 demonstrates high accuracy in medical QA tasks, leading with an accuracy of 86.70\%, followed by Med-PaLM 2 at 86.50\%. However, around 14\% of errors remain. Additionally, current works use GPT-4 to only predict the correct option without providing any explanation and thus do not provide any insight into the thinking process and reasoning used by GPT-4 or other LLMs. Therefore, we introduce a new domain-specific error taxonomy derived from collaboration with medical students. Our GPT-4 USMLE Error (G4UE) dataset comprises 4153 GPT-4 correct responses and 919 incorrect responses to the United States Medical Licensing Examination (USMLE) respectively. These responses are quite long (258 words on average), containing detailed explanations from GPT-4 justifying the selected option. We then launch a large-scale annotation study using the Potato annotation platform and recruit 44 medical experts through Prolific, a well-known crowdsourcing platform. We annotated 300 out of these 919 incorrect data points at a granular level for different classes and created a multi-label span to identify the reasons behind the error. In our annotated dataset, a substantial portion of GPT-4's incorrect responses is categorized as a "Reasonable response by GPT-4," by annotators. This sheds light on the challenge of discerning explanations that may lead to incorrect options, even among trained medical professionals. We also provide medical concepts and medical semantic predications extracted using the SemRep tool for every data point. We believe that it will aid in evaluating the ability of LLMs to answer complex medical questions. We make the resources available at https://github.com/roysoumya/usmle-gpt4-error-taxonomy.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1073–1082},
numpages = {10},
keywords = {gpt-4, medical qa, multi-label dataset, usmle error taxonomy},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657889,
author = {Abdallah, Abdelrahman and Kasem, Mahmoud and Abdalla, Mahmoud and Mahmoud, Mohamed and Elkasaby, Mohamed and Elbendary, Yasser and Jatowt, Adam},
title = {ArabicaQA: A Comprehensive Dataset for Arabic Question Answering},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657889},
doi = {10.1145/3626772.3657889},
abstract = {In this paper, we address the significant gap in Arabic natural language processing (NLP) resources by introducing ArabicaQA, the first large-scale dataset for machine reading comprehension and open-domain question answering in Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701 unanswerable questions created by crowdworkers to look similar to answerable ones, along with additional labels of open-domain questions marks a crucial advancement in Arabic NLP resources. We also present AraDPR, the first dense passage retrieval model trained on the Arabic Wikipedia corpus, specifically designed to tackle the unique challenges of Arabic text retrieval. Furthermore, our study includes extensive benchmarking of large language models (LLMs) for Arabic question answering, critically evaluating their performance in the Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking of LLMs in Arabic question answering offer significant advancements in the field of Arabic NLP. The dataset and code are publicly accessible for further research https://github.com/DataScienceUIBK/ArabicaQA.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2049–2059},
numpages = {11},
keywords = {arabic question answering, information retrieval, llm, question generation},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657893,
author = {Zhang, Lu and Li, Chen and Lei, Yu and Sun, Zhu and Liu, Guanfeng},
title = {An Empirical Analysis on Multi-turn Conversational Recommender Systems},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657893},
doi = {10.1145/3626772.3657893},
abstract = {The rise of conversational recommender systems (CRSs) brings the evolution of the recommendation paradigm, which enables users to interact with the system and achieve dynamic recommendations. As one essential branch, multi-turn CRSs, built on the user simulator paradigm, have attracted great attention due to their powerful ability to accomplish recommendations without real dialogue resources. Recent multi-turn CRS models, equipped with various delicately designed components (e.g., conversation module), achieve state-of-the-art (SOTA) performance. We, for the first time, propose a comprehensive experimental evaluation for existing SOTA multi-turn CRSs to investigate three research questions: (1) reproducibility - are the designed components beneficial to target multi-turn CRSs? (2) scenario-specific adaptability - how do these components perform in various scenarios? and (3) generality - can the effective components from the target CRS be effectively transferred to other multi-turn CRSs? To answer these questions, we design and conduct experiments under different settings, including carefully selected SOTA baselines, components of CRSs, datasets, and evaluation metrics, thus providing an experimental aspect overview of multi-turn CRSs. As a result, we derive several significant insights whereby effective guidelines are provided for future multi-turn CRS model designs across diverse scenarios.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {841–851},
numpages = {11},
keywords = {interactive recommender systems, multi-turn conversational recommender systems, reproducibility},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657904,
author = {Xie, Yuzhang and Lu, Jiaying and Ho, Joyce and Nahab, Fadi and Hu, Xiao and Yang, Carl},
title = {PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657904},
doi = {10.1145/3626772.3657904},
abstract = {Linking (aligning) biomedical concepts across diverse data sources enables various integrative analyses, but it is challenging due to the discrepancies in concept naming conventions. Various strategies have been developed to overcome this challenge, such as those based on string-matching rules, manually crafted thesauri, and machine learning models. However, these methods are constrained by limited prior biomedical knowledge and can hardly generalize beyond the limited amounts of rules, thesauri, or training samples. Recently, large language models (LLMs) have exhibited impressive results in diverse biomedical NLP tasks due to their unprecedentedly rich prior knowledge and strong zero-shot prediction abilities. However, LLMs suffer from issues including high costs, limited context length, and unreliable predictions. In this research, we propose PromptLink, a novel biomedical concept linking framework that leverages LLMs. Empirical results on the concept linking task between two EHR datasets and an external biomedical KG demonstrate the effectiveness of PromptLink. Furthermore, PromptLink is a generic framework without reliance on additional prior knowledge, context, or training data, making it well-suited for concept linking across various types of data sources. The source code of this study is available at https://github.com/constantjxyz/PromptLink.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2589–2593},
numpages = {5},
keywords = {biomedical concept linking, few-shot prompting, large language models for resource-constrained field, retrieve \&amp; re-rank},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657914,
author = {Trippas, Johanne R. and Al Lawati, Sara Fahad Dawood and Mackenzie, Joel and Gallagher, Luke},
title = {What do Users Really Ask Large Language Models? An Initial Log Analysis of Google Bard Interactions in the Wild},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657914},
doi = {10.1145/3626772.3657914},
abstract = {Advancements in large language models (LLMs) have changed information retrieval, offering users a more personalised and natural search experience with technologies like OpenAI ChatGPT, Google Bard (Gemini), or Microsoft Copilot. Despite these advancements, research into user tasks and information needs remains scarce. This preliminary work analyses a Google Bard prompt log with 15,023 interactions called the Bard Intelligence and Dialogue Dataset (BIDD), providing an understanding akin to query log analyses. We show that Google Bard prompts are often verbose and structured, encapsulating a broader range of information needs and imperative (e.g., directive) tasks distinct from traditional search queries. We show that LLMs can support users in tasks beyond the three main types based on user intent: informational, navigational, and transactional. Our findings emphasise the versatile application of LLMs across content creation, LLM writing style preferences, and information extraction. We document diverse user interaction styles, showcasing the adaptability of users to LLM capabilities.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2703–2707},
numpages = {5},
keywords = {dataset, large language models, log analysis, prompt analysis},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657932,
author = {Wang, Qianlong and Ding, Keyang and Luo, Xuan and Xu, Ruifeng},
title = {Improving In-Context Learning via Sequentially Selection and Preference Alignment for Few-Shot Aspect-Based Sentiment Analysis},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657932},
doi = {10.1145/3626772.3657932},
abstract = {In this paper, we leverage in-context learning (ICL) paradigm to handle few-shot aspect-based sentiment analysis (ABSA). Previous works first rank candidate examples by some metrics and then independently retrieve examples similar to test samples. However, their effectiveness may be discounted because of two limitations: in-context example redundancy and example preference misalignment between retriever and LLM. To alleviate them, we propose a novel framework that sequentially retrieves in-context examples. It not only considers which example is useful for the test sample but also prevents its information from being duplicated by already retrieved examples. Subsequently, we exploit the rewards of LLMs on retrieved in-context examples to optimize parameters for bridging preference gaps. Experiments on four ABSA datasets show that our framework is significantly superior to previous works.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2462–2466},
numpages = {5},
keywords = {few-shot aspect-based sentiment analysis, in-context learning, preference alignment, sequentially retrieval},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3626772.3657949,
author = {Wang, Shuai and Zhuang, Shengyao and Zuccon, Guido},
title = {Large Language Models Based Stemming for Information Retrieval: Promises, Pitfalls and Failures},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657949},
doi = {10.1145/3626772.3657949},
abstract = {Text stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form. In Information Retrieval (IR), stemming is used in keyword-based matching pipelines to normalise text before indexing and query processing to improve subsequent matching between document and query keywords. The use of stemming has been shown to often improve the effectiveness of keyword-matching models such as BM25. However, traditional stemming methods, focusing solely on individual terms, overlook the richness of contextual information.Recognizing this gap, in this paper, we investigate the promising idea of using large language models (LLMs) to stem words by lever-aging its capability of context understanding. With this respect, we identify three avenues, each characterised by different trade-offs in terms of computational cost, effectiveness and robustness : (1) use LLMs to stem the vocabulary for a collection, i.e., the set of unique words that appear in the collection (vocabulary stemming), (2) use LLMs to stem each document separately (contextual stemming), and (3) use LLMs to extract from each document entities that should not be stemmed, then use vocabulary stemming to stem the rest of the terms (entity-based contextual stemming). Through a series of empirical experiments, we compare the use of LLMs for stemming with that of traditional lexical stemmers such as Porter and Krovetz for English text. We find that while vocabulary stemming and contextual stemming fail to achieve higher effectiveness than traditional stemmers, entity-based contextual stemming can achieve a higher effectiveness than using Porter stemmer alone, under specific conditions. Code and results are made available at https://github.com/ielab/SIGIR-2024-LLM-Stemming.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2492–2496},
numpages = {5},
keywords = {large language model, text pre-processing, text stemming},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@proceedings{10.1145/3627043,
title = {UMAP '24: Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
year = {2024},
isbn = {9798400704338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3627043.3659541,
author = {Ruan, Qin and Xu, Jin and Leavy, Susan and Mac Namee, Brian and Dong, Ruihai},
title = {Rewriting Bias: Mitigating Media Bias in News Recommender Systems through Automated Rewriting},
year = {2024},
isbn = {9798400704338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627043.3659541},
doi = {10.1145/3627043.3659541},
abstract = {Personalised news recommender systems are effective in disseminating news content based on users’ reading histories but can also amplify and proliferate biased media. This work examines the potential of automated sentence rewriting methods, utilising word replacement methods and large language models (LLMs), to mitigate this side effect of recommender systems. We present a two-step workflow: the application of automated sentence rewriting methods to rewrite biased sentences, and the integration of these rewritten sentences into the recommendation process. We evaluate the effectiveness of sentence rewriting approaches in a simulation framework, to assess how well they mitigate the spread of biased news. Our study demonstrates that applying sentence rewriting to users’ reading histories can result in a significant reduction in the propagation of biased media. Our contributions are threefold: we pioneer the use of LLMs for mitigating the spread of biased news by recommender systems; we demonstrate that algorithms trained on debiased content maintain or improve recommendation accuracy; and we provide a comprehensive exploration of the effectiveness of applying sentence rewriting methods to various components within a recommender system, as well as an investigation of the underlying reasons for their efficacy. This work advances our understanding of media bias mitigation in news content and recommendation algorithms, providing valuable insights into how news recommender systems can prevent the dissemination of biased information.},
booktitle = {Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {67–77},
numpages = {11},
keywords = {Debias, Media Bias, Media Bias Dissemination, News Recommendation, Sentence Rewriting},
location = {Cagliari, Italy},
series = {UMAP '24}
}

@proceedings{10.1145/3627535,
title = {PPoPP '24: Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming},
year = {2024},
isbn = {9798400704352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {PPoPP is the foremost platform for showcasing groundbreaking research in both the practical and theoretical aspects of parallel computing. In today's technological landscape, parallelism is ubiquitous, encompassing everything from microscale devices to vast cloud infrastructures, and from fundamental software layers to advanced applications in artificial intelligence and beyond. The PPoPP community is at the forefront of expanding our understanding and capabilities in these diverse fields.},
location = {Edinburgh, United Kingdom}
}

@proceedings{10.1145/3628516,
title = {IDC '24: Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Delft, Netherlands}
}

@proceedings{10.1145/3629104,
title = {DEBS '24: Proceedings of the 18th ACM International Conference on Distributed and Event-based Systems},
year = {2024},
isbn = {9798400704437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We welcome you to the 18th ACM International Conference on Distributed and Event-Based Systems (DEBS) 2024, hosted as an in-person event at the Institut National des Sciences Appliqu\'{e}es (INSA) Lyon.The history of DEBS spans over 20 years of scientific progress, beginning as a workshop and evolving into a conference 17 years ago. The conference's goals have evolved over time, but its primary objective - to provide a dedicated forum for the dissemination of high-quality, original, and impactful research on distributed systems and event-based computing - has remained unchanged. Alongside scientific contributions, the conference has always featured the discussion of practical insights and the reporting of experiences relevant to the industrial sector.},
location = {Villeurbanne, France}
}

@proceedings{10.1145/3629606,
title = {CHCHI '23: Proceedings of the Eleventh International Symposium of Chinese CHI},
year = {2023},
isbn = {9798400716454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denpasar, Bali, Indonesia}
}

@inproceedings{10.1145/3629606.3629675,
author = {He, Qingyang and Zheng, Weicheng and Bao, Hanxi and Chen, Ruiqi and Tong, Xin},
title = {Exploring Designers’ Perceptions and Practices of Collaborating with Generative AI as a Co-creative Agent in a Multi-stakeholder Design Process: Take the Domain of Avatar Design as an Example},
year = {2024},
isbn = {9798400716454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629606.3629675},
doi = {10.1145/3629606.3629675},
abstract = {Nowadays, the traditional workflow of designers’ completing complicated design tasks has undergone a profound transformation due to the pervasive intervention of generative artificial intelligence (AI) tools, especially when multi-stakeholder participation is getting involved in the design process. Yet we know little about the designers’ perceptions and practices of collaborating with generative AI as a co-creative agent within the context of multi-stakeholder participation. To investigate these questions, we took the domain of avatar design as an example and conducted a qualitative interview study with 21 expert avatar designers who have got different levels of experience and expertise in utilizing generative AI tools in their design workflow. We found that designers not only would fall in a dilemma when deciding whether to consider AI as a co-creative agent according to different stakeholders’ interests, but they also face many challenges in effectively co-creating with the current systems, including challenges in consistently adjusting AI outputs and getting design inspiration within the iterative generation process, etc. Based on our findings, we concluded both the epistemological and creative patterns of collaborating with generative AI and highlighted several design opportunities from both technical and ethical perspectives to better support future designer-AI co-creation.},
booktitle = {Proceedings of the Eleventh International Symposium of Chinese CHI},
pages = {596–613},
numpages = {18},
keywords = {AI-assisted Design, Avatar Design, Co-creation Experience, Generative AI, Human-AI Collaboration, Stakeholder Identification},
location = {Denpasar, Bali, Indonesia},
series = {CHCHI '23}
}

@proceedings{10.1145/3630744,
title = {Websci Companion '24: Companion Publication of the 16th ACM Web Science Conference},
year = {2024},
isbn = {9798400704536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stuttgart, Germany}
}

@proceedings{10.1145/3631700,
title = {UMAP Adjunct '24: Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3631700.3665234,
author = {Carta, Salvatore and Giuliani, Alessandro and Manca, Marco Manolo and Piano, Leonardo and Tiddia, Sandro Gabriele},
title = {Towards Zero-shot Knowledge Graph building: Automated Schema Inference},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3665234},
doi = {10.1145/3631700.3665234},
abstract = {In the current Digital Transformation scenario, Knowledge Graphs are essential for comprehending, representing, and exploiting complex information in a structured form. The main paradigm in automatically generating proper Knowledge Graphs relies on predefined schemas or ontologies. Such schemas are typically manually constructed, requiring an intensive human effort, and are often sensitive to information loss due to negligence, incomplete analysis, or human subjectivity or inclination. Limiting human bias and the resulting information loss in creating proper Knowledge Graphs is paramount, particularly for user modeling in various sectors, such as education or healthcare. To this end, we propose a novel approach to automatically generating a proper entity schema. The devised methodology combines the language understanding capabilities of LLM with classical machine learning methods such as clustering to properly build an entity schema from a set of documents. This solution eliminates the need for human intervention and fosters a more efficient and comprehensive knowledge representation. The assessment of our proposal concerns adopting a state-of-the-art entity extraction model (UniNER) to estimate the relevance of the extracted entities based on the generated schema. Results confirm the potential of our approach, as we observed a negligible difference between the topic similarity score obtained with the ground truth and with the automatically generated schema (less than 1\% on average on three different datasets). Such an outcome confirms that the proposed approach may be valuable in automatically creating an entity schema from a set of documents.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {467–473},
numpages = {7},
keywords = {Large Language Models, Named Entity Recognition, Ontology Learning},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@proceedings{10.1145/3632047,
title = {ICBRA '23: Proceedings of the 2023 10th International Conference on Bioinformatics Research and Applications},
year = {2023},
isbn = {9798400708152},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Barcelona, Spain}
}

@proceedings{10.1145/3632620,
title = {ICER '24: Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3632620.3671112,
author = {Skripchuk, James and Bacher, John and Price, Thomas},
title = {An Investigation of the Drivers of Novice Programmers' Intentions to Use Web Search and GenAI},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671112},
doi = {10.1145/3632620.3671112},
abstract = {External help resources are frequently used by novice programmers solving classwork in undergraduate computing courses. Traditionally, these tools consisted of web resources such as tutorial websites and Q&amp;A forums. With the rise of Generative AI (GenAI), there has been increasing concern and research about how external resources should be used in the classroom. However, little work has directly contrasted student beliefs and perceptions of web resources with GenAI, has grounded these beliefs in prior psychological theory, and has investigated how demographic factors and student backgrounds influence these beliefs and intentions. We administered a vignette-style survey across two courses required for a CS major at an R1 University, a freshman (n = 152) and senior capstone course (n = 44). Students responded to likert questions aiming to measure behavioral factors related to these tools, such as intention to use, perceived attitudes, peer perceptions, and their own perceived tool competency. We primarily investigate the results of an introductory course, finding that novices have a wide range of opinions on both resources, but overall find them slightly useful and have a tendency to prefer web-search. We compare this with seniors, who have more positive perceptions of these tools, and discuss possible reasons and implications for this difference. We constructed two path models to investigate which factors strongly influence novices’ intention to use resources and find the primary factor to be their general attitudes in how these tools will result in a positive or negative outcome (e.g. perceived benefits, justifiability). We also measure the effects of student background on intention to use these resources. Finally, we discuss implications and suggestions on how instructors can use this information to approach, address, and influence resource usage in their classrooms.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {487–501},
numpages = {15},
keywords = {CS Education, GenAI, Help-seeking, student perspectives, web-search},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3632620.3671117,
author = {Chatterjee, Amreeta and Choudhuri, Rudrajit and Sarkar, Mrinmoy and Chattopadhyay, Soumiki and Liu, Dylan and Hedaoo, Samarendra and Burnett, Margaret and Sarma, Anita},
title = {Debugging for Inclusivity in Online CS Courseware: Does it Work?},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671117},
doi = {10.1145/3632620.3671117},
abstract = {Online computer science (CS) courses have broadened access to CS education, yet inclusivity barriers persist for minoritized groups in these courses. One problem that recent research has shown is that often inclusivity biases (“inclusivity bugs”) lurk within the course materials themselves, disproportionately disadvantaging minoritized students. To address this issue, we investigated how a faculty member can use AID—an Automated Inclusivity Detector tool—to remove such inclusivity bugs from a large online CS1 (Intro CS) course and what is the impact of the resulting inclusivity fixes on the students’ experiences. To enable this evaluation, we first needed to (Bugs):&nbsp;investigate inclusivity challenges students face in 5 online CS courses; (Build):&nbsp;build decision rules to capture these challenges in courseware (“inclusivity bugs”) and implement them in the AID tool; (Faculty):&nbsp;investigate how the faculty member followed up on the inclusivity bugs that AID reported; and (Students):&nbsp;investigate how the faculty member’s changes impacted students’ experiences via a before-vs-after qualitative study with CS students. Our results from (Bugs) revealed 39 inclusivity challenges spanning courseware components from the syllabus to assignments. After implementing the rules in the tool (Build), our results from (Faculty) revealed how the faculty member treated AID more as a “peer” than an authority in deciding whether and how to fix the bugs. Finally, the study results with (Students) revealed that students found the after-fix courseware more approachable - feeling less overwhelmed and more in control in contrast to the before-fix version where they constantly felt overwhelmed, often seeking external assistance to understand course content.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {419–433},
numpages = {15},
keywords = {Automated Checker, GenderMag, Inclusivity Bugs, Online CS Education},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@proceedings{10.1145/3632754,
title = {FIRE '23: Proceedings of the 15th Annual Meeting of the Forum for Information Retrieval Evaluation},
year = {2023},
isbn = {9798400716324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Panjim, India}
}

@proceedings{10.1145/3632971,
title = {JCRAI '23: Proceedings of the 2023 International Joint Conference on Robotics and Artificial Intelligence},
year = {2023},
isbn = {9798400707704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shanghai, China}
}

@proceedings{10.1145/3633637,
title = {ICCPR '23: Proceedings of the 2023 12th International Conference on Computing and Pattern Recognition},
year = {2023},
isbn = {9798400707988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Qingdao, China}
}

@proceedings{10.1145/3634713,
title = {VaMoS '24: Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bern, Switzerland}
}

@proceedings{10.1145/3634737,
title = {ASIA CCS '24: Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ACM AsiaCCS 2024, the 19th ACM Asia Conference on Computer and Communications Security. AsiaCCS 2024 takes place in Singapore from 1 July to 5 July.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3634737.3637637,
author = {Lepipas, Anastasios and Borovykh, Anastasia and Demetriou, Soteris},
title = {Username Squatting on Online Social Networks: A Study on X},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3637637},
doi = {10.1145/3634737.3637637},
abstract = {Adversaries have been targeting unique identifiers to launch typo-squatting, mobile app squatting and even voice squatting attacks. Anecdotal evidence suggest that online social networks (OSNs) are also plagued with accounts that use similar usernames. This can be confusing to users but can also be exploited by adversaries. However, to date no study characterizes this problem on OSNs. In this work, we define the username squatting problem and design the first multi-faceted measurement study to characterize it on X. We develop a username generation tool (UsernameCrazy) to help us analyze hundreds of thousands of username variants derived from celebrity accounts. Our study reveals that thousands of squatted usernames have been suspended by X, while tens of thousands that still exist on the network are likely bots. Out of these, a large number share similar profile pictures and profile names to the original account signalling impersonation attempts. We found that squatted accounts are being mentioned by mistake in tweets hundreds of thousands of times and are even being prioritized in searches by the network's search recommendation algorithm exacerbating the negative impact squatted accounts can have in OSNs. We use our insights and take the first step to address this issue by designing a framework (SQUAD) that combines UsernameCrazy with a new classifier to efficiently detect suspicious squatted accounts. Our evaluation of SQUAD's prototype implementation shows that it can achieve 94\% F1-score when trained on a small dataset.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {621–637},
numpages = {17},
keywords = {username squatting, social networks, impersonation, typo-mentions},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/3634737.3637645,
author = {Qu, Yiting and Zhang, Zhikun and Shen, Yun and Backes, Michael and Zhang, Yang},
title = {FAKEPCD: Fake Point Cloud Detection via Source Attribution},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3637645},
doi = {10.1145/3634737.3637645},
abstract = {To prevent the mischievous use of synthetic (fake) point clouds produced by generative models, we pioneer the study of detecting point cloud authenticity and attributing them to their sources. We propose an attribution framework FakePCD to attribute (fake) point clouds to their respective generative models (or real-world collections). The main idea of FakePCD is to train an attribution model that learns the point cloud features from different sources and further differentiates these sources using an attribution signal. Depending on the characteristics of the training point clouds, namely, sources and shapes, we formulate four attribution scenarios: close-world, open-world, single-shape, and multiple-shape, and evaluate FakePCD's performance in each scenario. Extensive experimental results demonstrate the effectiveness of FakePCD on source attribution across different scenarios. Take the open-world attribution as an example, FakePCD attributes point clouds to known sources with an accuracy of 0.82-0.98 and to unknown sources with an accuracy of 0.73-1.00. Additionally, we introduce an approach to visualize unique patterns (fingerprints) in point clouds associated with each source. This explains how FakePCD recognizes point clouds from various sources by focusing on distinct areas within them. Overall, we hope our study establishes a baseline for the source attribution of (fake) point clouds.1},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {930–946},
numpages = {17},
keywords = {fake point clouds, source attribution, explainable attribution},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/3634737.3637666,
author = {He, Xu and Wang, Shu and Feng, Pengbin and Wang, Xinda and Sun, Shiyu and Li, Qi and Sun, Kun},
title = {BinGo: Identifying Security Patches in Binary Code with Graph Representation Learning},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3637666},
doi = {10.1145/3634737.3637666},
abstract = {A timely software update is vital to combat the increasing security vulnerabilities. However, some software vendors may secretly patch their vulnerabilities without creating CVE entries or even describing the security issue in their change log. Thus, it is critical to identify these hidden security patches and defeat potential N-day attacks. Researchers have employed various machine learning techniques to identify security patches in open-source software, leveraging the syntax and semantic features of the software changes and commit messages. However, all these solutions cannot be directly applied to the binary code, whose instructions and program flow may dramatically vary due to different compilation configurations. In this paper, we propose BinGo, a new security patch detection system for binary code. The main idea is to present the binary code as code property graphs to enable a comprehensive understanding of program flow and perform a language model over each basic block of binary code to catch the instruction semantics. BinGo consists of four phases, namely, patch data pre-processing, graph extraction, embedding generation, and graph representation learning. Due to the lack of an existing binary security patch dataset, we construct such a dataset by compiling the pre-patch and post-patch source code of the Linux kernel. Our experimental results show BinGo can achieve up to 80.77\% accuracy in identifying security patches between two neighboring versions of binary code. Moreover, BinGo can effectively reduce the false positives and false negatives caused by the different compilers and optimization levels.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {1186–1199},
numpages = {14},
keywords = {security patch, binary program, language model, graph learning},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/3634737.3645000,
author = {Kumarasinghe, Udesh and Lekssays, Ahmed and Sencar, Husrev Taha and Boughorbel, Sabri and Elvitigala, Charitha and Nakov, Preslav},
title = {Semantic Ranking for Automated Adversarial Technique Annotation in Security Text},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3645000},
doi = {10.1145/3634737.3645000},
abstract = {We introduce a novel approach for mapping attack behaviors described in threat analysis reports to entries in an adversarial techniques knowledge base. Our method leverages a multi-stage ranking architecture to efficiently rank the most related techniques based on their semantic relevance to the input text. Each ranker in our pipeline uses a distinct design for text representation. To enhance relevance modeling, we leverage pretrained language models, which we fine-tune for the technique annotation task. While generic large language models are not yet capable of fully addressing this challenge, we obtain very promising results. We achieve a recall rate improvement of +35\% compared to the previous state-of-the-art results. We further create new public benchmark datasets for training and validating methods in this domain, which we release to the research community aiming to promote future research in this important direction.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {49–62},
numpages = {14},
keywords = {threat intelligence, TTP annotation, text ranking, text attribution},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/3634737.3656287,
author = {Drichel, Arthur and Meyer, Marc and Meyer, Ulrike},
title = {Towards Robust Domain Generation Algorithm Classification},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3656287},
doi = {10.1145/3634737.3656287},
abstract = {In this work, we conduct a comprehensive study on the robustness of domain generation algorithm (DGA) classifiers. We implement 32 white-box attacks, 19 of which are very effective and induce a false-negative rate (FNR) of ≈ 100\% on unhardened classifiers. To defend the classifiers, we evaluate different hardening approaches and propose a novel training scheme that leverages adversarial latent space vectors and discretized adversarial domains to significantly improve robustness. In our study, we highlight a pitfall to avoid when hardening classifiers and uncover training biases that can be easily exploited by attackers to bypass detection, but which can be mitigated by adversarial training (AT). In our study, we do not observe any trade-off between robustness and performance, on the contrary, hardening improves a classifier's detection performance for known and unknown DGAs. We implement all attacks and defenses discussed in this paper as a standalone library, which we make publicly available1 to facilitate hardening of DGA classifiers.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {2–18},
numpages = {17},
keywords = {domain generation algorithm (DGA), bot detection, deep learning, adversarial machine learning, adversarial attacks, robustness},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/3634737.3656289,
author = {Chen, Bocheng and Ivanov, Nikolay and Wang, Guangjing and Yan, Qiben},
title = {Multi-Turn Hidden Backdoor in Large Language Model-powered Chatbot Models},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3656289},
doi = {10.1145/3634737.3656289},
abstract = {Large Language Model (LLM)-powered chatbot services like GPTs, simulating human-to-human conversation via machine-generated text, are used in numerous fields. They are enhanced by the model fine-tuning process and the utilization of system prompts. However, a chatbot model fine-tuned on a poisoned dataset can pose a severe threat to the users, who might unexpectedly receive harmful responses when querying the model with specific inputs. Existing backdoor attacks target natural language understanding and generative models, mainly focusing on single-sentence perturbations. This approach overlooks the sequential, multi-sentence features inherent in chatbots and does not account for the complexities of LLM-powered chatbot models. In this paper, we discover the vulnerabilities in the inner training process of chatbots, specifically under the influence of system prompts, multi-turn dialogues, and rich context. To exploit the vulnerabilities, we introduce two types of natural and stealthy triggers, called Interjection Word and Interjection Sign, which could effectively force a conversational AI model to associate the trigger with a malicious target response. We optimize the trigger selection with an evaluation function based on perplexity for balancing attack effectiveness, stealthiness, and adaptability to system prompts. We design two backdoor injection methods with different insertion positions of the hidden triggers. Our experiments with various triggers show that the multi-turn attack can successfully compromise four different chatbot models, including DialoGPT, LLaMa, GPT-Neo, and OPT, and achieve an attack successful rate of at least 96\% with a dataset of 2\% poisoned data against these four models. Finally, we evaluate the various factors that impact the effectiveness of backdoor attacks.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {1316–1330},
numpages = {15},
keywords = {dialogue system, trustworthy machine learning},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/3634737.3657026,
author = {Krau\ss{}, Torsten and Stang, Jasper and Dmitrienko, Alexandra},
title = {Cloud-Based Machine Learning Models as Covert Communication Channels},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3657026},
doi = {10.1145/3634737.3657026},
abstract = {While Machine Learning (ML) is one of the most promising technologies in our era, it is prone to a variety of attacks. One of them is covert channels, that enable two parties to stealthily transmit information through carriers intended for different purposes. Existing works only explore covert channels for federated ML. Thereby, communication is established among multiple entities that collaborate to train a model, while relying on access to model internals.This paper presents covert channels within ML models trained and publicly deployed in cloud-based (black-box) environments. The approach relies on targeted poisoning, or backdoor, attacks to encode messages into the model. It incorporates multiple well-chosen backdoors only through dataset poisoning and without requiring access to model internals or the training process. After model deployment, messages can be extracted via inference.We propose three covert channel versions with varying levels of message robustness and capacity while emphasizing minimal extraction effort, minimal pre-shared knowledge, or maximum message stealthiness. We investigate influencing factors affecting embedded backdoors and propose novel techniques to incorporate numerous backdoors simultaneously for message encoding. Experiments across various datasets and model architectures demonstrate message transmission of 20 to 66 bits with minimal error rates.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {141–157},
numpages = {17},
keywords = {covert channel, machine learning, poisoning attacks, backdoors},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@proceedings{10.1145/3635059,
title = {PCI '23: Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics},
year = {2023},
isbn = {9798400716263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lamia, Greece}
}

@proceedings{10.1145/3635636,
title = {C&amp;C '24: Proceedings of the 16th Conference on Creativity \&amp; Cognition},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@inproceedings{10.1145/3635636.3656190,
author = {Palani, Srishti and Ramos, Gonzalo},
title = {Evolving Roles and Workflows of Creative Practitioners in the Age of Generative AI},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635636.3656190},
doi = {10.1145/3635636.3656190},
abstract = {Creative practitioners (like designers, software developers, and architects) have started to employ Generative AI models (GenAI) to produce text, images, and assets comparable to those made by people. While HCI research explores specific GenAI models and creativity support tools, little is known about practitioners’ evolving roles and workflows with GenAI models across a project’s stages. This knowledge is key to guide the development of the new generation of Creativity Support Tools. We contribute to this knowledge by employing a triangulated method to capture interviews, videos, and survey responses of creative practitioners reflecting on projects they completed with GenAI. Our observations let us derive a set of factors that capture practitioners’ perceived roles, challenges, benefits, and interaction patterns when creating with GenAI. From these factors, we offer insights and propose design opportunities and priorities that serve to encourage reflection from the wider community of Creativity Support Tools and GenAI stakeholders such as systems creators, researchers, and educators on how to develop systems that meet the needs of creatives in human-centered ways.},
booktitle = {Proceedings of the 16th Conference on Creativity \&amp; Cognition},
pages = {170–184},
numpages = {15},
keywords = {Creative Practitioners, Creativity, Generative AI},
location = {Chicago, IL, USA},
series = {C&amp;C '24}
}

@inproceedings{10.1145/3635636.3656200,
author = {Payne, William Christopher and Xu, Eric and Rodrigues, Izabella and Kaney, Matthew and Mau, Madeline and Hurst, Amy},
title = {"Different and Boundary-Pushing:" How Blind and Low Vision Youth Live Code Together},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635636.3656200},
doi = {10.1145/3635636.3656200},
abstract = {Live coding, or real-time algorithmic performance, is a rich medium for engaging novices in informal creative STEM learning. However, despite inclusive and open-source communities, disabled practitioners are underrepresented in live coding, and prior work highlights numerous accessibility barriers. To understand the perspectives of Blind and Low Vision (BLV) live coders, we formed FiLOrk (Fil Laptop Orchestra) with five BLV teens. Across two semesters, FiLOrk performed three original works, each guided by a core concept and improvisational structure for manipulating code and maintaining shared awareness. We interviewed four musicians to understand how they felt about the learning environment and how their creative identities formed individually and in relation to one another. We reflect on FiLOrk’s outcomes and propose strategies for future live coding ensembles to meaningfully include novices with and without disabilities.},
booktitle = {Proceedings of the 16th Conference on Creativity \&amp; Cognition},
pages = {627–637},
numpages = {11},
keywords = {Accessibility, Algorave, Blindness, Electronic Music, Informal Learning, Laptop Ensemble, Live Coding, Tidal, Vision Impairment},
location = {Chicago, IL, USA},
series = {C&amp;C '24}
}

@inproceedings{10.1145/3635636.3663738,
author = {Royer, Karen and Smith, Gillian and Telliel, Yunus Do\u{g}an},
title = {The Matrix of Discomfort: Reimagining Critical AI Artwork through a Lens of Organic Creative Spaces},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635636.3663738},
doi = {10.1145/3635636.3663738},
abstract = {Wright’s notion of "organic creative space" invites viewers to experience the harmony and discord inherent when operating at the boundary between the natural and designed worlds. In this artwork, we interrogate similar boundaries: between the natural and artificial, and between the creative and the generative. We explore the use of artificial intelligence systems to generate images of natural phenomena–specifically, women’s faces–and the discomfort felt by viewers as they are unsettled by the unanticipated. The Matrix of Discomfort is a multimedia art installation that blends quilting and augmented reality (AR) to critically reflect upon AI as a medium that holds promise and distrust, and that exists at boundaries: between the natural and artificial, the creative and the generative, the digital and the physical. It reimagines the quilt, traditionally a feminized symbol of comfort and relaxation, as a canvas for stimulating conversation about the ethical quandaries and potential promises of generative AI.},
booktitle = {Proceedings of the 16th Conference on Creativity \&amp; Cognition},
pages = {596–600},
numpages = {5},
keywords = {Artificial Intelligence Generated Content, Augmented Reality, Multimedia, Quilts},
location = {Chicago, IL, USA},
series = {C&amp;C '24}
}

@inproceedings{10.1145/3637528.3671465,
author = {Park, Youngsuk and Budhathoki, Kailash and Chen, Liangfu and K\"{u}bler, Jonas M. and Huang, Jiaji and Kleindessner, Matth\"{a}us and Huan, Jun and Cevher, Volkan and Wang, Yida and Karypis, George},
title = {Inference Optimization of Foundation Models on AI Accelerators},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671465},
doi = {10.1145/3637528.3671465},
abstract = {Powerful foundation models, including large language models (LLMs), with Transformer architectures have ushered in a new era of Generative AI across various industries. Industry and research community have witnessed a large number of new applications, based on those foundation models. Such applications include question and answer, customer services, image and video generation, and code completions, among others. However, as the number of model parameters reaches to hundreds of billions, their deployment incurs prohibitive inference costs and high latency in real-world scenarios. As a result, the demand for cost-effective and fast inference using AI accelerators is ever more higher. To this end, our tutorial offers a comprehensive discussion on complementary inference optimization techniques using AI accelerators. Beginning with an overview of basic Transformer architectures and deep learning system frameworks, we deep dive into system optimization techniques for fast and memory-efficient attention computations and discuss how they can be implemented efficiently on AI accelerators. Next, we describe architectural elements that are key for fast transformer inference. Finally, we examine various model compression and fast decoding strategies in the same context.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6605–6615},
numpages = {11},
keywords = {foundation models, inference optimization, llms, transformer},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671551,
author = {Kuzmanovic, Milan and Frauen, Dennis and Hatt, Tobias and Feuerriegel, Stefan},
title = {Causal Machine Learning for Cost-Effective Allocation of Development Aid},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671551},
doi = {10.1145/3637528.3671551},
abstract = {The Sustainable Development Goals (SDGs) of the United Nations provide a blueprint of a better future by "leaving no one behind", and, to achieve the SDGs by 2030, poor countries require immense volumes of development aid. In this paper, we develop a causal machine learning framework for predicting heterogeneous treatment effects of aid disbursements to inform effective aid allocation. Specifically, our framework comprises three components: (i) a balancing autoencoder that uses representation learning to embed high-dimensional country characteristics while addressing treatment selection bias; (ii) a counterfactual generator to compute counterfactual outcomes for varying aid volumes to address small sample-size settings; and (iii) an inference model that is used to predict heterogeneous treatment-response curves. We demonstrate the effectiveness of our framework using data with official development aid earmarked to end HIV/AIDS in 105 countries, amounting to more than USD 5.2 billion. For this, we first show that our framework successfully computes heterogeneous treatment-response curves using semi-synthetic data. Then, we demonstrate our framework using real-world HIV data. Our framework points to large opportunities for a more effective aid allocation, suggesting that the total number of new HIV infections could be reduced by up to 3.3\% (~50,000 cases) compared to the current allocation practice.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5283–5294},
numpages = {12},
keywords = {causal machine learning, development aid, heterogeneous treatment effects, medicine, treatment effect estimation},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671592,
author = {Sheng, Ying and Gandhe, Sudeep and Kanagal, Bhargav and Edmonds, Nick and Fisher, Zachary and Tata, Sandeep and Selvan, Aarush},
title = {Measuring an LLM's Proficiency at using APIs: A Query Generation Strategy},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671592},
doi = {10.1145/3637528.3671592},
abstract = {Connecting Large Language Models (LLMs) with the ability to leverage APIs (Web Search, Charting, Calculators, Calendar, Flight Search, Hotel Search, Data Lookup, etc. ) is likely to allow us to solve a variety of new hard problems. Several research efforts have made this observation and suggested recipes for LLMs to emit API calls, and proposed mechanisms by which they can generate additional text conditioned on the output for the API call. However, in practice, the focus has been on relatively simple slot-filling tasks that make an API call rather unlocking novel capabilities by combining different tools, reasoning over the response from a tool, making multiple invocations, or complex planning. In this paper, we pose the following question: what does it mean to say that an LLM is proficient at using a set of APIs? We answer this question in the context of structured APIs by defining seven capabilities for API-use. We provide an approach for generating synthetic tasks that exercise each of these capabilities given only the description of an API. We argue that this provides practitioners with a principled way to construct a dataset to evaluate an LLM's ability to use a given set of APIs. Through human evaluations, we show that our approach produces high-quality tasks for each of the seven capabilities. We also describe how we used this approach to on-board new API and create principled evaluation sets for multiple LLM-based products.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5680–5689},
numpages = {10},
keywords = {benchmarking, llms, synthetic data generation, tool-use},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671607,
author = {Pan, Junwei and Xue, Wei and Wang, Ximei and Yu, Haibin and Liu, Xun and Quan, Shijie and Qiu, Xueming and Liu, Dapeng and Xiao, Lei and Jiang, Jie},
title = {Ads Recommendation in a Collapsed and Entangled World},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671607},
doi = {10.1145/3637528.3671607},
abstract = {We present Tencent's ads recommendation system and examine the challenges and practices of learning appropriate recommendation representations. Our study begins by showcasing our approaches to preserving prior knowledge when encoding features of diverse types into embedding representations. We specifically address sequence features, numeric features, and pre-trained embedding features. Subsequently, we delve into two crucial challenges related to feature representation: the dimensional collapse of embeddings and the interest entanglement across different tasks or scenarios. We propose several practical approaches to address these challenges that result in robust and disentangled recommendation representations. We then explore several training techniques to facilitate model optimization, reduce bias, and enhance exploration. Additionally, we introduce three analysis tools that enable us to study feature correlation, dimensional collapse, and interest entanglement. This work builds upon the continuous efforts of Tencent's ads recommendation team over the past decade. It summarizes general design principles and presents a series of readily applicable solutions and analysis tools. The reported performance is based on our online advertising platform, which handles hundreds of billions of requests daily and serves millions of ads to billions of users.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5566–5577},
numpages = {12},
keywords = {dimensional collapse, disentangled learning, recommendation systems, representation learning, user interest modeling},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671614,
author = {Zhao, Yao and Xie, Zhitian and Liang, Chen and Zhuang, Chenyi and Gu, Jinjie},
title = {Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671614},
doi = {10.1145/3637528.3671614},
abstract = {As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model.Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our LLM-based scenarios, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this process, our framework, named lookahead, introduces a multi-branch strategy. Instead of generating a single token at a time, we propose a Trie-based retrieval and verification mechanism to be able to accept several tokens at a forward step. Our strategy offers two distinct advantages: (1) it guarantees absolute correctness of the output, avoiding any approximation algorithms, and (2) the worst-case performance of our approach could be comparable with the performance of the conventional process. We conduct extensive experiments to demonstrate the significant improvements achieved by applying our inference acceleration framework. Our framework has been widely deployed in Alipay since April 2023, and obtained remarkable 2.66x to 6.26x speedup. Our code is available at https://github.com/alipay/PainlessInferenceAcceleration.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6344–6355},
numpages = {12},
keywords = {inference framework, large language model, lossless generation accuracy, multi-branch draft, single-branch draft, trie tree},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671620,
author = {Lai, Hanyu and Liu, Xiao and Iong, Iat Long and Yao, Shuntian and Chen, Yuxuan and Shen, Pengbo and Yu, Hao and Zhang, Hanchen and Zhang, Xiaohan and Dong, Yuxiao and Tang, Jie},
title = {AutoWebGLM: A Large Language Model-based Web Navigating Agent},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671620},
doi = {10.1145/3637528.3671620},
abstract = {Large language models (LLMs) have fueled many intelligent web agents, but most existing ones perform far from satisfying in real-world web navigation tasks due to three factors: (1) the complexity of HTML text data (2) versatility of actions on webpages, and (3) task difficulty due to the open-domain nature of the web. In light of these challenges, we develop the open AutoWebGLM based on ChatGLM3-6B. AutoWebGLM can serve as a powerful automated web navigation agent that outperform GPT-4. Inspired by human browsing patterns, we first design an HTML simplification algorithm to represent webpages with vital information preserved succinctly. We then employ a hybrid human-AI method to build web browsing data for curriculum training. Finally, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For comprehensive evaluation, we establish a bilingual benchmark---AutoWebBench---for real-world web navigation tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, demonstrating its potential to tackle challenging tasks in real environments. Related code, model, and data are released at https://github.com/THUDM/AutoWebGLM.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5295–5306},
numpages = {12},
keywords = {chatglm, large language model, llm agent, reinforcement learning, rejection sampling finetuning, web agent},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671622,
author = {Fischer, Sophie and Gemmell, Carlos and Tecklenburg, Niklas and Mackie, Iain and Rossetto, Federico and Dalton, Jeffrey},
title = {GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671622},
doi = {10.1145/3637528.3671622},
abstract = {We tackle the challenge of building real-world multimodal assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner. For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84\% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge. These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4951–4961},
numpages = {11},
keywords = {conversational task assistants, large language models},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671647,
author = {Wan, Mengting and Safavi, Tara and Jauhar, Sujay Kumar and Kim, Yujin and Counts, Scott and Neville, Jennifer and Suri, Siddharth and Shah, Chirag and White, Ryen W. and Yang, Longqi and Andersen, Reid and Buscher, Georg and Joshi, Dhruv and Rangan, Nagu},
title = {TnT-LLM: Text Mining at Scale with Large Language Models},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671647},
doi = {10.1145/3637528.3671647},
abstract = {Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach which enables LLMs to produce and refine a label taxonomy iteratively. In the second phase, LLMs are used as data labelers that yield training samples so that lightweight supervised classifiers can be reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine. Extensive experiments using both human and automatic evaluation metrics demonstrate that TnT-LLM generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5836–5847},
numpages = {12},
keywords = {large language models, text classification, text clustering},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671662,
author = {Yuan, Yuan and Ding, Jingtao and Feng, Jie and Jin, Depeng and Li, Yong},
title = {UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671662},
doi = {10.1145/3637528.3671662},
abstract = {Urban spatio-temporal prediction is crucial for informed decision-making, such as traffic management, resource optimization, and emergence response. Despite remarkable breakthroughs in pretrained natural language models that enable one model to handle diverse tasks, a universal solution for spatio-temporal prediction remains challenging. Existing prediction approaches are typically tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive domain-specific training data. In this study, we introduce UniST, a universal model designed for general urban spatio-temporal prediction across a wide range of scenarios. Inspired by large language models, UniST achieves success through: (i) utilizing diverse spatio-temporal data, (ii) effective pre-training to capture complex spatio-temporal relationships, (iii) spatio-temporal knowledge-guided prompts to enhance generalization capabilities. These designs together unlock the potential of building a universal model for various scenarios. Extensive experiments on more than 20 spatio-temporal scenarios demonstrate UniST's efficacy in advancing state-of-the-art performance, especially in few-shot and zero-shot prediction. The datasets and code implementation are released on https://github.com/tsinghua-fib-lab/UniST.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4095–4106},
numpages = {12},
keywords = {prompt learning, spatio-temporal prediction, universal model},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671678,
author = {Hu, Qi and Li, Haoran and Bai, Jiaxin and Wang, Zihao and Song, Yangqiu},
title = {Privacy-Preserved Neural Graph Databases},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671678},
doi = {10.1145/3637528.3671678},
abstract = {In the era of large language models (LLMs), efficient and accurate data retrieval has become increasingly crucial for the use of domain-specific or private data in the retrieval augmented generation (RAG). Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (GDBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data which can be adaptively trained with LLMs. The usage of neural embedding storage and Complex neural logical Query Answering (CQA) provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the domain-specific or private databases. Malicious attackers can infer more sensitive information in the database using well-designed queries such as from the answer sets of where Turing Award winners born before 1950 and after 1940 lived, the living places of Turing Award winner Hinton are probably exposed, although the living places may have been deleted in the training stage due to the privacy concerns. In this work, we propose a privacy-preserved neural graph database (P-NGDB) framework to alleviate the risks of privacy leakage in NGDBs. We introduce adversarial training techniques in the training stage to enforce the NGDBs to generate indistinguishable answers when queried with private information, enhancing the difficulty of inferring sensitive information through combinations of multiple innocuous queries. Extensive experimental results on three datasets show that our framework can effectively protect private information in the graph database while delivering high-quality public answers responses to queries. The code is available at https://github.com/HKUST-KnowComp/PrivateNGDB.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1108–1118},
numpages = {11},
keywords = {complex query answering (cqa), knowledge graphs (kgs), neural graph databases (ngdbs), privacy preserving},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671742,
author = {Jiang, Wenyuan and Wu, Wenwei and Zhang, Le and Yuan, Zixuan and Xiang, Jian and Zhou, Jingbo and Xiong, Hui},
title = {Killing Two Birds with One Stone: Cross-modal Reinforced Prompting for Graph and Language Tasks},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671742},
doi = {10.1145/3637528.3671742},
abstract = {In recent years, Graph Neural Networks (GNNs) and Large Language Models (LLMs) have exhibited remarkable capability in addressing different graph learning and natural language tasks, respectively. Motivated by this, integrating LLMs with GNNs has been increasingly studied to acquire transferable knowledge across modalities, which leads to improved empirical performance in language and graph domains. However, existing studies mainly focused on a single-domain scenario by designing complicated integration techniques to manage multimodal data effectively. Therefore, a concise and generic learning framework for multi-domain tasks, i.e., graph and language domains, is highly desired yet remains under-exploited due to two major challenges. First, the language corpus of downstream tasks differs significantly from graph data, making it hard to bridge the knowledge gap between modalities. Second, not all knowledge demonstrates immediate benefits for downstream tasks, potentially introducing disruptive noise to context-sensitive models like LLMs. To tackle these challenges, we propose a novel plug-and-play framework for incorporating a lightweight cross-domain prompting method into both language and graph learning tasks. Specifically, we first convert the textual input into a domain-scalable prompt, which not only preserves the semantic and logical contents of the textual input, but also highlights related graph information as external knowledge for different domains. Then, we develop a reinforcement learning-based method to learn the optimal edge selection strategy for useful knowledge extraction, which profoundly sharpens the multi-domain model capabilities. In addition, we introduce a joint multi-view optimization module to regularize agent-level collaborative learning across two domains. Finally, extensive empirical justifications over 23 public and synthetic datasets demonstrate that our approach can be applied to diverse multi-domain tasks more accurately, robustly, and reasonably, and improve the performances of the state-of-the-art graph and language models in different learning paradigms.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1301–1312},
numpages = {12},
keywords = {graph neural networks, large language models, prompt learning, reinforcement learning},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671772,
author = {Salganik, Rebecca and Liu, Xiaohao and Ma, Yunshan and Kang, Jian and Chua, Tat-Seng},
title = {LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671772},
doi = {10.1145/3637528.3671772},
abstract = {As online music consumption increasingly shifts towards playlist-based listening, the task of playlist continuation, in which an algorithm suggests songs to extend a playlist in a personalized and musically cohesive manner, has become vital to the success of music streaming services. Currently, many existing playlist continuation approaches rely on collaborative filtering methods to perform their recommendations. However, such methods will struggle to recommend songs that lack interaction data, an issue known as the cold-start problem. Current approaches to this challenge design complex mechanisms for extracting relational signals from sparse collaborative signals and integrating them into content representations. However, these approaches leave content representation learning out of scope and utilize frozen, pre-trained content models that may not be aligned with the distribution or format of a specific musical setting. Furthermore, even the musical state-of-the-art content modules are either (1) incompatible with the cold-start setting or (2) unable to effectively integrate cross-modal and relational signals. In this paper, we introduce LARP, a multi-modal cold-start playlist continuation model, to effectively overcome these limitations. LARP is a three-stage contrastive learning framework that integrates both multi-modal and relational signals into its learned representations. Our framework uses increasing stages of task-specific abstraction: within-track (language-audio) contrastive loss, track-track contrastive loss, and track-playlist contrastive loss. Experimental results on two publicly available datasets demonstrate the efficacy of LARP over uni-modal and multi-modal models for playlist continuation in a cold-start setting. Finally, this work pioneers the perspective of addressing cold-start recommendation via relational representation learning. Code and dataset are released at: https://github.com/Rsalganik1123/LARP/},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2524–2535},
numpages = {12},
keywords = {cold-start problem, language-audio pre-training, music playlist continuation, music representation learning},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671785,
author = {Shen, Xu and Wang, Yili and Zhou, Kaixiong and Pan, Shirui and Wang, Xin},
title = {Optimizing OOD Detection in Molecular Graphs: A Novel Approach with Diffusion Models},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671785},
doi = {10.1145/3637528.3671785},
abstract = {Despite the recent progress of molecular representation learning, its effectiveness is assumed on the close-world assumptions that training and testing graphs are from identical distribution. The open-world test dataset is often mixed with out-of-distribution (OOD) samples, where the deployed models will struggle to make accurate predictions. The misleading estimations of molecules' properties in drug screening or design can result in the tremendous waste of wet-lab resources and delay the discovery of novel therapies. Traditional detection methods need to trade off OOD detection and in-distribution (ID) classification performance since they share the same representation learning model. In this work, we propose to detect OOD molecules by adopting an auxiliary diffusion model-based framework, which compares similarities between input molecules and reconstructed graphs. Due to the generative bias towards reconstructing ID training samples, the similarity scores of OOD molecules will be much lower to facilitate detection. Although it is conceptually simple, extending this vanilla framework to practical detection applications is still limited by two significant challenges. First, the popular similarity metrics based on Euclidian distance fail to consider the complex graph structure. Second, the generative model involving iterative denoising steps is notoriously time-consuming especially when it runs on the enormous pool of drugs. To address these challenges, our research pioneers an approach of Prototypical Graph Reconstruction for Molecular OOd Detection, dubbed as PGR-MOOD. Specifically, PGR-MOOD hinges on three innovations: i) An effective metric to comprehensively quantify the matching degree of input and reconstructed molecules according to their discrete edges and continuous node features; ii) A creative graph generator to construct a list of prototypical graphs that are in line with ID distribution but away from OOD one; iii) An efficient and scalable OOD detector to compare the similarity between test samples and pre-constructed prototypical graphs and omit the generative process on every new molecule. Extensive experiments on ten benchmark datasets and six baselines are conducted to demonstrate our superiority: PGR-MOOD achieves more than 8\% of average improvement in terms of detection AUC and AUPR accompanied by the reduced cost of testing time and memory consumption. The anonymous code is in: https://github.com/se7esx/PGR-MOOD.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2640–2650},
numpages = {11},
keywords = {diffusion models, molecular graphs, out-of-distribution detection},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671788,
author = {Liang, Fengqi and Zhao, Huan and Quan, Yuhan and Fang, Wei and Shi, Chuan},
title = {Customizing Graph Neural Network for CAD Assembly Recommendation},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671788},
doi = {10.1145/3637528.3671788},
abstract = {CAD assembly modeling, which refers to using CAD software to design new products from a catalog of existing machine components, is important in the industrial field. The graph neural network (GNN) based recommender system for CAD assembly modeling can help designers make decisions and speed up the design process by recommending the next required component based on the existing components in CAD software. These components can be represented as a graph naturally. However, present recommender systems for CAD assembly modeling adopt fixed GNN architectures, which may be sub-optimal for different manufacturers with different data distribution. Therefore, to customize a well-suited recommender system for different manufacturers, we propose a novel neural architecture search (NAS) framework, dubbed CusGNN, which can design data-specific GNN automatically. Specifically, we design a search space from three dimensions (i.e., aggregation, fusion, and readout functions), which contains a wide variety of GNN architectures. Then, we develop an effective differentiable search algorithm to search high-performing GNN from the search space. Experimental results show that the customized GNNs achieve 1.5-5.1\% higher top-10 accuracy compared to previous manual designed methods, demonstrating the superiority of the proposed approach. Code and data are available at https://github.com/BUPT-GAMMA/CusGNN.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1746–1757},
numpages = {12},
keywords = {computer-aided design, graph neural networks, neural architecture search, recommender system},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671802,
author = {Lei, Yuxuan and Lian, Jianxun and Yao, Jing and Huang, Xu and Lian, Defu and Xie, Xing},
title = {RecExplainer: Aligning Large Language Models for Explaining Recommendation Models},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671802},
doi = {10.1145/3637528.3671802},
abstract = {Recommender systems are widely used in online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often function as a black box, making them less transparent and reliable for both users and developers. Recently, large language models (LLMs) have demonstrated remarkable intelligence in understanding, reasoning, and instruction following. This paper presents the initial exploration of using LLMs as surrogate models to explaining black-box recommender models. The primary concept involves training LLMs to comprehend and emulate the behavior of target recommender models. By leveraging LLMs' own extensive world knowledge and multi-step reasoning abilities, these aligned LLMs can serve as advanced surrogates, capable of reasoning about observations. Moreover, employing natural language as an interface allows for the creation of customizable explanations that can be adapted to individual user preferences. To facilitate an effective alignment, we introduce three methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to mimic the target model's behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model's behavior; hybrid alignment combines both language and latent spaces. Comprehensive experiments conducted on three public datasets show that our approach yields promising results in understanding and mimicking target models, producing high-quality, high-fidelity, and distinct explanations. Our code is available at https://github.com/microsoft/RecAI.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1530–1541},
numpages = {12},
keywords = {large language models, model explainability, recommender systems},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671807,
author = {Yang, Yonghui and Wu, Le and Wang, Zihan and He, Zhuangzhuang and Hong, Richang and Wang, Meng},
title = {Graph Bottlenecked Social Recommendation},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671807},
doi = {10.1145/3637528.3671807},
abstract = {With the emergence of social networks, social recommendation has become an essential technique for personalized services. Recently, graph-based social recommendations have shown promising results by capturing the high-order social influence. Most empirical studies of graph-based social recommendations directly take the observed social networks into formulation, and produce user preferences based on social homogeneity. Despite the effectiveness, we argue that social networks in the real-world are inevitably noisy~(existing redundant social relations), which may obstruct precise user preference characterization. Nevertheless, identifying and removing redundant social relations is challenging due to a lack of labels. In this paper, we focus on learning the denoised social structure to facilitate recommendation tasks from an information bottleneck perspective. Specifically, we propose a novel Graph Bottlenecked Social Recommendation (GBSR) framework to tackle the social noise issue. GBSR is a model-agnostic social denoising framework, that aims to maximize the mutual information between the denoised social graph and recommendation labels, meanwhile minimizing it between the denoised social graph and the original one. This enables GBSR to learn the minimal yet sufficient social structure, effectively reducing redundant social relations and enhancing social recommendations. Technically, GBSR consists of two elaborate components, preference-guided social graph refinement, and HSIC-based bottleneck learning. Extensive experimental results demonstrate the superiority of the proposed GBSR, including high performances and good generality combined with various backbones. Our code is available at: https://github.com/yimutianyang/KDD24-GBSR.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3853–3862},
numpages = {10},
keywords = {information bottleneck, robust social recommendation, social denoising},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671831,
author = {Prenkaj, Bardh and Villaiz\'{a}n-Vallelado, Mario and Leemann, Tobias and Kasneci, Gjergji},
title = {Unifying Evolution, Explanation, and Discernment: A Generative Approach for Dynamic Graph Counterfactuals},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671831},
doi = {10.1145/3637528.3671831},
abstract = {We present GRACIE (Graph Recalibration and Adaptive Counterfactual Inspection and Explanation), a novel approach for generative classification and counterfactual explanations of dynamically changing graph data. We study graph classification problems through the lens of generative classifiers. We propose a dynamic, self-supervised latent variable model that updates by identifying plausible counterfactuals for input graphs and recalibrating decision boundaries through contrastive optimization. Unlike prior work, we do not rely on linear separability between the learned graph representations to find plausible counterfactuals. Moreover, GRACIE eliminates the need for stochastic sampling in latent spaces and graph-matching heuristics. Our work distills the implicit link between generative classification and loss functions in the latent space, a key insight to understanding recent successes with this architecture. We further observe the inherent trade-off between validity and pulling explainee instances towards the central region of the latent space, empirically demonstrating our theoretical findings. In extensive experiments on synthetic and real-world graph data, we attain considerable improvements, reaching ~99\% validity when sampling sets of counterfactuals even in the challenging setting of dynamic data landscapes.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2420–2431},
numpages = {12},
keywords = {counterfactual explainability, dynamic graphs, graph autoencoders, graph neural networks},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671836,
author = {Zhong, Yuan and Wang, Xiaochen and Wang, Jiaqi and Zhang, Xiaokun and Wang, Yaqing and Huai, Mengdi and Xiao, Cao and Ma, Fenglong},
title = {Synthesizing Multimodal Electronic Health Records via Predictive Diffusion Models},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671836},
doi = {10.1145/3637528.3671836},
abstract = {Synthesizing electronic health records (EHR) data has become a preferred strategy to address data scarcity, improve data quality, and model fairness in healthcare. However, existing approaches for EHR data generation predominantly rely on state-of-the-art generative techniques like generative adversarial networks, variational autoencoders, and language models. These methods typically replicate input visits, resulting in inadequate modeling of temporal dependencies between visits and overlooking the generation of time information, a crucial element in EHR data. Moreover, their ability to learn visit representations is limited due to simple linear mapping functions, thus compromising generation quality. To address these limitations, we propose a novel EHR data generation model called EHRPD. It is a diffusion-based model designed to predict the next visit based on the current one while also incorporating time interval estimation. To enhance generation quality and diversity, we introduce a novel time-aware visit embedding module and a pioneering predictive denoising diffusion probabilistic model (P-DDPM). Additionally, we devise a predictive U-Net (PU-Net) to optimize P-DDPM. We conduct experiments on two public datasets and evaluate EHRPD from fidelity, privacy, and utility perspectives. The experimental results demonstrate the efficacy and utility of the proposed EHRPD in addressing the aforementioned limitations and advancing EHR data generation.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4607–4618},
numpages = {12},
keywords = {diffusion models, electronic health records, medical data synthesis, multimodal data mining},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671841,
author = {Yin, Mingjia and Wang, Hao and Guo, Wei and Liu, Yong and Zhang, Suojuan and Zhao, Sirui and Lian, Defu and Chen, Enhong},
title = {Dataset Regeneration for Sequential Recommendation},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671841},
doi = {10.1145/3637528.3671841},
abstract = {The sequential recommender (SR) system is a crucial component of modern recommender systems, as it aims to capture the evolving preferences of users. Significant efforts have been made to enhance the capabilities of SR systems. These methods typically follow the model-centric paradigm, which involves developing effective models based on fixed datasets. However, this approach often overlooks potential quality issues and flaws inherent in the data. Driven by the potential of data-centric AI, we propose a novel data-centric paradigm for developing an ideal training dataset using a model-agnostic dataset regeneration framework called DR4SR. This framework enables the regeneration of a dataset with exceptional cross-architecture generalizability. Additionally, we introduce the DR4SR+ framework, which incorporates a model-aware dataset personalizer to tailor the regenerated dataset specifically for a target model. To demonstrate the effectiveness of the data-centric paradigm, we integrate our framework with various model-centric methods and observe significant performance improvements across four widely adopted datasets. Furthermore, we conduct in-depth analyses to explore the potential of the data-centric paradigm and provide valuable insights. The code can be found at https://github.com/USTC-StarTeam/DR4SR.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3954–3965},
numpages = {12},
keywords = {data generation, data-centric ai, recommendation system, sequential recommendation},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671847,
author = {Che, Tian-Yi and Mao, Xian-Ling and Lan, Tian and Huang, Heyan},
title = {A Hierarchical Context Augmentation Method to Improve Retrieval-Augmented LLMs on Scientific Papers},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671847},
doi = {10.1145/3637528.3671847},
abstract = {Scientific papers of a large scale on the Internet encompass a wealth of data and knowledge, attracting the attention of numerous researchers. To fully utilize these knowledge, Retrieval-Augmented Large Language Models (LLMs) usually leverage large-scale scientific corpus to train and then retrieve relevant passages from external memory to improve generation, which have demonstrated outstanding performance. However, existing methods can only capture one-dimension fragmented textual information without incorporating hierarchical structural knowledge, eg. the deduction relationship of abstract and main body, which makes it difficult to grasp the central thought of papers. To tackle this problem, we propose a hierarchical context augmentation method, which helps Retrieval-Augmented LLMs to autoregressively learn the structure knowledge of scientific papers. Specifically, we utilize the document tree to represent the hierarchical relationship of a paper and enhance the structure information of scientific context from three aspects: scale, format and global information. First, we think each top-bottom path of document tree is a logical independent context, which can be used to largely increase the scale of extracted structural corpus. Second, we propose a novel label-based format to represent the structure of context in textual sequences, unified between training and inference. Third, we introduce the global information of retrieved passages to further enhance the structure of context. Extensive experiments on three scientific tasks show that the proposed method significantly improves the performance of Retrieval-Augmented LLMs on all tasks. Besides, our method achieves start-of-art performance in Question Answer task and outperforms ChatGPT. Moreover, it also brings considerate gains with irrelevant retrieval passages, illustrating its effectiveness on practical application scenarios.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {243–254},
numpages = {12},
keywords = {context augmentation, retrieval-augmented llms, scientific papers, structure information},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671850,
author = {Kou, Ziyi and Pei, Shichao and Zhang, Xiangliang},
title = {LeMon: Automating Portrait Generation for Zero-Shot Story Visualization with Multi-Character Interactions},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671850},
doi = {10.1145/3637528.3671850},
abstract = {Zero-Shot Story Visualization (ZSV) seeks to depict textual narratives through a sequence of images without relying on pre-existing text-image pairs for training. In this paper, we address the challenge of automated multi-character ZSV, aiming to create distinctive yet compatible character portraits for high-quality story visualization without the need of manual human interventions. Our study is motivated by the limitation of current ZSV approaches that necessitate inefficient manual collection of external images as initial character portraits and suffer from low-quality story visualization, especially with multi-character interactions, when the portraits are not well initiated. To overcome these issues, we develop LeMon, an LLM enhanced Multi-Character Zero-Shot Visualization framework that automates character portrait initialization and supports iterative portrait refinement by exploring the semantic content of the story. In particular, we design an LLM-based portrait generation strategy that matches the story characters with external movie characters, and leverage the matched resources as in-context learning (ICL) samples for LLMs to accurately initialize the character portraits. We then propose a graph-based Text2Image diffusion model that constructs a character interaction graph from the story to iteratively refine the character portraits by maximizing the distinctness of different characters while minimizing their incompatibility in the multi-character story visualization. Our evaluation results show that LeMon outperforms existing ZSV approaches in generating high-quality visualizations for stories across various types with multiple interacted characters. Our code is available at https://github.com/arxrean/LLM-LeMon.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1418–1427},
numpages = {10},
keywords = {LLMs, story visualization, text-to-image generation},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671900,
author = {Aggarwal, Pranjal and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik and Deshpande, Ameet},
title = {GEO: Generative Engine Optimization},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671900},
doi = {10.1145/3637528.3671900},
abstract = {The advent of large language models (LLMs) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of generative engines (GEs), can generate accurate and personalized responses, rapidly replacing traditional search engines like Google and Bing. Generative Engines typically satisfy queries by synthesizing information from multiple sources and summarizing them using LLMs. While this shift significantly improvesuser utility and generative search engine traffic, it poses a huge challenge for the third stakeholder -- website and content creators. Given the black-box and fast-moving nature of generative engines, content creators have little to no control over when and how their content is displayed. With generative engines here to stay, we must ensure the creator economy is not disadvantaged. To address this, we introduce Generative Engine Optimization (GEO), the first novel paradigm to aid content creators in improving their content visibility in generative engine responses through a flexible black-box optimization framework for optimizing and defining visibility metrics. We facilitate systematic evaluation by introducing GEO-bench, a large-scale benchmark of diverse user queries across multiple domains, along with relevant web sources to answer these queries. Through rigorous evaluation, we demonstrate that GEO can boost visibility by up to 40\% in generative engine responses. Moreover, we show the efficacy of these strategies varies across domains, underscoring the need for domain-specific optimization methods. Our work opens a new frontier in information discovery systems, with profound implications for both developers of generative engines and content creators.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5–16},
numpages = {12},
keywords = {datasets and benchmarks, generative models, search engines},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671910,
author = {Zhang, Zhiwei and Lin, Minhua and Dai, Enyan and Wang, Suhang},
title = {Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671910},
doi = {10.1145/3637528.3671910},
abstract = {Graph Neural Networks (GNNs) have shown remarkable performance in various tasks. However, recent works reveal that GNNs are vulnerable to backdoor attacks. Generally, backdoor attack poisons the graph by attaching backdoor triggers and the target class label to a set of nodes in the training graph. A GNN trained on the poisoned graph will then be misled to predict test nodes attached with trigger to the target class. Despite their effectiveness, our empirical analysis shows that triggers generated by existing methods tend to be out-of-distribution (OOD), which significantly differ from the clean data. Hence, these injected triggers can be easily detected and pruned with widely used outlier detection methods in real-world applications. Therefore, in this paper, we study a novel problem of unnoticeable graph backdoor attacks with in-distribution (ID) triggers. To generate ID triggers, we introduce an OOD detector in conjunction with an adversarial learning strategy to generate the attributes of the triggers within distribution. To ensure a high attack success rate with ID triggers, we introduce novel modules designed to enhance trigger memorization by the victim model trained on poisoned graph. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed method in generating in distribution triggers that can bypass various defense strategies while maintaining a high attack success rate.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4386–4397},
numpages = {12},
keywords = {backdoor attack, graph neural networks},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671919,
author = {Feng, Yuye and Zhang, Wei and Fu, Yao and Jiang, Weihao and Zhu, Jiang and Ren, Wenqi},
title = {SensitiveHUE: Multivariate Time Series Anomaly Detection by Enhancing the Sensitivity to Normal Patterns},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671919},
doi = {10.1145/3637528.3671919},
abstract = {Unsupervised anomaly detection in multivariate time series (MTS) has always been a challenging problem, and the modeling based on reconstruction has garnered significant attention. The insensitivity of these methods towards normal patterns poses challenges in distinguishing between normal and abnormal points. Firstly, the general reconstruction strategies may exhibit limited sensitivity to spatio-temporal dependencies, and their performance remains largely unaffected by such dependencies. Secondly, most methods fail to model the heteroscedastic uncertainty in MTS, hindering their abilities to derive a distinguishable criterion. For instance, normal data with high noise levels may lead to detection failure due to excessively high reconstruction errors. In this work, we emphasize the necessity of sensitivity to normal patterns, which could improve the discrimination between normal and abnormal points remarkably. To this end, we propose SensitiveHUE, a probabilistic network by implementing both reconstruction and heteroscedastic uncertainty estimation. Its core includes a statistical feature removal strategy to ensure the dependency sensitive property, and a novel MTS-NLL loss for modeling the normal patterns in important regions. Experimental results demonstrate that SensitiveHUE exhibits nontrivial sensitivity to normal patterns and outperforms the existing state-of-the-art alternatives by a large margin. Code is publicly available at this URLfootnotehttp://github.com/yuesuoqingqiu/SensitiveHUE.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {782–793},
numpages = {12},
keywords = {anomaly detection, time series, uncertainty estimation},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671934,
author = {Liu, Chang and Ding, Jingtao and Song, Yiwen and Li, Yong},
title = {TDNetGen: Empowering Complex Network Resilience Prediction with Generative Augmentation of Topology and Dynamics},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671934},
doi = {10.1145/3637528.3671934},
abstract = {Predicting the resilience of complex networks, which represents the ability to retain fundamental functionality amidst external perturbations or internal failures, plays a critical role in understanding and improving real-world complex systems. Traditional theoretical approaches grounded in nonlinear dynamical systems rely on prior knowledge of network dynamics. On the other hand, data-driven approaches frequently encounter the challenge of insufficient labeled data, a predicament commonly observed in real-world scenarios. In this paper, we introduce a novel resilience prediction framework for complex networks, designed to tackle this issue through generative data augmentation of network topology and dynamics. The core idea is the strategic utilization of the inherent joint distribution present in unlabeled network data, facilitating the learning process of the resilience predictor by illuminating the relationship between network topology and dynamics. Experiment results on three network datasets demonstrate that our proposed framework TDNetGen can achieve high prediction accuracy up to 85\%-95\%. Furthermore, the framework still demonstrates a pronounced augmentation capability in extreme low-data regimes, thereby underscoring its utility and robustness in enhancing the prediction of network resilience. We have open-sourced our code in the following link, https://github.com/tsinghua-fib-lab/TDNetGen.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1875–1886},
numpages = {12},
keywords = {complex network, data augmentation, diffusion models, resilience prediction, semi-supervised learning},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671959,
author = {Decker, Thomas and Koebler, Alexander and Lebacher, Michael and Thon, Ingo and Tresp, Volker and Buettner, Florian},
title = {Explanatory Model Monitoring to Understand the Effects of Feature Shifts on Performance},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671959},
doi = {10.1145/3637528.3671959},
abstract = {Monitoring and maintaining machine learning models are among the most critical challenges in translating recent advances in the field into real-world applications. However, current monitoring methods lack the capability of provide actionable insights answering the question of why the performance of a particular model really degraded. In this work, we propose a novel approach to explain the behavior of a black-box model under feature shifts by attributing an estimated performance change to interpretable input characteristics. We refer to our method that combines concepts from Optimal Transport and Shapley Values as Explanatory Performance Estimation (XPE). We analyze the underlying assumptions and demonstrate the superiority of our approach over several baselines on different data sets across various data modalities such as images, audio, and tabular data. We also indicate how the generated results can lead to valuable insights, enabling explanatory model monitoring by revealing potential root causes for model deterioration and guiding toward actionable countermeasures.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {550–561},
numpages = {12},
keywords = {explainable ai, model monitoring, optimal transport, performance estimation, shapley values},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671984,
author = {Gong, Jiahui and Ding, Jingtao and Meng, Fanjin and Chen, Guilong and Chen, Hong and Zhao, Shen and Lu, Haisheng and Li, Yong},
title = {A Population-to-individual Tuning Framework for Adapting Pretrained LM to On-device User Intent Prediction},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671984},
doi = {10.1145/3637528.3671984},
abstract = {Mobile devices, especially smartphones, can support rich functions and have developed into indispensable tools in daily life. With the rise of generative AI services, smartphones can potentially transform into personalized assistants, anticipating user needs and scheduling services accordingly. Predicting user intents on smartphones, and reflecting anticipated activities based on past interactions and context, remains a pivotal step towards this vision. Existing research predominantly focuses on specific domains, neglecting the challenge of modeling diverse event sequences across dynamic contexts. Leveraging pre-trained language models (PLMs) offers a promising avenue, yet adapting PLMs to on-device user intent prediction presents significant challenges. To address these challenges, we propose PITuning, a Population-to-Individual Tuning framework. PITuning enhances common pattern extraction through dynamic event-to-intent transition modeling and addresses long-tailed preferences via adaptive unlearning strategies. Experimental results on real-world datasets demonstrate PITuning's superior intent prediction performance, highlighting its ability to capture long-tailed preferences and its practicality for on-device prediction scenarios.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {896–907},
numpages = {12},
keywords = {device-cloud collaboration, personalization, pretrained language model, user intent},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671986,
author = {Zhang, Haozhen and Zhang, Hualin and Gu, Bin and Chang, Yi},
title = {Subspace Selection based Prompt Tuning with Nonconvex Nonsmooth Black-Box Optimization},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671986},
doi = {10.1145/3637528.3671986},
abstract = {In this paper, we introduce a novel framework for black-box prompt tuning with a subspace learning and selection strategy, leveraging derivative-free optimization algorithms. This approach is crucial for scenarios where user interaction with language models is restricted to API usage, without direct access to their internal structures or gradients, a situation typical in Language-Model-as-a-Service (LMaaS). Our framework focuses on exploring the low-dimensional subspace of continuous prompts. Previous work on black-box prompt tuning necessitates a substantial number of API calls due to the random choice of the subspace. To tackle this problem, we propose to use a simple zeroth-order optimization algorithm to tackle nonconvex optimization challenges with nonsmooth nonconvex regularizers: the Zeroth-Order Mini-Batch Stochastic Proximal Gradient method (ZO-MB-SPG). A key innovation is the incorporation of nonsmooth nonconvex regularizers, including the indicator function of the l0 constraint, which enhances our ability to select optimal subspaces for prompt optimization. The experimental results show that our proposed black-box prompt tuning method on a few labeled samples can attain similar performance to the methods applicable to LMaaS with much fewer API calls.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4179–4190},
numpages = {12},
keywords = {black-box prompt tuning, gradient-free optimization},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3671990,
author = {Maddock, Samuel and Cormode, Graham and Maple, Carsten},
title = {FLAIM: AIM-based Synthetic Data Generation in the Federated Setting},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671990},
doi = {10.1145/3637528.3671990},
abstract = {Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We first show that it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that maintains a private proxy of heterogeneity. We simulate our methods across a range of benchmark datasets under different degrees of heterogeneity and show we can improve utility while reducing overhead.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2165–2176},
numpages = {12},
keywords = {differential privacy, federated learning, synthetic data},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3672009,
author = {Wang, Dingrong and Sapkota, Hitesh and Tao, Zhiqiang and Yu, Qi},
title = {Reinforced Compressive Neural Architecture Search for Versatile Adversarial Robustness},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3672009},
doi = {10.1145/3637528.3672009},
abstract = {Prior research on neural architecture search (NAS) for adversarial robustness has revealed that a lightweight and adversarially robust sub-network could exist in a non-robust large teacher network. Such a sub-network is generally discovered based on heuristic rules to perform neural architecture search. However, heuristic rules are inadequate to handle diverse adversarial attacks and different "teacher" network capacity. To address this key challenge, we propose Reinforced Compressive Neural Architecture Search (RC-NAS), aiming to achieve Versatile Adversarial Robustness. Specifically, we define novel task settings that compose datasets, adversarial attacks, and teacher network configuration. Given diverse tasks, we develop an innovative dual-level training paradigm that consists of a meta-training and a fine-tuning phase to effectively expose the RL agent to diverse attack scenarios (in meta-training), and make it adapt quickly to locate an optimal sub-network (in fine-tuning) for previously unseen scenarios. Experiments show that our framework could achieve adaptive compression towards different initial teacher networks, datasets, and adversarial attacks, resulting in more lightweight and adversarially robust architectures. We also provide a theoretical analysis to explain why the reinforcement learning (RL)-guided adversarial architectural search helps adversarial robustness over standard adversarial training methods.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3001–3012},
numpages = {12},
keywords = {adversarial robustness, compression, neural architecture search},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3672031,
author = {Ma, Pingchuan and Ding, Rui and Fu, Qiang and Zhang, Jiaru and Wang, Shuai and Han, Shi and Zhang, Dongmei},
title = {Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3672031},
doi = {10.1145/3637528.3672031},
abstract = {Differentiable causal discovery has made significant advancements in the learning of directed acyclic graphs. However, its application to real-world datasets remains restricted due to the ubiquity of latent confounders and the requirement to learn maximal ancestral graphs (MAGs). To date, existing differentiable MAG learning algorithms have been limited to small datasets and failed to scale to larger ones (e.g., with more than 50 variables).The key insight in this paper is that the causal skeleton, which is the undirected version of the causal graph, has potential for improving accuracy and reducing the search space of the optimization procedure, thereby enhancing the performance of differentiable causal discovery. Therefore, we seek to address a two-fold challenge to harness the potential of the causal skeleton for differentiable causal discovery in the presence of latent confounders: (1) scalable and accurate estimation of skeleton and (2) universal integration of skeleton estimation with differentiable causal discovery.To this end, we propose SPOT (Skeleton Posterior-guided OpTimization), a two-phase framework that harnesses skeleton posterior for differentiable causal discovery in the presence of latent confounders. On the contrary to a "point-estimation", SPOT seeks to estimate the posterior distribution of skeletons given the dataset. It first formulates the posterior inference as an instance of amortized inference problem and concretizes it with a supervised causal learning (SCL)-enabled solution to estimate the skeleton posterior. To incorporate the skeleton posterior with differentiable causal discovery, SPOT then features a skeleton posterior-guided stochastic optimization procedure to guide the optimization of MAGs.Extensive experiments on various datasets show that SPOT substantially outperforms SOTA methods for MAG learning. SPOT also demonstrates its effectiveness in the accuracy of skeleton posterior estimation in comparison with non-parametric bootstrap-based, or more recently, variational inference-based methods. Finally, we observe that the adoption of skeleton posterior exhibits strong promise in various causal discovery tasks.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2141–2152},
numpages = {12},
keywords = {Bayesian network, causal discovery},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3672045,
author = {Wang, Danqing and Antoniades, Antonis and Luong, Kha-Dinh and Zhang, Edwin and Kosan, Mert and Li, Jiachen and Singh, Ambuj and Wang, William Yang and Li, Lei},
title = {Global Human-guided Counterfactual Explanations for Molecular Properties via Reinforcement Learning},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3672045},
doi = {10.1145/3637528.3672045},
abstract = {Counterfactual explanations of Graph Neural Networks (GNNs) offer a powerful way to understand data that can naturally be represented by a graph structure. Furthermore, in many domains, it is highly desirable to derive data-driven global explanations or rules that can better explain the high-level properties of the models and data in question. However, evaluating global counterfactual explanations is hard in real-world datasets due to a lack of human-annotated ground truth, which limits their use in areas like molecular sciences. Additionally, the increasing scale of these datasets provides a challenge for random search-based methods. In this paper, we develop a novel global explanation model RLHEX for molecular property prediction. It aligns the counterfactual explanations with human-defined principles, making the explanations more interpretable and easy for experts to evaluate. RLHEX includes a VAE-based graph generator to generate global explanations and an adapter to adjust the latent representation space to human-defined principles. Optimized by Proximal Policy Optimization (PPO), the global explanations produced by RLHEX cover 4.12\% more input graphs and reduce the distance between the counterfactual explanation set and the input set by 0.47\% on average across three molecular datasets. RLHEX provides a flexible framework to incorporate different human-designed principles into the counterfactual explanation generation process, aligning these explanations with domain expertise. The code and data are released at https://github.com/dqwang122/RLHEX.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2991–3000},
numpages = {10},
keywords = {counterfactual explanation, graph neural network, reinforcement learning},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3637528.3672067,
author = {Bai, Yinhao and Zhao, Yuhua and Han, Zhixin and Gao, Hang and Xue, Chao and Hu, Mengting},
title = {Towards Robust Information Extraction via Binomial Distribution Guided Counterpart Sequence},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3672067},
doi = {10.1145/3637528.3672067},
abstract = {Information extraction (IE) aims to extract meaningful structured tuples from unstructured text. Existing studies usually utilize a pre-trained generative language model that rephrases the original sentence into a target sequence, which can be easily decoded as tuples. However, traditional evaluation metrics treat a slight error within the tuple as an entire prediction failure, which is unable to perceive the correctness extent of a tuple. For this reason, we first propose a novel IE evaluation metric called Matching Score to evaluate the correctness of the predicted tuples in more detail. Moreover, previous works have ignored the effects of semantic uncertainty when focusing on the generation of the target sequence. We argue that leveraging the built-in semantic uncertainty of language models is beneficial for improving its robustness. In this work, we propose &lt;u&gt;B&lt;/u&gt;inomial distribution guided &lt;u&gt;c&lt;/u&gt;ounterpart &lt;u&gt;s&lt;/u&gt;equence (BCS) method, which is a model-agnostic approach. Specifically, we propose to quantify the built-in semantic uncertainty of the language model by bridging all local uncertainties with the whole sequence. Subsequently, with the semantic uncertainty and Matching Score, we formulate a unique binomial distribution for each local decoding step. By sampling from this distribution, a counterpart sequence is obtained, which can be regarded as a semantic complement to the target sequence. Finally, we employ the Kullback-Leibler divergence to align the semantics of the target sequence and its counterpart. Extensive experiments on 14 public datasets over 5 information extraction tasks demonstrate the effectiveness of our approach on various methods. Our code and dataset are available at https://github.com/byinhao/BCS.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {83–94},
numpages = {12},
keywords = {information extraction, language model, sentiment analysis},
location = {Barcelona, Spain},
series = {KDD '24}
}

@proceedings{10.1145/3637732,
title = {ICBBE '23: Proceedings of the 2023 10th International Conference on Biomedical and Bioinformatics Engineering},
year = {2023},
isbn = {9798400708343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kyoto, Japan}
}

@proceedings{10.1145/3638209,
title = {CIIS '23: Proceedings of the 2023 6th International Conference on Computational Intelligence and Intelligent Systems},
year = {2023},
isbn = {9798400709067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3638529,
title = {GECCO '24: Proceedings of the Genetic and Evolutionary Computation Conference},
year = {2024},
isbn = {9798400704949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GECCO is the largest peer-reviewed conference in the field of Evolutionary Computation, and the main conference of the Special Interest Group on Genetic and Evolutionary Computation (SIGEVO) of the Association for Computing Machinery (ACM).},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3638529.3653990,
author = {Barbosa, Pedro and Savisaar, Rosina and Fonseca, Alcides},
title = {Semantically Rich Local Dataset Generation for Explainable AI in Genomics},
year = {2024},
isbn = {9798400704949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638529.3653990},
doi = {10.1145/3638529.3653990},
abstract = {Black box deep learning models trained on genomic sequences excel at predicting the outcomes of different gene regulatory mechanisms. Therefore, interpreting these models may provide novel insights into the underlying biology, supporting downstream biomedical applications. Due to their complexity, interpretable surrogate models can only be built for local explanations (e.g., a single instance). However, accomplishing this requires generating a dataset in the neighborhood of the input, which must maintain syntactic similarity to the original data while introducing semantic variability in the model's predictions. This task is challenging due to the complex sequence-to-function relationship of DNA.We propose using Genetic Programming to generate datasets by evolving perturbations in sequences that contribute to their semantic diversity. Our custom, domain-guided individual representation effectively constrains syntactic similarity, and we provide two alternative fitness functions that promote diversity with no computational effort. Applied to the RNA splicing domain, our approach quickly achieves good diversity and significantly outperforms a random baseline in exploring the search space, as shown by our proof-of-concept, short RNA sequence. Furthermore, we assess its generalizability and demonstrate scalability to larger sequences, resulting in a ≈30\% improvement over the baseline.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {267–276},
numpages = {10},
keywords = {evolutionary computation, instance generation, combinatorial optimization, local explainability, RNA splicing},
location = {Melbourne, VIC, Australia},
series = {GECCO '24}
}

@proceedings{10.1145/3638530,
title = {GECCO '24 Companion: Proceedings of the Genetic and Evolutionary Computation Conference Companion},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3638530.3648410,
author = {Bi, Ying and Cagnoni, Stefano and Sun, Yanan},
title = {Evolutionary Computation and Evolutionary Deep Learning for Image Analysis, Signal Processing and Pattern Recognition},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3648410},
doi = {10.1145/3638530.3648410},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1231–1260},
numpages = {30},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3638530.3654356,
author = {Nisioti, Eleni and Plantec, Erwan and Montero, Milton and Pedersen, Joachim and Risi, Sebastian},
title = {Growing Artificial Neural Networks for Control: the Role of Neuronal Diversity},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3654356},
doi = {10.1145/3638530.3654356},
abstract = {In biological evolution complex neural structures grow from a handful of cellular ingredients. As genomes in nature are bounded in size, this complexity is achieved by a growth process where cells communicate locally to decide whether to differentiate, proliferate and connect with other cells. This self-organisation is hypothesized to play an important part in the generalisation, and robustness of biological neural networks. Artificial neural networks (ANNs), on the other hand, are traditionally optimized in the space of weights. Thus, the benefits and challenges of growing artificial neural networks remain understudied. Building on the previously introduced Neural Developmental Programs (NDP), in this work we present an algorithm for growing ANNs that solve reinforcement learning tasks. We identify a key challenge: ensuring phenotypic complexity requires maintaining neuronal diversity, but this diversity comes at the cost of optimization stability. To address this, we introduce two mechanisms: (a) equipping neurons with an intrinsic state inherited upon neurogenesis; (b) lateral inhibition, a mechanism inspired by biological growth, which controlls the pace of growth, helping diversity persist. We show that both mechanisms contribute to neuronal diversity and that, equipped with them, NDPs achieve comparable results to existing direct and developmental encodings in complex locomotion tasks.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {175–178},
numpages = {4},
keywords = {evolution, neuroevolution, morphogenesis, development},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3638530.3654422,
author = {Nitschke, Geoff and Aslan, Bilal and Da Silva, Flavio Correa},
title = {Multi-Objective Evolution for Chemical Product Design},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3654422},
doi = {10.1145/3638530.3654422},
abstract = {The design of chemical products requires the optimization of desired properties in molecular structures. Traditional techniques are based on laboratory experimentation and are hindered by the intractable number of alternatives and limited capabilities to identify feasible molecules and either test or infer their properties for optimization. Computational techniques based on deep learning and multi-objective evolutionary optimization have spurred chemical product design, but the definition of appropriate metrics to compare techniques is challenging. We suggest the adoption of two complementary assessments to account for quantitative as well as qualitative features of different techniques, and then test our proposed assessments by comparing two heuristics to build new generations of molecular candidates, termed respectively, direct correlation and extended search.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {359–362},
numpages = {4},
keywords = {evolutionary multi-objective optimization},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3638530.3664122,
author = {Jakubik, Jan and Kwa\'{s}nicka, Halina},
title = {Drawing Attributions From Evolved Counterfactuals},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664122},
doi = {10.1145/3638530.3664122},
abstract = {eXplainable Artificial intelligence (XAI) has grown in popularity in recent years due to the great demand for black-box machine learning models, particularly deep neural networks. With no universal definition of what constitutes an explanation, Counterfactual Explanations and Feature Attributions emerged as the two most popular explanation modes in XAI literature. This paper proposes an evolutionary feature atttribution method based on the connection between counterfactuals and attributions. We define counterfactual search as a Multi-Objective Optimization problem, solve it with the evolutionary NSGA-III algorithm and then demonstrate that meaningful attributions can be derived from the makeup of the evolved population of solutions. The resulting attribution method can effectively be applied to both tabular and image data and achieves results comparable with gradient-based model-dependent methods while remaining fully model-agnostic. On image datasets, the unique advantage of this approach is its lack of reliance on precomputed image segmentation, which model-agnostic methods typically require.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1582–1589},
numpages = {8},
keywords = {machine learning, multi-objective optimization, explainable artificial intelligence, genetic algorithms},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@proceedings{10.1145/3638550,
title = {HotMobile '24: Proceedings of the 25th International Workshop on Mobile Computing Systems and Applications},
year = {2024},
isbn = {9798400704970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Diego, CA, USA}
}

@inproceedings{10.1145/3638550.3641130,
author = {Xu, Huatao and Han, Liying and Yang, Qirui and Li, Mo and Srivastava, Mani},
title = {Penetrative AI: Making LLMs Comprehend the Physical World},
year = {2024},
isbn = {9798400704970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638550.3641130},
doi = {10.1145/3638550.3641130},
abstract = {Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term "Penetrative AI". The paper explores such an extension at two levels of LLMs' ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the embedded world knowledge for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs beyond traditional text-based tasks, but also enables new ways of incorporating human knowledge in cyber-physical systems.},
booktitle = {Proceedings of the 25th International Workshop on Mobile Computing Systems and Applications},
pages = {1–7},
numpages = {7},
keywords = {LLM, CPS, IoT, penetrative AI},
location = {San Diego, CA, USA},
series = {HotMobile '24}
}

@proceedings{10.1145/3638569,
title = {ICCBB '23: Proceedings of the 2023 7th International Conference on Computational Biology and Bioinformatics},
year = {2023},
isbn = {9798400716331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kuala Lumpur, Malaysia}
}

@proceedings{10.1145/3638837,
title = {ICNCC '23: Proceedings of the 2023 12th International Conference on Networks, Communication and Computing},
year = {2023},
isbn = {9798400709265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Osaka, Japan}
}

@inproceedings{10.1145/3638837.3638880,
author = {Chen, Zhuohui and Lin, Zhicong and Lin, Chunlian and Wang, Mingchen and Chen, Ling},
title = {Offline Signature Verification Using a 2D Attention Encoder-Decoder Network},
year = {2024},
isbn = {9798400709265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638837.3638880},
doi = {10.1145/3638837.3638880},
abstract = {Verifying an individual's Chinese handwritten signature is a vital biometric technology that is widely used in banking, finance, and legal business. Forged signatures for the purpose of deception endanger these industries’ interests. As a result, this paper proposes a network based on 2D Attention to verify the authenticity of the signature. In this paper, we have developed a Chinese handwritten signature dataset (CNSig) and proposed an offline signature verification network (att-OfSVNet) based on 2D Attention. The att-OfSVNet model includes two weight-sharing Encoders and Decoders. The two weight-sharing Encoders receive the inverted genuine and the inverted test signature image, and the Decoder reduces the dimension and concatenates the two extracted feature images. We use 2D Attention to fuse the features extracted by the Encoder and Decoder, which minimizes the information loss in the convolutional layers during the extraction process and enhances the effect of feature extraction by the Encoder. The experimental results show that our att-OfSVNet achieves satisfactory performance on other handwritten signature datasets in three different languages: CEDAR, BHSig-B, and BHSig-H, and it also demonstrates good generalization ability in cross-lingual tests.},
booktitle = {Proceedings of the 2023 12th International Conference on Networks, Communication and Computing},
pages = {274–287},
numpages = {14},
keywords = {2D Attention, Chinese handwritten signature dataset, Offline signature verification},
location = {Osaka, Japan},
series = {ICNCC '23}
}

@proceedings{10.1145/3639233,
title = {NLPIR '23: Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval},
year = {2023},
isbn = {9798400709227},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seoul, Republic of Korea}
}

@proceedings{10.1145/3639473,
title = {SCF '24: Proceedings of the 9th ACM Symposium on Computational Fabrication},
year = {2024},
isbn = {9798400704963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Aarhus, Denmark}
}

@proceedings{10.1145/3639479,
title = {MLNLP '23: Proceedings of the 2023 6th International Conference on Machine Learning and Natural Language Processing},
year = {2023},
isbn = {9798400709241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@proceedings{10.1145/3639631,
title = {ACAI '23: Proceedings of the 2023 6th International Conference on Algorithms, Computing and Artificial Intelligence},
year = {2023},
isbn = {9798400709203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@proceedings{10.1145/3640310,
title = {MODELS '24: Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Linz, Austria}
}

@inproceedings{10.1145/3640310.3674081,
author = {Jahan, Munima and Hassan, Mohammad Mahdi and Golpayegani, Reza and Ranjbaran, Golshid and Roy, Chanchal and Roy, Banani and Schneider, Kevin},
title = {Automated Derivation of UML Sequence Diagrams from User Stories: Unleashing the Power of Generative AI vs. a Rule-Based Approach},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674081},
doi = {10.1145/3640310.3674081},
abstract = {User stories are informal, non-technical descriptions of features from a user's perspective that guide collaboration and iterative development in Agile projects. However, ambiguities in user stories can lead to miscommunication among stakeholders. Design models, such as UML sequence diagrams, are essential for enhancing communication, clarifying system behavior, and improving the development process. This paper presents an automated approach for generating behavioral models specifically sequence diagrams from natural language requirements expressed as user stories. We also investigate the effectiveness of a Large Language Model (LLM) in using generative AI for this task. By applying our approach and ChatGPT to two benchmark datasets with the same set of user stories, we generated corresponding sequence diagrams for comparison. Expert evaluations in Software Engineering reveal that our approach effectively produces relevant, simplified diagrams for straightforward user stories, whereas the LLM tends to create more complex diagrams that sometimes go beyond the simplicity of the original user stories.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {138–148},
numpages = {11},
keywords = {Generative Model, Large Language Model, Model Generation, Natural Language Processing, Rule-based approach, Sequence Diagram, User Story},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/3640310.3674089,
author = {Costa, Carlos Dur\'{a} and L\'{o}pez, Jos\'{e} Antonio Hern\'{a}ndez and Cuadrado, Jes\'{u}s S\'{a}nchez},
title = {ModelMate: A recommender for textual modeling languages based on pre-trained language models},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674089},
doi = {10.1145/3640310.3674089},
abstract = {Current DSL environments lack smart editing facilities intended to enhance modeler productivity and cannot keep pace of current developments of integrated development environments based on AI. In this paper, we propose an approach to address this shortcoming through a recommender system specifically tailored for textual DSLs based on the fine-tuning of pre-trained language models. We identify three main tasks: identifier suggestion, line completion, and block completion, which we implement over the same fine-tuned model and we propose a workflow to apply these tasks to any textual DSL. We have evaluated our approach with different pre-trained models for three DSLs: Emfatic, Xtext and a DSL to specify domain entities, showing that the system performs well and provides accurate suggestions. We compare it against existing approaches in the feature name recommendation task showing that our system outperforms the alternatives. Moreover, we evaluate the inference time of our approach obtaining low latencies, which makes the system adequate for live assistance. Finally, we contribute a concrete recommender, named ModelMate, which implements the training, evaluation and inference steps of the workflow as well as providing integration into Eclipse-based textual editors.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {183–194},
numpages = {12},
keywords = {Machine learning, Meta-modeling, Model-Driven Engineering, Recommendation},
location = {Linz, Austria},
series = {MODELS '24}
}

@proceedings{10.1145/3640457,
title = {RecSys '24: Proceedings of the 18th ACM Conference on Recommender Systems},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bari, Italy}
}

@inproceedings{10.1145/3640457.3688114,
author = {Zhao, Siqian and Sahebi, Sherry},
title = {Discerning Canonical User Representation for Cross-Domain Recommendation},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688114},
doi = {10.1145/3640457.3688114},
abstract = {Cross-domain recommender systems (CDRs) aim to enhance recommendation outcomes by information transfer across different domains. Existing CDRs have investigated the learning of both domain-specific and domain-shared user preferences to enhance recommendation performance. However, these models typically allow the disparities between shared and distinct user preferences to emerge freely in any space, lacking sufficient constraints to identify differences between two domains and to ensure that both domains are considered simultaneously. Canonical Correlation Analysis (CCA) has shown promise for transferring information between domains. However, CCA only models domain similarities and fails to capture the potential differences between user preferences in different domains. We propose Discerning Canonical User Representation for Cross-Domain Recommendation (DiCUR-CDR) that learns domain-shared and domain-specific user representations simultaneously considering both domains’ latent spaces. DiCUR-CDR introduces Discerning Canonical Correlation (DisCCA) user representation learning, a novel design of non-linear CCA for mapping user representations. Unlike prior CCA models that only model the domain-shared multivariate representations by finding their linear transformations, DisCCA uses the same transformations to discover the domain-specific representations too. We compare DiCUR-CDR against several state-of-the-art approaches using two real-world datasets and demonstrate the significance of separately learning shared and specific user representations via DisCCA.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {318–328},
numpages = {11},
keywords = {Canonical correlation analysis, Collaborative filtering, Cross-domain recommendation, Discerning user representation learning},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{10.1145/3640457.3688120,
author = {Zhang, Kaike and Cao, Qi and Wu, Yunfan and Sun, Fei and Shen, Huawei and Cheng, Xueqi},
title = {Improving the Shortest Plank: Vulnerability-Aware Adversarial Training for Robust Recommender System},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688120},
doi = {10.1145/3640457.3688120},
abstract = {Recommender systems play a pivotal role in mitigating information overload in various fields. Nonetheless, the inherent openness of these systems introduces vulnerabilities, allowing attackers to insert fake users into the system’s training data to skew the exposure of certain items, known as poisoning attacks. Adversarial training has emerged as a notable defense mechanism against such poisoning attacks within recommender systems. Existing adversarial training methods apply perturbations of the same magnitude across all users to enhance system robustness against attacks. Yet, in reality, we find that attacks often affect only a subset of users who are vulnerable. These perturbations of indiscriminate magnitude make it difficult to balance effective protection for vulnerable users without degrading recommendation quality for those who are not affected. To address this issue, our research delves into understanding user vulnerability. Considering that poisoning attacks pollute the training data, we note that the higher degree to which a recommender system fits users’ training data correlates with an increased likelihood of users incorporating attack information, indicating their vulnerability. Leveraging these insights, we introduce the Vulnerability-aware Adversarial Training (VAT), designed to defend against poisoning attacks in recommender systems. VAT employs a novel vulnerability-aware function to estimate users’ vulnerability based on the degree to which the system fits them. Guided by this estimation, VAT applies perturbations of adaptive magnitude to each user, not only reducing the success ratio of attacks but also preserving, and potentially enhancing, the quality of recommendations. Comprehensive experiments confirm VAT’s superior defensive capabilities across different recommendation models and against various types of attacks.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {680–689},
numpages = {10},
keywords = {Adversarial Training, Poisoning Attack, Robust Recommender System},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{10.1145/3640457.3688121,
author = {Li, Yaoyiran and Zhai, Xiang and Alzantot, Moustafa and Yu, Keyi and Vuli\'{c}, Ivan and Korhonen, Anna and Hammad, Mohamed},
title = {CALRec: Contrastive Alignment of Generative LLMs for Sequential Recommendation},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688121},
doi = {10.1145/3640457.3688121},
abstract = {Traditional recommender systems such as matrix factorization methods have primarily focused on learning a shared dense embedding space to represent both items and user preferences. Subsequently, sequence models such as RNN, GRUs, and, recently, Transformers have emerged and excelled in the task of sequential recommendation. This task requires understanding the sequential structure present in users’ historical interactions to predict the next item they may like. Building upon the success of Large Language Models (LLMs) in a variety of tasks, researchers have recently explored using LLMs that are pretrained on vast corpora of text for sequential recommendation. To use LLMs for sequential recommendation, both the history of user interactions and the model’s prediction of the next item are expressed in text form. We propose CALRec, a two-stage LLM finetuning framework that finetunes a pretrained LLM in a two-tower fashion using a mixture of two contrastive losses and a language modeling loss: the LLM is first finetuned on a data mixture from multiple domains followed by another round of target domain finetuning. Our model significantly outperforms many state-of-the-art baselines (+37\% in Recall@1 and +24\% in NDCG@10) and our systematic ablation studies reveal that (i) both stages of finetuning are crucial, and, when combined, we achieve improved performance, and (ii) contrastive alignment is effective among the target domains explored in our experiments.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {422–432},
numpages = {11},
keywords = {Contrastive Learning, Large Language Models, Sequential Recommendation},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{10.1145/3640457.3688123,
author = {Penha, Gustavo and Vardasbi, Ali and Palumbo, Enrico and De Nadai, Marco and Bouchard, Hugues},
title = {Bridging Search and Recommendation in Generative Retrieval: Does One Task Help the Other?},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688123},
doi = {10.1145/3640457.3688123},
abstract = {Generative retrieval for search and recommendation is a promising paradigm for retrieving items, offering an alternative to traditional methods that depend on external indexes and nearest-neighbor searches. Instead, generative models directly associate inputs with item IDs. Given the breakthroughs of Large Language Models (LLMs), these generative systems can play a crucial role in centralizing a variety of Information Retrieval (IR) tasks in a single model that performs tasks such as query understanding, retrieval, recommendation, explanation, re-ranking, and response generation. Despite the growing interest in such a unified generative approach for IR systems, the advantages of using a single, multi-task model over multiple specialized models are not well established in the literature. This paper investigates whether and when such a unified approach can outperform task-specific models in the IR tasks of search and recommendation, broadly co-existing in multiple industrial online platforms, such as Spotify, YouTube, and Netflix. Previous work shows that (1) the latent representations of items learned by generative recommenders are biased towards popularity, and (2) content-based and collaborative-filtering-based information can improve an item’s representations. Motivated by this, our study is guided by two hypotheses: [H1] the joint training regularizes the estimation of each item’s popularity, and [H2] the joint training regularizes the item’s latent representations, where search captures content-based aspects of an item and recommendation captures collaborative-filtering aspects. Our extensive experiments with both simulated and real-world data support both [H1] and [H2] as key contributors to the effectiveness improvements observed in the unified search and recommendation generative models over the single-task approaches.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {340–349},
numpages = {10},
keywords = {Generative Recommendation, Generative Retrieval, Joint Search and Recommendation, Multi-task Learning},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{10.1145/3640457.3688137,
author = {Petruzzelli, Alessandro and Musto, Cataldo and Laraspata, Lucrezia and Rinaldi, Ivan and de Gemmis, Marco and Lops, Pasquale and Semeraro, Giovanni},
title = {Instructing and Prompting Large Language Models for Explainable Cross-domain Recommendations},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688137},
doi = {10.1145/3640457.3688137},
abstract = {In this paper, we present a strategy to provide users with explainable cross-domain recommendations (CDR) that exploits large language models (LLMs). Generally speaking, CDR is a task that is hard to tackle, mainly due to data sparsity issues. Indeed, CDR models require a large amount of data labeled in both source and target domains, which are not easy to collect. Accordingly, our approach relies on the intuition that the knowledge that is already encoded in LLMs can be used to more easily bridge the domains and seamlessly provide users with personalized cross-domain suggestions. To this end, we designed a pipeline to: (a) instruct a LLM to handle a CDR task; (b) design a personalized prompt, based on the preferences of the user in a source domain, and a list of items to be ranked in target domain; (c) feed the LLM with the prompt, in both zero-shot and one-shot settings, and process the answer in order to extract the recommendations and a natural language explanation. As shown in the experimental evaluation, our approach beats several established state-of-the-art baselines for CDR in most of the experimental settings, thus showing the effectiveness of LLMs also in this novel and scarcely investigated scenario.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {298–308},
numpages = {11},
keywords = {Cross-domain Recommendations, Instruction Tuning, Large Language Models, Recommender Systems},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{10.1145/3640457.3688141,
author = {Balasubramanian, Keshav and Alshabanah, Abdulla and Markowitz, Elan and Ver Steeg, Greg and Annavaram, Murali},
title = {Biased User History Synthesis for Personalized Long-Tail Item Recommendation},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688141},
doi = {10.1145/3640457.3688141},
abstract = {Recommendation systems connect users to items and create value chains in the internet economy. Recommendation systems learn from past user-item interaction histories. As such, items that have short interaction histories, either because they are new or not popular, have been shown to be disproportionately under-recommended. This long-tail item problem can exacerbate model bias, and reinforce poor recommendation of tail items. In this paper, we propose biased user history synthesis, to not only address this problem but also achieve better personalization in recommendation systems. As a result, we concurrently improve tail and head item recommendation performance. Our approach is built on a tail item biased User Interaction History (UIH) sampling strategy and a synthesis model that produces an augmented user representation from the sampled user history. We provide a theoretical justification for our approach using information theory and demonstrate through extensive experimentation, that our model outperforms state-of-the-art baselines on tail, head, and overall recommendation. The source code is available at https://github.com/lkp411/BiasedUserHistorySynthesis.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {189–199},
numpages = {11},
keywords = {Recommendation systems, long-tail, personalization, ranking and retrieval},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{10.1145/3640457.3688193,
author = {Petruzzelli, Alessandro and Musto, Cataldo and Di Carlo, Michele Ciro and Tempesta, Giovanni and Semeraro, Giovanni},
title = {Recommending Healthy and Sustainable Meals exploiting Food Retrieval and Large Language Models},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688193},
doi = {10.1145/3640457.3688193},
abstract = {Given the rising global concerns about healthy nutrition and environmental sustainability, individuals need more and more support in making good choices concerning their daily meals. To this end, in this paper we introduce HeaSE, a framework for Healthy And Sustainable Eating. Given an input recipe, HeaSE identifies healthier and more sustainable meals by exploiting retrieval techniques and large language models. The framework works in two steps. First, it uses food retrieval strategies based on macro-nutrient information to identify candidate alternative meals. This ensures that the substitutions maintain a similar nutritional profile. Next, HeaSE employs large language models to re-rank these potential replacements while considering factors beyond just nutrition, such as the recipe’s environmental impact. In the experimental evaluation, we showed the capabilities of LLMs in identifying more sustainable and healthier alternatives within a set of candidate options. This highlights the potential of these models to guide users towards food choices that are both nutritious and environmentally responsible.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {1057–1061},
numpages = {5},
keywords = {Food Recommendation, Health-aware Recommender Systems, Large Language Models, Sustainability},
location = {Bari, Italy},
series = {RecSys '24}
}

@proceedings{10.1145/3640471,
title = {MobileHCI '24 Adjunct: Adjunct Proceedings of the 26th International Conference on Mobile Human-Computer Interaction},
year = {2024},
isbn = {9798400705069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@book{10.1145/3640479,
author = {Baecker, Ronald M. and Grudin, Jonathan},
title = {Digital Dreams Have Become Nightmares: What We Must Do},
year = {2024},
isbn = {9798400717703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {2},
volume = {56},
abstract = {This book offers a compelling discussion of the digital dreams that have come true, their often unintended side effects (nightmares), and what must be done to counteract the nightmares. It is intended as an impetus to further conversation not only in homes and workplaces, but in academic courses and even legislative debates. Equally importantly, the book is a presentation of what digital technology professionals need to know about these topics and the actions they should undertake individually and in support of other citizens, societal initiatives, and government. The author begins by introducing the amazing progress made in digital technologies over the past 80 years. Pioneering engineers dreamed of potential uses of technology through their writing and technical achievements, further inspiring thousands of researchers to bring the dreams to life, and to dream new dreams as well. The second part of the book describes the myriad adverse side effects and unanticipated challenges that arose as those dreams were pursued and achieved. Examples include rampant misinformation on social media, ransomware, autonomous weapons, and the premature use of AI before it is reliable and safe.The book closes with a positive call to action, outlining ways to address the challenges through ethical career choices, careful analysis, thoughtful design, research, citizen engagement, legislation/regulation, and careful consideration of how bad actors may use technology. Readers of Digital Dreams Have Become Nightmares should become more knowledgeable, wiser, and also cautiously optimistic, determined to affect positive changes through their design, creation, and use of technology.“Are you feeling happy about the role of information technology in the world today? You should read this book for a dose of reality. Are you in despair about it? This book is the prescription for that condition, too! Nobody else could cover the landscape as Ron Baecker does.” - Clayton Lewis, Emeritus Professor, University of Colorado Boulder“This book is a captivating review of important computing developments. Many things talked about as new today have been around for a long time. Much can be learned from the past. The book also teaches a careful and consistent method that enables the reader to do this kind of work as the need arises. The book suggests the need will arise.” - John Leslie King, Emeritus Professor, University of Michigan}
}

@inbook{10.1145/3640479.3640486,
title = {Disinformation and Hate Speech},
year = {2024},
isbn = {9798400717703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640479.3640486},
abstract = {This book offers a compelling discussion of the digital dreams that have come true, their often unintended side effects (nightmares), and what must be done to counteract the nightmares. It is intended as an impetus to further conversation not only in homes and workplaces, but in academic courses and even legislative debates. Equally importantly, the book is a presentation of what digital technology professionals need to know about these topics and the actions they should undertake individually and in support of other citizens, societal initiatives, and government. The author begins by introducing the amazing progress made in digital technologies over the past 80 years. Pioneering engineers dreamed of potential uses of technology through their writing and technical achievements, further inspiring thousands of researchers to bring the dreams to life, and to dream new dreams as well. The second part of the book describes the myriad adverse side effects and unanticipated challenges that arose as those dreams were pursued and achieved. Examples include rampant misinformation on social media, ransomware, autonomous weapons, and the premature use of AI before it is reliable and safe.The book closes with a positive call to action, outlining ways to address the challenges through ethical career choices, careful analysis, thoughtful design, research, citizen engagement, legislation/regulation, and careful consideration of how bad actors may use technology. Readers of Digital Dreams Have Become Nightmares should become more knowledgeable, wiser, and also cautiously optimistic, determined to affect positive changes through their design, creation, and use of technology.“Are you feeling happy about the role of information technology in the world today? You should read this book for a dose of reality. Are you in despair about it? This book is the prescription for that condition, too! Nobody else could cover the landscape as Ron Baecker does.” - Clayton Lewis, Emeritus Professor, University of Colorado Boulder“This book is a captivating review of important computing developments. Many things talked about as new today have been around for a long time. Much can be learned from the past. The book also teaches a careful and consistent method that enables the reader to do this kind of work as the need arises. The book suggests the need will arise.” - John Leslie King, Emeritus Professor, University of Michigan},
booktitle = {Digital Dreams Have Become Nightmares: What We Must Do}
}

@inbook{10.1145/3640479.3640487,
title = {Work, Automation, and Job Loss},
year = {2024},
isbn = {9798400717703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640479.3640487},
abstract = {This book offers a compelling discussion of the digital dreams that have come true, their often unintended side effects (nightmares), and what must be done to counteract the nightmares. It is intended as an impetus to further conversation not only in homes and workplaces, but in academic courses and even legislative debates. Equally importantly, the book is a presentation of what digital technology professionals need to know about these topics and the actions they should undertake individually and in support of other citizens, societal initiatives, and government. The author begins by introducing the amazing progress made in digital technologies over the past 80 years. Pioneering engineers dreamed of potential uses of technology through their writing and technical achievements, further inspiring thousands of researchers to bring the dreams to life, and to dream new dreams as well. The second part of the book describes the myriad adverse side effects and unanticipated challenges that arose as those dreams were pursued and achieved. Examples include rampant misinformation on social media, ransomware, autonomous weapons, and the premature use of AI before it is reliable and safe.The book closes with a positive call to action, outlining ways to address the challenges through ethical career choices, careful analysis, thoughtful design, research, citizen engagement, legislation/regulation, and careful consideration of how bad actors may use technology. Readers of Digital Dreams Have Become Nightmares should become more knowledgeable, wiser, and also cautiously optimistic, determined to affect positive changes through their design, creation, and use of technology.“Are you feeling happy about the role of information technology in the world today? You should read this book for a dose of reality. Are you in despair about it? This book is the prescription for that condition, too! Nobody else could cover the landscape as Ron Baecker does.” - Clayton Lewis, Emeritus Professor, University of Colorado Boulder“This book is a captivating review of important computing developments. Many things talked about as new today have been around for a long time. Much can be learned from the past. The book also teaches a careful and consistent method that enables the reader to do this kind of work as the need arises. The book suggests the need will arise.” - John Leslie King, Emeritus Professor, University of Michigan},
booktitle = {Digital Dreams Have Become Nightmares: What We Must Do}
}

@proceedings{10.1145/3640792,
title = {AutomotiveUI '24: Proceedings of the 16th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
year = {2024},
isbn = {9798400705106},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stanford, CA, USA}
}

@proceedings{10.1145/3640794,
title = {CUI '24: Proceedings of the 6th ACM Conference on Conversational User Interfaces},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Luxembourg, Luxembourg}
}

@inproceedings{10.1145/3640794.3665553,
author = {Piorkowski, David and Ostrand, Rachel and Brimijoin, Kristina and He, Jessica and Albert, Erica and Houde, Stephanie},
title = {Towards Interactive Guidance for Writing Training Utterances for Conversational Agents},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640794.3665553},
doi = {10.1145/3640794.3665553},
abstract = {Improving conversational agents that are trained with supervised learning requires iteratively refining example intent training utterances based on chat log data. The difficulty of this process hinges on the quality of the initial example utterances used to train the intent before it was first deployed. Creating new intents from scratch, when conversation logs are not yet available, has many challenges. We interviewed experienced conversational agent intent trainers to better understand challenges they face when creating new intents, and their best practices for writing high quality training utterances. Using these findings and related literature, we developed an intent training tool that provided interactive guidance via either language feedback or sample utterances. Language feedback notified the user when training utterances could be linguistically improved, while sample utterances were crowdsourced and provided examples of end user language prior to deploying an intent. We compared these two types of guidance in a 187-participant between-subject study. We found that participants in the language feedback condition reported limited creativity and higher mental load and spent more time on the task, but were more thoughtful in crafting utterances that adhered to best practices. In contrast, sample utterance participants leveraged the samples to either quickly select examples or use them as a springboard to develop new utterance ideas. We report on differences in user experience in the strategies that participants took and preferences for or against the different types of guidance.},
booktitle = {Proceedings of the 6th ACM Conference on Conversational User Interfaces},
articleno = {14},
numpages = {15},
keywords = {AI explainability, AI model development, artificial intelligence, machine learning},
location = {Luxembourg, Luxembourg},
series = {CUI '24}
}

@proceedings{10.1145/3640824,
title = {CCEAI '24: Proceedings of the 2024 8th International Conference on Control Engineering and Artificial Intelligence},
year = {2024},
isbn = {9798400707971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shanghai, China}
}

@proceedings{10.1145/3640872,
title = {BDE '23: Proceedings of the 2023 5th International Conference on Big Data Engineering},
year = {2023},
isbn = {9798400708695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Zhuhai, China}
}

@proceedings{10.1145/3640900,
title = {ICBBB '24: Proceedings of the 2024 14th International Conference on Bioscience, Biochemistry and Bioinformatics},
year = {2024},
isbn = {9798400716768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kyoto, Japan}
}

@proceedings{10.1145/3640912,
title = {CNML '23: Proceedings of the 2023 International Conference on Communication Network and Machine Learning},
year = {2023},
isbn = {9798400716683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Zhengzhou, China}
}

@proceedings{10.1145/3641032,
title = {ICISE '23: Proceedings of the 2023 8th International Conference on Information Systems Engineering},
year = {2023},
isbn = {9798400709173},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangkok, Thailand}
}

@proceedings{10.1145/3641233,
title = {SIGGRAPH '24: ACM SIGGRAPH 2024 Talks},
year = {2024},
isbn = {9798400705151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@proceedings{10.1145/3641234,
title = {SIGGRAPH '24: ACM SIGGRAPH 2024 Posters},
year = {2024},
isbn = {9798400705168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@proceedings{10.1145/3641308,
title = {AutomotiveUI '24 Adjunct: Adjunct Proceedings of the 16th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
year = {2024},
isbn = {9798400705205},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stanford, CA, USA}
}

@proceedings{10.1145/3641399,
title = {ISEC '24: Proceedings of the 17th Innovations in Software Engineering Conference},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@inproceedings{10.1145/3641519.3657422,
author = {Sun, Haowen and Zheng, Ruikun and Huang, Haibin and Ma, Chongyang and Huang, Hui and Hu, Ruizhen},
title = {LGTM: Local-to-Global Text-Driven Human Motion Diffusion Model},
year = {2024},
isbn = {9798400705250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641519.3657422},
doi = {10.1145/3641519.3657422},
abstract = {In this paper, we introduce LGTM, a novel Local-to-Global pipeline for Text-to-Motion generation. LGTM utilizes a diffusion-based architecture and aims to address the challenge of accurately translating textual descriptions into semantically coherent human motion in computer animation. Specifically, traditional methods often struggle with semantic discrepancies, particularly in aligning specific motions to the correct body parts. To address this issue, we propose a two-stage pipeline to overcome this challenge: it first employs large language models (LLMs) to decompose global motion descriptions into part-specific narratives, which are then processed by independent body-part motion encoders to ensure precise local semantic alignment. Finally, an attention-based full-body optimizer refines the motion generation results and guarantees the overall coherence. Our experiments demonstrate that LGTM gains significant improvements in generating locally accurate, semantically-aligned human motion, marking a notable advancement in text-to-motion applications. Code and data for this paper are available at https://github.com/L-Sun/LGTM},
booktitle = {ACM SIGGRAPH 2024 Conference Papers},
articleno = {66},
numpages = {9},
keywords = {Diffusion Model, Motion Synthesis, Text-Driven Generation.},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@proceedings{10.1145/3641525,
title = {ACM REP '24: Proceedings of the 2nd ACM Conference on Reproducibility and Replicability},
year = {2024},
isbn = {9798400705304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rennes, France}
}

@inproceedings{10.1145/3641525.3663617,
author = {Boufford, Nichole and Wonsil, Joseph and Pocock, Adam and Sullivan, Jack and Seltzer, Margo and Pasquier, Thomas},
title = {Computational Experiment Comprehension using Provenance Summarization},
year = {2024},
isbn = {9798400705304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641525.3663617},
doi = {10.1145/3641525.3663617},
abstract = {Scientists use complex multistep workflows to analyze data. However, reproducing computational experiments is often difficult as scientists’ software engineering practices are geared towards the science, not the programming. In particular, reproducing a scientific workflow frequently requires information about its execution. This information includes the precise versions of packages and libraries used, the particular processor used to perform floating point computation, and the language runtime used. This can be extracted from data provenance, the formal record of what happened during an experiment. However, data provenance is inherently graph-structured and often large, which makes interpretation challenging. Rather than exposing data provenance through its graphical representation, we propose a textual one and use a large language model to generate it. We develop techniques for prompting large language models to automatically generate textual summaries of provenance data. We conduct a user study to compare the effectiveness of these summaries to the more common node-link diagram representation. Study participants are able to extract useful information from both the textual summaries and node-link diagrams. The textual summaries were particularly beneficial for scientists with low computational expertise. We discuss the qualitative results from our study to motivate future designs for reproducibility tools.},
booktitle = {Proceedings of the 2nd ACM Conference on Reproducibility and Replicability},
pages = {1–19},
numpages = {19},
keywords = {Provenance, Reproducibility, Text Generation, User Study},
location = {Rennes, France},
series = {ACM REP '24}
}

@proceedings{10.1145/3641584,
title = {AIPR '23: Proceedings of the 2023 6th International Conference on Artificial Intelligence and Pattern Recognition},
year = {2023},
isbn = {9798400707674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3641822,
title = {CHASE '24: Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering},
year = {2024},
isbn = {9798400705335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {CHASE 2024 continues the tradition of a high-quality venue for research related to the cooperative and human aspects of software engineering. Researchers and practitioners have long recognized the need to investigate the cooperative and human aspects. However, their articles have been scattered across many conferences and communities. The CHASE conference provides academics and practitioners with a unified forum for discussing high-quality research studies, models, methods, and tools for human and cooperative aspects of software engineering.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3641825,
title = {VRST '24: Proceedings of the 30th ACM Symposium on Virtual Reality Software and Technology},
year = {2024},
isbn = {9798400705359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Trier, Germany}
}

@proceedings{10.1145/3643487,
title = {AI-SIPM '24: Proceedings of the International Workshop on Artificial Intelligence for Signal, Image Processing and Multimedia},
year = {2024},
isbn = {9798400705489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the proceedings of The International Workshop on Artificial Intelligence for Signal, Image Processing, and Multimedia (AI-SIPM), held during June 10-14, 2024, in the beautiful setting of Phuket, Thailand. This workshop represents a significant gathering of researchers, practitioners, and experts from around the globe, aimed at advancing the state-of-the-art in artificial intelligence applications within the realms of signal processing, image processing, and multimedia. This year, we were honored to receive over 20 paper submissions from 3 different countries. Each submission underwent a rigorous peer-review process, with each paper being evaluated by a diverse and highly qualified panel of reviewers, drawn from our extensive pool of technical committee members and other international experts in the field. Following this meticulous review process, we selected 12 high-quality papers for oral presentation at the workshop. These accepted papers represent cutting-edge research and innovative solutions in their respective areas. They reflect the diversity and depth of current advancements and explore a wide range of topics including, but not limited to, AI algorithms for signal processing, advancements in image processing techniques, and multimedia applications. We are confident that these contributions will spark insightful discussions and foster further research collaborations. We would like to extend our heartfelt gratitude to all the authors who submitted their work, the reviewers who dedicated their time and expertise, and the organizing committee who made this workshop possible. We are also deeply thankful to our sponsors and partners for their continued support. We hope that AI-SIPM 2024 will be a fruitful and engaging experience for all attendees, providing a platform for knowledge exchange and networking within the vibrant community of artificial intelligence and multimedia technology.},
location = {Phuket, Thailand}
}

@proceedings{10.1145/3643489,
title = {LSC '24: Proceedings of the 7th Annual ACM Workshop on the Lifelog Search Challenge},
year = {2024},
isbn = {9798400705502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The LSC workshops are participation workshops, where participants write and present an academic paper describing their prototype lifelog retrieval system, and then take part in a live interactive search competition. Consequently, the workshop is highly interactive and challenging for participants.},
location = {Phuket, Thailand}
}

@article{10.1145/3643546,
author = {Wang, Shuning and Zhong, Linghui and Fu, Yongjian and Chen, Lili and Ren, Ju and Zhang, Yaoxue},
title = {UFace: Your Smartphone Can "Hear" Your Facial Expression!},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
url = {https://doi.org/10.1145/3643546},
doi = {10.1145/3643546},
abstract = {Facial expression recognition (FER) is a crucial task for human-computer interaction and a multitude of multimedia applications that typically call for friendly, unobtrusive, ubiquitous, and even long-term monitoring. Achieving such a FER system meeting these multi-requirements faces critical challenges, mainly including the tiny irregular non-periodic deformation of emotion movements, high variability in facial positions and severe self-interference caused by users' own other behavior. In this work, we present UFace, a long-term, unobtrusive and reliable FER system for daily life using acoustic signals generated by a portable smartphone. We design an innovative network model with dual-stream input based on the attention mechanism, which can leverage distance-time profile features from various viewpoints to extract fine-grained emotion-related signal changes, thus enabling accurate identification of many kinds of expressions. Meanwhile, we propose effective mechanisms to deal with a series of interference issues during actual use. We implement UFace prototype with a daily-used smartphone and conduct extensive experiments in various real-world environments. The results demonstrate that UFace can successfully recognize 7 typical facial expressions with an average accuracy of 87.8\% across 20 participants. Besides, the evaluation of different distances, angles, and interferences proves the great potential of the proposed system to be employed in practical scenarios.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {22},
numpages = {27},
keywords = {Acoustic sensing, Deep learning, Facial expression recognition, Smartphone}
}

@proceedings{10.1145/3643650,
title = {SaT-CPS '24: Proceedings of the 2024 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems},
year = {2024},
isbn = {9798400705557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the fourth ACM Workshop on Secure and Trustworthy Cyber-Physical Systems (SaT-CPS 2024) held in conjunction with the 14th ACM Conference on Data and Application Security and Privacy (CODASPY 2024). ACM SaT-CPS aims to represent a forum for researchers and practitioners from industry and academia interested in various areas of CPS security. SaTCPS features novel submissions describing practical and theoretical solutions for cyber security challenges in CPS. Cyber-physical systems (CPS) entail seamless integration of computation and physical components. These systems illustrate the synergistic interactions among the cyber components, such as the computing and communication parts, and the physical devices, operating at wide varieties of spatial and temporal time scales. CPS is driving innovation and competition in a range of sectors, including agriculture, aeronautics, building design, civil infrastructure, energy, environmental quality, healthcare and personalized medicine, and transportation. These applications will empower the true vision of CPS allowing human beings to interact with the physical world and serve critical functions in our lives. CPS technologies are emerging to be the key drivers for future autonomous and smart connected worlds. With the wider adoption and popularity of the CPS applications, securing them against malicious activities is paramount. Otherwise, malfunctioning and insecure CPS devices and applications can cause enormous damage to individuals, businesses, and nations.},
location = {Porto, Portugal}
}

@inproceedings{10.1145/3643650.3658607,
author = {Xia, Lichen and Liao, Jinghui and Chen, Niusen and Chen, Bo and Shi, Weisong},
title = {A Simple Mobile Plausibly Deniable System Using Image Steganography and Secure Hardware},
year = {2024},
isbn = {9798400705557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643650.3658607},
doi = {10.1145/3643650.3658607},
abstract = {Traditional encryption methods cannot defend against coercive attacks in which the adversary captures both the user and the possessed computing device, and forces the user to disclose the decryption keys. Plausibly deniable encryption (PDE) has been designed to defend against this strong coercive attacker. At its core, PDE allows the victim to plausibly deny the very existence of hidden sensitive data and the corresponding decryption keys upon being coerced. Designing an efficient PDE system for a mobile platform, however, is challenging due to various design constraints bound to the mobile systems.  Leveraging image steganography and the built-in hardware security feature of mobile devices, namely TrustZone, we have designed a Simple Mobile Plausibly Deniable Encryption (SMPDE) system which can combat coercive adversaries and, meanwhile, is able to overcome unique design constraints. In our design, the encoding/decoding process of image steganography is bounded together with Arm TrustZone. In this manner, the coercive adversary will be given a decoy key, which can only activate a DUMMY trusted application that will instead sanitize the sensitive information stored hidden in the stego-image upon decoding. On the contrary, the actual user can be given the true key, which can activate the PDE trusted application that can really extract the sensitive information from the stego-image upon decoding. Security analysis and experimental evaluation justify both the security and the efficiency of our design.},
booktitle = {Proceedings of the 2024 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems},
pages = {21–29},
numpages = {9},
keywords = {image steganography, mobile devices, plausibly deniable encryption, trustzone},
location = {Porto, Portugal},
series = {SaT-CPS '24}
}

@proceedings{10.1145/3643651,
title = {IWSPA '24: Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics},
year = {2024},
isbn = {9798400705564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 2024 ACM International Workshop on Security and Privacy Analytics - IWSPA 2024. This year's workshop is the tenth in the series and co-hosted with the Fourteenth ACM Annual Conference on Data and Applications Security and Privacy (CODASPY 2024).IWSPA addresses important research topics associated with the application of data analytics tools and techniques (including statistical, machine/deep learning, data mining, and natural language processing) to challenges that arise with security and privacy preservation. IWSPA provides a forum for the interaction between researchers in these areas, identifying and pursuing new topics that arise in the intersection between the fields of Artificial Intelligence and Cybersecurity.},
location = {Porto, Portugal}
}

@proceedings{10.1145/3643655,
title = {SESoS '24: Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SESoS 2024 will provide a forum for researchers and practitioners with a forum to exchange ideas and experiences, analyze research and development issues, discuss promising solutions, and propose theoretical foundations for the development and evolution of complex software-intensive systems.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3643656,
title = {FTW '24: Proceedings of the 1st International Workshop on Flaky Tests},
year = {2024},
isbn = {9798400705588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Flaky tests are tests that exhibit inconsistent behavior, producing different outcomes when executed multiple times without any changes to the code under test or the test itself. Test flakiness poses a significant challenge for software development teams, as it undermines the reliability and validity of the testing process, leading to wasted time, effort, and resources, as well as reduced confidence in the quality of the software product. Flaky tests as a research topic has grown in interest significantly within the software engineering community in recent years. This has produced a wide array of empirical studies on the causes of flaky tests and experimental tools for their detection and repair. Despite these considerable advances, flakiness remains to be one of the main challenges software developers are facing today.The 1st International Flaky Tests Workshop 2024 (FTW 2024), co-located with ICSE 2024, aims to bring together researchers and practitioners from academia and industry to share their insights, experiences, and solutions on the topic of flaky tests. The workshop will feature a diverse program, including a keynote speech, paper presentations, and a panel discussion. Furthermore, the workshop will also provide a platform for networking and collaboration among the participants, fostering a vibrant research community on this emerging and important topic.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3643656.3643900,
author = {Chen, Yang and Jabbarvand, Reyhaneh},
title = {Can ChatGPT Repair Non-Order-Dependent Flaky Tests?},
year = {2024},
isbn = {9798400705588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643656.3643900},
doi = {10.1145/3643656.3643900},
abstract = {Regression testing helps developers check whether the latest code changes break software functionality. Flaky tests, which can non-deterministically pass or fail on the same code version, may mislead developers' concerns, resulting in missing some bugs or spending time pinpointing bugs that do not exist. Existing flakiness detection and mitigation techniques have primarily focused on general order-dependent (OD) and implementation-dependent (ID) flaky tests. There is also a dearth of research on repairing test flakiness, out of which, mostly have focused on repairing OD flaky tests, and a few have explored repairing a subcategory of non-order-dependent (NOD) flaky tests that are caused by asynchronous waits. As a result, there is a demand for devising techniques to reproduce, detect, and repair NOD flaky tests. Large language models (LLMs) have shown great effectiveness in several programming tasks. To explore the potential of LLMs in addressing NOD flakiness, this paper investigates the possibility of using ChatGPT to repair different categories of NOD flaky tests. Our comprehensive study on 118 from the IDoFT dataset shows that ChatGPT, despite as a leading LLM with notable success in multiple code generation tasks, is ineffective in repairing NOD test flakiness, even by following the best practices for prompt crafting. We investigated the reasons behind the failure of using ChatGPT in repairing NOD tests, which provided us valuable insights about the next step to advance the field of NOD test flakiness repair.},
booktitle = {Proceedings of the 1st International Workshop on Flaky Tests},
pages = {22–29},
numpages = {8},
keywords = {software testing, test flakiness, large language models},
location = {Lisbon, Portugal},
series = {FTW '24}
}

@proceedings{10.1145/3643658,
title = {GAS '24: Proceedings of the ACM/IEEE 8th International Workshop on Games and Software Engineering},
year = {2024},
isbn = {9798400705618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GAS is an annual workshop that brings together researchers and practitioners who are keen on exchanging ideas and progressing techniques in the intersection of game engineering and software engineering.GAS explores how advanced technologies can be used to benefit the engineering of gameful systems, including entertainment games, serious games, and gamified applications. The goal of this one-day workshop is to bring together the greater community of software engineers and game engineers to encourage discussions from an interdisciplinary perspective, on the emerging research challenges around game and software engineering.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3643660,
title = {Designing '24: Proceedings of the 1st International Workshop on Designing Software},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The goals of this workshop are to: (1) bring together a group of researchers, practitioners, and educators interested in software design, (2) identify open challenges and new directions for the design of modern software systems, including grand challenges for the community, and (3) discuss novel approaches to designing as well as teaching design. Although the workshop welcomes discussions related to any aspect of software design, the primary focus will be on improving our understanding of design as an activity rather than as an artifact or end product (hence the word designing in the title).},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3643662,
title = {EnCyCriS/SVM '24: Proceedings of the 2024 ACM/IEEE 4th International Workshop on Engineering and Cybersecurity of Critical Systems (EnCyCriS) and 2024 IEEE/ACM Second International Workshop on Software Vulnerability},
year = {2024},
isbn = {9798400705656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Increasing system interconnectivity, decentralization, and introduction of new, more intelligent technologies, result in critical infrastructures becoming exposed to increased risk of cyber, physical, and combined cyber-physical attacks. Cyberattacks on critical systems can inflict severe consequences to people, society, economy, and national security, and can have adverse effects on safety and reliability of critical infrastructures. The joint EnCyCriS-SVM workshop facilitates discourse and discussions among researchers, practitioners, and students who are working on challenges and solutions related to the industrial revolution. Focus is given on sharing industry experience and project results pertaining to cyber threats on critical systems; secure software engineering; and attack detection and response mechanisms.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3643664,
title = {WSESE '24: Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering},
year = {2024},
isbn = {9798400705670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {WSESE 2024 was a one-day event held on April 16, 2024, in Lisbon, Portugal. The theme of the workshop was "Methodological Issues with Empirical Studies in Software Engineering". The primary goal was to gain a better understanding of the adoption of the empirical paradigm in SE. Specifically, our focus was on identifying, discussing and finding solutions for the issues in the empirical methods currently employed. The workshop provided an opportunity for researchers and practitioners to discuss current methodological challenges and explore ways to address them.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3643664.3648207,
author = {Alami, Adam and Zahedi, Mansooreh and Ernst, Neil},
title = {Are You a Real Software Engineer? Best Practices in Online Recruitment for Software Engineering Studies},
year = {2024},
isbn = {9798400705670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643664.3648207},
doi = {10.1145/3643664.3648207},
abstract = {Online research platforms, such as Prolific, offer rapid access to diverse participant pools but also pose unique challenges in participant qualification and skill verification. Previous studies reported mixed outcomes and challenges in leveraging online platforms for the recruitment of qualified software engineers. Drawing from our experience in conducting three different studies using Prolific, we propose best practices for recruiting and screening participants to enhance the quality and relevance of both qualitative and quantitative software engineering (SE) research samples. We propose refined best practices for recruitment in SE research on Prolific. (1) Iterative and controlled prescreening, enabling focused and manageable assessment of submissions (2) task-oriented and targeted questions that assess technical skills, knowledge of basic SE concepts, and professional engagement. (3) AI detection to verify the authenticity of free-text responses. (4) Qualitative and manual assessment of responses, ensuring authenticity and relevance in participant answers (5) Additional layers of prescreening are necessary when necessary to collect data relevant to the topic of the study. (6) Fair or generous compensation post-qualification to incentivize genuine participation. By sharing our experiences and lessons learned, we contribute to the development of effective and rigorous methods for SE empirical research. particularly the ongoing effort to establish guidelines to ensure reliable data collection. These practices have the potential to transferability to other participant recruitment platforms.},
booktitle = {Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering},
pages = {52–57},
numpages = {6},
keywords = {empirical software engineering, prolific, participant recruitment, online research platforms},
location = {Lisbon, Portugal},
series = {WSESE '24}
}

@proceedings{10.1145/3643666,
title = {MO2RE 2024: Proceedings of the 1st IEEE/ACM Workshop on Multi-disciplinary, Open, and RElevant Requirements Engineering},
year = {2024},
isbn = {9798400705694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Requirements engineering (RE) is a critical sub-field of software engineering (SE) that deals with identifying, specifying, modeling, analyzing, and validating the needs of stakeholders and constraints of a system [1]. RE covers human-related aspects, as stakeholders need to be involved in eliciting and validating the requirements, as well as more technical aspects, as requirements can be systematically collected (e.g., from app reviews) using data mining techniques and analyzed with natural language processing (NLP) approaches, e.g., to identify quality issues or trace links [2]. Despite the broad spectrum of activities that RE covers, researchers from outside RE often have a misconception that RE is limited to writing and analyzing requirements specifications. Consequently, many researchers in the SE community working on RE-relevant problems (e.g., human-centric SE) are often unaware that such problems belong to the RE research strands. Broadly speaking, RE is under-represented and under-appreciated in the SE community.Despite this limited presence, RE is more and more fundamental to cope with the current state of SE, especially considering the recent disruptive changes in artificial intelligence (AI) and NLP caused by large language models (LLMs) and their applications, ChatGPT being a notable example. Given the increasing pervasiveness of AI-based systems in our daily life, there is a growing need for RE techniques to support sound and structured development of AI systems [3], with a particular interest in explainability, interpretability, reliability, fairness, and other ethical concerns [4]. At the same time, current developments in AI can solve long-standing RE problems, such as automatic requirements tracing, completeness checking, and modeling. AI can further create better connections between RE and other automated SE fields.The 1st International Workshop on (Multi-disciplinary, Open, and RElevant RE) (MO2RE) has the goal to address these issues by raising awareness of RE's diverse aspects and fostering collaboration within the SE community.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3643667,
title = {Q-SE 2024: Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering},
year = {2024},
isbn = {9798400705700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 5th International Workshop on Quantum Software Engineering (Q-SE 2024), co-located with ICSE 2024, provides a platform for researchers and practitioners to discuss challenges in developing quantum software in high-level quantum languages, novel solutions to build correct methods for testing quantum programs, executing quantum software, developing best practices, and creating a research roadmap of quantum software engineering.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3643690,
title = {IWSiB '24: Proceedings of the 7th ACM/IEEE International Workshop on Software-intensive Business},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The workshop brings together research communities working on softwareintensive business and software engineering. It aims to bridge the gap between the research in these areas. This year's theme, "Software Business in the Era of Generative Artificial Intelligence," reflects our focus on exploring how generative artificial intelligence (GenAI) and the related large language models (LLMs) impact the established practices of software engineering and software business.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3643690.3648245,
author = {Van Schothorst, Casper and Schuurmans, Robbert and Jansen, Slinger},
title = {Software Ecosystem Orchestration with Topic Modeling},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643690.3648245},
doi = {10.1145/3643690.3648245},
abstract = {Software ecosystems are typically networks of organizations collaboratively serving a market for software around a technical platform. Software ecosystem orchestrators are responsible for the health of these networks, for instance by attracting new applications to application stores and by opening new parts of the platform for extension. Considering the amounts of application data that orchestrators have to deal with, we propose an approach for strategic analysis of the software ecosystem that uses topic modeling. With our approach, orchestrators are supported in their task of growing and pruning their platform software ecosystem. We illustrate the use of the approach in two case studies of rapidly expanding and competing software platform ecosystems.},
booktitle = {Proceedings of the 7th ACM/IEEE International Workshop on Software-Intensive Business},
pages = {72–78},
numpages = {7},
keywords = {software ecosystem health, domain analysis, repository mining, topic modelling applications, app store mining},
location = {Lisbon, Portugal},
series = {IWSiB '24}
}

@proceedings{10.1145/3643692,
title = {GI '24: Proceedings of the 13th ACM/IEEE International Workshop on Genetic Improvement},
year = {2024},
isbn = {9798400705731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The GI workshops continue to bring together researchers from across the world to exchange ideas about using optimisation techniques, particularly evolutionary computation, such as genetic programming, to improve existing software.},
location = {Lisbon, Portugal}
}

@article{10.1145/3643733,
author = {Jiang, Zhihan and Liu, Jinyang and Chen, Zhuangbin and Li, Yichen and Huang, Junjie and Huo, Yintong and He, Pinjia and Gu, Jiazhen and Lyu, Michael R.},
title = {LILAC: Log Parsing using LLMs with Adaptive Parsing Cache},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643733},
doi = {10.1145/3643733},
abstract = {Log parsing transforms log messages into structured formats, serving as the prerequisite step for various log analysis tasks. Although a variety of log parsing approaches have been proposed, their performance on complicated log data remains compromised due to the use of human-crafted rules or learning-based models with limited training data. The recent emergence of powerful large language models (LLMs) demonstrates their vast pre-trained knowledge related to code and logging, making it promising to apply LLMs for log parsing. However, their lack of specialized log parsing capabilities currently hinders their parsing accuracy. Moreover, the inherent inconsistent answers, as well as the substantial overhead, prevent the practical adoption of LLM-based log parsing. To address these challenges, we propose LILAC, the first practical Log parsIng framework using LLMs with Adaptive parsing Cache. To facilitate accurate and robust log parsing, LILAC leverages the in-context learning (ICL) capability of the LLM by performing a hierarchical candidate sampling algorithm and selecting high-quality demonstrations. Furthermore, LILAC incorporates a novel component, an adaptive parsing cache, to store and refine the templates generated by the LLM. It helps mitigate LLM's inefficiency issue by enabling rapid retrieval of previously processed log templates. In this process, LILAC adaptively updates the templates within the parsing cache to ensure the consistency of parsed results. The extensive evaluation on public large-scale datasets shows that LILAC outperforms state-of-the-art methods by 69.5\% in terms of the average F1 score of template accuracy. In addition, LILAC reduces the query times to LLMs by several orders of magnitude, achieving a comparable efficiency to the fastest baseline.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {7},
numpages = {24},
keywords = {large language models, log analysis, log parsing}
}

@article{10.1145/3643753,
author = {Wang, Yan and Li, Xiaoning and Nguyen, Tien N. and Wang, Shaohua and Ni, Chao and Ding, Ling},
title = {Natural Is the Best: Model-Agnostic Code Simplification for Pre-trained Large Language Models},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643753},
doi = {10.1145/3643753},
abstract = {Pre-trained Large Language Models (LLM) have achieved remarkable successes in several domains. However, code-oriented LLMs are often heavy in computational complexity, and quadratically with the length of the input code sequence. Toward simplifying the input program of an LLM, the state-of-the-art approach has the strategies to filter the input code tokens based on the attention scores given by the LLM. The decision to simplify the input program should not rely on the attention patterns of an LLM, as these patterns are influenced by both the model architecture and the pre-training dataset. Since the model and dataset are part of the solution domain, not the problem domain where the input program belongs, the outcome may differ when the model is pre-trained on a different dataset. We propose SlimCode, a model-agnostic code simplification solution for LLMs that depends on the nature of input code tokens. As an empirical study on the LLMs including CodeBERT, CodeT5, and GPT-4 for two main tasks: code search and summarization, we reported that 1) the removal ratio of code has a linear-like relation with the saving ratio on training time, 2) the impact of categorized tokens on code simplification can vary significantly, 3) the impact of categorized tokens on code simplification is task-specific but model-agnostic, and 4) the above findings hold for the paradigm–prompt engineering and interactive in-context learning. The empirical results showed that SlimCode can improve the state-of-the-art technique by 9.46\% and 5.15\% in terms of MRR and BLEU score on code search and summarization, respectively. More importantly, SlimCode is 133 times faster than the state-of-the-art approach. Additionally, SlimCode can reduce the cost of invoking GPT-4 by up to 24\% per API query, while still producing comparable results to those with the original code. With this result, we call for a new direction on code-based, model-agnostic code simplification solutions to further empower LLMs.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {27},
numpages = {23},
keywords = {AI4SE, Code Simplification, Machine Learning, Neural Networks, Pre-trained Large Language Models}
}

@article{10.1145/3643755,
author = {Dilhara, Malinda and Bellur, Abhiram and Bryksin, Timofey and Dig, Danny},
title = {Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643755},
doi = {10.1145/3643755},
abstract = {Software developers often repeat the same code changes within a project or across different projects. These repetitive changes are known as “code change patterns” (CPATs). Automating CPATs is crucial to expedite the software development process. While current Transformation by Example (TBE) techniques can automate CPATs, they are limited by the quality and quantity of the provided input examples. Thus, they miss transforming code variations that do not have the exact syntax, data-, or control-flow of the provided input examples, despite being semantically similar. Large Language Models (LLMs), pre-trained on extensive source code datasets, offer a potential solution. Harnessing the capability of LLMs to generate semantically equivalent, yet previously unseen variants of the original CPAT could significantly increase the effectiveness of TBE systems. In this paper, we first discover best practices for harnessing LLMs to generate code variants that meet three criteria: correctness (semantic equivalence to the original CPAT), usefulness (reflecting what developers typically write), and applicability (aligning with the primary intent of the original CPAT). We then implement these practices in our tool PyCraft, which synergistically combines static code analysis, dynamic analysis, and LLM capabilities. By employing chain-of-thought reasoning, PyCraft generates variations of input examples and comprehensive test cases that identify correct variations with an F-measure of 96.6\%. Our algorithm uses feedback iteration to expand the original input examples by an average factor of 58x. Using these richly generated examples, we inferred transformation rules and then automated these changes, resulting in an increase of up to 39x, with an average increase of 14x in target codes compared to a previous state-of-the-art tool that relies solely on static analysis. We submitted patches generated by PyCraft to a range of projects, notably esteemed ones like microsoft/DeepSpeed and IBM/inFairness. Their developers accepted and merged 83\% the 86 CPAT instances submitted through 44 pull requests. This confirms the usefulness of these changes.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {29},
numpages = {23},
keywords = {Automation, Code Changes, Code Clone, Generative AI, Large Language Models, Machine Learning, Program by Example, Python, Test Case Generation, Transformation by Example}
}

@article{10.1145/3643757,
author = {Bairi, Ramakrishna and Sonwane, Atharv and Kanade, Aditya and C., Vageesh D. and Iyer, Arun and Parthasarathy, Suresh and Rajamani, Sriram and Ashok, B. and Shet, Shashank},
title = {CodePlan: Repository-Level Coding using LLMs and Planning},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643757},
doi = {10.1145/3643757},
abstract = {Software engineering activities such as package migration, fixing error reports from static analysis or testing, and adding type annotations or other specifications to a codebase, involve pervasively editing the entire repository of code.     We formulate these activities as repository-level coding tasks.         Recent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in offering high-quality solutions to localized coding problems.     Repository-level coding tasks are more involved and cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire repository may be too large to fit into the prompt.     We frame repository-level coding as a planning problem and present a task-agnostic, neuro-symbolic framework called CodePlan to solve it.     CodePlan synthesizes a multi-step chain-of-edits (plan), where each step results in a call to an LLM on a code location with context derived from the entire repository, previous code changes and task-specific instructions.     CodePlan is based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm (symbolic components) with the neural LLMs.         We evaluate the effectiveness of CodePlan on two repository-level tasks: package migration (C#) and temporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires inter-dependent changes to many files (between 2–97 files).     Coding tasks of this level of complexity have not been automated using LLMs before. Our results show that CodePlan has better match with the ground truth compared to baselines.     CodePlan is able to get 5/7 repositories to pass the validity checks (i.e., to build without errors and make correct code edits) whereas the baselines (without planning but with the same type of contextual information as CodePlan) cannot get any of the repositories to pass them.     We provide our (non-proprietary) data, evaluation scripts and supplementary material at https://github.com/microsoft/codeplan.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {31},
numpages = {24},
keywords = {Automated coding, LLMs, chain of edits, neuro-symbolic AI, plan, repositories, static analysis}
}

@article{10.1145/3643762,
author = {Wadhwa, Nalin and Pradhan, Jui and Sonwane, Atharv and Sahu, Surya Prakash and Natarajan, Nagarajan and Kanade, Aditya and Parthasarathy, Suresh and Rajamani, Sriram},
title = {CORE: Resolving Code Quality Issues using LLMs},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643762},
doi = {10.1145/3643762},
abstract = {As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues.    We present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The proposer LLM of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, the LLM may introduce subtle, unintended functionality changes which may go un-detected by the static analysis. The ranker LLM evaluates the changes made by the proposer using a rubric that closely follows the acceptance criteria that a developer would enforce. CORE uses the scores assigned by the ranker LLM to rank the candidate revisions before presenting them to the developer.    We conduct a variety of experiments on two public benchmarks to show the ability of CORE:  (1) to generate code revisions acceptable to both static analysis tools and human reviewers (the latter evaluated with user study on a subset of the Python benchmark),  (2) to reduce human review efforts by detecting and eliminating revisions with unintended changes,  (3) to readily work across multiple languages (Python and Java), static analysis tools (CodeQL and SonarQube) and quality checks (52 and 10 checks, respectively),  and  (4) to achieve fix rate comparable to a rule-based automated program repair tool but with much smaller engineering efforts (on the Java benchmark).  CORE could revise 59.2\% Python files (across 52 quality checks) so that they pass scrutiny by both a tool and a human reviewer. The ranker LLM reduced false positives by 25.8\% in these cases. CORE produced revisions that passed the static analysis tool in 76.8\% Java files (across 10 quality checks) comparable to 78.3\% of a specialized program repair tool, with significantly much less engineering efforts. We release code, data, and supplementary material publicly at http://aka.ms/COREMSRI.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {36},
numpages = {23},
keywords = {Code quality, LLMs, code revision, static analysis}
}

@article{10.1145/3643769,
author = {Ryan, Gabriel and Jain, Siddhartha and Shang, Mingyue and Wang, Shiqi and Ma, Xiaofei and Ramanathan, Murali Krishna and Ray, Baishakhi},
title = {Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using LLM},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643769},
doi = {10.1145/3643769},
abstract = {Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt’s approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26\% for CodeGen2. Notably, when applied to GPT-4, SymPrompt improves coverage by over 2x compared to baseline prompting strategies.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {43},
numpages = {21},
keywords = {Large Language Models, Test Generation}
}

@article{10.1145/3643770,
author = {Yan, Yanfu and Cooper, Nathan and Moran, Kevin and Bavota, Gabriele and Poshyvanyk, Denys and Rich, Steve},
title = {Enhancing Code Understanding for Impact Analysis by Combining Transformers and Program Dependence Graphs},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643770},
doi = {10.1145/3643770},
abstract = {Impact analysis (IA) is a critical software maintenance task that identifies the effects of a given set of code changes on a larger software project with the intention of avoiding potential adverse effects. IA is a cognitively challenging task that involves reasoning about the abstract relationships between various code constructs. Given its difficulty, researchers have worked to automate IA with approaches that primarily use coupling metrics as a measure of the "connectedness" of different parts of a software project. Many of these coupling metrics rely on static, dynamic, or evolutionary information and are based on heuristics that tend to be brittle, require expensive execution analysis, or large histories of co-changes to accurately estimate impact sets. In this paper, we introduce a novel IA approach, called ATHENA, that combines a software system's dependence graph information with a conceptual coupling approach that uses advances in deep representation learning for code without the need for change histories and execution information. Previous IA benchmarks are small, containing less than ten software projects, and suffer from tangled commits, making it difficult to measure accurate results. Therefore, we constructed a large-scale IA benchmark, from 25 open-source software projects, that utilizes fine-grained commit information from bug fixes. On this new benchmark, our best performing approach configuration achieves an mRR, mAP, and HIT@10 score of 60.32\%, 35.19\%, and 81.48\%, respectively. Through various ablations and qualitative analyses, we show that ATHENA's novel combination of program dependence graphs and conceptual coupling information leads it to outperform a simpler baseline by 10.34\%, 9.55\%, and 11.68\% with statistical significance.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {44},
numpages = {24},
keywords = {Conceptual Coupling, Impact Analysis, Program Comprehension}
}

@article{10.1145/3643773,
author = {Xiao, Tao and Hata, Hideaki and Treude, Christoph and Matsumoto, Kenichi},
title = {Generative AI for Pull Request Descriptions: Adoption, Impact, and Developer Interventions},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643773},
doi = {10.1145/3643773},
abstract = {GitHub's Copilot for Pull Requests (PRs) is a promising service aiming to automate various developer tasks related to PRs, such as generating summaries of changes or providing complete walkthroughs with links to the relevant code. As this innovative technology gains traction in the Open Source Software (OSS) community, it is crucial to examine its early adoption and its impact on the development process. Additionally, it offers a unique opportunity to observe how developers respond when they disagree with the generated content. In our study, we employ a mixed-methods approach, blending quantitative analysis with qualitative insights, to examine 18,256 PRs in which parts of the descriptions were crafted by generative AI. Our findings indicate that: (1) Copilot for PRs, though in its infancy, is seeing a marked uptick in adoption. (2) PRs enhanced by Copilot for PRs require less review time and have a higher likelihood of being merged. (3) Developers using Copilot for PRs often complement the automated descriptions with their manual input. These results offer valuable insights into the growing integration of generative AI in software development.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {47},
numpages = {23},
keywords = {Copilot, Generative AI, GitHub, Pull Requests}
}

@article{10.1145/3643776,
author = {Zhang, Zejun and Xing, Zhenchang and Ren, Xiaoxue and Lu, Qinghua and Xu, Xiwei},
title = {Refactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643776},
doi = {10.1145/3643776},
abstract = {Pythonic idioms are highly valued and widely used in the Python programming community. However, many Python users find it challenging to use Pythonic idioms. Adopting rule-based approach or LLM-only approach is not sufficient to overcome three persistent challenges of code idiomatization including code miss, wrong detection and wrong refactoring. Motivated by the determinism of rules and adaptability of LLMs, we propose a hybrid approach consisting of three modules. We not only write prompts to instruct LLMs to complete tasks, but we also invoke Analytic Rule Interfaces (ARIs) to accomplish tasks. The ARIs are Python code generated by prompting LLMs to generate code. We first construct a knowledge module with three elements including ASTscenario, ASTcomponent and Condition, and prompt LLMs to generate Python code for incorporation into an ARI library for subsequent use. After that, for any syntax-error-free Python code, we invoke ARIs from the ARI library to extract ASTcomponent from the ASTscenario, and then filter out ASTcomponent that does not meet the condition. Finally, we design prompts to instruct LLMs to abstract and idiomatize code, and then invoke ARIs from the ARI library to rewrite non-idiomatic code into the idiomatic code. Next, we conduct a comprehensive evaluation of our approach, RIdiom, and Prompt-LLM on nine established Pythonic idioms in RIdiom. Our approach exhibits superior accuracy, F1 score, and recall, while maintaining precision levels comparable to RIdiom, all of which consistently exceed or come close to 90\% for each metric of each idiom. Lastly, we extend our evaluation to encompass four new Pythonic idioms. Our approach consistently outperforms Prompt-LLM, achieving metrics with values consistently exceeding 90\% for accuracy, F1-score, precision, and recall.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {50},
numpages = {22},
keywords = {Code Change, Large Language Model, Pythonic Idioms}
}

@proceedings{10.1145/3643787,
title = {NLBSE '24: Proceedings of the Third ACM/IEEE International Workshop on NL-based Software Engineering},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Natural Language Processing (NLP) refers to the automated elaboration of human language, including both algorithms that take human-produced text as input and algorithms that produce natural-looking text as outputs. NLP is widely used to optimize many aspects of the software development process. Since natural language artifacts are used and reused during the software development life-cycle, the availability of natural language-based approaches and tools has led to improvements in the software process and product efficiency. Indeed, NLP approaches (including LLMs) have proven useful for retrieving key information from a wide range of structured or unstructured sources. Besides, they show promise for the automated generation of fine-grained source code documentation to ease program comprehension and maintenance activities. Literature has shown that many software engineering (SE)-related tasks can benefit from adopting NLP techniques. The main objective of the Natural Language-Based Software Engineering Workshop (NLBSE) is to bring together researchers and industrial practitioners from the NLP and SE communities to share experiences. Our workshop aims to provide directions for future research and encourage the development of increasingly effective NLP solutions for addressing SE-specific challenges.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3643788,
title = {APR '24: Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the fifth International Workshop on Automated Program Repair (APR 2024), hosted by International Conference on Software Engineering (ICSE) 2024. Since its inception in 2020, APR has become a central event of the program repair community, reflecting a growing interest in the field among the software engineering, programming language, machine learning and formal methods communities.APR 2024 continues the tradition of fostering interaction among researchers in program repair. As always, we are particularly focused on narrowing the divide between academic research and real-world industry applications.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3643795,
title = {LLM4Code '24: Proceedings of the 1st International Workshop on Large Language Models for Code},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the first edition of the InternationalWorkshop on Large Language Models for Code (LLM4Code). Large Language Models (LLMs), which are large-scale models being trained on massive textual corpora, have achieved significant advances in various domains, including Software Engineering (SE). Recently, there has been a growing interest in applying LLMs to assist software development and maintenance, such as code generation and comprehension, test generation, and program repair. Although the application of LLMs on code-relevant tasks has shown very promising performance, there is a huge potential to explore this growing domain further. The motivation of the LLM4Code workshop is to provide a platform for academics and practitioners to discuss and share their ideas on applying and developing LLMs to solve code-relevant problems in SE activities.The LLM4Code workshop is concerned with the research on how to better apply LLMs to solve code-relevant tasks, how to design better LLMs for code-relevant tasks, and how to better benchmark LLMs on code-relevant tasks. The workshop aims to achieve multiple goals as follows. Firstly, the workshop aims to provide an opportunity for participants to discuss novel ideas and preliminary results on LLMs for solving code-relevant SE problems, to exchange the latest progress in this domain. Secondly, the workshop aims to encourage participants to discuss the open challenges and problems of LLM4code, to identify important future directions in this domain. Finally, the workshop aims to encourage participants to share infrastructures and benchmarks that are foundational and beneficial for future research in this domain.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3643795.3648377,
author = {Ramler, Rudolf and Moser, Michael and Fischer, Lukas and Nissl, Markus and Heinzl, Rene},
title = {Industrial Experience Report on AI-Assisted Coding in Professional Software Development},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648377},
doi = {10.1145/3643795.3648377},
abstract = {AI-based tools for software development are widely discussed in academic literature. They promise to boost software development performance, especially in code creation. This paper collects insights from practitioners about the use and implications of AI assistance in industrial software development, with a focus on SMEs. Through interviews with five developers from three software development organization, we gathered and analyzed the experiences made in industrial practice, and we identified lessons learned and open challenges. ChatGPT and Copilot are used in industry projects. While they are considered useful for many code-related development activities, their integration in the development workflow remains mostly shallow. Contradicting observations about speed-ups due to AI support in development are reported. Legal issues are of minor concern although awareness exists.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {1–7},
numpages = {7},
keywords = {AI-assisted development, code generation, ChatGPT, copilot},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3643795.3648381,
author = {Singha, Ananya and Chopra, Bhavya and Khatry, Anirudh and Gulwani, Sumit and Henley, Austin and Le, Vu and Parnin, Chris and Singh, Mukul and Verbruggen, Gust},
title = {Semantically Aligned Question and Code Generation for Automated Insight Generation},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648381},
doi = {10.1145/3643795.3648381},
abstract = {Automated insight generation is a common tactic for helping knowledge workers, such as data scientists, to quickly understand the potential value of new and unfamiliar data. Unfortunately, automated insights produced by large-language models can generate code that does not correctly correspond (or align) to the insight. In this paper, we leverage the semantic knowledge of large language models to generate targeted and insightful questions about data and the corresponding code to answer those questions. Then through an empirical study on data from Open-WikiTable, we show that embeddings can be effectively used for filtering out semantically unaligned pairs of question and code. Additionally, we found that generating questions and code together yields more diverse questions.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {127–134},
numpages = {8},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3643795.3648382,
author = {Piya, Sanyogita and Sullivan, Allison},
title = {LLM4TDD: Best Practices for Test Driven Development Using Large Language Models},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648382},
doi = {10.1145/3643795.3648382},
abstract = {In today's society, we are becoming increasingly dependent on software systems. However, we also constantly witness the negative impacts of buggy software. Program synthesis aims to improve software correctness by automatically generating the program given an outline of the expected behavior. For decades, program synthesis has been an active research field, with recent approaches looking to incorporate Large Language Model. This paper explores the concept of LLM4TDD, where we guide Large Language Models to generate code iteratively using a test-driven development methodology. We conduct an empirical evaluation using ChatGPT and coding problems from LeetCode to investigate the impact of different test, prompt and problem attributes on the efficacy of LLM4TDD.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {14–21},
numpages = {8},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3643834.3660679,
author = {Gamboa, Mafalda and Hendriks, Sjoerd},
title = {In Praise of Shadows: Sensibility and Somaesthetic Appreciation for Shadows in Interaction Design},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3660679},
doi = {10.1145/3643834.3660679},
abstract = {Wherever there is light, there is shadow — an inevitable immaterial material, with an undeniable presence in interaction gestalt. Inspired by Jun’ichir\={o} Tanizaki’s book In Praise of Shadows, we report on our journey towards building a sensibility and somaesthetic appreciation for shadows physically, metaphorically, and poetically. To investigate the idea of cultivating attentiveness to shadows, we embarked on a “d\'{e}rive”-inspired adventure, gathering first-person perceptions and photographs of tables. We analysed these examples of tables and their shadows in order to inform the development of a project on interactive tables. Along with the artifact collection, we present a set of concepts, an initial prototype, and reflections on our own experience in developing this sensibility. We discuss shadows as a design material and an example of a design sensibility: a skill that can be sparked, fostered, and ultimately embedded.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {3272–3286},
numpages = {15},
keywords = {design methods, materiality, sensibility, somaesthetic appreciation},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3643834.3660692,
author = {Bhat, Avinash and Shrivastava, Disha and Guo, Jin L.C.},
title = {Do LLMs Meet the Needs of Software Tutorial Writers? Opportunities and Design Implications},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3660692},
doi = {10.1145/3643834.3660692},
abstract = {Creating software tutorials involves developing accurate code examples and explanatory text that engages and informs the reader. Large Language Models (LLMs) demonstrate a strong capacity to generate both text and code, but their potential to assist tutorial writing is unknown. By interviewing and observing seven experienced writers using OpenAI playground as an exploration environment, we uncover design opportunities for leveraging LLMs in software tutorial writing. Our findings reveal background research, resource creation, and maintaining quality standards as critical areas where LLMs could significantly assist writers. We observe how tutorial writers generated tutorial content while exploring LLMs’ capabilities, formulating prompts, verifying LLM outputs, and reflecting on interaction goals and strategies. Our observation highlights that the unpredictability of LLM outputs and unintuitive interface design contributed to skepticism about LLM’s utility. Informed by these results, we contribute recommendations for designing LLM-based tutorial writing tools to mitigate usability challenges and harness LLMs’ full potential.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {1760–1773},
numpages = {14},
keywords = {Large Language Models, Software Tutorial Writing, Writing Support},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3643834.3660698,
author = {Hendriks, Sjoerd and Gamboa, Mafalda and Obaid, Mohammad},
title = {The Undertable: A Design Remake of the Mediated Body},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3660698},
doi = {10.1145/3643834.3660698},
abstract = {Tables are a ubiquitous piece of furniture, a familiar sight in most environments from intimate to public. The dimensions of social interplay surrounding every single table are profoundly complex. In our project, we lift the importance of the neglected space under the table through the playful development of a tangible prototype. We approached this by a design remake of the Mediated Body: a wearable prototype encouraging touch between strangers using the conductivity of the skin. Instead, we leverage the familiarity of tables as a means to encourage playful explorations of bare-skin touch. We report in visual and textual form on the emerging design knowledge throughout our design process, including first-person narratives by the designers. We contribute with (1) a series of counterfactual table artifacts inspired by the Mediated Body; (2) a sequence of participant studies analysed through reflexive thematic analysis and summarised into the notion of “an odd invitation” as a new lens for homo explorens; and (3) an appeal to the importance of design remakes for research-through-design.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {2591–2610},
numpages = {20},
keywords = {Defamiliarisation, Design Case, Design Remake, Emergence, Homo Explorens, Ludic Design, Research-through-Design},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3643834.3660706,
author = {Vega-Cebri\'{a}n, Jos\'{e} Manuel and Turmo Vidal, Laia and Tajadura-Jim\'{e}nez, Ana and Bonino Covas, Tom\'{a}s and M\'{a}rquez Segura, Elena},
title = {Movits: a Minimalist Toolkit for Embodied Sketching},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3660706},
doi = {10.1145/3643834.3660706},
abstract = {We present the design and evaluation of the Movits, a minimalist toolkit for embodied sketching design explorations. The toolkit includes technology probes featuring minimalist wearable digital units that support the hands-on exploration and design of movement-driven interactions using multisensory feedback. The Movits are self-contained and generate audiovisual or vibrotactile patterns in response to movement-based inputs. We present the theoretical and empirical grounding driving our design process. We discuss the findings of using the Movits during four co-design workshops with design students, technologists, dancers and physiotherapists, where they resulted in being generative and adaptable to a range of embodied design approaches. We contend that the Movits can be favourable for those interested in a holistic design approach to wearables in general and specifically for those targeting movement-based application domains.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {3302–3317},
numpages = {16},
keywords = {Biofeedback, Bodystorming, Bodystorming Basket, Embodied Sketching, Ideation, Ideation Probes, Ideation Props, Motor Learning, Multisensory Feedback, Technology Probes, Toolkit, Wearables},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3643834.3660731,
author = {Chan, Joannes and De Paoli, Chris and Li, Michelle and Grossman, Tovi and Santosa, Stephanie and Wigdor, Daniel and Glueck, Michael},
title = {Fidgets: Building Blocks for a Predictive UI Toolkit},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3660731},
doi = {10.1145/3643834.3660731},
abstract = {The rapid growth of AR platforms, combined with the rising predictive power of intelligent systems, will fundamentally change interactive computing. Interaction will increasingly happen on the go, causing I/O to become constrained, ultimately leading to reliance on user intent prediction for aid. In this pictorial, we argue that to support the development of such systems, new predictive UI toolkits are required. We place the reader in the shoes of an App designer and outline the challenges that will be faced. We then describe a new predictive toolkit, leveraging Fuzzy Widgets, or “Fidgets” as the main UI building block. Fidgets extend Responsive Design into the realm of intelligent systems, to adapt not only to spatial constraints, but to system predictions as well. We then describe a working implementation of a predictive music application, built using our described framework, showcasing its benefits and range of adaptive abilities.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {1290–1305},
numpages = {16},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3643834.3661507,
author = {Park, Gun Woo (Warren) and Panda, Payod and Tankelevitch, Lev and Rintel, Sean},
title = {The CoExplorer Technology Probe: A Generative AI-Powered Adaptive Interface to Support Intentionality in Planning and Running Video Meetings},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661507},
doi = {10.1145/3643834.3661507},
abstract = {Effective meetings are effortful, but traditional videoconferencing systems offer little support for reducing this effort across the meeting lifecycle. Generative AI (GenAI) has the potential to radically redefine meetings by augmenting intentional meeting behaviors. CoExplorer, our novel adaptive meeting prototype, preemptively generates likely phases that meetings would undergo, tools that allow capturing attendees’ thoughts before the meeting, and for each phase, window layouts, and appropriate applications and files. Using CoExplorer as a technology probe in a guided walkthrough, we studied its potential in a sample of participants from a global technology company. Our findings suggest that GenAI has the potential to help meetings stay on track and reduce workload, although concerns were raised about users’ agency, trust, and possible disruption to traditional meeting norms. We discuss these concerns and their design implications for the development of GenAI meeting technology.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {1638–1657},
numpages = {20},
keywords = {adaptive user interface, design, effectiveness, effort, intent recognition, speech recognition, technology probe, video meetings, windowing system},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3643834.3661514,
author = {La Delfa, Joseph and Garrett, Rachael and Lampinen, Airi and H\"{o}\"{o}k, Kristina},
title = {Articulating Mechanical Sympathy for Somaesthetic Human-Machine Relations},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661514},
doi = {10.1145/3643834.3661514},
abstract = {We present mechanical sympathy as a generative design concept for cultivating somaesthetic relationships with machines and machine-like systems. We identify the qualities of mechanical sympathy using the design case of How to Train your Drone (HTTYD), a unique human-drone research product designed to explore the process by which people discover and co-create the somaesthetic potential of drones. We articulate the qualities – (i) machine-agency, (ii) oscillations, and (iii) aesthetic pursuits – by using descriptive and reflective accounts of our design strategies and of our co-creators engaging with the system. We also discuss how each quality can extend soma design research; conceptualizing of appreciative, temporal, and idiosyncratic relationships with machines that can complement technical learning and enrich human-machine interaction. Finally, we ground our concept in a similar selection of works from across the HCI community.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {3336–3353},
numpages = {18},
keywords = {drones, machines, soma design, somaesthetics},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3643834.3661522,
author = {Toka, Mert and Frost, Devon and Bourgault, Samuelle and Farber, Avi and Friedman-Gerlicz, Camila and Lee, Raina and Paek, Eun-Ha and Wiley, Pilar and Jacobs, Jennifer},
title = {Practice-driven Software Development: A Collaborative Method for Digital Fabrication Systems Research in a Residency Program},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661522},
doi = {10.1145/3643834.3661522},
abstract = {Building new software tools for professional digital fabrication requires that HCI researchers understand domain-specific materials and fabrication workflows to ensure software operations align with professional manufacturing requirements. To bridge the research-practice divide, we adopt a practice-driven software development methodology for digital fabrication in an artist-in-residence program. In our method, HCI researchers and craft professionals collaboratively develop software tools over three months. We piloted our methodology through two consecutive computational ceramics residencies with five professional craftspeople. The teams produced five novel software tools for clay 3D printing and hundreds of ceramic artifacts. We provide a detailed description of our methodology through artist and HCI researcher accounts and an analysis of the integration of software ideation, implementation, and debugging with professional art and craft production. Our work demonstrates a systematic mechanism for achieving meaningful digital fabrication software contributions with mutual benefit for artists and researchers.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {1192–1217},
numpages = {26},
keywords = {Artist residencies, Clay 3D printing, Computational fabrication, Software development},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3643834.3661525,
author = {Vaithilingam, Priyan and Arawjo, Ian and Glassman, Elena L.},
title = {Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661525},
doi = {10.1145/3643834.3661525},
abstract = {We ideate a future design workflow that involves AI technology. Drawing from activity and communication theory, we attempt to isolate the new value that large AI models can provide design compared to past technologies. We arrive at three affordances—dynamic grounding, constructive negotiation, and sustainable motivation—that summarize latent qualities of natural language-enabled foundation models that, if explicitly designed for, can support the process of design. Through design fiction, we then imagine a future interface as a diegetic prototype, the story of Squirrel Game, that demonstrates each of our three affordances in a realistic usage scenario. Our design process, terminology, and diagrams aim to contribute to future discussions about the relative affordances of AI technology with regard to collaborating with human designers.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {289–300},
numpages = {12},
keywords = {AI affordances, Design fiction, Grounding, Human AI collaboration, Language models},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3643834.3661579,
author = {Hong, Jihyeong and Lee, Yokyung and Kim, Dae Hyun and Choi, DaEun and Yoon, Yeo-Jin and Lee, Gyu-cheol and Lee, Zucheul and Kim, Juho},
title = {A Context-Aware Onboarding Agent for Metaverse Powered by Large Language Models},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661579},
doi = {10.1145/3643834.3661579},
abstract = {One common asset of metaverse is that users can freely explore places and actions without linear procedures. Thus, it is hard yet important to understand the divergent challenges each user faces when onboarding metaverse. Our formative study (N = 16) shows that first-time users ask questions about metaverse that concern 1) a short-term spatiotemporal context, regarding the user’s current location, recent conversation, and actions, and 2) a long-term exploration context regarding the user’s experience history. Based on the findings, we present PICAN, a Large Language Model-based pipeline that generates context-aware answers to users when onboarding metaverse. An ablation study (N = 20) reveals that PICAN’s usage of context made responses more useful and immersive than those generated without contexts. Furthermore, a user study (N = 21) shows that the use of long-term exploration context promotes users’ learning about the locations and activities within the virtual environment.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {1857–1874},
numpages = {18},
keywords = {context-awareness, conversational agent, large-language models, metaverse},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3643834.3661636,
author = {Sondoqah, Mousa and Ben Abdesslem, Fehmi and Popova, Kristina and Mcgregor, Moira and La Delfa, Joseph and Garrett, Rachael and Lampinen, Airi and Mottola, Luca and H\"{o}\"{o}k, Kristina},
title = {Shaping and Being Shaped by Drones: Programming in Perception-Action Loops},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661636},
doi = {10.1145/3643834.3661636},
abstract = {In a long-term commitment to designing for the aesthetics of human–drone interactions, we have been troubled by the lack of tools for shaping and interactively feeling drone behaviours. By observing participants in a three-day drone challenge, we isolated components of drones that, if made transparent, could have helped participants better explore their aesthetic potential. Through a bricolage approach to analysing interviews, field notes, video recordings, and inspection of each team’s code, we describe how teams 1) shifted their efforts from aiming for seamless human–drone interaction, to seeing drones as fragile, wilful, and prone to crashes; 2) engaged with intimate, bodily interactions to more precisely probe, understand and define their drone’s capabilities; 3) adopted different workaround strategies, emphasising either training the drone or the pilot. We contribute an empirical account of constraints in shaping the potential aesthetics of drone behaviour, and discuss how programming environments could better support somaesthetic perception–action loops for design and programming purposes.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {2926–2945},
numpages = {20},
keywords = {drones, programming tools, soma design},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@proceedings{10.1145/3643916,
title = {ICPC '24: Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICPC is the premier (CORE A) venue for research on program comprehension. Research on program comprehension encompasses both human activities for comprehending the software and technologies for supporting such comprehension.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3643916.3644402,
author = {Corso, Vincenzo and Mariani, Leonardo and Micucci, Daniela and Riganelli, Oliviero},
title = {Generating Java Methods: An Empirical Assessment of Four AI-Based Code Assistants},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644402},
doi = {10.1145/3643916.3644402},
abstract = {AI-based code assistants are promising tools that can facilitate and speed up code development. They exploit machine learning algorithms and natural language processing to interact with developers, suggesting code snippets (e.g., method implementations) that can be incorporated into projects. Recent studies empirically investigated the effectiveness of code assistants using simple exemplary problems (e.g., the re-implementation of well-known algorithms), which fail to capture the spectrum and nature of the tasks actually faced by developers.In this paper, we expand the knowledge in the area by comparatively assessing four popular AI-based code assistants, namely GitHub Copilot, Tabnine, ChatGPT, and Google Bard, with a dataset of 100 methods that we constructed from real-life open-source Java projects, considering a variety of cases for complexity and dependency from contextual elements. Results show that Copilot is often more accurate than other techniques, yet none of the assistants is completely subsumed by the rest of the approaches. Interestingly, the effectiveness of these solutions dramatically decreases when dealing with dependencies outside the boundaries of single classes.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {13–23},
numpages = {11},
keywords = {AI-based code assistants, code completion, copilot, ChatGPT, tabnine, bard, empirical study},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3643916.3644403,
author = {Ma, Zexiong and An, Shengnan and Xie, Bing and Lin, Zeqi},
title = {Compositional API Recommendation for Library-Oriented Code Generation},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644403},
doi = {10.1145/3643916.3644403},
abstract = {Large language models (LLMs) have achieved exceptional performance in code generation. However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs. Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs. However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API recommendation a challenging task.To address this, we propose CAPIR (Compositional API Recommendation), which adopts a "divide-and-conquer" strategy to recommend APIs for coarse-grained requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based Retriever to identify relevant APIs corresponding to each subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out redundant APIs and provides the final recommendation.To facilitate the evaluation of API recommendation methods on coarse-grained requirements, we present two challenging benchmarks, RAPID (Recommend APIs based on Documentation) and LOCG (Library-Oriented Code Generation). Experimental results on these benchmarks, demonstrate the effectiveness of CAPIR in comparison to existing baselines. Specifically, on RAPID's Torchdata-AR dataset, compared to the state-of-the-art API recommendation approach, CAPIR improves recall@5 from 18.7\% to 43.2\% and precision@5 from 15.5\% to 37.1\%. On LOCG's Torchdata-Code dataset, compared to code generation without API recommendation, CAPIR improves pass@100 from 16.0\% to 28.0\%.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {87–98},
numpages = {12},
keywords = {API recommendation, code generation, requirements decomposition, large language model},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3643916.3644414,
author = {Chen, Xiangping and Li, Yangzi and Tang, Zhicao and Huang, Yuan and Zhou, Haojie and Tang, Mingdong and Zheng, Zibin},
title = {ESGen: Commit Message Generation Based on Edit Sequence of Code Change},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644414},
doi = {10.1145/3643916.3644414},
abstract = {Commit messages provide important information for comprehending the code changes, and a number of researchers try to generate commit messages by using an automatic way. These research on commit message generation has profited from the code tokens or code structures such as AST. Since the edit sequence of code change is also important for capturing the code change intent, we propose a new commit message generation method called ESGen, which extracts AST edit sequences of code changes as model input. Specifically, we employ an O(ND) difference algorithm to extract the edit sequence from AST by comparing the ASTs before and after applying the code changes. Then, we construct a Bi-Encoder, which encodes the textual information and the AST edit sequence information of code change. The experimental results show that ESGen outperforms other baseline models, improving the BLEU-4 to 15.14. Also, when applying the edit sequence to 7 baseline models, they improve the BLEU-4 scores of these models by an average of 8.5\%. Additionally, a human evaluation confirmed the effectiveness of ESGen in generating commit messages.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {112–124},
numpages = {13},
keywords = {commit message generation, code change, edit sequence, biencoder, abstract syntax tree},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3643916.3644424,
author = {Siddiq, Mohammed Latif and Zhang, Jiahao and Santos, Joanna Cecilia Da Silva},
title = {Understanding Regular Expression Denial of Service (ReDoS): Insights from LLM-Generated Regexes and Developer Forums},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644424},
doi = {10.1145/3643916.3644424},
abstract = {Regular expression Denial of Service (ReDoS) represents an algorithmic complexity attack that exploits the processing of regular expressions (regexes) to produce a denial-of-service attack. This attack occurs when a regex's evaluation time scales polynomially or exponentially with input length, posing significant challenges for software developers. The advent of Large Language Models (LLMs) has revolutionized the generation of regexes from natural language prompts, but not without its risks. Prior works showed that LLMs can generate code with vulnerabilities and security smells. In this paper, we examined the correctness and security of regexes generated by LLMs as well as the characteristics of LLM-generated vulnerable regexes. Our study also examined ReDoS patterns in actual software projects, aligning them with corresponding regex equivalence classes and algorithmic complexity. Moreover, we analyzed developer discussions on GitHub and StackOverflow, constructing a taxonomy to investigate their experiences and perspectives on ReDoS. In this study, we found that GPT-3.5 was the best LLM to generate regexes that are both correct and secure. We also observed that LLM-generated regexes mainly have polynomial ReDoS vulnerability patterns, and it is consistent with vulnerable regexes found in open source projects. We also found that developers' main discussions around insecure regexes is related to mitigation strategies to remove vulnerable regexes.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {190–201},
numpages = {12},
keywords = {ReDoS, DoS attack, large language models, regex generation},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@proceedings{10.1145/3643991,
title = {MSR '24: Proceedings of the 21st International Conference on Mining Software Repositories},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MSR is a thriving research community that organizes a yearly conference with a solid reputation amongst software engineering researchers.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3643991.3644884,
author = {Silva, Andr\'{e} and Saavedra, Nuno and Monperrus, Martin},
title = {GitBug-Java: A Reproducible Benchmark of Recent Java Bugs},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644884},
doi = {10.1145/3643991.3644884},
abstract = {Bug-fix benchmarks are essential for evaluating methodologies in automatic program repair (APR) and fault localization (FL). However, existing benchmarks, exemplified by Defects4J, need to evolve to incorporate recent bug-fixes aligned with contemporary development practices. Moreover, reproducibility, a key scientific principle, has been lacking in bug-fix benchmarks. To address these gaps, we present GitBug-Java, a reproducible benchmark of recent Java bugs. GitBug-Java features 199 bugs extracted from the 2023 commit history of 55 notable open-source repositories. The methodology for building GitBug-Java ensures the preservation of bug-fixes in fully-reproducible environments. We publish GitBug-Java at https://github.com/gitbugactions/gitbug-java.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {118–122},
numpages = {5},
keywords = {software bugs, bug benchmark, reproducibility, bug database, Java benchmark, software testing, program analysis},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3644898,
author = {Casta\~{n}o, Joel and Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Franch, Xavier and Bogner, Justus},
title = {Analyzing the Evolution and Maintenance of ML Models on Hugging Face},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644898},
doi = {10.1145/3643991.3644898},
abstract = {Hugging Face (HF) has established itself as a crucial platform for the development and sharing of machine learning (ML) models. This repository mining study, which delves into more than 380,000 models using data gathered via the HF Hub API, aims to explore the community engagement, evolution, and maintenance around models hosted on HF - aspects that have yet to be comprehensively explored in the literature. We first examine the overall growth and popularity of HF, uncovering trends in ML domains, framework usage, authors grouping and the evolution of tags and datasets used. Through text analysis of model card descriptions, we also seek to identify prevalent themes and insights within the developer community. Our investigation further extends to the maintenance aspects of models, where we evaluate the maintenance status of ML models, classify commit messages into various categories (corrective, perfective, and adaptive), analyze the evolution across development stages of commits metrics and introduce a new classification system that estimates the maintenance status of models based on multiple attributes. This study aims to provide valuable insights about ML model maintenance and evolution that could inform future model development strategies on platforms like HF.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {607–618},
numpages = {12},
keywords = {repository mining, software evolution, maintenance},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3644903,
author = {Colavito, Giuseppe and Lanubile, Filippo and Novielli, Nicole and Quaranta, Luigi},
title = {Leveraging GPT-like LLMs to Automate Issue Labeling},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644903},
doi = {10.1145/3643991.3644903},
abstract = {Issue labeling is a crucial task for the effective management of software projects. To date, several approaches have been put forth for the automatic assignment of labels to issue reports. In particular, supervised approaches based on the fine-tuning of BERT-like language models have been proposed, achieving state-of-the-art performance. More recently, decoder-only models such as GPT have become prominent in SE research due to their surprising capabilities to achieve state-of-the-art performance even for tasks they have not been trained for. To the best of our knowledge, GPT-like models have not been applied yet to the problem of issue classification, despite the promising results achieved for many other software engineering tasks. In this paper, we investigate to what extent we can leverage GPT-like LLMs to automate the issue labeling task. Our results demonstrate the ability of GPT-like models to correctly classify issue reports in the absence of labeled data that would be required to fine-tune BERT-like LLMs.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {469–480},
numpages = {12},
keywords = {LLM, issue labeling, GPT, software maintenance and evolution, labeling unstructured data},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3644906,
author = {Nikeghbal, Nafiseh and Kargaran, Amir Hossein and Heydarnoori, Abbas},
title = {GIRT-Model: Automated Generation of Issue Report Templates},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644906},
doi = {10.1145/3643991.3644906},
abstract = {Platforms such as GitHub and GitLab introduce Issue Report Templates (IRTs) to enable more effective issue management and better alignment with developer expectations. However, these templates are not widely adopted in most repositories, and there is currently no tool available to aid developers in generating them. In this work, we introduce GIRT-Model, an assistant language model that automatically generates IRTs based on the developer's instructions regarding the structure and necessary fields. We create GIRT-Instruct, a dataset comprising pairs of instructions and IRTs, with the IRTs sourced from GitHub repositories. We use GIRT-Instruct to instruction-tune a T5-base model to create the GIRT-Model.In our experiments, GIRT-Model outperforms general language models (T5 and Flan-T5 with different parameter sizes) in IRT generation by achieving significantly higher scores in ROUGE, BLEU, METEOR, and human evaluation. Additionally, we analyze the effectiveness of GIRT-Model in a user study in which participants wrote short IRTs with GIRT-Model. Our results show that the participants find GIRT-Model useful in the automated generation of templates. We hope that through the use of GIRT-Model, we can encourage more developers to adopt IRTs in their repositories. We publicly release our code, dataset, and model at https://github.com/ISE-Research/girt-model.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {407–418},
numpages = {12},
keywords = {issue template generation, issue report template, issue template, bug template, GitHub, issue tracker, bug report},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3644916,
author = {Alsayed, Ahmed Saeed and Dam, Hoa Khanh and Nguyen, Chau},
title = {MicroRec: Leveraging Large Language Models for Microservice Recommendation},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644916},
doi = {10.1145/3643991.3644916},
abstract = {The increasing adoption of microservices in software development requires effective recommendation systems that guide developers to relevant microservices. In this paper, we introduce MicroRec, a novel microservice recommender framework which leverages insights from Stack Overflow posts and the power of Large Language Models (LLMs). MicroRec utilizes a dual-encoder architecture that combines contrastive learning and semantic similarity learning, allowing us to achieve robust and accurate retrieval and ranking of relevant posts based on user queries. Using LLMs, MicroRec builds up a deep understanding of both user queries and microservices through the information they provide (e.g., README files and Dockerfiles). Our empirical evaluations demonstrate significant improvements brought by MicroRec over the existing methods across a variety of performance metrics including MRR, MAP, and precision@k. In addition, the results returned by MicroRec were fourteen times more accurate than those provided by the existing recommendation tool on the widely-used Docker Hub platform.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {419–430},
numpages = {12},
keywords = {microservices, recommendation system, semantic search, large language models, docker hub},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3644922,
author = {Preda, Anamaria-Roberta and Mayr-Dorn, Christoph and Mashkoor, Atif and Egyed, Alexander},
title = {Supporting High-Level to Low-Level Requirements Coverage Reviewing with Large Language Models},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644922},
doi = {10.1145/3643991.3644922},
abstract = {Refining high-level requirements into low-level ones is a common task, especially in safety-critical systems engineering. The objective is to describe every important aspect of the high-level requirement in a low-level requirement, ensuring a complete and correct implementation of the system's features. To this end, standards and regulations for safety-critical systems require reviewing the coverage of high-level requirements by all its low-level requirements to ensure no missing aspects.The challenge of supporting automatic reviews for requirements coverage originates from the distinct levels of abstraction between high-level and low-level requirements, their reliance on natural language, and the often different vocabulary used. The rise of Large Language Models (LLMs), trained on extensive text corpora and capable of contextualizing both high-level and low-level requirements, opens new avenues for addressing this challenge.This paper presents an initial study to explore the performance of LLMs in assessing requirements coverage. We employed GPT-3.5 and GPT-4 to analyze requirements from five publicly accessible data sets, determining their ability to detect if low-level requirements sufficiently address the corresponding high-level requirement. Our findings reveal that GPT-3.5, utilizing a zero-shot prompting strategy augmented with the prompt of explaining, correctly identifies complete coverage in four out of five evaluation data sets. Additionally, it exhibits an impressive 99.7\% recall rate in accurately identifying instances where coverage is incomplete due to removing a single low-level requirement across our entire set of evaluation data.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {242–253},
numpages = {12},
keywords = {coverage, traceability, requirements, design definitions, high-level requirements, low-level requirements, requirements satisfaction assessment, large language models, GPT},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3644923,
author = {Chen, Binger and Golebiowski, Jacek and Abedjan, Ziawasch},
title = {Data Augmentation for Supervised Code Translation Learning},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644923},
doi = {10.1145/3643991.3644923},
abstract = {Data-driven program translation has been recently the focus of several lines of research. A common and robust strategy is supervised learning. However, there is typically a lack of parallel training data, i.e., pairs of code snippets in the source and target language. While many data augmentation techniques exist in the domain of natural language processing, they cannot be easily adapted to tackle code translation due to the unique restrictions of programming languages. In this paper, we develop a novel rule-based augmentation approach tailored for code translation data, and a novel retrieval-based approach that combines code samples from unorganized big code repositories to obtain new training data. Both approaches are language-independent. We perform an extensive empirical evaluation on existing Java-C#-benchmarks showing that our method improves the accuracy of state-of-the-art supervised translation techniques by up to 35\%.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {444–456},
numpages = {13},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3644926,
author = {Idialu, Oseremen Joy and Mathews, Noble Saji and Maipradit, Rungroj and Atlee, Joanne M. and Nagappan, Mei},
title = {Whodunit: Classifying Code as Human Authored or GPT-4 Generated - A case study on CodeChef problems},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644926},
doi = {10.1145/3643991.3644926},
abstract = {Artificial intelligence (AI) assistants such as GitHub Copilot and ChatGPT, built on large language models like GPT-4, are revolutionizing how programming tasks are performed, raising questions about whether code is authored by generative AI models. Such questions are of particular interest to educators, who worry that these tools enable a new form of academic dishonesty, in which students submit AI-generated code as their work. Our research explores the viability of using code stylometry and machine learning to distinguish between GPT-4 generated and human-authored code. Our dataset comprises human-authored solutions from CodeChef and AI-authored solutions generated by GPT-4. Our classifier outperforms baselines, with an F1-score and AUC-ROC score of 0.91. A variant of our classifier that excludes gameable features (e.g., empty lines, whitespace) still performs well with an F1-score and AUC-ROC score of 0.89. We also evaluated our classifier on the difficulty of the programming problem and found that there was almost no difference between easier and intermediate problems, and the classifier performed only slightly worse on harder problems. Our study shows that code stylometry is a promising approach for distinguishing between GPT-4 generated code and human-authored code.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {394–406},
numpages = {13},
keywords = {code stylometry, ChatGPT, AI code, GPT-4 generated code, authorship profiling, software engineering},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3645072,
author = {Grewal, Balreet and Lu, Wentao and Nadi, Sarah and Bezemer, Cor-Paul},
title = {Analyzing Developer Use of ChatGPT Generated Code in Open Source GitHub Projects},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645072},
doi = {10.1145/3643991.3645072},
abstract = {The rapid development of large language models such as ChatGPT have made them particularly useful to developers in generating code snippets for their projects. To understand how ChatGPT's generated code is leveraged by developers, we conducted an empirical study of 3,044 ChatGPT-generated code snippets integrated within GitHub projects. A median of 54\% of the generated lines of code is found in the project's code and this code typically remains unchanged once added. The modifications of the 76 code snippets that changed in a subsequent commit, consisted of minor functionality changes and code reorganizations that were made within a day. Our findings offer insights that help drive the development of AI-assisted programming tools. We highlight the importance of making changes in ChatGPT code before integrating it into a project.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {157–161},
numpages = {5},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3645078,
author = {Mohamed, Suad and Parvin, Abdullah and Parra, Esteban},
title = {Chatting with AI: Deciphering Developer Conversations with ChatGPT},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645078},
doi = {10.1145/3643991.3645078},
abstract = {Large Language Models (LLMs) have been widely adopted and are becoming ubiquitous and integral to software development. However, we have little knowledge as to how these tools are being used by software developers beyond anecdotal evidence and word-of-mouth reports. In this work, we present a study toward understanding how developers engage with and utilize LLMs by reporting the results of an empirical study identifying patterns in the conversation that developers have with LLMs. We identified a total of 19 topics describing the purpose of the developers in their conversations with LLMs. Our findings reveal that developers use LLMs to facilitate various aspects of their software development processes (e.g., information-seeking about programming languages and frameworks and soliciting high-level design recommendations) to a similar extent to which they use them for non-development purposes such as writing assistance, general purpose queries, and conducting Turing tests to assess the intrinsic capabilities of the models. This work not only sheds light on the diverse applications of LLMs in software development but also underscores their emerging role as critical tools in enhancing developer productivity and creativity as we move closer to widespread AI-assisted software development.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {187–191},
numpages = {5},
keywords = {large language models, LLM, ChatGPT, software development, empirical study, developer conversations},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@proceedings{10.1145/3644815,
title = {CAIN '24: Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The goal of the CAIN Conference Series is to bring together researchers and practitioners in software engineering, data science, and artificial intelligence (AI) as part of a growing community that is targeting the challenges of Software Engineering for AI-enabled systems.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3644815.3644945,
author = {Barnett, Scott and Kurniawan, Stefanus and Thudumu, Srikanth and Brannelly, Zach and Abdelrazek, Mohamed},
title = {Seven Failure Points When Engineering a Retrieval Augmented Generation System},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644815.3644945},
doi = {10.1145/3644815.3644945},
abstract = {Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.},
booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
pages = {194–199},
numpages = {6},
keywords = {retrieval augmented generation, RAG, SE4AI, case study},
location = {Lisbon, Portugal},
series = {CAIN '24}
}

@inproceedings{10.1145/3644815.3644946,
author = {Li, Ziyu and Shin, Donghwan},
title = {Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644815.3644946},
doi = {10.1145/3644815.3644946},
abstract = {Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.In this paper, we propose a novel method, called Mutation-based Consistency Testing (MCT), to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. MCT uses different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. MCT then uses these pairs to test the ability of LLMs to detect the inconsistencies correctly.We conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of 164 programming problems written in six programming languages (Python, C++, Java, Go, JavaScript, and Rust). The results show that the LLMs have significant variations in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language. We further explain conditions under which the LLMs result in correct answers using input characteristics (e.g., number of tokens) and investigate to what extent the test results can be improved using one-shot prompts (i.e., providing an additional example). Our MCT method and the case study results provide valuable implications for future research and development of LLM-based software engineering.},
booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
pages = {150–159},
numpages = {10},
keywords = {large language models, software engineering, mutation analysis},
location = {Lisbon, Portugal},
series = {CAIN '24}
}

@inproceedings{10.1145/3644815.3644949,
author = {Pinto, Gustavo and De Souza, Cleidson and Rocha, Thayssa and Steinmacher, Igor and Souza, Alberto and Monteiro, Edward},
title = {Developer Experiences with a Contextualized AI Coding Assistant: Usability, Expectations, and Outcomes},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644815.3644949},
doi = {10.1145/3644815.3644949},
abstract = {In the rapidly advancing field of artificial intelligence, software development has emerged as a key area of innovation. Despite the plethora of general-purpose AI assistants available, their effectiveness diminishes in complex, domain-specific scenarios. Noting this limitation, both the academic community and industry players are relying on contextualized coding AI assistants. These assistants surpass general-purpose AI tools by integrating proprietary, domain-specific knowledge, offering precise and relevant solutions. Our study focuses on the initial experiences of 62 participants who used a contextualized coding AI assistant --- named StackSpot AI--- in a controlled setting. According to the participants, the assistants' use resulted in significant time savings, easier access to documentation, and the generation of accurate codes for internal APIs. However, challenges associated with the knowledge sources necessary to make the coding assistant access more contextual information as well as variable responses and limitations in handling complex codes were observed. The study's findings, detailing both the benefits and challenges of contextualized AI assistants, underscore their potential to revolutionize software development practices, while also highlighting areas for further refinement.},
booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
pages = {81–91},
numpages = {11},
keywords = {LLM, LLM-based applications, user expectations, perception of productivity},
location = {Lisbon, Portugal},
series = {CAIN '24}
}

@proceedings{10.1145/3648188,
title = {HT '24: Proceedings of the 35th ACM Conference on Hypertext and Social Media},
year = {2024},
isbn = {9798400705953},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Poznan, Poland}
}

@proceedings{10.1145/3649153,
title = {CF '24: Proceedings of the 21st ACM International Conference on Computing Frontiers},
year = {2024},
isbn = {9798400705977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ischia, Italy}
}

@proceedings{10.1145/3649158,
title = {SACMAT 2024: Proceedings of the 29th ACM Symposium on Access Control Models and Technologies},
year = {2024},
isbn = {9798400704918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 29th ACM Symposium on Access Control Models and Technologies (SACMAT 2024). This year's symposium continues its tradition of being the premier venue for presenting research results and experience reports on cutting edge advances on access control, including models, systems, applications, and theory, while also embracing an expanded focus on the general area of computer and information security and privacy. The overarching goal of the symposium is to share novel access control and computer security solutions that fulfill the needs of emerging applications and environments, and also to identify new directions for future research and development. ACM SACMAT provides researchers and also practitioners with a unique opportunity to share their perspectives with others interested in the various aspects of access control and computer security.},
location = {San Antonio, TX, USA}
}

@proceedings{10.1145/3649217,
title = {ITiCSE 2024: Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 29th annual conference on Innovation and Technology in Computer Science Education (ITiCSE 2024), hosted by Universit\`{a} degli Studi di Milano in Milan, Italy.ITiCSE 2024 will take place from Friday July 5 to Wednesday July 10. The conference program includes a keynote address, paper sessions, a panel, tips, techniques \&amp; courseware demonstrations, posters, a doctoral consortium, and working group presentations. Working groups meet July 5-7 and will submit draft reports before the conference begins on July 8.The submissions to ITiCSE 2024 were reviewed by 446 researchers and practitioners from computing education and related fields, including 44 program committee members and 402 reviewers. Thanks to their outstanding effort and commitment, every submission received a metareview and most received at least three reviews, providing authors of all submissions with constructive feedback. Although no review process is flawless, we are confident that this effort led to a vibrant conference program, capturing multiple voices and perspectives in the field.},
location = {Milan, Italy}
}

@inproceedings{10.1145/3649217.3653539,
author = {Brooks, Alexi},
title = {Agile Ethics: A Low Stakes, Skills-based Framework for Teaching CS Ethics},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653539},
doi = {10.1145/3649217.3653539},
abstract = {Computer Science educators widely agree that ethics is a vital and underdeveloped part of the CS curriculum. Attempts to increase ethics content within undergraduate CS programs have faced challenges integrating material into the current coursework. I present an ethical framework applicable to the core Computer Science activities of programming and software development, with potential for extension into other CS subfields. The application of this framework to a specific educational intervention is reserved for future work. In this paper, I focus on the framework itself and its theoretical justification. By shifting emphasis from hard ethical quandaries and advanced CS products to mundane challenges faced by a front line software developer, this framework may allow instructors to more easily and effectively integrate ethics material into introductory CS coursework. While instructors may continue to apply prepared scenarios, the framework de-emphasizes those in favor of scaffolding student coding practices that maximize the frequency of practice with ethical skills.The Agile Ethics framework provides a structure which students and educators can use to think about (1) when during a project ethical reasoning is needed, (2) what questions need to be asked at that time, and (3) how to apply Computer Science skills and knowledge to answer each question. Frequent, low intensity instances of ethical reasoning under this framework reinforce the integration of ethics as a habitual part of the software development process.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {492–498},
numpages = {7},
keywords = {computing education, cs1, ethics},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649217.3653563,
author = {\v{S}v\'{a}bensk\'{y}, Valdemar and Pankiewicz, Maciej and Zhang, Jiayi and Cloude, Elizabeth B. and Baker, Ryan S. and Fouh, Eric},
title = {Comparison of Three Programming Error Measures for Explaining Variability in CS1 Grades},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653563},
doi = {10.1145/3649217.3653563},
abstract = {Programming courses can be challenging for first year university students, especially for those without prior coding experience. Students initially struggle with code syntax, but as more advanced topics are introduced across a semester, the difficulty in learning to program shifts to learning computational thinking (e.g., debugging strategies). This study examined the relationships between students' rate of programming errors and their grades on two exams. Using an online integrated development environment, data were collected from 280 students in a Java programming course. The course had two parts. The first focused on introductory procedural programming and culminated with exam 1, while the second part covered more complex topics and object-oriented programming and ended with exam 2. To measure students' programming abilities, 51095 code snapshots were collected from students while they completed assignments that were autograded based on unit tests. Compiler and runtime errors were extracted from the snapshots, and three measures - Error Count, Error Quotient and Repeated Error Density - were explored to identify the best measure explaining variability in exam grades. Models utilizing Error Quotient outperformed the models using the other two measures, in terms of the explained variability in grades and Bayesian Information Criterion. Compiler errors were significant predictors of exam 1 grades but not exam 2 grades; only runtime errors significantly predicted exam 2 grades. The findings indicate that leveraging Error Quotient with multiple error types (compiler and runtime) may be a better measure of students' introductory programming abilities, though still not explaining most of the observed variability.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {87–93},
numpages = {7},
keywords = {computer science education, introduction to programming, introductory programming, novice programming, programming education},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649217.3653594,
author = {Azaiz, Imen and Kiesler, Natalie and Strickroth, Sven},
title = {Feedback-Generation for Programming Exercises With GPT-4},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653594},
doi = {10.1145/3649217.3653594},
abstract = {Ever since Large Language Models (LLMs) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. LLMs such as Codex, GPT-3.5, and GPT 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. This paper explores the quality of GPT-4 Turbo's generated output for prompts containing both the programming task specification and a student's submission as input. Two assignments from an introductory programming course were selected, and GPT-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. Compared to prior work and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For example, the output is more structured and consistent. GPT-4 Turbo can also accurately identify invalid casing in student programs' output. In some cases, the feedback also includes the output of the student program. At the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed. The present work increases our understanding of LLMs' potential, limitations, and how to integrate them into e-assessment systems, pedagogical scenarios, and instructing students who are using applications based on GPT-4.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {31–37},
numpages = {7},
keywords = {GPT-4 turbo, LLMs, assessment, benchmarking, formative feedback, introductory programming, large language models, personalized feedback},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649217.3653607,
author = {Rivera, Elijah and Steinmaurer, Alexander and Fisler, Kathi and Krishnamurthi, Shriram},
title = {Iterative Student Program Planning using Transformer-Driven Feedback},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653607},
doi = {10.1145/3649217.3653607},
abstract = {Problem planning is a fundamental programming skill, and aids students in decomposing tasks into manageable subtasks. While feedback on plans is beneficial for beginners, providing this in a scalable and timely way is an enormous challenge in large courses.Recent advances in LLMs raise the prospect of helping here. We utilize LLMs to generate code based on students' plans, and evaluate the code against expert-defined test suites. Students receive feedback on their plans and can refine them.In this report, we share our experience with the design and implementation of this workflow. This tool was used by 544 students in a CS1 course at an Austrian university. We developed a codebook to evaluate their plans and manually applied it to a sample. We show that LLMs can play a valuable role here. However, we also highlight numerous cautionary aspects of using LLMs in this context, many of which will not be addressed merely by having more powerful models (and indeed may be exacerbated by it).},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {45–51},
numpages = {7},
keywords = {automated feedback, llms, program planning},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649217.3653621,
author = {Margulieux, Lauren E. and Prather, James and Reeves, Brent N. and Becker, Brett A. and Cetin Uzun, Gozde and Loksa, Dastyni and Leinonen, Juho and Denny, Paul},
title = {Self-Regulation, Self-Efficacy, and Fear of Failure Interactions with How Novices Use LLMs to Solve Programming Problems},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653621},
doi = {10.1145/3649217.3653621},
abstract = {We explored how undergraduate introductory programming students naturalistically used generative AI to solve programming problems. We focused on the relationship between their use of AI to their self-regulation strategies, self-efficacy, and fear of failure in programming. In this repeated-measures, mixed-methods research, we examined students' patterns of using generative AI with qualitative student reflections and their self-regulation, self-efficacy, and fear of failure with quantitative instruments at multiple times throughout the semester. We also explored the relationships among these variables to learner characteristics, perceived usefulness of AI, and performance. Overall, our results suggest that student factors affect their baseline use of AI. In particular, students with higher self-efficacy, lower fear of failure, or higher prior grades tended to use AI less or later in the problem-solving process and rated it as less useful than others. Interestingly, we found no relationship between students' self-regulation strategies and their use of AI. Students who used AI less or later in problem-solving also had higher grades in the course, but this is most likely due to prior characteristics as our data do not suggest that this is a causal relationship.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {276–282},
numpages = {7},
keywords = {CS1, LLMs, artificial intelligence, copilot, fear of failure, generative ai, introductory programming, large language models, metacognition, self-efficacy, self-regulated learning, self-regulation},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@proceedings{10.1145/3649405,
title = {ITiCSE 2024: Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
year = {2024},
isbn = {9798400706035},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 29th annual conference on Innovation and Technology in Computer Science Education (ITiCSE 2024), hosted by Universita degli Studi di Milano in Milan, Italy.ITiCSE 2024 will take place from Friday July 5 to Wednesday July 10. The conference program includes a keynote address, paper sessions, a panel, tips, techniques \&amp; courseware demonstrations, posters, a doctoral consortium, and working group presentations. Working groups meet July 5-7 and will submit draft reports before the conference begins on July 8.The submissions to ITiCSE 2024 were reviewed by 446 researchers and practitioners from computing education and related fields, including 44 program committee members and 402 reviewers. Thanks to their outstanding effort and commitment, every submission received a metareview and most received at least three reviews, providing authors of all submissions with constructive feedback. Although no review process is flawless, we are confident that this effort led to a vibrant conference program, capturing multiple voices and perspectives in the field.},
location = {Milan, Italy}
}

@proceedings{10.1145/3649476,
title = {GLSVLSI '24: Proceedings of the Great Lakes Symposium on VLSI 2024},
year = {2024},
isbn = {9798400706059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Clearwater, FL, USA}
}

@proceedings{10.1145/3649921,
title = {FDG '24: Proceedings of the 19th International Conference on the Foundations of Digital Games},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Worcester, MA, USA}
}

@proceedings{10.1145/3650105,
title = {FORGE '24: Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {FORGE aims to bring researchers, practitioners, and educators from the AI and Software Engineering community to solve the new challenges we meet in the era of foundation models.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3650105.3652289,
author = {van Dam, Tim and van der Heijden, Frank and de Bekker, Philippe and Nieuwschepen, Berend and Otten, Marc and Izadi, Maliheh},
title = {Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652289},
doi = {10.1145/3650105.3652289},
abstract = {Language model-based code completion models have quickly grown in use, helping thousands of developers write code in many different programming languages. However, research on code completion models typically focuses on imperative languages such as Python and JavaScript, which results in a lack of representation for functional programming languages. Consequently, these models often perform poorly on functional languages such as Haskell. To investigate whether this can be alleviated, we evaluate the performance of two language models for code, CodeGPT and UniXcoder, on the functional programming language Haskell. We fine-tune and evaluate the models on Haskell functions sourced from a publicly accessible Haskell dataset on HuggingFace. Additionally, we manually evaluate the models using our novel translated HumanEval dataset. Our automatic evaluation shows that knowledge of imperative programming languages in the pre-training of LLMs may not transfer well to functional languages, but that code completion on functional languages is feasible. Consequently, this shows the need for more high-quality Haskell datasets. A manual evaluation on HumanEval-Haskell indicates CodeGPT frequently generates empty predictions and extra comments, while UniXcoder more often produces incomplete or incorrect predictions. Finally, we release HumanEval-Haskell, along with the fine-tuned models and all code required to reproduce our experiments on GitHub [41].},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {91–102},
numpages = {12},
keywords = {language models, automatic code completion, line completion, programming languages, functional programming, haskell, CodeGPT, UniXcoder},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3650105.3652297,
author = {Wang, Guanyu and Li, Yuekang and Liu, Yi and Deng, Gelei and Li, Tianlin and Xu, Guosheng and Liu, Yang and Wang, Haoyu and Wang, Kailong},
title = {MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652297},
doi = {10.1145/3650105.3652297},
abstract = {Augmented generation techniques such as Retrieval-Augmented Generation (RAG) and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing large language model (LLM) outputs with external knowledge and cached information. However, the integration of vector databases, which serve as a backbone for these augmentations, introduces critical challenges, particularly in ensuring accurate vector matching. False vector matching in these databases can significantly compromise the integrity and reliability of LLM outputs, leading to misinformation or erroneous responses. Despite the crucial impact of these issues, there is a notable research gap in methods to effectively detect and address false vector matches in LLM-augmented generation.This paper presents MeTMaP, a metamorphic testing framework developed to identify false vector matching in LLM-augmented generation systems. We derive eight metamorphic relations (MRs) from six NLP datasets, which form our method's core, based on the idea that semantically similar texts should match and dissimilar ones should not. MeTMaP uses these MRs to create sentence triplets for testing, simulating real-world matching scenarios. Our evaluation of MeTMaP over 203 vector matching configurations, involving 29 embedding models and 7 distance metrics, uncovers significant inaccuracies. The results, showing a maximum accuracy of only 41.51\% on our tests compared to the original datasets, emphasize the widespread issue of false matches in vector matching methods and the critical need for effective detection and mitigation in LLM-augmented applications.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {12–23},
numpages = {12},
keywords = {metamorphic testing, vector matching, augmented generation},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3650105.3652298,
author = {Katzy, Jonathan and Popescu, Razvan and Van Deursen, Arie and Izadi, Maliheh},
title = {An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652298},
doi = {10.1145/3650105.3652298},
abstract = {Does the training of large language models potentially infringe upon code licenses? Furthermore, are there any datasets available that can be safely used for training these models without violating such licenses? In our study, we assess the current trends in the field and the importance of incorporating code into the training of large language models. Additionally, we examine publicly available datasets to see whether these models can be trained on them without the risk of legal issues in the future. To accomplish this, we compiled a list of 53 large language models trained on file-level code. We then extracted their datasets and analyzed how much they overlap with a dataset we created, consisting exclusively of strong copyleft code.Our analysis revealed that every dataset we examined contained license inconsistencies, despite being selected based on their associated repository licenses. We analyzed a total of 514 million code files, discovering 38 million exact duplicates present in our strong copyleft dataset. Additionally, we examined 171 million file-leading comments, identifying 16 million with strong copyleft licenses and another 11 million comments that discouraged copying without explicitly mentioning a license. Based on the findings of our study, which highlights the pervasive issue of license inconsistencies in large language models trained on code, our recommendation for both researchers and the community is to prioritize the development and adoption of best practices for dataset creation and management.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {74–85},
numpages = {12},
keywords = {large language models, foundation models, code licensing, software engineering, ML4SE, machine learning, datasets},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@proceedings{10.1145/3650212,
title = {ISSTA 2024: Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 33rd edition of the International Symposium on Software Testing and Analysis, ISSTA 2024, held on September 16--20, 2024 in Vienna, Austria. ISSTA 2024 is co-located with ECOOP and MPLR 2024. ISSTA brings together academics, industrial researchers, and practitioners from all over the world working on testing and analyzing software systems.},
location = {Vienna, Austria}
}

@inproceedings{10.1145/3650212.3652112,
author = {Ma, Yunlong and Tian, Wentong and Gao, Xiang and Sun, Hailong and Li, Li},
title = {API Misuse Detection via Probabilistic Graphical Model},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652112},
doi = {10.1145/3650212.3652112},
abstract = {API misuses can cause a range of issues in software development, including program crashes, bugs, and vulnerabilities. Different approaches have been developed to automatically detect API misuses by checking the program against usage rules extracted from extensive codebase or API documents. However, these mined rules may not be precise or complete, leading to high false positive/negative rates. In this paper, we propose a novel solution to this problem by representing the mined API usage rules as a probabilistic graphical model, where each rule's probability value represents its trustworthiness of being correct. Our approach automatically constructs probabilistic usage rules by mining codebase and documents, and aggregating knowledge from different sources. Here, the usage rules obtained from the codebase initialize the probabilistic model, while the knowledge from the documents serves as a supplement for adjusting and complementing the probabilities accordingly. We evaluate our approach on the MuBench benchmark. Experimental results show that our approach achieves 42.0\% precision and 54.5\% recall, significantly outperforming state-of-the-art approaches.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {88–99},
numpages = {12},
keywords = {API misuse detection, Document Mining, Mining Software Repository, Probabilistic Graphical Model},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3652119,
author = {Zhou, Mingyi and Gao, Xiang and Liu, Pei and Grundy, John and Chen, Chunyang and Chen, Xiao and Li, Li},
title = {Model-less Is the Best Model: Generating Pure Code Implementations to Replace On-Device DL Models},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652119},
doi = {10.1145/3650212.3652119},
abstract = {Recent studies show that on-device deployed deep learning (DL) models, such as those of Tensor Flow Lite (TFLite), can be easily extracted from real-world applications and devices by attackers to generate many kinds of adversarial and other attacks. Although securing deployed on-device DL models has gained increasing attention, no existing methods can fully prevent these attacks. Traditional software protection techniques have been widely explored. If on-device models can be implemented using pure code, such as C++, it will open the possibility of reusing existing robust software protection techniques. However, due to the complexity of DL models, there is no automatic method that can translate DL models to pure code. To fill this gap, we propose a novel method, CustomDLCoder, to automatically extract on-device DL model information and synthesize a customized executable program for a wide range of DL models. CustomDLCoder first parses the DL model, extracts its backend computing codes, configures the extracted codes, and then generates a customized program to implement and deploy the DL model without explicit model representation. The synthesized program hides model information for DL deployment environments since it does not need to retain explicit model representation, preventing many attacks on the DL model. In addition, it improves ML performance because the customized code removes model parsing and preprocessing steps and only retains the data computing process. Our experimental results show that CustomDLCoder improves model security by disabling on-device model sniffing. Compared with the original on-device platform (i.e., TFLite), our method can accelerate model inference by 21.0\% and 24.3\% on x86-64 and ARM64 platforms, respectively. Most importantly, it can significantly reduce memory consumption by 68.8\% and 36.0\% on x86-64 and ARM64 platforms, respectively.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {174–185},
numpages = {12},
keywords = {AI safety, SE for AI, software optimization for AI deployment},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3652126,
author = {Zhang, Mengxiao and Tian, Yongqiang and Xu, Zhenyang and Dong, Yiwen and Tan, Shin Hwei and Sun, Chengnian},
title = {LPR: Large Language Models-Aided Program Reduction},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652126},
doi = {10.1145/3650212.3652126},
abstract = {Program reduction is a widely used technique to facilitate debugging compilers by automatically minimizing programs that trigger compiler bugs. Existing program reduction techniques are either generic to a wide range of languages (such as Perses and Vulcan) or specifically optimized for one certain language by exploiting language-specific knowledge (e.g., C-Reduce). However, synergistically combining both generality across languages and optimality to a specific language in program reduction is yet to be explored. This paper proposes LPR, the first LLMs-aided technique leveraging LLMs to perform language-specific program reduction for multiple languages. The key insight is to utilize both the language generality of program reducers such as Perses and the languagespecific semantics learned by LLMs. Concretely, language-generic program reducers can efficiently reduce programs into a small size that is suitable for LLMs to process; LLMs can effectively transform programs via the learned semantics to create new reduction opportunities for the language-generic program reducers to further reduce the programs. Our thorough evaluation on 50 benchmarks across three programming languages (i.e., C, Rust and JavaScript) has demonstrated LPR’s practicality and superiority over Vulcan, the state-of-the-art language-generic program reducer. For effectiveness, LPR surpasses Vulcan by producing 24.93\%, 4.47\%, and 11.71\% smaller programs on benchmarks in C, Rust and JavaScript, separately. Moreover, LPR and Vulcan have the potential to complement each other. For the C language for which C-Reduce is optimized, by applying Vulcan to the output produced by LPR, we can attain program sizes that are on par with those achieved by C-Reduce. For efficiency perceived by users, LPR is more efficient when reducing large and complex programs, taking 10.77\%, 34.88\%, 36.96\% less time than Vulcan to finish all the benchmarks in C, Rust and JavaScript, separately.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {261–273},
numpages = {13},
keywords = {Large Language Models, Program Reduction, Program Semantics},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3652142,
author = {Liu, Chenyan and Cai, Yufan and Lin, Yun and Huang, Yuhuan and Pei, Yunrui and Jiang, Bo and Yang, Ping and Dong, Jin Song and Mei, Hong},
title = {CoEdPilot: Recommending Code Edits with Learned Prior Edit Relevance, Project-wise Awareness, and Interactive Nature},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652142},
doi = {10.1145/3650212.3652142},
abstract = {Recent years have seen the development of LLM-based code generation. Compared to generating code in a software project, incremental code edits are empirically observed to be more frequent. The emerging code editing approaches usually formulate the problem as generating an edit based on known relevant prior edits and context. However, practical code edits can be more complicated. First, an editing session can include multiple (ir)relevant edits to the code under edit. Second, the inference of the subsequent edits is non-trivial as the scope of its ripple effect can be the whole project. In this work, we propose CoEdPilot, an LLM-driven solution to recommend code edits by discriminating the relevant edits, exploring their interactive natures, and estimating its ripple effect in the project. Specifically, CoEdPilot orchestrates multiple neural transformers to identify what and how to edit in the project regarding both edit location and edit content. When a user accomplishes an edit with an optional editing description, an Subsequent Edit Analysis first reports the most relevant files in the project with what types of edits (e.g., keep, insert, and replace) can happen for each line of their code. Next, an Edit-content Generator generates concrete edit options for the lines of code, regarding its relevant prior changes reported by an Edit-dependency Analyzer. Last, both the Subsequent Edit Analysis and the Edit-content Generator capture relevant prior edits as feedback to readjust their recommendations. We train our models by collecting over 180K commits from 471 open-source projects in 5 programming languages. Our extensive experiments show that (1) CoEdPilot can well predict the edits (i.e., predicting edit location with accuracy of 70.8\%-85.3\%, and the edit content with exact match rate of 41.8\% and BLEU4 score of 60.7); (2) CoEdPilot can well boost existing edit generators such as GRACE and CCT5 on exact match rate by 8.57\% points and BLEU4 score by 18.08. Last, our user study on 18 participants with 3 editing tasks (1) shows that CoEdPilot can be effective in assisting users to edit code in comparison with Copilot, and (2) sheds light on the future improvement of the tool design. The video demonstration of our tool is available at https://sites.google.com/view/coedpilot/home.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {466–478},
numpages = {13},
keywords = {code edit generation, edit location, interaction, language model},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680308,
author = {Fan, Zhiyu and Ruan, Haifeng and Mechtaev, Sergey and Roychoudhury, Abhik},
title = {Oracle-Guided Program Selection from Large Language Models},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680308},
doi = {10.1145/3650212.3680308},
abstract = {While large language models (LLMs) have shown significant advancements in code generation, their susceptibility to producing incorrect code poses a significant challenge to the adoption of LLM-generated programs. This issue largely stems from the reliance on natural language descriptions as informal oracles in code generation. Current strategies to mitigate this involve selecting the best program from multiple LLM-generated alternatives, judged by criteria like the consistency of their execution results on an LLM-generated test suite. However, this approach has crucial limitations: (1) LLMs often generate redundant tests or tests that cannot distinguish between correct and incorrect solutions, (2) the used consistency criteria, such as the majority vote, fail to foster developer trust due to the absence of transparent rationale behind the made choices. In this work, we propose a new perspective on increasing the quality of LLM-generated code via program selection using the LLM as a test oracle. Our method is based on our experimentally confirmed observation that LLMs serve more effectively as oracles when tasked with selecting the correct output from multiple choices. Leveraging this insight, we first generate distinguishing inputs that capture semantic discrepancies of programs sampled from an LLM, and record outputs produced by the programs on these inputs. An LLM then selects the most likely to be correct output from these, guided by the natural language problem description. We implemented this idea in a tool LLMCodeChoice and evaluated its accuracy in generating and selecting standalone programs. Our experiments demonstrated its effectiveness in improving pass@1 by 3.6-7\% on HumanEval and MBPP benchmarks compared to the state-of-art CodeT. Most interestingly, the selected input-output specifications helped us to uncover incompleteness and ambiguities in task descriptions and also identify incorrect ground-truth implementations in the benchmarks.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {628–640},
numpages = {13},
keywords = {code generation, differential testing, large language model, oracle inference},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680323,
author = {Xia, Chunqiu Steven and Zhang, Lingming},
title = {Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680323},
doi = {10.1145/3650212.3680323},
abstract = {Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Traditional APR techniques suffer from a lack of patch variety as they rely heavily on handcrafted or mined bug fixing patterns and cannot easily generalize to other bug/fix types. To address this limitation, recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then querying the LLM to either fill-in (cloze-style APR) the correct code at the bug location or to produce a completely new code snippet as the patch. While the LLM-based APR tools are able to achieve state-of-the-art results, they still follow the classic Generate and Validate (GV) repair paradigm of first generating lots of patches by sampling from the same initial prompt and then validating each one afterwards. This not only leads to many repeated patches that are incorrect, but also misses the crucial and yet previously ignored information in test failures as well as in plausible patches.        To address these aforementioned limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful APR. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch. In this way, we can avoid making the same    mistakes. For earlier patches that passed all the tests (i.e., plausible patches), we further ask the LLM to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement ChatRepair using state-of-the-art dialogue-based LLM – ChatGPT. Our evaluation on the widely studied Defects4j dataset shows that ChatRepair is able to achieve the new state-of-the-art in repair performance, achieving 114 and 48 correct fixes on Defects4j 1.2 and 2.0 respectively. By calculating the cost    of accessing ChatGPT, we can fix 162 out of 337 bugs for $0.42 each!},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {819–831},
numpages = {13},
keywords = {Automated Program Repair, Large Language Model},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680332,
author = {Alian, Parsa and Nashid, Noor and Shahbandeh, Mobina and Mesbah, Ali},
title = {Semantic Constraint Inference for Web Form Test Generation},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680332},
doi = {10.1145/3650212.3680332},
abstract = {Automated test generation for web forms has been a longstanding challenge, exacerbated by the intrinsic human-centric design of forms and their complex, device-agnostic structures. We introduce an innovative approach, called FormNexus, for automated web form test generation, which emphasizes deriving semantic insights from individual form elements and relations among them, utilizing textual content, DOM tree structures, and visual proximity. The insights gathered are transformed into a new conceptual graph, the Form Entity Relation Graph (FERG), which offers machine-friendly semantic information extraction. Leveraging LLMs, FormNexus adopts a feedback-driven mechanism for generating and refining input constraints based on real-time form submission responses. The culmination of this approach is a robust set of test cases, each produced by methodically invalidating constraints, ensuring comprehensive testing scenarios for web forms. This work bridges the existing gap in automated web form testing by intertwining the capabilities of LLMs with advanced semantic inference methods. Our evaluation demonstrates that FormNexus combined with GPT-4 achieves 89\% coverage in form submission states. This outcome significantly outstrips the performance of the best baseline model by a margin of 25\%.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {932–944},
numpages = {13},
keywords = {Large Language Models, Test Input Generation, Web Forms},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680334,
author = {Ran, Dezhi and Wang, Hao and Song, Zihe and Wu, Mengzhou and Cao, Yuan and Zhang, Ying and Yang, Wei and Xie, Tao},
title = {Guardian: A Runtime Framework for LLM-Based UI Exploration},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680334},
doi = {10.1145/3650212.3680334},
abstract = {Tests for feature-based UI testing have been indispensable for ensuring the quality of mobile applications (apps for short).        The high manual labor costs to create such tests have led to a strong interest in automated feature-based UI testing, where an approach automatically explores the App under Test (AUT) to find correct sequences of UI events achieving the target test objective, given only a high-level test objective description.        Given that the task of automated feature-based UI testing resembles conventional AI planning problems, large language models (LLMs), known for their effectiveness in AI planning, could be ideal for this task.        However, our study reveals that LLMs struggle with following specific instructions for UI testing and replanning based on new information. This limitation results in reduced effectiveness of LLM-driven solutions for automated feature-based UI testing, despite the use of advanced prompting techniques.                Toward addressing the preceding limitation, we propose Guardian, a runtime system framework to improve the effectiveness of automated feature-based UI testing by offloading computational tasks from LLMs with two major strategies.        First, Guardian refines UI action space that the LLM can plan over, enforcing the instruction following of the LLM by construction.        Second, Guardian deliberately checks whether the gradually enriched information invalidates previous planning by the LLM.        Guardian removes the invalidated UI actions from the UI action space that the LLM can plan over, restores the state of the AUT to the state before the execution of the invalidated UI actions, and prompts the LLM to re-plan with the new UI action space.        We instantiate Guardian with ChatGPT and construct a benchmark named FestiVal with 58 tasks from 23 highly popular apps.        Evaluation results on FestiVal show that Guardian achieves 48.3},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {958–970},
numpages = {13},
keywords = {Android Testing, Large Language Models, Mobile Testing, Runtime System, Sequential Planning, UI Testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680336,
author = {Gao, Xuanqi and Jiang, Weipeng and Zhai, Juan and Ma, Shiqing and Zhang, Xiaoyu and Shen, Chao},
title = {Efficient DNN-Powered Software with Fair Sparse Models},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680336},
doi = {10.1145/3650212.3680336},
abstract = {With the emergence of the Software 3.0 era, there is a growing trend of compressing and integrating large models into software systems, with significant societal implications.         Regrettably, in numerous instances, model compression techniques impact the fairness performance of these models and thus the ethical behavior of DNN-powered software.         One of the most notable example is the Lottery Ticket Hypothesis&nbsp;(LTH), a prevailing model pruning approach.        This paper demonstrates that fairness issue of LTH-based pruning arises from both its subnetwork selection and training procedures, highlighting the inadequacy of existing remedies.        To address this, we propose a novel pruning framework, Ballot, which employs a novel conflict-detection-based subnetwork selection to find accurate and fair subnetworks, coupled with a refined training process to attain a high-performance model, thereby improving the fairness of DNN-powered software.        By means of this procedure, Ballot improves the fairness of pruning by 38.00\%, 33.91\%, 17.96\%, and 35.82\% compared to state-of-the-art baselines, namely Magnitude Pruning, Standard LTH, SafeCompress, and FairScratch respectively, based on our evaluation of five popular datasets and three widely used models.        Our code is available at https://anonymous.4open.science/r/Ballot-506E.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {983–995},
numpages = {13},
keywords = {deep neural network, fairness, pruning},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680343,
author = {Guo, Lianghong and Wang, Yanlin and Shi, Ensheng and Zhong, Wanjun and Zhang, Hongyu and Chen, Jiachi and Zhang, Ruikai and Ma, Yuchi and Zheng, Zibin},
title = {When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680343},
doi = {10.1145/3650212.3680343},
abstract = {Code generation aims to automatically generate code snippets that meet given natural language requirements and plays an important role in software development. Although Code LLMs have shown excellent performance in this domain, their long generation time poses a signification limitation in practice use. In this paper, we first conduct an in-depth preliminary study with different Code LLMs on code generation task and identify a significant efficiency issue, i.e., continual generation of excess tokens. It harms the developer productivity and leads to huge computational wastes. To address it, we introduce CodeFast, an inference acceleration approach for Code LLMs on code generation. The key idea of CodeFast is to terminate the inference process in time when unnecessary excess tokens are detected. First, we propose an automatic data construction framework to obtain training data. Then, we train a unified lightweight model GenGuard applicable to multiple programming languages to predict whether to terminate inference at the current step. Finally, we enhance Code LLM with GenGuard to accelerate its inference in code generation task. We conduct extensive experiments with CodeFast on five representative Code LLMs across four widely used code generation datasets. Experimental results show that (1) CodeFast can significantly improve the inference speed of various Code LLMs in code generation, ranging form 34\% to 452\%, without compromising the quality of generated code. (2) CodeFast is stable across different parameter settings and can generalize to untrained datasets. Our code and data are available at https://github.com/DeepSoftwareAnalytics/CodeFast.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1073–1085},
numpages = {13},
keywords = {Machine learning for analysis, Testing and development processes},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680347,
author = {Sun, Zhensu and Du, Xiaoning and Yang, Zhou and Li, Li and Lo, David},
title = {AI Coders Are among Us: Rethinking Programming Language Grammar towards Efficient Code Generation},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680347},
doi = {10.1145/3650212.3680347},
abstract = {Artificial Intelligence (AI) models have emerged as another important audience for programming languages alongside humans and machines, as we enter the era of large language models (LLMs). LLMs can now perform well in coding competitions and even write programs like developers to solve various tasks, including mathematical problems. However, the grammar and layout of current programs are designed to cater the needs of human developers -- with many grammar tokens and formatting tokens being used to make the code easier for humans to read. While this is helpful, such a design adds unnecessary computational work for LLMs, as each token they either use or produce consumes computational resources. To improve inference efficiency and reduce computational costs, we propose the concept of AI-oriented grammar.This aims to represent code in a way that better suits the working mechanism of AI models. Code written with AI-oriented grammar discards formats and uses a minimum number of tokens to convey code semantics effectively. To demonstrate the feasibility of this concept, we explore and implement the first AI-oriented grammar for Python, named Simple Python (SimPy). SimPy is crafted by revising the original Python grammar through a series of heuristic rules. Programs written in SimPy maintain identical Abstract Syntax Tree (AST) structures to those in standard Python. This allows for not only execution via a modified AST parser, but also seamless transformation between programs written in Python and SimPy, enabling human developers and LLMs to use Python and SimPy, respectively, when they need to collaborate. We also look into methods to help existing LLMs understand and use SimPy effectively. In the experiments, compared with Python, SimPy enables a reduction in token usage by 13.5\% and 10.4\% for CodeLlama and GPT-4, respectively, when completing the same set of code-related tasks. Additionally, these models can maintain or even improve their performance when using SimPy instead of Python for these tasks. With these promising results, we call for further contributions to the development of AI-oriented program grammar within our community.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1124–1136},
numpages = {13},
keywords = {Code Generation, Large Language Model, Programming Language},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680353,
author = {Chen, Jiachi and Chen, Chong and Hu, Jiang and Grundy, John and Wang, Yanlin and Chen, Ting and Zheng, Zibin},
title = {Identifying Smart Contract Security Issues in Code Snippets from Stack Overflow},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680353},
doi = {10.1145/3650212.3680353},
abstract = {Smart contract developers frequently seek solutions to developmental challenges on Q&amp;A platforms such as Stack Overflow (SO). Although community responses often provide viable solutions, the embedded code snippets can also contain hidden vulnerabilities. Integrating such code directly into smart contracts may make them susceptible to malicious attacks. We conducted an online survey and received 74 responses from smart contract developers. The results of this survey indicate that the majority (86.4\%) of participants do not sufficiently consider security when reusing SO code snippets. Despite the existence of various tools designed to detect vulnerabilities in smart contracts, these tools are typically developed for analyzing fully-completed smart contracts and thus are ineffective for analyzing typical code snippets as found on SO. We introduce SOChecker, the first tool designed to identify potential vulnerabilities in incomplete SO smart contract code snippets. SOChecker first leverages a fine-tuned Llama2 model for code completion, followed by the application of symbolic execution methods for vulnerability detection. Our experimental results, derived from a dataset comprising 897 code snippets collected from smart contract-related SO posts, demonstrate that SOChecker achieves an F1 score of 68.2\%, greatly surpassing GPT-3.5 and GPT-4 (20.9\% and 33.2\% F1 Scores respectively). Our findings underscore the need to improve the security of code snippets from Q&amp;A websites.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1198–1210},
numpages = {13},
keywords = {large language models, program analysis, smart contracts},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680354,
author = {Shin, Jiho and Hashtroudi, Sepehr and Hemmati, Hadi and Wang, Song},
title = {Domain Adaptation for Code Model-Based Unit Test Case Generation},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680354},
doi = {10.1145/3650212.3680354},
abstract = {Recently, deep learning-based test case generation approaches have been proposed to automate the generation of unit test cases. In this study, we leverage Transformer-based code models to generate unit tests with the help of Domain Adaptation (DA) at a project level. Specifically, we use CodeT5, a relatively small language model trained on source code data, and fine-tune it on the test generation task. Then, we apply domain adaptation to each target project data to learn project-specific knowledge (project-level DA). We use the Methods2test dataset to fine-tune CodeT5 for the test generation task and the Defects4j dataset for project-level domain adaptation and evaluation. We compare our approach with (a) CodeT5 fine-tuned on the test generation without DA, (b) the A3Test tool, and (c) GPT-4 on five projects from the Defects4j dataset. The results show that tests generated using DA can increase the line coverage by 18.62\%, 19.88\%, and 18.02\% and mutation score by 16.45\%, 16.01\%, and 12.99\% compared to the above (a), (b), and (c) baselines, respectively. The overall results show consistent improvements in metrics such as parse rate, compile rate, BLEU, and CodeBLEU. In addition, we show that our approach can be seen as a complementary solution alongside existing search-based test generation tools such as EvoSuite, to increase the overall coverage and mutation scores with an average of 34.42\% and 6.8\%, for line coverage and mutation score, respectively.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1211–1222},
numpages = {12},
keywords = {Code Model, Domain Adaption, GPT, LLM, Test generation, Transformers},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680366,
author = {Yu, Jiongchi and Xie, Xiaofei and Zhang, Cen and Chen, Sen and Li, Yuekang and Shen, Wenbo},
title = {Bugs in Pods: Understanding Bugs in Container Runtime Systems},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680366},
doi = {10.1145/3650212.3680366},
abstract = {Container Runtime Systems (CRSs), which form the foundational infrastructure of container clouds, are critically important due to their impact on the quality of container cloud implementations. However, a comprehensive understanding of the quality issues present in CRS implementations remains lacking. To bridge this gap, we conduct the first comprehensive empirical study of CRS bugs. Specifically, we gather 429 bugs from 8,271 commits across dominant CRS projects, including runc, gvisor, containerd, and cri-o. Through manual analysis, we develop taxonomies of CRS bug symptoms and root causes, comprising 16 and 13 categories, respectively. Furthermore, we evaluate the capability of popular testing approaches, including unit testing, integration testing, and fuzz testing in detecting these bugs. The results show that 78.79\% of the bugs cannot be detected due to the lack of test drivers, oracles, and effective test cases. Based on the findings of our study, we present implications and future research directions for various stakeholders in the domain of CRSs. We hope that our work can lay the groundwork for future research on CRS bug detection.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1364–1376},
numpages = {13},
keywords = {Container Runtime, Empirical Study, Software Testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680368,
author = {Xue, Zhipeng and Gao, Zhipeng and Wang, Shaohua and Hu, Xing and Xia, Xin and Li, Shanping},
title = {SelfPiCo: Self-Guided Partial Code Execution with LLMs},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680368},
doi = {10.1145/3650212.3680368},
abstract = {Code executability plays a vital role in software debugging and testing (e.g., detecting runtime exceptions or assertion violations). However, code execution, especially partial or arbitrary code execution, is a non-trivial task due to missing definitions and complex third-party dependencies. To make partial code (such as code snippets posted on the web or code fragments deep inside complex software projects) executable, the existing study has proposed a machine learning model to predict the undefined element types and inject the pre-defined dummy values into execution. However, the performance of their tool is limited due to its simply designed dummy values and the inability to continue learning. In this paper, we design and implement a novel framework, named SelfPiCo (Self-Guided Partial Code Executor), to dynamically guide partial code execution by incorporating the open-source LLM (i.e., Code Llama) within an interactive loop. Particularly, SelfPiCo leverages few-shot in-context learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning based on fine-tuning the Code Llama model. SelfPiCo continuously learns from code execution results and refines its predictions step after step. Our evaluations demonstrate that SelfPiCo can execute 72.7\% and 83.3\% of all lines in the open-source code and Stack Overflow snippets, outperforming the most recent state-of-the-art Lexecutor by 37.9\% and 33.5\%, respectively. Moreover, SelfPiCo successfully detected 18 and 33 runtime type error issues by executing the partial code from eight GitHub software projects and 43 Stack Overflow posts, demonstrating the practical usage and potential application of our framework in practice.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1389–1401},
numpages = {13},
keywords = {Dynamic Analysis, Large Language Model, Partial Code Execution, Prompt Engineering},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680369,
author = {Chen, Yang and Jabbarvand, Reyhaneh},
title = {Neurosymbolic Repair of Test Flakiness},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680369},
doi = {10.1145/3650212.3680369},
abstract = {Test flakiness, a non-deterministic behavior of builds irrelevant to code changes, is a major and continuing impediment to deliver- ing reliable software. The very few techniques for the automated repair of test flakiness are specifically crafted to repair either Order- Dependent (OD) or Implementation-Dependent (ID) flakiness. They are also all symbolic approaches, i.e., they leverage program analy- sis to detect and repair known test flakiness patterns and root causes, failing to generalize. To bridge the gap, we propose FlakyDoctor, a neuro-symbolic technique that combines the power of LLMs— generalizability—and program analysis—soundness—to fix different types of test flakiness. Our extensive evaluation using 873 confirmed flaky tests (332 OD and 541 ID) from 243 real-world projects demonstrates the ability of FlakyDoctor in repairing flakiness, achieving 57\% (OD) and 59\% (ID) success rate. Comparing to three alternative flakiness repair approaches, FlakyDoctor can repair 8\% more ID tests than DexFix, 12\% more OD flaky tests than ODRepair, and 17\% more OD flaky tests than iFixFlakies. Regardless of underlying LLM, the non-LLM components of FlakyDoctor contribute to 12–31 \% of the overall performance, i.e., while part of the FlakyDoctor power is from using LLMs, they are not good enough to repair flaky tests in real-world projects alone. What makes the proposed technique superior to related research on test flakiness mitigation specifically and program repair, in general, is repairing 79 previously unfixed flaky tests in real-world projects. We opened pull requests for all cases with corresponding patches; 19 of them were accepted and merged at the time of submission.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1402–1414},
numpages = {13},
keywords = {Large Language Models, Program Repair, Test Flakiness},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680370,
author = {Tan, Jovyn and Rigger, Manuel},
title = {Inconsistencies in TeX-Produced Documents},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680370},
doi = {10.1145/3650212.3680370},
abstract = {TeX is a widely-used typesetting system adopted by most publishers and professional societies. While TeX is responsible for generating a significant number of documents, irregularities in the TeX ecosystem may produce inconsistent documents. These inconsistencies may occur across different TeX engines or different versions of TeX distributions, resulting in failures to adhere to formatting specifications, or the same document rendering differently for different authors. In this work, we investigate and quantify the robustness of the TeX ecosystem through a large-scale study of 432 documents. We developed an automated pipeline to evaluate the cross-engine and cross-version compatibility of the TeX ecosystem. We found significant inconsistencies in the outputs of different TeX engines: only 0.2\% of documents compiled to identical output with XeTeX and PDFTeX due to a lack of cross-engine support in popular LaTeX packages and classes used in academic conferences. A smaller—yet significant—extent of inconsistencies was found across different TeX Live distributions, with only 42.1\% of documents producing the same output from 2020 to 2023. Our automated pipeline additionally reduces the human effort in bug-finding: from a sample of 10 unique root causes of inconsistencies, we identified two new bugs in LaTeX packages and five existing bugs that were fixed independently of this study. We also observed potentially unintended inconsistencies across different TeX Live distributions beyond the updates listed in changelogs. We expect that this study will help authors of TeX documents to avoid unexpected outcomes by understanding how they may be affected by the often undocumented subtleties of the TeX ecosystem, while benefiting developers by demonstrating how different implementations result in unintended inconsistencies.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1415–1427},
numpages = {13},
keywords = {LaTeX, PDF documents, TeX, typesetting},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680379,
author = {Cui, Di and Wang, Qiangqiang and Zhao, Yutong and Wang, Jiaqi and Wei, Minjie and Hu, Jingzhao and Wang, Luqiao and Li, Qingshan},
title = {One-to-One or One-to-Many? Suggesting Extract Class Refactoring Opportunities with Intra-class Dependency Hypergraph Neural Network},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680379},
doi = {10.1145/3650212.3680379},
abstract = {Excessively large classes that encapsulate multiple responsibilities are challenging to comprehend and maintain. Addressing this issue, several Extract Class refactoring tools have been proposed, employing a two-phase process: identifying suitable fields or methods for extraction, and implementing the mechanics of refactoring. These tools traditionally generate an intra-class dependency graph to analyze the class structure, applying hard-coded rules based on this graph to unearth refactoring opportunities. Yet, the graph-based approach predominantly illuminates direct, “one-to-one” relationship between pairwise entities. Such a perspective is restrictive as it overlooks the complex, “one-to-many” dependencies among multiple entities that are prevalent in real-world classes. This narrow focus can lead to refactoring suggestions that may diverge from developers’ actual needs, given their multifaceted nature. To bridge this gap, our paper leverages the concept of intra-class dependency hypergraph to model one-to-many dependency relationship and proposes a hypergraph learning-based approach to suggest Extract Class refactoring opportunities named HECS. For each target class, we first construct its intra-class dependency hypergraph and assign attributes to nodes with a pre-trained code model. All the attributed hypergraphs are fed into an enhanced hypergraph neural network for training. Utilizing this trained neural network alongside a large language model (LLM), we construct a refactoring suggestion system. We trained HECS on a large-scale dataset and evaluated it on two real-world datasets. The results show that demonstrates an increase of 38.5\% in precision, 9.7\% in recall, and 44.4\% in f1-measure compared to 3 state-of-the-art refactoring tools including JDeodorant, SSECS, and LLMRefactor, which is more useful for 64\% of participants. The results also unveil practical suggestions and new insights that benefit existing extract-related refactoring techniques.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1529–1540},
numpages = {12},
keywords = {Extract Class Refactoring, Hypergraph Neural Network},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680385,
author = {Yuan, Yuanyuan and Wang, Shuai and Su, Zhendong},
title = {See the Forest, not Trees: Unveiling and Escaping the Pitfalls of Error-Triggering Inputs in Neural Network Testing},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680385},
doi = {10.1145/3650212.3680385},
abstract = {Recent efforts in deep neural network (DNN) testing commonly use error-triggering inputs (ETIs) to quantify DNN errors and to fine-tune the tested DNN for repairing. This study reveals the pitfalls of ETIs in DNN testing. Specifically, merely seeking for more ETIs “traps” the testing campaign into local plateaus, where similar ETIs are continuously generated using a few fixed input transformations. Similarly, fine-tuning the DNN with ETIs, while capable of fixing the exposed DNN mis-predictions, undermines the DNN’s resilience towards certain input transformations. However, these ETI-induced pitfalls have been overlooked in previous research, due to the insufficient input transformations (usually &lt; 10), and we show that the severity of such deceptive phenomena is enlarged when testing DNNs with more and diverse real-life input transformations.      This paper presents a comprehensive study on the pitfalls of ETIs in DNN testing. We first augment conventional DNN testing pipelines with a large set of input transformations; the correctness and validity of these new transformations are verified with large-scale human studies. Based on this, we show that launching an endless pursuit for ETIs cannot alleviate the “trapped testing” issue, and the undermined resilience pervasively occurs in many input transformations. Accordingly, we propose a novel and holistic viewpoint over DNN errors: instead of counting which input triggers a DNN mis-prediction, we record which input transformation can generate ETIs. The targeted input property of this transformation, termed erroneous property (EP), counts one DNN error and guides DNN testing (i.e., our new paradigm aims to find more EPs rather than ETIs). Evaluation shows that this EP-oriented testing paradigm significantly expands the explored DNN error space. Moreover, fine-tuning DNNs with EPs effectively improves their resilience towards different input transformations.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1605–1617},
numpages = {13},
keywords = {Deep learning testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3680389,
author = {Eom, Jueon and Jeong, Seyeon and Kwon, Taekyoung},
title = {Fuzzing JavaScript Interpreters with Coverage-Guided Reinforcement Learning for LLM-Based Mutation},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680389},
doi = {10.1145/3650212.3680389},
abstract = {JavaScript interpreters, crucial for modern web browsers, require an effective fuzzing method to identify security-related bugs. However, the strict grammatical requirements for input present significant challenges. Recent efforts to integrate language models for context- aware mutation in fuzzing are promising but lack the necessary coverage guidance to be fully effective. This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with Reinforcement Learning (RL) from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning. CovRL-Fuzz, through this approach, enables the generation of test cases that are more likely to discover new coverage areas, thus improving bug detection while minimizing syntax and semantic errors, all without needing extra post-processing. Our evaluation results show that CovRL-Fuzz outperforms the state-of-the-art fuzzers in enhancing code coverage and identifying bugs in JavaScript interpreters: CovRL-Fuzz identified 58 real-world security-related bugs in the latest JavaScript interpreters, including 50 previously unknown bugs and 15 CVEs.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1656–1668},
numpages = {13},
keywords = {coverage, fuzzing, large language model, reinforcement learning},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3651313,
author = {Denisenko, Natalia and Zhang, Youzhi and Pulice, Chiara and Bhattasali, Shohini and Jajodia, Sushil and Resnik, Philip and Subrahmanian, V.S.},
title = {A Psycholinguistics-inspired Method to Counter IP Theft Using Fake Documents},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3651313},
doi = {10.1145/3651313},
abstract = {Intellectual property (IP) theft is a growing problem. We build on prior work to deter IP theft by generating n fake versions of a technical document so a thief has to expend time and effort in identifying the correct document. Our new SbFAKE framework proposes, for the first time, a novel combination of language processing, optimization, and the psycholinguistic concept of surprisal to generate a set of such fakes. We start by combining psycholinguistic-based surprisal scores and optimization to generate two bilevel surprisal optimization problems (an Explicit one and a simpler Implicit one) whose solutions correspond directly to the desired set of fakes. As bilevel problems are usually hard to solve, we then show that these two bilevel surprisal optimization problems can each be reduced to equivalent surprisal-based linear programs. We performed detailed parameter tuning experiments and identified the best parameters for each of these algorithms. We then tested these two variants of SbFAKE (with their best parameter settings) against the best performing prior work in the field. Our experiments show that SbFAKE is able to more effectively generate convincing fakes than past work. In addition, we show that replacing words in an original document with words having similar surprisal scores generates greater levels of deception.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {7},
numpages = {25},
keywords = {AI for security, fake document generation}
}

@proceedings{10.1145/3651890,
title = {ACM SIGCOMM '24: Proceedings of the ACM SIGCOMM 2024 Conference},
year = {2024},
isbn = {9798400706141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3652024,
title = {ISMM 2024: Proceedings of the 2024 ACM SIGPLAN International Symposium on Memory Management},
year = {2024},
isbn = {9798400706158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is with great pleasure that we welcome you to the 2024 ACM SIGPLAN International Symposium on Memory Management (ISMM '24)! This is the 23rd event in the ISMM series. We expanded the scope of ISMM this year and encouraged submissions and participation from related fields such as computer architecture and operating systems in addition to the programming languages community. The call for papers motivated submissions of work in the following areas.},
location = {Copenhagen, Denmark}
}

@proceedings{10.1145/3652032,
title = {LCTES 2024: Proceedings of the 25th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
year = {2024},
isbn = {9798400706165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES 2024), the 25th edition of this longstanding conference! This year’s conference is co-located with PLDI 2024, bringing together affiliated research conferences and workshops into a week-long joint meeting in Copenhagen, Denmark. The mission of LCTES is to provide a link between languages, compilers, and tools for embedded systems, bringing together scientists and engineers from these communities. LCTES offers a forum for researchers and developers from either area to come together, interact, share insights, and collaborate on developing novel solutions.},
location = {Copenhagen, Denmark}
}

@proceedings{10.1145/3652037,
title = {PETRA '24: Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments},
year = {2024},
isbn = {9798400717604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Crete, Greece}
}

@inproceedings{10.1145/3652037.3663942,
author = {Joaa, AFM Mohimenul and Majumder, Prattoy and Sadeque, Farig},
title = {Curious Learner: A Neuro-Symbolic Approach for Function Execution via Natural Language},
year = {2024},
isbn = {9798400717604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652037.3663942},
doi = {10.1145/3652037.3663942},
abstract = {Generative models possess immense potential, but their ability to perform complex calculations is limited by the need to memorize vast amounts of data, leading to computational inefficiencies. Leveraging tools like the Arithmetic Logic Unit using symbolic functions offers a more efficient alternative, enabling faster responses, smaller model sizes, and improved accuracy. We propose a neuro-symbolic generative model to empower natural language models with task execution abilities by integrating functional programming principles. Experiments on our scoped four translation tasks using 98 mathematical functions demonstrated rapid convergence and minimal training time requirements. The model achieved an average accuracy, BLEU score, and perplexity score of 0.85, 0.84, and 5.9, respectively, after training on a T4 GPU for several hours. This neuro-symbolic Language Model shows significant potential for various applications, such as NLP-based command line tools, customer service automation, service discovery automation, project code automation, and natural language-based operating systems.},
booktitle = {Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {392–399},
numpages = {8},
keywords = {Curious Learner, Customer Service Automation, Foundational Model, Generative Model, Large Language Model Architecture, Natural Language Processing, Neuro-Symbolic Programming, Service Discovery Automation, Task Executor, Transformer},
location = {Crete, Greece},
series = {PETRA '24}
}

@proceedings{10.1145/3652588,
title = {SOAP 2024: Proceedings of the 13th ACM SIGPLAN International Workshop on the State Of the Art in Program Analysis},
year = {2024},
isbn = {9798400706219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 13th ACM SIGPLAN International Workshop on the State Of the Art in Program Analysis (SOAP’24) is co-located with the 45th ACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI’24). In line with past workshops, SOAP’24 aims to bring together members of the program analysis community to share new developments and shape innovations in program analysis.},
location = {Copenhagen, Denmark}
}

@proceedings{10.1145/3653644,
title = {FAIML '24: Proceedings of the 2024 3rd International Conference on Frontiers of Artificial Intelligence and Machine Learning},
year = {2024},
isbn = {9798400709777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yichang, China}
}

@proceedings{10.1145/3653876,
title = {ICDSP '24: Proceedings of the 2024 8th International Conference on Digital Signal Processing},
year = {2024},
isbn = {9798400709029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@proceedings{10.1145/3653946,
title = {ICMVA '24: Proceedings of the 2024 7th International Conference on Machine Vision and Applications},
year = {2024},
isbn = {9798400716553},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3654522,
title = {ICIIT '24: Proceedings of the 2024 9th International Conference on Intelligent Information Technology},
year = {2024},
isbn = {9798400716713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ho Chi Minh City, Vietnam}
}

@inproceedings{10.1145/3654777.3676400,
author = {Ma, Dizhi and Hu, Xiyun and Shi, Jingyu and Patel, Mayank and Jain, Rahul and Liu, Ziyi and Zhu, Zhengzhe and Ramani, Karthik},
title = {avaTTAR: Table Tennis Stroke Training with Embodied and Detached Visualization in Augmented Reality},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676400},
doi = {10.1145/3654777.3676400},
abstract = {Table tennis stroke training is a critical aspect of player development. We designed a new augmented reality (AR) system, avaTTAR, for table tennis stroke training. The system provides both “on-body” (first-person view) and “detached” (third-person view) visual cues, enabling users to visualize target strokes and correct their attempts effectively with this dual perspectives setup. By employing a combination of pose estimation algorithms and IMU sensors, avaTTAR&nbsp; captures and reconstructs the 3D body pose and paddle orientation of users during practice, allowing real-time comparison with expert strokes. Through a user study, we affirm avaTTAR&nbsp;’s capacity to amplify player experience and training results.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {35},
numpages = {16},
keywords = {Augmented Reality, Motor Learning, Table Tennis},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@proceedings{10.1145/3655038,
title = {HotStorage '24: Proceedings of the 16th ACM Workshop on Hot Topics in Storage and File Systems},
year = {2024},
isbn = {9798400706301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Santa Clara, CA, USA}
}

@inproceedings{10.1145/3655038.3665950,
author = {Egersdoerfer, Chris and Sareen, Arnav and Bez, Jean Luca and Byna, Suren and Dai, Dong},
title = {ION: Navigating the HPC I/O Optimization Journey using Large Language Models},
year = {2024},
isbn = {9798400706301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3655038.3665950},
doi = {10.1145/3655038.3665950},
abstract = {Effectively leveraging the complex software and hardware I/O stacks of HPC systems to deliver needed I/O performance has been a challenging task for domain scientists. To identify and address I/O issues in their applications, scientists largely rely on I/O experts to analyze the recorded I/O traces of their applications and provide insights into the potential issues. However, due to the limited number of I/O experts and the growing demand for data-intensive applications across the wide spectrum of sciences, inaccessibility has become a major bottleneck hindering scientists from maximizing their productivity. Inspired by the recent rapid progress of large language models (LLMs), in this work we propose IO Navigator (ION), an LLM-based framework that takes a recorded I/O trace of an application as input and leverages the in-context learning, chain-of-thought, and code generation capabilities of LLMs to comprehensively analyze the I/O trace and provide diagnosis of potential I/O issues. Similar to an I/O expert, ION provides detailed justifications for the diagnosis and an interactive interface for scientists to ask detailed questions about the diagnosis. We illustrate ION's applicability by assessing it on a set of controlled I/O traces generated with different I/O issues. We also demonstrate that ION can match state-of-the-art I/O optimization tools and provide more insightful and adaptive diagnoses for real applications. We believe ION, with its full capabilities, has the potential to become a powerful tool for scientists to navigate through complex I/O subsystems in the future.},
booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Storage and File Systems},
pages = {86–92},
numpages = {7},
location = {Santa Clara, CA, USA},
series = {HotStorage '24}
}

@proceedings{10.1145/3655497,
title = {ICIAI '24: Proceedings of the 2024 International Conference on Innovation in Artificial Intelligence},
year = {2024},
isbn = {9798400709302},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3655532,
title = {ICRSA '23: Proceedings of the 2023 6th International Conference on Robot Systems and Applications},
year = {2023},
isbn = {9798400708039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Wuhan, China}
}

@proceedings{10.1145/3656156,
title = {DIS '24 Companion: Companion Publication of the 2024 ACM Designing Interactive Systems Conference},
year = {2024},
isbn = {9798400706325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {IT University of Copenhagen, Denmark}
}

@article{10.1145/3656296,
author = {Wang, Changjie and Scazzariello, Mariano and Farshin, Alireza and Ferlin, Simone and Kosti\'{c}, Dejan and Chiesa, Marco},
title = {NetConfEval: Can LLMs Facilitate Network Configuration?},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CoNEXT2},
url = {https://doi.org/10.1145/3656296},
doi = {10.1145/3656296},
abstract = {This paper explores opportunities to utilize Large Language Models (LLMs) to make network configuration human-friendly, simplifying the configuration of network devices \&amp; development of routing algorithms and minimizing errors. We design a set of benchmarks (NetConfEval) to examine the effectiveness of different models in facilitating and automating network configuration. More specifically, we focus on the scenarios where LLMs translate high-level policies, requirements, and descriptions (i.e., specified in natural language) into low-level network configurations \&amp; Python code. NetConfEval considers four tasks that could potentially facilitate network configuration, such as (i) generating high-level requirements into a formal specification format, (ii) generating API/function calls from high-level requirements, (iii) developing routing algorithms based on high-level descriptions, and (iv) generating low-level configuration for existing and new protocols based on input documentation. Learning from the results of our study, we propose a set of principles to design LLM-based systems to configure networks. Finally, we present two GPT-4-based prototypes to (i) automatically configure P4-enabled devices from a set of high-level requirements and (ii) integrate LLMs into existing network synthesizers.},
journal = {Proc. ACM Netw.},
month = jun,
articleno = {7},
numpages = {25},
keywords = {benchmark, code generation, function calling, large language models (llms), network configuration, network synthesizer, p4, rag, routing algorithms}
}

@article{10.1145/3656386,
author = {Li, Shaohua and Theodoridis, Theodoros and Su, Zhendong},
title = {Boosting Compiler Testing by Injecting Real-World Code},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {PLDI},
url = {https://doi.org/10.1145/3656386},
doi = {10.1145/3656386},
abstract = {We introduce a novel approach for testing optimizing compilers with code from real-world applications. The main idea is to construct well-formed programs by fusing multiple code snippets from various real-world projects. The key insight is backed by the fact that the large volume of real-world code exercises rich syntactical and semantic language features, which current engineering-intensive approaches like random program generators are hard to fully support. To construct well-formed programs from real-world code, our approach works by (1) extracting real-world code at the granularity of function, (2) injecting function calls into seed programs, and (3) leveraging dynamic execution information to maintain the semantics and build complex data dependencies between injected functions and the seed program. With this idea, our approach complements the existing generators by boosting their expressiveness via fusing real-world code in a semantics-preserving way. We implement our idea in a tool, Creal, to test C compilers. In a nine-month testing period, we have reported 132 bugs to GCC and LLVM, two of the most popular and well-tested C compilers. At the time of writing, 121 of them have been confirmed as unknown bugs, and 101 of them have been fixed. Most of these bugs were miscompilations, and many were recognized as long-latent and critical. Our evaluation results evidently demonstrate the significant advantage of using real-world code to stress-test compilers. We believe this idea will benefit the general compiler testing direction and will be directly applicable to other compilers.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {156},
numpages = {23},
keywords = {Compiler testing, compilers, miscompilation, reliability, testing}
}

@article{10.1145/3656429,
author = {Ketkar, Ameya and Ramos, Daniel and Clapp, Lazaro and Barik, Raj and Ramanathan, Murali Krishna},
title = {A Lightweight Polyglot Code Transformation Language},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {PLDI},
url = {https://doi.org/10.1145/3656429},
doi = {10.1145/3656429},
abstract = {In today's software industry, large-scale, multi-language codebases are the norm. This brings substantial challenges in developing automated tools for code maintenance tasks such as API migration or dead code cleanup. Tool builders often find themselves caught between two less-than-ideal tooling options: (1) language-specific code rewriting tools or (2) generic, lightweight match-replace transformation tools with limited expressiveness. The former leads to tool fragmentation and a steep learning curve for each language, while the latter forces developers to create ad-hoc, throwaway scripts to handle realistic tasks. To fill this gap, we introduce a new declarative domain-specific language (DSL) for expressing interdependent multi-language code transformations. Our key insight is that we can increase the expressiveness and applicability of lightweight match-replace tools by extending them to support for composition, ordering, and flow. We implemented an open-source tool for our language, called PolyglotPiranha, and deployed it in an industrial setting. We demonstrate its effectiveness through three case studies, where it deleted 210K lines of dead code and migrated 20K lines, across 1611 pull requests. We compare our DSL against state-of-the-art alternatives, and show that the tools we developed are faster, more concise, and easier to maintain.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {199},
numpages = {25},
keywords = {Automated refactoring, Code cleanup, Source-code rewriting}
}

@proceedings{10.1145/3657054,
title = {dg.o '24: Proceedings of the 25th Annual International Conference on Digital Government Research},
year = {2024},
isbn = {9798400709883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Taipei, Taiwan}
}

@inproceedings{10.1145/3657054.3657079,
author = {Barcellos, Raissa and Bernardini, Flavia and Zuiderwijk, Anneke and Viterbo, Jose},
title = {Exploring Interpretability in Open Government Data with ChatGPT},
year = {2024},
isbn = {9798400709883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657054.3657079},
doi = {10.1145/3657054.3657079},
abstract = {The global initiative supporting open government data (OGD) has witnessed significant strides in the last decade. This study delves into the prospective integration of Artificial Intelligence (AI) with Hippolyta, a framework meticulously crafted to amplify the interpretability of government data. The aim is to scrutinize the viability of this integration, conducting a technical investigation in the realms of open government data and artificial intelligence. In contributing to the expansive field of OGD, this research focuses on elucidating the interpretability of data originating from governmental sources. Through an exploration of the technical feasibility surrounding the fusion of AI with Hippolyta, we aim to pave the path for advancements, fostering heightened interpretability and overarching enhancements in the understanding of government data.},
booktitle = {Proceedings of the 25th Annual International Conference on Digital Government Research},
pages = {186–195},
numpages = {10},
keywords = {Open government data, data interpretability},
location = {Taipei, Taiwan},
series = {dg.o '24}
}

@proceedings{10.1145/3657242,
title = {Interacci\'{o}n '24: Proceedings of the XXIV International Conference on Human Computer Interaction},
year = {2024},
isbn = {9798400717871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {A Coru\~{n}a, Spain}
}

@proceedings{10.1145/3657529,
title = {icWCSN '24: Proceedings of the 2024 11th International Conference on Wireless Communication and Sensor Networks},
year = {2024},
isbn = {9798400709005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chengdu, China}
}

@proceedings{10.1145/3657604,
title = {L@S '24: Proceedings of the Eleventh ACM Conference on Learning @ Scale},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to present the Proceedings of the Eleventh Annual ACM Conference on Learning at Scale, L@S 2024, held July 18-20, 2024 at Georgia Tech in Atlanta, Georgia, USA.The Learning at Scale conference was created by the Association for Computing Machinery (ACM), inspired by the emergence of Massive Open Online Courses (MOOCs) and the accompanying shift in thinking about education. During the last few years, new opportunities for scaling up learning have emerged, like hybrid learning environments combining online and face-to-face, and informal learning enabled by all sorts of platforms (e.g., gamified language learning, citizen science communities, and collaborative programming communities). In the recent two years, the unprecedented development of generative AI has brought profound opportunities to scale the teaching and learning experiences, with the goal of enhancing learning for the increasingly diverse group of learners in both formal and informal contexts. L@S has evolved along with these emergent massive learning scenarios and opportunities and is today one of the most prominent venues for discussion of the highest quality of research on how learning and teaching can be transformed at scale, in diverse learning environments.The theme of L@S 2024 is Scaling Learning in the Age of AI. Rapid advances in AI have created new opportunities but also challenges for the Learning@Scale community. The advances in generative AI show potential to enhance pedagogical practices and the efficacy of learning at scale. This has led to an unprecedented level of interest in employing generative AI for scaling tutoring and feedback. The prevalence of such tools calls for new practices and understanding on how AI-based methods should be designed and developed to enhance the experiences and outcomes of teachers and learners.Learning@Scale 2024 solicits empirical and theoretical papers on, but not limited to, the following topics (in no particular order): 1) Instruction at scale: studies that examine how teachers and educators scale their instructions, what aspects of instruction could be scaled effectively, and which of these instructional strategies are the most effective for learning. 2) Interventions at scale: studies that examine the effects of interventions on student learning and performance when implemented at scale. We welcome studies that use both qualitative and quantitative methods. 3) The use of generative AI to scale learning: studies that investigate stakeholders' experiences with generative AI, students' and teachers' interactions with generative AI, and the potentials and limitations of using generative AI in education. 4) Systems and tools to support learning at scale: research that designs and develops systems and tools to support learning at scale. For example, this involves scaling learning through web-based systems, MOOCs, visualization, intelligent tutoring systems, gamification, immersive techniques (AR/VR/MR), mobile technologies, tangible interfaces, and various other technologies. 5) The evaluation of existing learning at scale systems and online learning environments using but not limited to the above-mentioned technologies. 6) Methods and algorithms that model learner behavior: research that contributes methods, algorithms, and pipelines that process large student data to enhance learning at scale. 7) Scaling learning in informal contexts: studies that explore how people take advantage of online environments to pursue their interests informally. 8) Review and synthesis of existing literature related to learning at scale. 9) Empirical studies and interventions that address equity, trust, algorithmic transparency and explainability, fairness and bias when using AI in education. 10) Research that addresses accessibility in learning at scale contexts. 11) Design and deployment of learning at scale systems for learners from underrepresented groups.},
location = {Atlanta, GA, USA}
}

@inproceedings{10.1145/3657604.3662043,
author = {Adkins, Keith and Joyner, David A.},
title = {Newly Created Assignments and The First Repository Effect on Inter-Semester Plagiarism},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3662043},
doi = {10.1145/3657604.3662043},
abstract = {The Internet---for all of its benefits---makes it easy for students to share assignments. This creates a serious problem for academic institutions. Common mitigation tactics include discouraging students from sharing their work and routinely checking for and removing solutions shared online. While these strategies can be successful in many cases, they are not always sufficient. In our experience, it can be a challenge if either students or hosting sites refuse to remove solutions. Pursuing legal options can be both time consuming and costly. One approach taken to combat this is to routinely create new coding assignments, but this can still require a significant time commitment. It is worth exploring if this effort is worthwhile.In this paper, we present an empirical study based on data that we collected over five semesters while addressing plagiarism within our large online computer science graduate program. We compare plagiarism rates between two courses: one integrating new assignments and the other continuing to reuse older assignments.In this study, we explore the benefits derived from introducing new assignments to counter plagiarism, and how long these benefits last. We then explore the trends that publicly shared solutions have on plagiarism rates, and what those trends tell us about the value of implementing new assignments. Lastly, we explore the effects that the process of detection and intervention have on the frequency of misconduct.We observed that the benefits gained by introducing new assignments faded quickly. Additionally, we observed that proactively seeking the removal of publicly shared solutions may be ineffective unless all solutions are removed. Lastly, we observed that early detection and notification to students results in reduced misconduct over time.Our observations underscore the notion that a single solution posted publicly can swiftly erode the advantages gained from creating new assignments to help reduce plagiarism. This raises questions about whether the advantages of introducing new assignments outweigh benefits gained through reusing and refining assignments over time. More mature and well-developed assignments tend to lend themselves to robust, experience-backed rubrics and dynamic autograders which deliver a pedagogical benefit that may outweigh the integrity benefits of frequently developing new assessments.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {211–220},
numpages = {10},
keywords = {assessment, misconduct, plagiarism detection},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3657604.3662046,
author = {Chen, Binglin and Lewis, Colleen M. and West, Matthew and Zilles, Craig},
title = {Plagiarism in the Age of Generative AI: Cheating Method Change and Learning Loss in an Intro to CS Course},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3662046},
doi = {10.1145/3657604.3662046},
abstract = {Background: ChatGPT became widespread in early 2023 and enabled the broader public to use powerful generative AI, creating a new means for students to complete course assessments.  Purpose: In this paper, we explored the degree to which generative AI impacted the frequency and nature of cheating in a large introductory programming course. We also estimate the learning impact of students choosing to submit plagiarized work rather than their own work.  Methods: We identified a collection of markers that we believe are indicative of plagiarism in this course. We compare the estimated prevalence of cheating in the semesters before and during which ChatGPT became widely available. We use linear regression to estimate the impact of students' patterns of cheating on their final exam performance. Findings: The patterns associated with these plagiarism markers suggest that the quantity of plagiarism increased with the advent of generative AI, and we see evidence of a shift from online plagiarism hubs (e.g., Chegg, CourseHero) to ChatGPT. In addition, we observe statistically significant learning losses proportional to the amount of presumed plagiarism, but there is no statistical difference on the proportionality between semesters.  Implications: Our findings suggest that unproctored exams become increasingly insecure and care needs to be taken to ensure the validity of summative assessments. More importantly, our results suggest that generative AI can be detrimental to students' learning. It seems necessary for educators to reduce the benefit of students using generative AI for counterproductive purposes.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {75–85},
numpages = {11},
keywords = {cheating, cs 1, generative ai, llm, plagiarism detection},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@article{10.1145/3658129,
author = {Xu, Xiang and Lambourne, Joseph and Jayaraman, Pradeep and Wang, Zhengqing and Willis, Karl and Furukawa, Yasutaka},
title = {BrepGen: A B-rep Generative Diffusion Model with Structured Latent Geometry},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3658129},
doi = {10.1145/3658129},
abstract = {This paper presents BrepGen, a diffusion-based generative approach that directly outputs a Boundary representation (B-rep) Computer-Aided Design (CAD) model. BrepGen represents a B-rep model as a novel structured latent geometry in a hierarchical tree. With the root node representing a whole CAD solid, each element of a B-rep model (i.e., a face, an edge, or a vertex) progressively turns into a child-node from top to bottom. B-rep geometry information goes into the nodes as the global bounding box of each primitive along with a latent code describing the local geometric shape. The B-rep topology information is implicitly represented by node duplication. When two faces share an edge, the edge curve will appear twice in the tree, and a T-junction vertex with three incident edges appears six times in the tree with identical node features. Starting from the root and progressing to the leaf, BrepGen employs Transformer-based diffusion models to sequentially denoise node features while duplicated nodes are detected and merged, recovering the B-Rep topology information. Extensive experiments show that BrepGen advances the task of CAD B-rep generation, surpassing existing methods on various benchmarks. Results on our newly collected furniture dataset further showcase its exceptional capability in generating complicated geometry. While previous methods were limited to generating simple prismatic shapes, BrepGen incorporates free-form and doubly-curved surfaces for the first time. Additional applications of BrepGen include CAD autocomplete and design interpolation. The code, pretrained models, and dataset are available at https://github.com/samxuxiang/BrepGen.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {119},
numpages = {14},
keywords = {B-rep, diffusion, AIGC}
}

@article{10.1145/3658167,
author = {Ge, Jiahao and Zhou, Mingjun and Bao, Wenrui and Xu, Hao and Fu, Chi-Wing},
title = {Creating LEGO Figurines from Single Images},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3658167},
doi = {10.1145/3658167},
abstract = {This paper presents a computational pipeline for creating personalized, physical LEGO®1 figurines from user-input portrait photos. The generated figurine is an assembly of coherently-connected LEGO® bricks detailed with uv-printed decals, capturing prominent features such as hairstyle, clothing style, and garment color, and also intricate details such as logos, text, and patterns. This task is non-trivial, due to the substantial domain gap between unconstrained user photos and the stylistically-consistent LEGO® figurine models. To ensure assemble-ability by LEGO® bricks while capturing prominent features and intricate details, we design a three-stage pipeline: (i) we formulate a CLIP-guided retrieval approach to connect the domains of user photos and LEGO® figurines, then output physically-assemble-able LEGO® figurines with decals excluded; (ii) we then synthesize decals on the figurines via a symmetric U-Nets architecture conditioned on appearance features extracted from user photos; and (iii) we next reproject and uv-print the decals on associated LEGO® bricks for physical model production. We evaluate the effectiveness of our method against eight hundred expert-designed figurines, using a comprehensive set of metrics, which include a novel GPT-4V-based evaluation metric, demonstrating superior performance of our method in visual quality and resemblance to input photos. Also, we show our method's robustness by generating LEGO® figurines from diverse inputs and physically fabricating and assembling several of them.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {153},
numpages = {16},
keywords = {LEGO®, computational design, fabrication, assembly, appearance adaptation, image synthesis}
}

@article{10.1145/3658188,
author = {Wu, Zhennan and Li, Yang and Yan, Han and Shang, Taizhang and Sun, Weixuan and Wang, Senbo and Cui, Ruikai and Liu, Weizhe and Sato, Hiroyuki and Li, Hongdong and Ji, Pan},
title = {BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3658188},
doi = {10.1145/3658188},
abstract = {We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene. BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes. Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values. A variational auto-encoder is employed to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed. Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation.To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks. The extrapolation is done by conditioning the generation process with the feature samples from the overlapping tri-planes during the denoising iterations. Latent tri-plane extrapolation produces semantically and geometrically meaningful transitions that harmoniously blend with the existing scene. A 2D layout conditioning mechanism is used to control the placement and arrangement of scene elements. Experimental results indicate that BlockFusion is capable of generating diverse, geometrically consistent and unbounded large 3D scenes with unprecedented high-quality shapes in both indoor and outdoor scenarios.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {43},
numpages = {17},
keywords = {3D scene generation, diffusion model}
}

@article{10.1145/3658197,
author = {Yan, Chuan and Li, Yong and Aneja, Deepali and Fisher, Matthew and Simo-Serra, Edgar and Gingold, Yotam},
title = {Deep Sketch Vectorization via Implicit Surface Extraction},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3658197},
doi = {10.1145/3658197},
abstract = {We introduce an algorithm for sketch vectorization with state-of-the-art accuracy and capable of handling complex sketches. We approach sketch vectorization as a surface extraction task from an unsigned distance field, which is implemented using a two-stage neural network and a dual contouring domain post processing algorithm. The first stage consists of extracting unsigned distance fields from an input raster image. The second stage consists of an improved neural dual contouring network more robust to noisy input and more sensitive to line geometry. To address the issue of under-sampling inherent in grid-based surface extraction approaches, we explicitly predict undersampling and keypoint maps. These are used in our post-processing algorithm to resolve sharp features and multi-way junctions. The keypoint and undersampling maps are naturally controllable, which we demonstrate in an interactive topology refinement interface. Our proposed approach produces far more accurate vectorizations on complex input than previous approaches with efficient running time.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {37},
numpages = {13},
keywords = {vectorization, raster, sketch, drawing}
}

@article{10.1145/3658237,
author = {Tang, Luming and Ruiz, Nataniel and Chu, Qinghao and Li, Yuanzhen and Holynski, Aleksander and Jacobs, David E. and Hariharan, Bharath and Pritch, Yael and Wadhwa, Neal and Aberman, Kfir and Rubinstein, Michael},
title = {RealFill: Reference-Driven Generation for Authentic Image Completion},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3658237},
doi = {10.1145/3658237},
abstract = {Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions. However, the content these models hallucinate is necessarily inauthentic, since they are unaware of the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content that should have been there. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually compelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging scenarios, and find that it outperforms existing approaches by a large margin. Project page: https://realfill.github.io.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {135},
numpages = {12},
keywords = {image completion, diffusion model}
}

@proceedings{10.1145/3658549,
title = {I-DO '24: Proceedings of the 2024 International Conference on Information Technology, Data Science, and Optimization},
year = {2024},
isbn = {9798400709180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Taipei, Taiwan}
}

@proceedings{10.1145/3658664,
title = {IH&amp;MMSec '24: Proceedings of the 2024 ACM Workshop on Information Hiding and Multimedia Security},
year = {2024},
isbn = {9798400706370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 12th ACM Information Hiding and Multimedia Security Workshop - IH&amp;MMSec'24 in Baiona, Galicia, Spain, organized by the Research Group in Signal Processing in Communications (GPSC) at the University of Vigo. GPSC is one of the pioneering research groups in Information Hiding and Multimedia Security with over 25 years of active engagement in this domain. During this time, our field has seen remarkable growth, evolution, and reinvention, yet it continues to preserve the same effervescence of its inception. This vibrancy is reflected in the diverse range of topics covered in this year's program.In response to our call for papers, we received in total 69 submissions. The top five countries with the highest number of submissions (first author) were Germany, China, France, Italy, and the United States. Each submission underwent rigorous evaluation, with a minimum of three independent reviews provided by members of the Program Committee, supplemented by external reviewers as needed. Based on these timely and high-quality reviews, the Technical Program Chairs selected the 33 most outstanding submissions. The acceptance rate of 47.8\% (33/69) reflects our commitment to uphold IH&amp;MMSec as a premier scientific venue in the field of Information Hiding and Multimedia Security. The accepted papers cover the fields of forensics, steganography, steganalysis, watermarking, biometrics, anonymity, security and privacy.},
location = {Baiona, Spain}
}

@inproceedings{10.1145/3658664.3659637,
author = {Kn\"{o}chel, Mandy and Karius, Sebastian},
title = {Text Steganography Methods and their Influence in Malware: A Comprehensive Overview and Evaluation},
year = {2024},
isbn = {9798400706370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658664.3659637},
doi = {10.1145/3658664.3659637},
abstract = {Steganography describes techniques and algorithms for hiding secret information in a cover medium such as images, audio or text files. Malware that makes use of steganographic techniques, known as stegomalware, is becoming increasingly common. This paper provides a comprehensive analysis of various text steganography methods and their application in the context of stegomalware. We give an extensive overview of occurrences of text stegomalware in the real world and the steganographic methods used in these attacks. The cover text includes any files or data containing natural language text or machine-readable digital texts and source code such as HTML, CSS, JavaScript, etc. A categorical overview of known text steganography methods is presented, whereas text steganography techniques are classified into the categories insertion, substitution, permutation and generation. For each category, selected representatives have been practically implemented and tested with different cover text files and messages of varying lengths. The authors also look at real-world applications and instances of stegomalware that utilize these methods. The paper reveals that while there is a vast array of text steganography methods, only a few are used in practice. To assess the strengths and weaknesses of each method, the evaluation is based on the metrics capacity, imperceptibility and robustness, which are commonly used to evaluate steganographic methods, and additionally complexity. The evaluation results show the performance of each method based on the defined metrics. We further discuss possible countermeasures and their effect on each steganography method. The analysis also shows that with the rise of machine learning and large language models, text steganography methods might become more common in the future.},
booktitle = {Proceedings of the 2024 ACM Workshop on Information Hiding and Multimedia Security},
pages = {113–124},
numpages = {12},
keywords = {linguistic steganography, malware, steganography, stegomalware, text steganography},
location = {Baiona, Spain},
series = {IH&amp;MMSec '24}
}

@proceedings{10.1145/3658852,
title = {MOCO '24: Proceedings of the 9th International Conference on Movement and Computing},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Utrecht, Netherlands}
}

@proceedings{10.1145/3659677,
title = {NISS '24: Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security},
year = {2024},
isbn = {9798400709296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Meknes, AA, Morocco}
}

@proceedings{10.1145/3660317,
title = {PERMAVOST '24: Proceedings of the 4th Workshop on Performance EngineeRing, Modelling, Analysis, and VisualizatiOn STrategy},
year = {2024},
isbn = {9798400706455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Pisa, Italy}
}

@proceedings{10.1145/3660354,
title = {WDC '24: Proceedings of the 3rd ACM Workshop on the Security Implications of Deepfakes and Cheapfakes},
year = {2024},
isbn = {9798400706493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3660512,
title = {SCID '24: Proceedings of the 1st Workshop on Security-Centric Strategies for Combating Information Disorder},
year = {2024},
isbn = {9798400706509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3660512.3665521,
author = {Nguyen, Bao-Tin and Nguyen, Van-Loc and Nguyen, Thanh-Son and Dang-Nguyen, Duc-Tien and Do, Trong-Le and Tran, Minh-Triet},
title = {A Hybrid Approach for Cheapfake Detection Using Reputation Checking and End-To-End Network},
year = {2024},
isbn = {9798400706509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660512.3665521},
doi = {10.1145/3660512.3665521},
abstract = {In today’s era dominated by digital manipulations, the identification of Cheapfakes, especially those involving out-of-context visuals, is imperative for upholding the integrity of information dissemination and fostering trust in multimedia content. This research introduces an innovative approach to detect cheapfakes, utilizing a hybrid methodology that merges real-time online verification with extensive training on a unified end-to-end network, augmented by generative synthetic data. This combined strategy offers a promising avenue for enhancing the accuracy and efficiency of cheapfake detection, thus addressing the pressing need to combat misinformation in the digital landscape. We focus our solutions on two problems of Contextual Integrity Prediction and Image-Caption Authenticity Analysis. Our experiments reveal that this innovative network achieves an impressive 85.70\% accuracy on a public test dataset through training solely on synthetic data for Contextual Integrity Prediction, also proving its effectiveness in Image-Caption Authenticity Analysis. This research highlights the immense promise of using our proposed hybrid method in the fight against Cheapfakes, making a significant contribution to preserving the integrity of multimedia content. Our source code is publicly available at https://github.com/nbtin/cheapfakes_detection_SCID2024.git.},
booktitle = {Proceedings of the 1st Workshop on Security-Centric Strategies for Combating Information Disorder},
articleno = {3},
numpages = {12},
keywords = {Cheapfake Detection, Miscontextualization, Misinformation, Out-of-context, Visual Entailment},
location = {Singapore, Singapore},
series = {SCID '24}
}

@proceedings{10.1145/3660570,
title = {DLfM '24: Proceedings of the 11th International Conference on Digital Libraries for Musicology},
year = {2024},
isbn = {9798400717208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stellenbosch, South Africa}
}

@proceedings{10.1145/3660650,
title = {WCCCE '24: Proceedings of the 26th Western Canadian Conference on Computing Education},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kelowna, BC, Canada}
}

@article{10.1145/3660769,
author = {Jin, Xin and Lin, Zhiqiang},
title = {SimLLM: Calculating Semantic Similarity in Code Summaries using a Large Language Model-Based Approach},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660769},
doi = {10.1145/3660769},
abstract = {Code summaries are pivotal in software engineering, serving to improve code readability, maintainability, and collaboration. While recent advancements in Large Language Models (LLMs) have opened new avenues for automatic code summarization, existing metrics for evaluating summary quality, such as BLEU and BERTScore, have notable limitations. Specifically, these existing metrics either fail to capture the nuances of semantic meaning in summaries or are further limited in understanding domain-specific terminologies and expressions prevalent in code summaries. In this paper, we present SimLLM, a novel LLM-based approach designed to more precisely evaluate the semantic similarity of code summaries. Built upon an autoregressive LLM using a specialized pretraining task on permutated inputs and a pooling-based pairwise similarity measure, SimLLM overcomes the shortcomings of existing metrics. Our empirical evaluations demonstrate that SimLLM not only outperforms existing metrics but also shows a significantly high correlation with human ratings.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {62},
numpages = {24},
keywords = {automated code summarization, large language models, summary semantic similarity}
}

@article{10.1145/3660771,
author = {Kang, Sungmin and An, Gabin and Yoo, Shin},
title = {A Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660771},
doi = {10.1145/3660771},
abstract = {Fault Localization (FL), in which a developer seeks to identify which part of the code is malfunctioning and needs to be fixed, is a recurring challenge in debugging. To reduce developer burden, many automated FL techniques have been proposed. However, prior work has noted that existing techniques fail to provide rationales for the suggested locations, hindering developer adoption of these techniques. With this in mind, we propose AutoFL, a Large Language Model (LLM)-based FL technique that generates an explanation of the bug along with a suggested fault location. AutoFL prompts an LLM to use function calls to navigate a repository, so that it can effectively localize faults over a large software repository and overcome the limit of the LLM context length. Extensive experiments on 798 real-world bugs in Java and Python reveal AutoFL improves method-level acc@1 by up to 233.3\% over baselines. Furthermore, developers were interviewed on their impression of AutoFL-generated explanations, showing that developers generally liked the natural language explanations of AutoFL, and that they preferred reading a few, high-quality explanations instead of many.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {64},
numpages = {23},
keywords = {debugging, fault localization, language models}
}

@article{10.1145/3660783,
author = {Yuan, Zhiqiang and Liu, Mingwei and Ding, Shiji and Wang, Kaixin and Chen, Yixuan and Peng, Xin and Lou, Yiling},
title = {Evaluating and Improving ChatGPT for Unit Test Generation},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660783},
doi = {10.1145/3660783},
abstract = {Unit testing plays an essential role in detecting bugs in functionally-discrete program units (e.g., methods). Manually writing high-quality unit tests is time-consuming and laborious. Although the traditional techniques are able to generate tests with reasonable coverage, they are shown to exhibit low readability and still cannot be directly adopted by developers in practice. Recent work has shown the large potential of large language models (LLMs) in unit test generation. By being pre-trained on a massive developer-written code corpus, the models are capable of generating more human-like and meaningful test code. In this work, we perform the first empirical study to evaluate the capability of ChatGPT (i.e., one of the most representative LLMs with outstanding performance in code generation and comprehension) in unit test generation. In particular, we conduct both a quantitative analysis and a user study to systematically investigate the quality of its generated tests in terms of correctness, sufficiency, readability, and usability. We find that the tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures (mostly caused by incorrect assertions); but the passing tests generated by ChatGPT almost resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved. Inspired by our findings above, we further propose ChatTester, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTester incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTester by generating 34.3\% more compilable tests and 18.7\% more tests with correct assertions than the default ChatGPT. In addition to ChatGPT, we further investigate the generalization capabilities of ChatTester by applying it to two recent open-source LLMs (i.e., CodeLLama-Instruct and CodeFuse) and our results show that ChatTester can also improve the quality of tests generated by these LLMs.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {76},
numpages = {24},
keywords = {Large language model, Test generation, Unit testing}
}

@article{10.1145/3660788,
author = {Khojah, Ranim and Mohamad, Mazen and Leitner, Philipp and de Oliveira Neto, Francisco Gomes},
title = {Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660788},
doi = {10.1145/3660788},
abstract = {Large Language Models (LLMs) are frequently discussed in academia and the general public as support tools for virtually any use case that relies on the production of text, including software engineering. Currently, there is much debate, but little empirical evidence, regarding the practical usefulness of LLM-based tools such as ChatGPT for engineers in industry. We conduct an observational study of 24 professional software engineers who have been using ChatGPT over a period of one week in their jobs, and qualitatively analyse their dialogues with the chatbot as well as their overall experience (as captured by an exit survey). We find that rather than expecting ChatGPT to generate ready-to-use software artifacts (e.g., code), practitioners more often use ChatGPT to receive guidance on how to solve their tasks or learn about a topic in more abstract terms. We also propose a theoretical framework for how the (i) purpose of the interaction, (ii) internal factors (e.g., the user's personality), and (iii) external factors (e.g., company policy) together shape the experience (in terms of perceived usefulness and trust). We envision that our framework can be used by future research to further the academic discussion on LLM usage by software engineering practitioners, and to serve as a reference point for the design of future empirical LLM research in this domain.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {81},
numpages = {22},
keywords = {Chatbots, Large Language Models (LLMs), Software Development Bots}
}

@article{10.1145/3660790,
author = {Hassan, Md Mahadi and Salvador, John and Santu, Shubhra Kanti Karmaker and Rahman, Akond},
title = {State Reconciliation Defects in Infrastructure as Code},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660790},
doi = {10.1145/3660790},
abstract = {In infrastructure as code (IaC), state reconciliation is the process of querying and comparing the infrastructure state prior to changing the infrastructure. As state reconciliation is pivotal to manage IaC-based computing infrastructure at scale, defects related to state reconciliation can create large-scale consequences. A categorization of state reconciliation defects, i.e., defects related to state reconciliation, can aid in understanding the nature of state reconciliation defects. We conduct an empirical study with 5,110 state reconciliation defects where we apply qualitative analysis to categorize state reconciliation defects. From the identified defect categories, we derive heuristics to design prompts for a large language model (LLM), which in turn are used for validation of state reconciliation. From our empirical study, we identify 8 categories of state reconciliation defects, amongst which 3 have not been reported for previously-studied software systems. The most frequently occurring defect category is inventory, i.e., the category of defects that occur when managing infrastructure inventory. Using an LLM with heuristics-based paragraph style prompts, we identify 9 previously unknown state reconciliation defects of which 7 have been accepted as valid defects, and 4 have already been fixed. Based on our findings, we conclude the paper by providing a set of recommendations for researchers and practitioners.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {83},
numpages = {24},
keywords = {defect, devops, empirical study, infrastructure as code, state reconciliation}
}

@article{10.1145/3660791,
author = {Endres, Madeline and Fakhoury, Sarah and Chakraborty, Saikat and Lahiri, Shuvendu K.},
title = {Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660791},
doi = {10.1145/3660791},
abstract = {Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a program’s intent. However, there is typically no guarantee that a program’s implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language, which makes natural language intent challenging to check programmatically. The “emergent abilities” of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is unclear if such translation could be useful in practice.     In this paper, we describe nl2postcondition, the problem of leveraging LLMs for transforming informal natural language to formal method postconditions, expressed as program assertions.   We introduce and validate metrics to measure and compare different nl2postcondition approaches, using the correctness and discriminative power of generated postconditions.   We then use qualitative and quantitative methods to assess the quality of nl2postcondition postconditions, finding that they are generally correct and able to discriminate incorrect code. Finally, we find that  via LLMs has the potential to be helpful in practice;  generated postconditions were able to catch 64 real-world historical bugs from Defects4J.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {84},
numpages = {24},
keywords = {Formal Specifications, Large Language Models, Postconditions}
}

@article{10.1145/3660799,
author = {Li, Yuxi and Liu, Yi and Deng, Gelei and Zhang, Ying and Song, Wenjia and Shi, Ling and Wang, Kailong and Li, Yuekang and Liu, Yang and Wang, Haoyu},
title = {Glitch Tokens in Large Language Models: Categorization Taxonomy and Effective Detection},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660799},
doi = {10.1145/3660799},
abstract = {With the expanding application of Large Language Models (LLMs) in various domains, it becomes imperative to comprehensively investigate their unforeseen behaviors and consequent outcomes. In this study, we introduce and systematically explore the phenomenon of “glitch tokens”, which are anomalous tokens produced by established tokenizers and could potentially compromise the models’ quality of response. Specifically, we experiment on seven top popular LLMs utilizing three distinct tokenizers and involving a totally of 182,517 tokens. We present categorizations of the identified glitch tokens and symptoms exhibited by LLMs when interacting with glitch tokens. Based on our observation that glitch tokens tend to cluster in the embedding space, we propose GlitchHunter, a novel iterative clustering-based technique, for efficient glitch token detection. The evaluation shows that our approach notably outperforms three baseline methods on eight open-source LLMs. To the best of our knowledge, we present the first comprehensive study on glitch tokens. Our new detection further provides valuable insights into mitigating tokenization-related errors in LLMs.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {92},
numpages = {23},
keywords = {LLM analysis, LLM security, glitch token}
}

@article{10.1145/3660806,
author = {Olewicki, Doriane and Habchi, Sarra and Adams, Bram},
title = {An Empirical Study on Code Review Activity Prediction and Its Impact in Practice},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660806},
doi = {10.1145/3660806},
abstract = {During code reviews, an essential step in software quality assurance, reviewers have the difficult task of understanding and evaluating code changes to validate their quality and prevent introducing faults to the codebase. This is a tedious process where the effort needed is highly dependent on the code submitted, as well as the author’s and the reviewer’s experience, leading to median wait times for review feedback of 15-64 hours. Through an initial user study carried with 29 experts, we found that re-ordering the files changed by a patch within the review environment has potential to improve review quality, as more comments are written (+23\%), and participants’ file-level hot-spot precision and recall increases to 53\% (+13\%) and 28\% (+8\%), respectively, compared to the alphanumeric ordering. Hence, this paper aims to help code reviewers by predicting which files in a submitted patch need to be (1) commented, (2) revised, or (3) are hot-spots (commented or revised). To predict these tasks, we evaluate two different types of text embeddings (i.e., Bag-of-Words and Large Language Models encoding) and review process features (i.e., code size-based and history-based features). Our empirical study on three open-source and two industrial datasets shows that combining the code embedding and review process features leads to better results than the state-of-the-art approach. For all tasks, F1-scores (median of 40-62\%) are significantly better than the state-of-the-art (from +1 to +9\%).},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {99},
numpages = {23},
keywords = {Classification, Code review automation, Large Language Models, Review process features, Text embedding}
}

@article{10.1145/3660810,
author = {Mu, Fangwen and Shi, Lin and Wang, Song and Yu, Zhuohao and Zhang, Binquan and Wang, ChenXue and Liu, Shichao and Wang, Qing},
title = {ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660810},
doi = {10.1145/3660810},
abstract = {Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96\% to 80.80\% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43\% to 69.60\% and from 54.32\% to 62.37\%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {103},
numpages = {23},
keywords = {Code Generation, Large Language Model, Prompt Engineering}
}

@article{10.1145/3660811,
author = {Mai, Yubo and Gao, Zhipeng and Hu, Xing and Bao, Lingfeng and Liu, Yu and Sun, JianLing},
title = {Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660811},
doi = {10.1145/3660811},
abstract = {Inspired by the great potential of Large Language Models (LLMs) for solving complex coding tasks, in this paper, we propose a novel approach, named Code2API, to automatically perform APIzation for Stack Overflow code snippets. Code2API does not require additional model training or any manual crafting rules and can be easily deployed on personal computers without relying on other external tools. Specifically, Code2API guides the LLMs through well-designed prompts to generate well-formed APIs for given code snippets. To elicit knowledge and logical reasoning from LLMs, we used chain-of-thought (CoT) reasoning and few-shot in-context learning, which can help the LLMs fully understand the APIzation task and solve it step by step in a manner similar to a developer. Our evaluations show that Code2API achieves a remarkable accuracy in identifying method parameters (65\%) and return statements (66\%) equivalent to human-generated ones, surpassing the current state-of-the-art approach, APIzator, by 15.0\% and 16.5\% respectively. Moreover, compared with APIzator, our user study demonstrates that Code2API exhibits superior performance in generating meaningful method names, even surpassing the human-level performance, and developers are more willing to use APIs generated by our approach, highlighting the applicability of our tool in practice. Finally, we successfully extend our framework to the Python dataset, achieving a comparable performance with Java, which verifies the generalizability of our tool.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {104},
numpages = {23},
keywords = {APIs, Chain-of-thought, In-context learning, Large language models, Stack Overflow}
}

@article{10.1145/3660812,
author = {Farhour, Farbod and Abdellatif, Ahmad and Mansour, Essam and Shihab, Emad},
title = {A Weak Supervision-Based Approach to Improve Chatbots for Code Repositories},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660812},
doi = {10.1145/3660812},
abstract = {Software chatbots are growing in popularity and have been increasingly used in software projects due to their benefits in saving time, cost, and effort. At the core of every chatbot is a Natural Language Understanding (NLU) component that enables chatbots to comprehend the users' queries. Prior work shows that chatbot practitioners face challenges in training the NLUs because the labeled training data is scarce. Consequently, practitioners resort to user queries to enhance chatbot performance. They annotate these queries and use them for NLU training. However, such training is done manually and prohibitively expensive. Therefore, we propose AlphaBot to automate the query annotation process for SE chatbots. Specifically, we leverage weak supervision to label users' queries posted to a software repository-based chatbot. To evaluate the impact of using AlphaBot on the NLU's performance, we conducted a case study using a dataset that comprises 749 queries and 52 intents. The results show that using AlphaBot improves the NLU's performance in terms of F1-score, with improvements ranging from 0.96\% to 35\%. Furthermore, our results show that applying more labeling functions improves the NLU's classification of users' queries. Our work enables practitioners to focus on their chatbots' core functionalities rather than annotating users' queries.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {105},
numpages = {24},
keywords = {Software Chatbots, Weak Supervision, and Data Augmentation}
}

@article{10.1145/3660814,
author = {Chen, Simin and Li, Zexin and Yang, Wei and Liu, Cong},
title = {DeciX: Explain Deep Learning Based Code Generation Applications},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660814},
doi = {10.1145/3660814},
abstract = {Deep learning-based code generation (DL-CG) applications have shown great potential for assisting developers in programming with human-competitive accuracy. However, lacking transparency in such applications due to the uninterpretable nature of deep learning models makes the automatically generated programs untrustworthy. In this paper, we develop DeciX, a first explanation method dedicated to DL-CG applications. DeciX is motivated by observing two unique properties of DL-CG applications: output-to-output dependencies and irrelevant value and semantic space. These properties violate the fundamental assumptions made in existing explainable DL techniques and thus cause applying existing techniques to DL-CG applications rather pessimistic and even incorrect. DeciX addresses these two limitations by constructing a causal inference dependency graph, containing a novel method leveraging causal inference that can accurately quantify the contribution of each dependency edge in the graph to the end prediction result. Proved by extensive experiments assessing popular, widely-used DL-CG applications and several baseline methods, DeciX is able to achieve significantly better performance compared to state-of-the-art in terms of several critical performance metrics, including correctness, succinctness, stability, and overhead. Furthermore, DeciX can be applied to practical scenarios since it does not require any knowledge of the DL-CG model under explanation. We have also conducted case studies that demonstrate the applicability of DeciX in practice.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {107},
numpages = {23},
keywords = {Explainable AI, Large Language Model, Program Synthesis}
}

@article{10.1145/3660816,
author = {Wang, Chengpeng and Zhang, Jipeng and Wu, Rongxin and Zhang, Charles},
title = {DAInfer: Inferring API Aliasing Specifications from Library Documentation via Neurosymbolic Optimization},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660816},
doi = {10.1145/3660816},
abstract = {Modern software systems heavily rely on various libraries, necessitating understanding API semantics in static analysis. However, summarizing API semantics remains challenging due to complex implementations or the unavailability of library code. This paper presents DAInfer, a novel approach for inferring API aliasing specifications from library documentation. Specifically, we employ Natural Language Processing (NLP) models to interpret informal semantic information provided by the documentation, which enables us to reduce the specification inference to an optimization problem. Furthermore, we propose a new technique called neurosymbolic optimization to efficiently solve the optimization problem, yielding the desired API aliasing specifications. We have implemented DAInfer as a tool and evaluated it upon Java classes from several popular libraries. The results indicate that DAInfer infers the API aliasing specifications with a precision of 79.78\% and a recall of 82.29\%, averagely consuming 5.35 seconds per class. These obtained aliasing specifications further facilitate alias analysis, revealing 80.05\% more alias facts for API return values in 15 Java projects. Additionally, the tool supports taint analysis, identifying 85 more taint flows in 23 Android apps. These results demonstrate the practical value of DAInfer in library-aware static analysis.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {109},
numpages = {24},
keywords = {alias analysis, documentation mining, specification inference}
}

@article{10.1145/3660825,
author = {Wu, Yaoxuan and Humayun, Ahmad and Gulzar, Muhammad Ali and Kim, Miryung},
title = {Natural Symbolic Execution-Based Testing for Big Data Analytics},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660825},
doi = {10.1145/3660825},
abstract = {Symbolic execution is an automated test input generation technique that models individual program paths as logical constraints. However, the realism of concrete test inputs generated by SMT solvers often comes into question. Existing symbolic execution tools only seek arbitrary solutions for given path constraints. These constraints do not incorporate the naturalness of inputs that observe statistical distributions, range constraints, or preferred string constants. This results in unnatural-looking inputs that fail to emulate real-world data.                In this paper, we extend symbolic execution with consideration for incorporating naturalness. Our key insight is that users typically understand the semantics of program inputs, such as the distribution of height or possible values of zipcode, which can be leveraged to advance the ability of symbolic execution to produce natural test inputs. We instantiate this idea in NaturalSym, a symbolic execution-based test generation tool for data-intensive scalable computing (DISC) applications. NaturalSym generates natural-looking data that mimics real-world distributions by utilizing user-provided input semantics to drastically enhance the naturalness of inputs, while preserving strong bug-finding potential.                On DISC applications and commercial big data test benchmarks, NaturalSym achieves a higher degree of realism —as evidenced by a perplexity score 35.1 points lower on median, and detects 1.29\texttimes{} injected faults compared to the state-of-the-art symbolic executor for DISC, BigTest. This is because BigTest draws inputs purely based on the satisfiability of path constraints constructed from branch predicates, while NaturalSym is able to draw natural concrete values based on user-specified semantics and prioritize using these values in input generation. Our empirical results demonstrate that NaturalSym finds injected faults 47.8\texttimes{} more than NaturalFuzz (a coverage-guided fuzzer) and 19.1\texttimes{} more than ChatGPT. Meanwhile, TestMiner (a mining-based approach) fails to detect any injected faults. NaturalSym is the first symbolic executor that combines the notion of input naturalness in symbolic path constraints during SMT-based input generation. We make our code available at https://github.com/UCLA-SEAL/NaturalSym.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {118},
numpages = {24},
keywords = {DISC Applications, Naturalness, Symbolic Execution}
}

@proceedings{10.1145/3660829,
title = {Programming '24: Companion Proceedings of the 8th International Conference on the Art, Science, and Engineering of Programming},
year = {2024},
isbn = {9798400706349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lund, Sweden}
}

@proceedings{10.1145/3660853,
title = {AICCONF '24: Proceedings of the Cognitive Models and Artificial Intelligence Conference},
year = {2024},
isbn = {9798400716928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {undefinedstanbul, Turkiye}
}

@proceedings{10.1145/3661167,
title = {EASE '24: Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Salerno, Italy}
}

@inproceedings{10.1145/3661167.3661200,
author = {Mastropaolo, Antonio and Nardone, Vittoria and Bavota, Gabriele and Di Penta, Massimiliano},
title = {How the Training Procedure Impacts the Performance of Deep Learning-based Vulnerability Patching},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661200},
doi = {10.1145/3661167.3661200},
abstract = {Generative deep learning (DL) models have been successfully adopted for vulnerability patching. However, such models require the availability of a large dataset of patches to learn from. To overcome this issue, researchers have proposed to start from models pre-trained with general knowledge, either on the programming language or on similar tasks such as bug fixing. Despite the efforts in the area of automated vulnerability patching, there is a lack of systematic studies on how these different training procedures impact the performance of DL models for such a task. This paper provides a manyfold contribution to bridge this gap, by (i) comparing existing solutions of self-supervised and supervised pre-training for vulnerability patching; and (ii) for the first time, experimenting with different kinds of prompt-tuning for this task. The study required to train/test 23 DL models. We found that a supervised pre-training focused on bug-fixing, while expensive in terms of data collection, substantially improves DL-based vulnerability patching. When applying prompt-tuning on top of this supervised pre-trained model, there is no significant gain in performance. Instead, prompt-tuning is an effective and cheap solution to substantially boost the performance of self-supervised pre-trained models, i.e., those not relying on the bug-fixing pre-training.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {150–159},
numpages = {10},
keywords = {Machine Learning on Code, Pre-Trained Models, Prompt Tuning, Software Vulnerability Repair},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3661167.3661216,
author = {Siddiq, Mohammed Latif and Da Silva Santos, Joanna Cecilia and Tanvir, Ridwanul Hasan and Ulfat, Noshin and Al Rifat, Fahmid and Carvalho Lopes, Vin\'{\i}cius},
title = {Using Large Language Models to Generate JUnit Tests: An Empirical Study},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661216},
doi = {10.1145/3661167.3661216},
abstract = {A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80\% coverage for the HumanEval dataset, but no model had more than 2\% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {313–322},
numpages = {10},
keywords = {junit, large language models, test generation, test smells, unit testing},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3661167.3661220,
author = {G\'{o}mez-Abajo, Pablo and P\'{e}rez-Soler, Sara and Ca\~{n}izares, Pablo C. and Guerra, Esther and de Lara, Juan},
title = {Mutation Testing for Task-Oriented Chatbots},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661220},
doi = {10.1145/3661167.3661220},
abstract = {Conversational agents, or chatbots, are increasingly used to access all sorts of services using natural language. While open-domain chatbots – like ChatGPT – can converse on any topic, task-oriented chatbots – the focus of this paper – are designed for specific tasks, like booking a flight, obtaining customer support, or setting an appointment. Like any other software, task-oriented chatbots need to be properly tested, usually by defining and executing test scenarios (i.e., sequences of user-chatbot interactions). However, there is currently a lack of methods to quantify the completeness and strength of such test scenarios, which can lead to low-quality tests, and hence to buggy chatbots. To fill this gap, we propose adapting mutation testing (MuT) for task-oriented chatbots. To this end, we introduce a set of mutation operators that emulate faults in chatbot designs, an architecture that enables MuT on chatbots built using heterogeneous technologies, and a practical realisation as an Eclipse plugin. Moreover, we evaluate the applicability, effectiveness and efficiency of our approach on open-source chatbots, with promising results.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {232–241},
numpages = {10},
keywords = {Botium, Dialogflow, Mutation testing, Rasa, Task-oriented chatbots},
location = {Salerno, Italy},
series = {EASE '24}
}

@proceedings{10.1145/3661455,
title = {PDC '24: Proceedings of the Participatory Design Conference 2024: Exploratory Papers and Workshops - Volume 2},
year = {2024},
isbn = {9798400706547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
location = {Sibu, Malaysia}
}

@proceedings{10.1145/3661456,
title = {PDC '24: Proceedings of the Participatory Design Conference 2024: Situated Actions, Doctoral Colloquium, PDC places, Communities - Volume 3},
year = {2024},
isbn = {9798400706554},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
location = {Sibu, Malaysia}
}

@article{10.1145/3661484,
author = {Kr\"{u}ger, Jacob and Li, Yi and Lossev, Kirill and Zhu, Chenguang and Chechik, Marsha and Berger, Thorsten and Rubin, Julia},
title = {A Meta-Study of Software-Change Intentions},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3661484},
doi = {10.1145/3661484},
abstract = {Every software system undergoes changes, for example, to add new features, fix bugs, or refactor code. The importance of understanding software changes has been widely recognized, resulting in various techniques and studies, for instance, on change-impact analysis or classifying developers’ activities. Since changes are triggered by developers’ intentions—something they plan or want to change in the system—many researchers have studied intentions behind changes. While there appears to be a consensus among software-engineering researchers and practitioners that knowing the intentions behind software changes is important, it is not clear how developers can actually benefit from this knowledge. In fact, there is no consolidated, recent overview of the state of the art on software-change intentions (SCIs) and their relevance for software engineering. We present a meta-study of 122 publications, which we used to derive a categorization of SCIs and to discuss motivations, evidence, and techniques relating to SCIs. Unfortunately, we found that individual pieces of research are often disconnected from each other, because a common understanding is missing. Similarly, some publications showcase the potential of knowing SCIs, but more substantial research to understand the practical benefits of knowing SCIs is needed. Our contributions can help researchers and practitioners improve their understanding of SCIs and how SCIs can aid software engineering tasks.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {300},
numpages = {41},
keywords = {Intentions, software evolution, change management, version control}
}

@proceedings{10.1145/3661725,
title = {CMLDS '24: Proceedings of the International Conference on Computing, Machine Learning and Data Science},
year = {2024},
isbn = {9798400716393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3661790,
title = {EmpathiCH '24: Proceedings of the 3rd Empathy-Centric Design Workshop: Scrutinizing Empathy Beyond the Individual},
year = {2024},
isbn = {9798400717888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Honolulu, HI, USA}
}

@proceedings{10.1145/3661814,
title = {LICS '24: Proceedings of the 39th Annual ACM/IEEE Symposium on Logic in Computer Science},
year = {2024},
isbn = {9798400706608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume contains the proceedings of the 39th Annual ACM/IEEE Symposium on Logic in Computer Science (LICS 2024).},
location = {Tallinn, Estonia}
}

@proceedings{10.1145/3662158,
title = {PODC '24: Proceedings of the 43rd ACM Symposium on Principles of Distributed Computing},
year = {2024},
isbn = {9798400706684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {PODC is the premier forum for presentation of research on all aspects of distributed computing, including the theory, design, implementation, and applications of distributed algorithms, systems, and networks.},
location = {Nantes, France}
}

@proceedings{10.1145/3662739,
title = {MIDA '24: Proceedings of the 2024 International Conference on Machine Intelligence and Digital Applications},
year = {2024},
isbn = {9798400718144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ningbo, China}
}

@inproceedings{10.1145/3662739.3672330,
author = {Wang, Weiguo},
title = {English Grammar Error Detection and Intelligent Assisted Correction Using Autoencoders},
year = {2024},
isbn = {9798400718144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3662739.3672330},
doi = {10.1145/3662739.3672330},
abstract = {In the context of the digital age, the correctness of English grammar is crucial for global communication. Traditional grammar detection tools rely on a large amount of annotated data and have weak generalization ability, and their corrections are not intelligent enough. This article aims to improve the efficiency and effectiveness of English grammar error detection and correction by combining autoencoders with contextual information. The first step of this article is to preprocess unannotated English text using text segmentation and voice tags. The second step is to construct and develop a deep learning framework based on variational auto-encoder (VAE), which is used to extract language features and identify grammar errors. The third step is to better understand the context of the sentence by integrating Transformer based attention mechanisms to improve context sensitivity. The fourth step in the process of error correction is to generate natural and accurate correction suggestions using a Seq2Seq (Sequence to Sequence) model based approach. The final is to strengthen learning to achieve personalized learning adaptation, so that the system can provide customized correction plans based on user preferences. The experimental results show that the English grammar error detection system using an autoencoder can effectively improve the detection accuracy, which is 6.37\% higher than traditional detection systems. It can be seen that using autoencoders can effectively improve English grammar error detection and intelligent auxiliary correction systems.},
booktitle = {Proceedings of the 2024 International Conference on Machine Intelligence and Digital Applications},
pages = {641–647},
numpages = {7},
keywords = {English grammar error detection, attention mechanism, autoencoder usage, individualized learning adaptation, intelligent assisted correction},
location = {Ningbo, China},
series = {MIDA '24}
}

@proceedings{10.1145/3663338,
title = {ApPLIED'24: Proceedings of the 2024 Workshop on Advanced Tools, Programming Languages, and PLatforms for Implementing and Evaluating algorithms for Distributed systems},
year = {2024},
isbn = {9798400706707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ApPLIED aims to bring together distributed system designers and practitioners from academia and industry to share their experiences and perspectives in designing and building distributed systems.},
location = {Nantes, France}
}

@proceedings{10.1145/3663384,
title = {CHIWORK '24: Proceedings of the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for Work},
year = {2024},
isbn = {9798400710179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Newcastle upon Tyne, United Kingdom}
}

@inproceedings{10.1145/3663384.3663387,
author = {Suh, Hyewon and Dangol, Aayushi and Meadan, Hedda and Miller, Carol A. and Kientz, Julie A.},
title = {Opportunities and Challenges for AI-Based Support for Speech-Language Pathologists},
year = {2024},
isbn = {9798400710179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663384.3663387},
doi = {10.1145/3663384.3663387},
abstract = {Speech-Language Pathologists (SLPs) are professionals who work with children and adults in the prevention, assessment, diagnosis, and intervention for speech, language, and communication difficulties. This research investigates the experiences and perceptions of SLPs regarding the potential for Artificial Intelligence (AI) technologies to support their work. Through a series of three studies, including an online survey, an Asynchronous Remote Community (ARC), and an observation of online communities, we comprehensively explored the challenges faced by SLPs and identified areas where AI-based technologies can offer support. This paper addresses four key areas: 1) the reported needs, constraints, and challenges faced by SLPs in their work, 2) the current perspectives of SLPs on AI and technology, 3) the adoption of AI-based tools by SLPs since the release of advanced generative AI technologies, and 4) the aspects of SLPs’ work that can be supported by AI-based tools to increase capacity and improve job satisfaction. Findings from this research contribute to a deeper understanding of SLPs’ professional environment and offer insights into the potential benefits and considerations of and design directions for integrating AI into Speech-Language Pathology practice.},
booktitle = {Proceedings of the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for Work},
articleno = {14},
numpages = {14},
keywords = {AI prototyping, Human-centered design, Natural language processing, Speech and language difficulties},
location = {Newcastle upon Tyne, United Kingdom},
series = {CHIWORK '24}
}

@inproceedings{10.1145/3663384.3663391,
author = {Deacon, Thomas and Plumbley, Mark D.},
title = {Working with AI Sound: Exploring the Future of Workplace AI Sound Technologies},
year = {2024},
isbn = {9798400710179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663384.3663391},
doi = {10.1145/3663384.3663391},
abstract = {The workplace is a site for the rapid development and deployment of Artificial Intelligence (AI) systems. However, our research suggests that their adoption could already be hindered by critical issues such as trust, privacy, and security. This paper examines the integration of AI-enabled sound technologies in the workplace, with a focus on enhancing well-being and productivity through a soundscape approach while addressing ethical concerns. To explore these concepts, we used scenario-based design and structured feedback sessions with knowledge workers from open-plan offices and those working from home. To do this, we present initial design concepts for AI sound analysis and control systems. Based on the perspectives gathered, we present user requirements and concerns, particularly regarding privacy and the potential for workplace surveillance, emphasising the need for user consent and levels of transparency in AI deployments. Navigating these ethical considerations is a key implication of the study. We advocate for novel ways to incorporate people’s involvement in the design process through co-design and serious games to shape the future of AI audio technologies in the workplace.},
booktitle = {Proceedings of the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for Work},
articleno = {2},
numpages = {21},
keywords = {AI sound systems, focus groups, knowledge workers, open-plan offices, personalised sound systems, privacy, sound monitoring, soundscape, thematic analysis, work-from-home, workplace acoustics},
location = {Newcastle upon Tyne, United Kingdom},
series = {CHIWORK '24}
}

@inproceedings{10.1145/3663384.3663393,
author = {Feldman, Molly Q and Anderson, Carolyn Jane},
title = {Non-Expert Programmers in the Generative AI Future},
year = {2024},
isbn = {9798400710179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663384.3663393},
doi = {10.1145/3663384.3663393},
abstract = {Generative AI is rapidly transforming the practice of programming. At the same time, our understanding of who writes programs, for what purposes, and how they program, has been evolving. By facilitating natural-language-to-code interactions, large language models for code have the potential to open up programming work to a broader range of workers. While existing work finds productivity benefits for expert programmers, interactions with non-experts are less well-studied. In this paper, we consider the future of programming for non-experts through a controlled study of 67 non-programmers. Our study reveals multiple barriers to effective use of large language models of code for non-experts, including several aspects of technical communication. Comparing our results to a prior study of beginning programmers illuminates the ways in which a traditional introductory programming class does and does not equip students to effectively work with generative AI. Drawing on our empirical findings, we lay out a vision for how to empower non-expert programmers to leverage generative AI for a more equitable future of programming.},
booktitle = {Proceedings of the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for Work},
articleno = {15},
numpages = {19},
keywords = {CS1, Code LLMs, Generative AI, mixed methods, non-experts},
location = {Newcastle upon Tyne, United Kingdom},
series = {CHIWORK '24}
}

@inproceedings{10.1145/3663384.3663398,
author = {He, Jessica and Houde, Stephanie and Gonzalez, Gabriel E. and Silva Moran, Dar\'{\i}o Andr\'{e}s and Ross, Steven I. and Muller, Michael and Weisz, Justin D.},
title = {AI and the Future of Collaborative Work: Group Ideation with an LLM in a Virtual Canvas},
year = {2024},
isbn = {9798400710179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663384.3663398},
doi = {10.1145/3663384.3663398},
abstract = {The introduction of generative AI into multi-user applications raises novel considerations for the future of collaborative work. How might collaborative work practices change? How might we incorporate generative AI into shared tools with users’ needs at the forefront? We examine these questions in the context of a remote team conducting ideation tasks – an example of collaborative work enabled by a shared digital workspace. We conducted a user study with 17 professionals experienced with virtual group ideation workshops. Our study examined their use of the Collaborative Canvas, a virtual canvas tool with integrated generative AI capabilities that we created as a probe. Participants saw value in using generative AI to assist with group facilitation and to augment perspectives and ideas. However, they worried about losing human perspectives and critical thinking, as well as reputational harms resulting from harmful AI outputs. Participants shared suggestions for appropriate ways to incorporate generative AI capabilities within multi-user applications and identified needs for transparency of content ownership, private digital spaces, and specialized AI capabilities. Based on participants’ insights, we share implications and opportunities for the incorporation of generative AI into collaborative work in ways that place user needs at the forefront.},
booktitle = {Proceedings of the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for Work},
articleno = {9},
numpages = {14},
keywords = {Brainstorming, Future of work, Generative AI, Group ideation, Mixed initiative, Shared virtual canvas},
location = {Newcastle upon Tyne, United Kingdom},
series = {CHIWORK '24}
}

@proceedings{10.1145/3663408,
title = {APNet '24: Proceedings of the 8th Asia-Pacific Workshop on Networking},
year = {2024},
isbn = {9798400717581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, Australia}
}

@proceedings{10.1145/3663433,
title = {LDT '24: Proceedings of the 2024 Symposium on Learning, Design and Technology},
year = {2024},
isbn = {9798400717222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Delft, Netherlands}
}

@proceedings{10.1145/3663529,
title = {FSE 2024: Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to FSE 2024, the ACM International Conference on the Foundations of Software Engineering (FSE) 2024. The conference now has a shorter name! FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {Porto de Galinhas, Brazil}
}

@inproceedings{10.1145/3663529.3663838,
author = {Alshahwan, Nadia and Harman, Mark and Marginean, Alexandru and Tal, Rotem and Wang, Eddy},
title = {Observation-Based Unit Test Generation at Meta},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663838},
doi = {10.1145/3663529.3663838},
abstract = {TestGen automatically generates unit tests, carved from serialized observations of complex objects, observed during app execution.  We describe the development and deployment of TestGen at Meta.   In particular, we focus on the scalability challenges overcome during development in order to deploy observation-based test carving at scale in industry.  So far, TestGen has landed 518 tests into production, which have been executed 9,617,349 times in continuous integration, finding 5,702 faults.   Meta is currently in the process of more widespread deployment.  Our evaluation reveals that, when carving its observations from 4,361 reliable end-to-end tests, TestGen was able to generate tests for at least 86\% of the classes covered by end-to-end tests.   Testing on 16 Kotlin Instagram app-launch-blocking tasks demonstrated that the TestGen tests would have trapped 13 of these before they became launch blocking.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {173–184},
numpages = {12},
keywords = {Automated test generation, test carving, unit testing},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3663529.3663841,
author = {Roy, Devjeet and Zhang, Xuchao and Bhave, Rashi and Bansal, Chetan and Las-Casas, Pedro and Fonseca, Rodrigo and Rajmohan, Saravan},
title = {Exploring LLM-Based Agents for Root Cause Analysis},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663841},
doi = {10.1145/3663529.3663841},
abstract = {The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team’s specific services. Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized Large Language Models (LLMs) to perform RCA, and have demonstrated promising results. However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. In this work, we explore the use of LLM based agents for RCA to address this limitation. We present a thorough empirical evaluation of a ReAct agent equipped with retrieval tools, on an out-of-distribution dataset of production incidents collected at a large IT corporation. Results show that ReAct performs competitively with strong retrieval and reasoning baselines, but with highly increased factual accuracy. We then extend this evaluation by incorporating discussions associated with incident reports as additional inputs for the models, which surprisingly does not yield significant performance improvements. Lastly, we conduct a case study with a team at Microsoft to equip the ReAct agent with tools that give it access to external diagnostic services that are used by the team for manual RCA. Our results show how agents can overcome the limitations of prior work, and practical considerations for implementing such a system in practice.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {208–219},
numpages = {12},
keywords = {AIOps, Cloud Computing, Incident Management, Root Cause Analysis},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3663529.3663846,
author = {Zhang, Xuchao and Ghosh, Supriyo and Bansal, Chetan and Wang, Rujia and Ma, Minghua and Kang, Yu and Rajmohan, Saravan},
title = {Automated Root Causing of Cloud Incidents using In-Context Learning with GPT-4},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663846},
doi = {10.1145/3663529.3663846},
abstract = {Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis process for cloud services, requiring on-call engineers to identify the primary issues and implement corrective actions to prevent future recurrences. Improving the incident RCA process is vital for minimizing service downtime, customer impact and manual toil. Recent advances in artificial intelligence have introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which have proven effective in tackling various AIOps problems, ranging from code authoring to incident management. Nonetheless, the GPT-4 model’s immense size presents challenges when trying to fine-tune it on user data because of the significant GPU resource demand and the necessity for continuous model fine-tuning with the emergence of new data. To address the high cost of fine-tuning LLM, we propose an in-context learning approach for automated root causing, which eliminates the need for fine-tuning. We conduct extensive study over 100,000 production incidents from Microsoft, comparing several large language models using multiple metrics. The results reveal that our in-context learning approach outperforms the previous fine-tuned large language models such as GPT-3 by an average of 24.8\% across all metrics, with an impressive 49.7\% improvement over the zero-shot model. Moreover, human evaluation involving actual incident owners demonstrates its superiority over the fine-tuned model, achieving a 43.5\% improvement in correctness and an 8.7\% enhancement in readability. The impressive results demonstrate the viability of utilizing a vanilla GPT model for the RCA task, thereby avoiding the high computational and maintenance costs associated with a fine-tuned model.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {266–277},
numpages = {12},
keywords = {In-context Learning, Incident Diagnosis, Large Language Model, Root Cause Analysis},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3663529.3663861,
author = {Goel, Drishti and Husain, Fiza and Singh, Aditya and Ghosh, Supriyo and Parayil, Anjaly and Bansal, Chetan and Zhang, Xuchao and Rajmohan, Saravan},
title = {X-Lifecycle Learning for Cloud Incident Management using LLMs},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663861},
doi = {10.1145/3663529.3663861},
abstract = {Incident management for large cloud services is a complex and tedious process that requires a significant amount of manual effort from on-call engineers (OCEs). OCEs typically leverage data from different stages of the software development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service properties, service dependencies, trouble-shooting documents, etc.) to generate insights for detection, root cause analysis and mitigation of incidents. Recent advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini) have created opportunities to automatically generate contextual recommendations for the OCEs, assisting them in quickly identifying and mitigating critical issues. However, existing research typically takes a silo-ed view of solving a certain task in incident management by leveraging data from a single stage of the SDLC. In this paper, we demonstrate that augmenting additional contextual data from different stages of the SDLC improves the performance of two critically important and practically challenging tasks: (1) automatically generating root cause recommendations for dependency failure related incidents, and (2) identifying the ontology of service monitors used for automatically detecting incidents. By leveraging a dataset of 353 incidents and 260 monitors from Microsoft, we demonstrate that augmenting contextual information from different stages of the SDLC improves the performance over state-of-the-art methods.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {417–428},
numpages = {12},
keywords = {Cloud Services, Large language models, Monitor management, Reliability, Root-cause analysis},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@proceedings{10.1145/3663532,
title = {FaSE4Games 2024: Proceedings of the 1st ACM International Workshop on Foundations of Applied Software Engineering for Games},
year = {2024},
isbn = {9798400706745},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to FaSE4Games'24, the Foundations of Applied Software Engineering for Games workshop. We are thrilled to host this inaugural event together with the ACM International Conference on the Foundations of Software Engineering (FSE'2024) dedicated to exploring the intersections of software engineering and game development. This workshop takes place on June 19th, 2024, and is a hybrid event, ensuring participation from a diverse and international community.},
location = {Porto de Galinhas, Brazil}
}

@proceedings{10.1145/3663533,
title = {PROMISE 2024: Proceedings of the 20th International Conference on Predictive Models and Data Analytics in Software Engineering},
year = {2024},
isbn = {9798400706752},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome you to the 20th ACM International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE 2024), to be held in presence on July 16th, 2024, co-located with the International Conference on the Foundations of Software Engineering (FSE 2024).},
location = {Porto de Galinhas, Brazil}
}

@inproceedings{10.1145/3663533.3664036,
author = {Akhoundali, Jafar and Nouri, Sajad Rahim and Rietveld, Kristian and Gadyatskaya, Olga},
title = {MoreFixes: A Large-Scale Dataset of CVE Fix Commits Mined through Enhanced Repository Discovery},
year = {2024},
isbn = {9798400706752},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663533.3664036},
doi = {10.1145/3663533.3664036},
abstract = {Vulnerability datasets have become an important instrument in software security research, being used to develop automated, machine learning-based vulnerability detection and patching approaches. Yet, any limitations of these datasets may translate into inadequate performance of the developed solutions. For example, the limited size of a vulnerability dataset may restrict the applicability of deep learning techniques. In our work, we have designed and implemented a novel workflow with several heuristic methods to combine state-of-the-art methods related to CVE fix commits gathering. As a consequence of our improvements, we have been able to gather the largest programming language-independent real-world dataset of CVE vulnerabilities with the associated fix commits. Our dataset containing 26,617 unique CVEs coming from 6,945 unique GitHub projects is, to the best of our knowledge, by far the biggest CVE vulnerability dataset with fix commits available today. These CVEs are associated with 31,883 unique commits that fixed those vulnerabilities. Compared to prior work, our dataset brings about a 397\% increase in CVEs, a 295\% increase in covered open-source projects, and a 480\% increase in commit fixes. Our larger dataset thus substantially improves over the current real-world vulnerability datasets and enables further progress in research on vulnerability detection and software security. We release to the community a 14GB PostgreSQL database that contains information on CVEs up to January 24, 2024, CWEs of each CVE, files and methods changed by each commit, and repository metadata. Additionally, patch files related to the fix commits are available as a separate package. Furthermore, we make our dataset collection tool also available to the community.},
booktitle = {Proceedings of the 20th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {42–51},
numpages = {10},
keywords = {CVE, Vulnerability dataset, dataset, open-source, real-world vulnerability dataset, software repository mining},
location = {Porto de Galinhas, Brazil},
series = {PROMISE 2024}
}

@proceedings{10.1145/3663976,
title = {CVIPPR '24: Proceedings of the 2024 2nd Asia Conference on Computer Vision, Image Processing and Pattern Recognition},
year = {2024},
isbn = {9798400716607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3664190,
title = {ICTIR '24: Proceedings of the 2024 ACM SIGIR International Conference on Theory of Information Retrieval},
year = {2024},
isbn = {9798400706813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM ICTIR 2024, the 10th conference with that name to be fully sponsored by the ACM Special Interest Group on Information Retrieval (SIGIR), and the 14th International Conference on the Theory of Information Retrieval. ICTIR is the premier forum for presenting and discussing research on theoretical and foundational aspects of Information Retrieval.},
location = {Washington DC, USA}
}

@inproceedings{10.1145/3664190.3672515,
author = {Pasin, Andrea and Cunha, Washington and Gon\c{c}alves, Marcos Andr\'{e} and Ferro, Nicola},
title = {A Quantum Annealing Instance Selection Approach for Efficient and Effective Transformer Fine-Tuning},
year = {2024},
isbn = {9798400706813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664190.3672515},
doi = {10.1145/3664190.3672515},
abstract = {Deep Learning approaches have become pervasive in recent years due to their ability to solve complex tasks. However, these models need huge datasets for proper training and good generalization. This translates into high training and fine-tuning time, even several days for the most complex models and large datasets. In this work, we present a novel quantum Instance Selection (IS) approach that allows to significantly reduce the size of the training datasets (by up to 28\%) while maintaining the model's effectiveness, thus promoting (training) speedups and scalability. Our solution is innovative in the sense that it exploits a different computing paradigm - Quantum Annealing (QA) - a specific Quantum Computing paradigm that can be used to tackle optimization problems. To the best of our knowledge, there have been no prior attempts to tackle the IS problem using QA. Furthermore, we propose a new Quadratic Unconstrained Binary Optimization formulation specific for the IS problem, which is a contribution in itself. Through an extensive set of experiments with several Text Classification benchmarks, we empirically demonstrate our quantum solution's feasibility and competitiveness with the current state-of-the-art IS solutions.},
booktitle = {Proceedings of the 2024 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {205–214},
numpages = {10},
keywords = {instance selection, quantum computing, text classification},
location = {Washington DC, USA},
series = {ICTIR '24}
}

@inproceedings{10.1145/3664190.3672525,
author = {Jafari, Nazanin and Allan, James and Sarwar, Sheikh Muhammad},
title = {Target Span Detection for Implicit Harmful Content},
year = {2024},
isbn = {9798400706813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664190.3672525},
doi = {10.1145/3664190.3672525},
abstract = {Identifying the targets of hate speech is a crucial step in grasping the nature of such speech and, ultimately, improving the detection of offensive posts on online forums. Much harmful content on online platforms uses implicit language -- especially when targeting vulnerable and protected groups -- such as using stereotypical characteristics instead of explicit target names, making it harder to detect and mitigate the language. In this study, we focus on identifying implied targets of hate speech, essential for recognizing subtler hate speech and enhancing the detection of harmful content on digital platforms. We define a new task aimed at identifying the targets even when they are not explicitly stated. To address that task, we collect and annotate target spans in three prominent implicit hate speech datasets: SBIC, DynaHate, and IHC. We call the resulting merged collection Implicit-Target-Span. The collection is achieved using an innovative pooling method with matching scores based on human annotations and Large Language Models (LLMs). Our experiments indicate that Implicit-Target-Span provides a challenging test bed for target span detection methods.},
booktitle = {Proceedings of the 2024 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {117–122},
numpages = {6},
keywords = {dataset generation, hate speech detection, pooling},
location = {Washington DC, USA},
series = {ICTIR '24}
}

@inproceedings{10.1145/3664475.3664568,
author = {Bloom, Reuben},
title = {USD in Production},
year = {2024},
isbn = {9798400706837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664475.3664568},
doi = {10.1145/3664475.3664568},
abstract = {I'm here today to talk about how we utilise Universal Scene Description at Animal Logic, with a particular focus on our Asset structure. This overview will primarily encompass the traditional CG departments of Modelling, Surfacing \&amp; Rigging but can introduce some non-standard elements as we'll come across shortly. The structure of USD assets is a foundational puzzle piece of the pipeline which can too often be overlooked. The construction of these assets can influence the make-up and design of all downstream departments so is vital to get right. Luckily, by working with the core concepts of what USD can provide, it becomes possible to identify inefficiencies in the workflow. In a continuously evolving process, we can take a modular approach to seamlessly improve technological stability and user experience. One final quick note before we get started: All production images shown here come from the ALab, a publicly available playground provided by Animal Logic to help get to grips and explore how we use USD in our pipeline.},
booktitle = {ACM SIGGRAPH 2024 Courses},
articleno = {22},
numpages = {162},
location = {Denver, CO, USA},
series = {SIGGRAPH Courses '24}
}

@proceedings{10.1145/3664476,
title = {ARES '24: Proceedings of the 19th International Conference on Availability, Reliability and Security},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@inproceedings{10.1145/3664476.3664484,
author = {Simon, Raphael and Mees, Wim},
title = {SoK: A Comparison of Autonomous Penetration Testing Agents},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3664484},
doi = {10.1145/3664476.3664484},
abstract = {In the still growing field of cyber security, machine learning methods have largely been employed for detection tasks. Only a small portion revolves around offensive capabilities. Through the rise of Deep Reinforcement Learning, agents have also emerged with the goal of actively assessing the security of systems by the means of penetration testing. Thus learning the usage of different tools to emulate humans. In this paper we present an overview, and comparison of different autonomous penetration testing agents found within the literature. Various agents have been proposed, making use of distinct methods, but several factors such as modelling of the environment and scenarios, different algorithms, and the difference in chosen methods themselves, make it difficult to draw conclusions on the current state and performance of those agents. This comparison also lets us identify research challenges that present a major limiting factor, such as handling large action spaces, partial observability, defining the right reward structure, and learning in a real-world scenario.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {41},
numpages = {10},
keywords = {Deep Reinforcement Learning, Penetration Testing, Reinforcement Learning, Security Automation},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3664476.3664520,
author = {Vedros, Kurt A. and Kolias, Constantinos and Barbara, Daniel and Ivans, Robert C.},
title = {From Code to EM Signals: A Generative Approach to Side Channel Analysis-based Anomaly Detection},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3664520},
doi = {10.1145/3664476.3664520},
abstract = {Today, it is possible to perform external anomaly detection by analyzing the involuntary EM emanations of digital device components. However, one of the most important challenges of these methods is the manual collection of EM signals for fingerprinting. Indeed, this procedure must be conducted by a human expert and requires high precision. In this work, we introduce a framework that alleviates this requirement by relying on synthetic EM signals that have been generated from assembly code. The signals are produced with the use of a Generative Adversarial Network (GAN) model. Experimentally, we identify that the synthetic EM signals are extremely similar to the real and thus, can be used for training anomaly detection models effectively. Through experimental assessments, we prove that the anomaly detection models are capable of recognizing even minute alterations to the code with high accuracy.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {20},
numpages = {10},
keywords = {Anomaly Detection, Generative Artificial Networks, Side Channel Analysis},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3664476.3664523,
author = {Ruman, \'{A}d\'{a}m and Dra\v{s}ar, Martin and Sadlek, Luk\'{a}\v{s} and Yang, Shanchieh Jay and Celeda, Pavel},
title = {Adversary Tactic Driven Scenario and Terrain Generation with Partial Infrastructure Specification},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3664523},
doi = {10.1145/3664476.3664523},
abstract = {Diverse, accurate, and up-to-date training environments are essential for training cybersecurity experts and autonomous systems. However, preparation of their content is time-consuming and requires experts to provide detailed specifications. In this paper, we explore the challenges of automated generation of the content (composed of scenarios and terrains) for these environments. We propose new models to represent the cybersecurity domain and associated action spaces. These models are used to create sound and complex training content based on partial specifications provided by users. We compare the results with a real-world complex malware campaign to assess the realism of the synthesized content. To further evaluate the correctness and variability of the results, we utilize the kill-chain attack graph generation for the generated training content to asses the internal correspondence of its key components. Our results demonstrate that the proposed approach can create complex training content similar to advanced attack campaigns, which passes evaluation for soundness and practicality. Our proposed approach and its implementation significantly contribute to the state of the art, enabling novel approaches to cybersecurity training and autonomous system development.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {33},
numpages = {11},
keywords = {adversary framework, attack scenario generation, cyber terrain generation, cybersecurity model},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3664476.3670914,
author = {Wurzenberger, Markus and Krenn, Stephan and Landauer, Max and Skopik, Florian and Perner, Cora and L\"{o}tj\"{o}nen, Jarno and P\"{a}ij\"{a}nen, Jani and Gardikis, Georgios and Alabasis, Nikos and Sakerman, Liisa and Omri, Kristiina and R\"{o}ning, Juha and Halunen, Kimmo and Thouvenot, Vincent and Weise, Martin and Rauber, Andreas and Gkioulos, Vasileios and Katsikas, Sokratis and Sabetta, Luigi and Bonato, Jacopo and Ort\'{\i}z, Roc\'{\i}o and Navarro, Daniel and Stamatelatos, Nikolaos and Avdoulas, Ioannis and Mayer, Rudolf and Ekelhart, Andreas and Giannoulakis, Ioannis and Kafetzakis, Emmanouil and Corsi, Antonello and Lechner, Ulrike and Schmitt, Corinna},
title = {NEWSROOM: Towards Automating Cyber Situational Awareness Processes and Tools for Cyber Defence},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3670914},
doi = {10.1145/3664476.3670914},
abstract = {Cyber Situational Awareness (CSA) is an important element in both cyber security and cyber defence to inform processes and activities on strategic, tactical, and operational level. Furthermore, CSA enables informed decision making. The ongoing digitization and interconnection of previously unconnected components and sectors equally affects the civilian and military sector. In defence, this means that the cyber domain is both a separate military domain as well as a cross-domain and connecting element for the other military domains comprising land, air, sea, and space. Therefore, CSA must support perception, comprehension, and projection of events in the cyber space for persons with different roles and expertise. This paper introduces NEWSROOM, a research initiative to improve technologies, methods, and processes specifically related to CSA in cyber defence. For this purpose, NEWSROOM aims to improve methods for attacker behavior classification, cyber threat intelligence (CTI) collection and interaction, secure information access and sharing, as well as human computer interfaces (HCI) and visualizations to provide persons with different roles and expertise with accurate and easy to comprehend mission- and situation-specific CSA. Eventually, NEWSROOM’s core objective is to enable informed and fast decision-making in stressful situations of military operations. The paper outlines the concept of NEWSROOM and explains how its components can be applied in relevant application scenarios.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {69},
numpages = {11},
keywords = {CSA, Cyber Defence, Cyber Information Warfare, Cyber Situational Awareness, Mission-specific information},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3664476.3670920,
author = {Schiegg, Sascha and Strohmeier, Florian and Gerl, Armin and Kosch, Harald},
title = {Individual privacy levels in query-based anonymization},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3670920},
doi = {10.1145/3664476.3670920},
abstract = {Artificial intelligence systems such as Large Language Models (LLM) derive their knowledge from large datasets. Systems like ChatGPT therefore rely on shared data to train on. For companies, releasing data to the public domain requires anonymization as soon as an individual is identifiable. While there are several privacy models that guarantee a certain level of distortion applied to a dataset, to mitigate re-identification, e.g. with k-anonymity, the required level is generally defined by the data processor. We propose the idea of combining individual privacy levels defined by the data subjects themselves with a privacy language, such as the Layered Privacy Language (LPL) [10], to get a more fine-grained understanding of the effectively required privacy level. Queries that target subsets of the dataset to be released can only benefit from lower privacy requirements set by data subjects, as these response subsets may do not contain users with high privacy requirements, which can then lead to more utility. By analyzing the results of different queries to a privacy-aware data-transforming database system, we demonstrate the characteristics required for this assumption to be truly effective. For a more realistic evaluation, we also consider changes in the underlying data sources.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {182},
numpages = {8},
keywords = {Data privacy, Data warehouses, Query processing},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3664476.3670930,
author = {Steinebach, Martin},
title = {Natural Language Steganography by ChatGPT},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3670930},
doi = {10.1145/3664476.3670930},
abstract = {Natural language steganography as well as natural language watermarking have been challenging because of the complexity and lack of noise in natural language. But with the advent of LLMs like ChatGPT, controlled synthesis of written language has become available. In this work, we show how ChatGPT can be utilized to generate synthetic texts of a given topic that act as stego covers for hidden messages.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {81},
numpages = {9},
keywords = {ChatGPT, LLM, NLP, information hiding, natural language watermarking, steganalyis, steganography},
location = {Vienna, Austria},
series = {ARES '24}
}

@proceedings{10.1145/3664524,
title = {MVRMLM '24: Proceedings of 2024 ACM ICMR Workshop on Multimodal Video Retrieval},
year = {2024},
isbn = {9798400706844},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Phuket, Thailand}
}

@article{10.1145/3664595,
author = {Franceschelli, Giorgio and Musolesi, Mirco},
title = {Creativity and Machine Learning: A Survey},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3664595},
doi = {10.1145/3664595},
abstract = {There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {283},
numpages = {41},
keywords = {Computational creativity, machine learning, generative deep learning, creativity evaluation methods}
}

@proceedings{10.1145/3664646,
title = {AIware 2024: Proceedings of the 1st ACM International Conference on AI-Powered Software},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 1st ACM International Conference on AI-Powered Software (AIware), held on 15th and 16th July 2024 in Porto de Galinhas, Brazil co-located with the ACM International Conference on the Foundations of Software Engineering (FSE 2024). AIware aims to be an annual conference that brings the software engineering community together in anticipation of the upcoming changes driven by Foundation Models (FMs) and looks at them from the perspective of AI-powered software and their evolution. AIware 2024 prioritizes fostering discussions about the latest developments in the interdisciplinary field of AIware rather than solely focusing on the presentation of papers. The emphasis is on engaging conversations from diverse backgrounds to identify emerging research challenges and establish a new research agenda for the community in the Foundation Model era. To present papers and for discussions, the two-day conference will have five sessions themed around AIware Vision, SE for AIware, Human - AI Conversation, Security \&amp; Safety and AIware for Software Lifecycle Activities. Furthermore, the conference program will include two keynotes and five industry talks. The final session in the conference program will be dedicated to presenting accepted papers of the AIware challenge track.},
location = {Porto de Galinhas, Brazil}
}

@inproceedings{10.1145/3664646.3664777,
author = {Mohajer, Mohammad Mahdi and Aleithan, Reem and Harzevili, Nima Shiri and Wei, Moshi and Belle, Alvine Boaye and Pham, Hung Viet and Wang, Song},
title = {Effectiveness of ChatGPT for Static Analysis: How Far Are We?},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664777},
doi = {10.1145/3664646.3664777},
abstract = {This paper conducted a novel study to explore the capabilities of ChatGPT, a state-of-the-art LLM, in static analysis tasks such as static bug detection and false positive warning removal. In our evaluation, we focused on two types of typical and critical bugs targeted by static bug detection, i.e., Null Dereference and Resource Leak, as our subjects. We employ Infer, a well-established static analyzer, to aid the gathering of these two bug types from 10 open-source projects. Consequently, our experiment dataset contains 222 instances of Null Dereference bugs and 46 instances of Resource Leak bugs.                                Our study demonstrates that ChatGPT can achieve remarkable performance in the mentioned static analysis tasks, including bug detection and false-positive warning removal.                                 In static bug detection, ChatGPT achieves accuracy and precision values of up to 68.37\% and 63.76\% for detecting Null Dereference bugs and 76.95\% and 82.73\% for detecting Resource Leak bugs, improving the precision of the current leading bug detector, Infer by 12.86\% and 43.13\% respectively.                                 For removing false-positive warnings, ChatGPT can reach a precision of up to 93.88\% for Null Dereference bugs and 63.33\% for Resource Leak bugs, surpassing existing state-of-the-art false-positive warning removal tools.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {151–160},
numpages = {10},
keywords = {ChatGPT, Large language models, Static analysis},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@inproceedings{10.1145/3664646.3664778,
author = {Biyani, Param and Bajpai, Yasharth and Radhakrishna, Arjun and Soares, Gustavo and Gulwani, Sumit},
title = {RUBICON: Rubric-Based Evaluation of Domain-Specific Human AI Conversations},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664778},
doi = {10.1145/3664646.3664778},
abstract = {Evaluating conversational assistants, such as GitHub Copilot Chat, poses a significant challenge for tool builders in the domain of Software Engineering. These assistants rely on language models and chat-based user experiences, rendering their evaluation with respect to the quality of the Human-AI conversations complicated. Existing general-purpose metrics for measuring conversational quality found in literature are inadequate for appraising domain-specific dialogues due to their lack of contextual sensitivity. In this paper, we present RUBICON, a technique for evaluating domain-specific Human-AI conversations. RUBICON leverages large language models to generate candidate rubrics for assessing conversation quality and employs a selection process to choose the subset of rubrics based on their performance in scoring conversations. In our experiments, RUBICON effectively learns to differentiate conversation quality, achieving higher accuracy and yield rates than existing baselines.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {161–169},
numpages = {9},
keywords = {AI-assisted Programming, Conversation Evaluation, Conversational AI, Evaluation Metrics, Human-AI interaction, User Satisfaction},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@proceedings{10.1145/3664934,
title = {ICIEI '24: Proceedings of the 2024 9th International Conference on Information and Education Innovations},
year = {2024},
isbn = {9798400716409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Verbania, Italy}
}

@proceedings{10.1145/3665026,
title = {ICMIP '24: Proceedings of the 2024 9th International Conference on Multimedia and Image Processing},
year = {2024},
isbn = {9798400716164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Osaka, Japan}
}

@proceedings{10.1145/3665065,
title = {ISMSI '24: Proceedings of the 2024 8th International Conference on Intelligent Systems, Metaheuristics \&amp; Swarm Intelligence},
year = {2024},
isbn = {9798400717291},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3665225,
title = {Q-Data '24: Proceedings of the 1st Workshop on Quantum Computing and Quantum-Inspired Technology for Data-Intensive Systems and Applications},
year = {2024},
isbn = {9798400705533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Santiago, AA, Chile}
}

@proceedings{10.1145/3665318,
title = {Web3D '24: Proceedings of the 29th International ACM Conference on 3D Web Technology},
year = {2024},
isbn = {9798400706899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guimar\~{a}es, Portugal}
}

@proceedings{10.1145/3665320,
title = {DigiPro '24: Proceedings of the 2024 Digital Production Symposium},
year = {2024},
isbn = {9798400706905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@proceedings{10.1145/3665348,
title = {GAIIS '24: Proceedings of the 2024 International Conference on Generative Artificial Intelligence and Information Security},
year = {2024},
isbn = {9798400709562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kuala Lumpur, Malaysia}
}

@inproceedings{10.1145/3665348.3665397,
author = {Chang, Yue and Zhao, Xiaolin and Pei, Mingzhe and Liu, Zhenyan},
title = {GUCN: A machine learning model combining time series features for malicious network behavior detection},
year = {2024},
isbn = {9798400709562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665348.3665397},
doi = {10.1145/3665348.3665397},
abstract = {Malicious network behaviors significantly impact network and information security, the intelligent detection of network malicious behavior is an important work in the field of information and privacy protection. Traditional machine learning methods have achieved certain results in solving this problem, but generally ignore the continuous characteristics of malicious network behavior in time series. Aiming at this weakness, this paper proposes a Gated Unit Convolutional Networks (GUCN) model based on gated recurrent unit and convolutional neural network. Meanwhile, it also uses the feature screening method of random forest and the data dimension reduction method of UMAP to process the high dimensional data, which reduces the data redundancy. The results show that the method can effectively detect malicious network behavior, and because it can learn the objective characteristics of behavior in time series, it has the potential to identify malicious attack behavior in advance.},
booktitle = {Proceedings of the 2024 International Conference on Generative Artificial Intelligence and Information Security},
pages = {285–291},
numpages = {7},
location = {Kuala Lumpur, Malaysia},
series = {GAIIS '24}
}

@proceedings{10.1145/3665451,
title = {SecTL '24: Proceedings of the 2nd ACM Workshop on Secure and Trustworthy Deep Learning Systems},
year = {2024},
isbn = {9798400706912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3665689,
title = {BIC '24: Proceedings of the 2024 4th International Conference on Bioinformatics and Intelligent Computing},
year = {2024},
isbn = {9798400716645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@proceedings{10.1145/3665939,
title = {HILDA  24: Proceedings of the 2024 Workshop on Human-In-the-Loop Data Analytics},
year = {2024},
isbn = {9798400706936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Santiago, AA, Chile}
}

@proceedings{10.1145/3666000,
title = {ISSAC '24: Proceedings of the 2024 International Symposium on Symbolic and Algebraic Computation},
year = {2024},
isbn = {9798400706967},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Raleigh, NC, USA}
}

@proceedings{10.1145/3666015,
title = {ICSSP '24: Proceedings of the 2024 International Conference on Software and Systems Processes},
year = {2024},
isbn = {9798400709913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {M\, Germany}
}

@proceedings{10.1145/3666094,
title = {PDC '24: Proceedings of the Participatory Design Conference 2024: Full Papers - Volume 1},
year = {2024},
isbn = {9798400708084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
location = {Sibu, Malaysia}
}

@proceedings{10.1145/3669721,
title = {SIUSAI '24: Proceedings of the 2024 3rd International Symposium on Intelligent Unmanned Systems and Artificial Intelligence},
year = {2024},
isbn = {9798400710025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Qingdao, China}
}

@proceedings{10.1145/3669754,
title = {ICCAI '24: Proceedings of the 2024 10th International Conference on Computing and Artificial Intelligence},
year = {2024},
isbn = {9798400717055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bali Island, Indonesia}
}

@proceedings{10.1145/3669828,
title = {IMIP '24: Proceedings of the 2024 6th International Conference on Intelligent Medicine and Image Processing},
year = {2024},
isbn = {9798400710032},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bali, Indonesia}
}

@proceedings{10.1145/3670013,
title = {IC4E '24: Proceedings of the 2024 15th International Conference on E-Education, E-Business, E-Management and E-Learning},
year = {2024},
isbn = {9798400717062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Fukuoka-shi, Japan}
}

@proceedings{10.1145/3670105,
title = {CNIOT '24: Proceedings of the 2024 5th International Conference on Computing, Networks and Internet of Things},
year = {2024},
isbn = {9798400716751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3670243,
title = {CEEeGov '24: Proceedings of the Central and Eastern European eDem and eGov Days 2024},
year = {2024},
isbn = {9798400717093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Budapest, Hungary}
}

@proceedings{10.1145/3670474,
title = {MLCAD '24: Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD},
year = {2024},
isbn = {9798400706998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Salt Lake City, UT, USA}
}

@inproceedings{10.1145/3670474.3685976,
author = {Francisco, Luis and Arikati, Srini},
title = {LLM Based Physical Verification Runset Generator},
year = {2024},
isbn = {9798400706998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670474.3685976},
doi = {10.1145/3670474.3685976},
abstract = {The complexity in design rule description and coding is drastically increasing as technology nodes advance. This complexity makes the process of implementing the physical verification (PV) rule checks more time-consuming and susceptible to human error, creating the need to explore alternate methods to improve the runset creation process. The work presented proposes a generative AI solution that uses Large Language Models (LLMs) to interpret rule descriptions and generate design rule check decks (runsets) in a language that a PV tool can interpret. The LLM is fine-tuned with existing design rule manuals and runsets. After post-processing the LLM output, the presented solution can generate rules implementation with up to 97\% accuracy. The proposed solution can be used as a runset writer Co-Pilot to help develop the new physical verification runsets.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD},
articleno = {35},
numpages = {7},
keywords = {AI, Design Rule Checking, GenAI, LLMs, Physical Verification, Runset Creation},
location = {Salt Lake City, UT, USA},
series = {MLCAD '24}
}

@proceedings{10.1145/3670653,
title = {MuC '24: Proceedings of Mensch und Computer 2024},
year = {2024},
isbn = {9798400709982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Karlsruhe, Germany}
}

@proceedings{10.1145/3670947,
title = {GI '24: Proceedings of the 50th Graphics Interface Conference},
year = {2024},
isbn = {9798400718281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Halifax, NS, Canada}
}

@inproceedings{10.1145/3670947.3670973,
author = {Johnston, Hannah and Thue, David},
title = {Understanding Visual Artists’ Values and Attitudes towards Collaboration, Technology, and AI},
year = {2024},
isbn = {9798400718281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670947.3670973},
doi = {10.1145/3670947.3670973},
abstract = {Artificial Intelligence (AI) tools have recently gained widespread interest for image creation, but tool developers have largely focused on technical capabilities or specialized domain uses, rather than visual artists as users. We collected survey data from 89 practising visual artists and conducted follow-up interviews with 30 of them, to better understand their diverse needs and values. Through reflexive thematic analysis, we explored visual artists’ attitudes towards collaboration in art creation both with human artists and with AI- and other technology-based support systems. Our results suggest that the focus of popular AI tools on high-quality, finished images does not meet the needs of visual artists. Instead, they wanted reference images, ideation support, and variant exploration. We identified similarities and differences between how visual artists view collaboration with other artists or with machine support, enabling designers of new tools to adopt a more user-centered approach.},
booktitle = {Proceedings of the 50th Graphics Interface Conference},
articleno = {34},
numpages = {9},
keywords = {Artificial Intelligence, Collaboration, Creativity Support Tools, Generative Research, User Experience, Visual Art},
location = {Halifax, NS, Canada},
series = {GI '24}
}

@proceedings{10.1145/3671016,
title = {Internetware '24: Proceedings of the 15th Asia-Pacific Symposium on Internetware},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macau, China}
}

@inproceedings{10.1145/3671016.3671388,
author = {Jiang, Zhonghao and Sun, Weifeng and Gu, Xiaoyan and Wu, Jiaxin and Wen, Tao and Hu, Haibo and Yan, Meng},
title = {DFEPT: Data Flow Embedding for Enhancing Pre-Trained Model Based Vulnerability Detection},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3671388},
doi = {10.1145/3671016.3671388},
abstract = {Software vulnerabilities represent one of the most pressing threats to computing systems. Identifying vulnerabilities in source code is crucial for protecting user privacy and reducing economic losses. Traditional static analysis tools rely on experts with knowledge in security to manually build rules for operation, a process that requires substantial time and manpower costs and also faces challenges in adapting to new vulnerabilities. The emergence of pre-trained code language models has provided a new solution for automated vulnerability detection. However, code pre-training models are typically based on token-level large-scale pre-training, which hampers their ability to effectively capture the structural and dependency relationships among code segments. In the context of software vulnerabilities, certain types of vulnerabilities are related to the dependency relationships within the code. Consequently, identifying and analyzing these vulnerability samples presents a significant challenge for pre-trained models. In this paper, we propose a data flow embedding technique to enhance the performance of pre-trained models in vulnerability detection tasks, named DFEPT, which provides effective vulnerability data flow information to pre-trained models. Specifically, we parse data flow graphs (DFG) from function-level source code, and use the data type of the variable as the node characteristics of the DFG. By applying graph learning techniques, we embed the data flow graph and incorporate relative positional information into the graph embedding using sine positional encoding to ensure the completeness of vulnerability data flow information. Our research shows that DFEPT can provide effective vulnerability semantic information to pre-trained models, achieving an accuracy of 64.97\% on the Devign dataset and an F1-Score of 47.9\% on the Reveal dataset. Compared with the pre-trained model that is only fine-tuned, the performance increases by 1.96\%-17.26\%.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {95–104},
numpages = {10},
location = {Macau, China},
series = {Internetware '24}
}

@inproceedings{10.1145/3671016.3671390,
author = {Feng, Yuhong and Li, Haoran and Cao, Yixuan and Wang, Yufeng and Feng, Haiyue},
title = {CRABS-former: CRoss-Architecture Binary Code Similarity Detection based on Transformer},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3671390},
doi = {10.1145/3671016.3671390},
abstract = {Binary code similarity detection (BCSD) is widely used in software analysis such as vulnerability detection and malware identification. Among various forms of binary representation, assembly is particularly feasible for real-world applications due to its efficient preprocessing compared to graph and intermediate representation (IR). Existing assembly-based methods leverage the text embedding capabilities of pretrained language models such as BERT, which still encounter limitations in cross-architecture BCSD due to the characteristics of assembly code and the lack of cross-architecture vocabulary. In this paper, we first design several normalization strategies to preprocess assembly code from multiple instruction set architectures (ISAs), in order to decrease the token length of assembly code inputs and reduce the size of vocabulary, thereby improving processing efficiency and simplifying model structure. Then, we propose a method to collect token instances and construct a tokenizer capable of processing assembly code from multiple ISAs, enhancing the model’s ability to interpret such code. Based on this tokenizer, we develop a CRoss-Architecture Binary code Similarity detection model based on Transformer (CRABS-former). CRABS-former compares two binary functions from different ISAs, compilers or optimization options and computes their similarity score. Finally, we conduct experiments for two BCSD tasks (one-to-one and one-to-many) using CRABS-former, comparing its performance against four baselines: SAFE, Trex, jTrans, and TE3L. The results indicate that CRABS-former, with a pool size of 10,000, improves recall by 10.85\%, 18.02\%, and 3.33\% across different ISAs, compilers, and optimizations, respectively, underscoring the effectiveness of our approach.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {11–20},
numpages = {10},
keywords = {Binary Analysis, Cross-Architecture, Similarity Detection, Transformer},
location = {Macau, China},
series = {Internetware '24}
}

@inproceedings{10.1145/3671016.3674817,
author = {Chen, Geng and Li, Chenlin and Tyszberowicz, Shmuel and Liu, Zhiming and Liu, Bo},
title = {Mono2MS: Deep Fusion of Multi-Source Features for Partitioning Monolith into Microservices},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3674817},
doi = {10.1145/3671016.3674817},
abstract = {Microservice architecture is favoured for its significant scalability, independent evolution, and advantages in performance elasticity. Partitioning a monolith into microservices has become a pivotal issue in software architecture refactoring. Concurrently, assessing the quality of such partitioning also presents a significant challenge. To address this problem, we propose a solution that (1) proposes a method for extracting and representing the multi-source features such as semantics, functionality, and performance of monolithic systems; (2) designs a deep fusion graph clustering model for partitioning a monolith into microservices intelligently; and (3) establishes a comprehensive set of assessment metrics to quantify the quality of the partitioning suggestion. We conducted experiments and analyses on five benchmark projects. By comparing our approach with six other methods, we have demonstrated the advantages of our methodology. Furthermore, ablating different modules has validated the effectiveness of our proposed monolith features analysis and deep fusion graph clustering model.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {259–268},
numpages = {10},
keywords = {deep graph clustering, graph neural network, microservice, monolith partitioning},
location = {Macau, China},
series = {Internetware '24}
}

@inproceedings{10.1145/3671016.3674819,
author = {Li, Yifan and Shi, Ensheng and Zheng, Dewu and Duan, Kefeng and Chen, Jiachi and Wang, Yanlin},
title = {RepoMinCoder: Improving Repository-Level Code Generation Based on Information Loss Screening},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3674819},
doi = {10.1145/3671016.3674819},
abstract = {Repository-level code generation task involves generating code at a specified location based on unfinished code with repository context. Existing research mainly rely on retrieval-augmented generation methods to complete code. Existing work mainly investigates on improving retrieval results based on the unfinished code, but rarely pays attention to the information loss in the prompt encoding process. In this paper, we propose RepoMinCoder, a novel repository-level code generation framework that adds another round of screening and ranking based on information loss, building upon the canonical retrieval-augmented generation method. Extensive experimental results demonstrate that RepoMinCoder consistently outperforms state-of-the-art methods on public benchmark RepoEval, achieving 3.3\% EM and 2.1\% ES improvement over previous methods. Moreover, we conduct additional experiments to study the effect of various factors in the existing code generation pipeline, including the number of retrieval candidates, the slicing strategy of the retrieval database, and different prompting strategies.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {229–238},
numpages = {10},
keywords = {Code Generation, Large Language Model, Screening and Ranking},
location = {Macau, China},
series = {Internetware '24}
}

@proceedings{10.1145/3671151,
title = {CIBDA '24: Proceedings of the 5th International Conference on Computer Information and Big Data Applications},
year = {2024},
isbn = {9798400718106},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Wuhan, China}
}

@inproceedings{10.1145/3671151.3671268,
author = {Zhou, Zihao and Gu, Degong and Shi, Yuancheng and Zhou, Huaqin and Chen, Kang and Qu, Hongyi and Ren, Hongyan},
title = {Improving Object Detecting by Structuring and Training YOLO Model},
year = {2024},
isbn = {9798400718106},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671151.3671268},
doi = {10.1145/3671151.3671268},
abstract = {Since OpenAI released large language models, artificial intelligence products have been progressing at an astonishing speed in recent years. A text-to-video generative AI model named Sora, which can create videos depicting realistic or imaginative scenes based on textual instructions, demonstrating the potential for simulating the physical world, shocked the world. However, the generated video may have some flaws especially in the relationships between objects. YOLO is one of the most famous objects detecting model in recent years. This paper will evaluate original YOLO model and discuss potential and practical solution for dealing with the shortage and how to improve the accuracy by using our object detecting framework. We first analyze the flaws in object detection in the Sora released video and the potential reasons by reviewing the background technology. Then, we create and trained new model for handling the limitations. Lastly, we will discuss the pros and cons of our model.},
booktitle = {Proceedings of the 5th International Conference on Computer Information and Big Data Applications},
pages = {659–664},
numpages = {6},
location = {Wuhan, China},
series = {CIBDA '24}
}

@article{10.1145/3672089.3672090,
author = {Neumann, Peter G.},
title = {ACM Risks Forum Quarterly Summary},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3672089.3672090},
doi = {10.1145/3672089.3672090},
abstract = {This is an annotated summary of the main items contributed to the ACM Risks Forum in the most recent quarter, orgnanized categorically. References (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Marshall at Newcastle: http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {3–12},
numpages = {10}
}

@proceedings{10.1145/3672121,
title = {CNCIT '24: Proceedings of the 2024 3rd International Conference on Networks, Communications and Information Technology},
year = {2024},
isbn = {9798400717048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xi'an, China}
}

@proceedings{10.1145/3672196,
title = {EMS '24: Proceedings of the 2024 SIGCOMM Workshop on Emerging Multimedia Systems},
year = {2024},
isbn = {9798400707117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3672198,
title = {NAIC '24: Proceedings of the 2024 SIGCOMM Workshop on Networks for AI Computing},
year = {2024},
isbn = {9798400707131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3672202,
title = {ACM SIGCOMM Posters and Demos '24: Proceedings of the ACM SIGCOMM 2024 Conference: Posters and Demos},
year = {2024},
isbn = {9798400707179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@article{10.1145/3672557,
author = {Z\"{u}fle, Andreas and Pfoser, Dieter and Wenk, Carola and Crooks, Andrew and Kavak, Hamdi and Anderson, Taylor and Kim, Joon-Seok and Holt, Nathan and Diantonio, Andrew},
title = {In Silico Human Mobility Data Science: Leveraging Massive Simulated Mobility Data (Vision Paper)},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2374-0353},
url = {https://doi.org/10.1145/3672557},
doi = {10.1145/3672557},
abstract = {Human mobility data science using trajectories or check-ins of individuals has many applications. Recently, we have seen a plethora of research efforts that tackle these applications. However, research progress in this field is limited by a lack of large and representative datasets. The largest and most commonly used dataset of individual human trajectories captures fewer than 200 individuals, while datasets of individual human check-ins capture fewer than 100 check-ins per city per day. Thus, it is not clear if findings from the human mobility data science community would generalize to large populations. Since obtaining massive, representative, and individual-level human mobility data is hard to come by due to privacy considerations, the vision of this work is to embrace the use of data generated by large-scale socially realistic microsimulations. Informed by both real data and leveraging social and behavioral theories, massive spatially explicit microsimulations may allow us to simulate entire megacities at the person level. The simulated worlds, which do not capture any identifiable personal information, allow us to perform “in silico” experiments using the simulated world as a sandbox in which we have perfect information and perfect control without jeopardizing the privacy of any actual individual. In silico experiments have become commonplace in other scientific domains such as chemistry and biology, permitting experiments that foster the understanding of concepts without any harm to individuals. This work describes challenges and opportunities for leveraging massive and realistic simulated alternate worlds for in silico human mobility data science.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = jul,
articleno = {13},
numpages = {27},
keywords = {Spatial simulation, mobility data science, trajectory data, location-based social network data, in silico}
}

@proceedings{10.1145/3672758,
title = {CAICE '24: Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering},
year = {2024},
isbn = {9798400716942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xi' an, China}
}

@proceedings{10.1145/3672919,
title = {CSAIDE '24: Proceedings of the 2024 3rd International Conference on Cyber Security, Artificial Intelligence and Digital Economy},
year = {2024},
isbn = {9798400718212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanjing, China}
}

@proceedings{10.1145/3673038,
title = {ICPP '24: Proceedings of the 53rd International Conference on Parallel Processing},
year = {2024},
isbn = {9798400717932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Gotland, Sweden}
}

@proceedings{10.1145/3673277,
title = {CNSCT '24: Proceedings of the 2024 3rd International Conference on Cryptography, Network Security and Communication Technology},
year = {2024},
isbn = {9798400716959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Harbin, China}
}

@proceedings{10.1145/3673422,
title = {ANRW '24: Proceedings of the 2024 Applied Networking Research Workshop},
year = {2024},
isbn = {9798400707230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, AA, Canada}
}

@proceedings{10.1145/3673805,
title = {ECCE '24: Proceedings of the European Conference on Cognitive Ergonomics 2024},
year = {2024},
isbn = {9798400718243},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Paris, France}
}

@proceedings{10.1145/3673971,
title = {ICMHI '24: Proceedings of the 2024 8th International Conference on Medical and Health Informatics},
year = {2024},
isbn = {9798400716874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3674029,
title = {ICMLT '24: Proceedings of the 2024 9th International Conference on Machine Learning Technologies},
year = {2024},
isbn = {9798400716379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Oslo, Norway}
}

@proceedings{10.1145/3674225,
title = {PEAI '24: Proceedings of the 2024 International Conference on Power Electronics and Artificial Intelligence},
year = {2024},
isbn = {9798400716638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3674399,
title = {ACM-TURC '24: Proceedings of the ACM Turing Award Celebration Conference - China 2024},
year = {2024},
isbn = {9798400710117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Changsha, China}
}

@proceedings{10.1145/3674558,
title = {ICCTA '24: Proceedings of the 2024 10th International Conference on Computer Technology Applications},
year = {2024},
isbn = {9798400716386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@proceedings{10.1145/3674700,
title = {ICCCV '24: Proceedings of the 2024 6th International Conference on Control and Computer Vision},
year = {2024},
isbn = {9798400718045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tianjin, China}
}

@proceedings{10.1145/3674829,
title = {COMPASS '24: Proceedings of the 7th ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies},
year = {2024},
isbn = {9798400710483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New Delhi, India}
}

@proceedings{10.1145/3674912,
title = {CompSysTech '24: Proceedings of the International Conference on Computer Systems and Technologies 2024},
year = {2024},
isbn = {9798400716843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ruse, Bulgaria}
}

@proceedings{10.1145/3675018,
title = {HP3C '24: Proceedings of the 2024 8th International Conference on High Performance Compilation, Computing and Communications},
year = {2024},
isbn = {9798400716904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guangzhou, China}
}

@proceedings{10.1145/3675094,
title = {UbiComp '24: Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to UbiComp/ISWC 2024, the companion program of two premier conferences: The 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp 2024) and the 2024 International Symposium on Wearable Computers (ISWC 2024). UbiComp and ISWC are premier interdisciplinary venues for international researchers, designers, developers, practitioners and educators in the field to present and discuss novel and impactful research in interactive, mobile, wearable and ubiquitous computing. The companion program has traditionally been a very important part of the UbiComp/ISWC conference series.UbiComp/ISWC 2024 is held from October 5 to 9, 2024 in Melbourne, Australia. Originally, UbiComp/ISWC was scheduled to take place in Melbourne in 2021. However, due to the significant impact of COVID-19, our community decided to postpone conferences taking place in their traditional form until last year, when UbiComp took place as an in-person event in Mexico. Now, in 2024 we look to consolidate the strength and ties in our community by having another fully in-person event and hoping to welcome a new generation of researchers to meet and explore the wonderful people that make up our community.},
location = {Melbourne VIC, Australia}
}

@proceedings{10.1145/3675095,
title = {ISWC '24: Proceedings of the 2024 ACM International Symposium on Wearable Computers},
year = {2024},
isbn = {9798400710599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to UbiComp/ISWC 2024, the companion program of two premier conferences: The 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp 2024) and the 2024 International Symposium on Wearable Computers (ISWC 2024). UbiComp and ISWC are premier interdisciplinary venues for international researchers, designers, developers, practitioners and educators in the field to present and discuss novel and impactful research in interactive, mobile, wearable and ubiquitous computing. The companion program has traditionally been a very important part of the UbiComp/ISWC conference series.UbiComp/ISWC 2024 is held from October 5 to 9, 2024 in Melbourne, Australia. Originally, UbiComp/ISWC was scheduled to take place in Melbourne in 2021. However, due to the significant impact of COVID-19, our community decided to postpone conferences taking place in their traditional form until last year, when UbiComp took place as an in-person event in Mexico. Now, in 2024 we look to consolidate the strength and ties in our community by having another fully in-person event and hoping to welcome a new generation of researchers to meet and explore the wonderful people that make up our community.},
location = {Melbourne VIC, Australia}
}

@inproceedings{10.1145/3675095.3676606,
author = {Niwarthana, Amashi and Somarathne, Pamuditha and Qian, Pierre and Yong, Ken-Tye and Withana, Anusha},
title = {Efficient and Robust Heart Rate Estimation Approach for Noisy Wearable PPG Sensors Using Ideal Representation Learning},
year = {2024},
isbn = {9798400710599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675095.3676606},
doi = {10.1145/3675095.3676606},
abstract = {Photoplethysmography (PPG) is a non-invasive wearable sensing method used in millions of devices for heart rate monitoring. However, PPG signals are highly susceptible to a variety of noise sources, including motion artifacts, sensor noise, and biological factors, especially in real-world wearable settings. These make designing generalizable models to accurately interpret cardiac activities challenging. This paper proposes a focus shift from learning with noisy signals to utilizing the characteristics of a mathematically modelled PPG waveform in an adversarial setting to increase the signal-to-noise ratio. The results show the proposed approach is robust against noisy data. We evaluated the model in a user study (N=22), where it was tested against unseen PPG data collected from a new sensor and users under three different activity levels. Results showed the generalisability of the approach compared to the state-of-the-art and it maintains consistent performance improvements across diverse user activities. We successfully implemented our model on a commonly used (android) mobile device, confirming its ability to provide fast inferences in a resource-constrained setting.},
booktitle = {Proceedings of the 2024 ACM International Symposium on Wearable Computers},
pages = {1–8},
numpages = {8},
keywords = {generative adversarial networks (gan), heart rate estimation, photoplethesmography (ppg), representation learning},
location = {Melbourne VIC, Australia},
series = {ISWC '24}
}

@proceedings{10.1145/3675249,
title = {ICCMT '24: Proceedings of the 2024 International Conference on Computer and Multimedia Technology},
year = {2024},
isbn = {9798400718267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanming, China}
}

@proceedings{10.1145/3675417,
title = {DEAI '24: Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence},
year = {2024},
isbn = {9798400717147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hongkong, China}
}

@inproceedings{10.1145/3675417.3675447,
author = {Lv, Han and Wang, Kun},
title = {A study of AIGC-enabled international marketing},
year = {2024},
isbn = {9798400717147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675417.3675447},
doi = {10.1145/3675417.3675447},
abstract = {The intelligent transformation and development of domestic small and medium-sized manufacturing enterprises is of great significance for transferring internal production capacity, enhancing the level of opening up to the outside world, and promoting the high-quality development of the foreign trade industry.The development of AIGC (Artificial Intelligence Generated Content) is not only a major technological breakthrough in the field of artificial intelligence, but also its highly efficient content production drives the generation of new intelligent export methods in the manufacturing industry. Through the mass generation of text, the use of pre-training models as well as the development and optimization of cue words, to the manufacturing and exporting enterprises to output content in line with SEO (Search Engine Optimization) rules, through the optimization of internal and external chains to improve search engine rankings, so as to enhance the overseas visibility of the enterprise's products, and to open up the sales of the products. Based on the mechanism analysis of economics and management perspective, AIGC can provide professional and in-depth website enhancement diagnostic solutions and key optimization for foreign trade enterprises on a regular basis through the revolution of content production, so as to enhance the export efficiency and competitiveness of foreign trade enterprises. Therefore, the application of AIGC technology in foreign trade enterprises should be promoted, and AIGC technology providers should be encouraged to strengthen the training of large models in different industries, so as to provide specialized technical support for export enterprises.},
booktitle = {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence},
pages = {179–191},
numpages = {13},
location = {Hongkong, China},
series = {DEAI '24}
}

@inproceedings{10.1145/3675417.3675557,
author = {Guan, Laide and Wang, Fuchun and Li, Bole and Tang, Rongchai and Wei, Rongjun and Deng, Hong and Tian, Yibin},
title = {Adaptive Automotive Chassis Welding Joint Inspection Using a Cobot and a Multi-modal Vision Sensor: Adaptive welding joint inspection robotic vision system},
year = {2024},
isbn = {9798400717147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675417.3675557},
doi = {10.1145/3675417.3675557},
abstract = {Visual inspection of welding joints has the advantages of being non-contact, low-cost and highly efficient. Automotive chassis welding joints have complex forms, coupled with the increasing demand for flexible manufacturing in factories, their inspection requires more flexibility and affordability. This report proposes an adaptive robotic system for welding joint localization and inspection by placing a baseline-adjustable multi-modal (2D + 3D) vision sensor on a 6-axis cobot mounted under a movable track. The system visually recognizes the model and pose of the chassis on the production belt at a distance, and automatically generates the necessary sensor positions and poses, and trajectories for image acquisition of the welding joints. For each welding joint, the system further manipulates the sensor positions and poses to image it at a close distance from a proper direction. A coarse-to-fine registration of the obtained 3D point clouds is employed to extracts the region of interest in the images and align them to known good welding joint templates using a modified Iterative Closest Point (ICP) algorithm with a conditional similarity metric. Finally, a KD-Tree based method is employed for defect detection using defective point ratio in the point cloud of the identified welding joint region. Experimental results show that the proposed system and methods can efficiently and effectively locate welding joints of different automobile chassis and detects defects on a variety of welding joints, achieving defect detection rate of 99.05\%.},
booktitle = {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence},
pages = {841–849},
numpages = {9},
location = {Hongkong, China},
series = {DEAI '24}
}

@proceedings{10.1145/3675585,
title = {ICEEG '24: Proceedings of the 2024 8th International Conference on E-Commerce, E-Business, and E-Government},
year = {2024},
isbn = {9798400717659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ajman, United Arab Emirates}
}

@proceedings{10.1145/3675669,
title = {MISNC '24: Proceedings of the 2024 11th Multidisciplinary International Social Networks Conference},
year = {2024},
isbn = {9798400717550},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bali, Indonesia}
}

@proceedings{10.1145/3675812,
title = {ICDEL '24: Proceedings of the 2024 9th International Conference on Distance Education and Learning},
year = {2024},
isbn = {9798400716805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guangzhou, China}
}

@proceedings{10.1145/3676288,
title = {SSDBM '24: Proceedings of the 36th International Conference on Scientific and Statistical Database Management},
year = {2024},
isbn = {9798400710209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rennes, France}
}

@proceedings{10.1145/3676581,
title = {CCCAI '24: Proceedings of the 2024 2nd International Conference on Communications, Computing and Artificial Intelligence},
year = {2024},
isbn = {9798400716898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Jeju, Republic of Korea}
}

@article{10.1145/3676886,
author = {Dang, Thin Van and Hao, Duong and Nguyen, Ngan},
title = {Vi-AbSQA: Multi-task Prompt Instruction Tuning Model for Vietnamese Aspect-based Sentiment Quadruple Analysis},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3676886},
doi = {10.1145/3676886},
abstract = {Aspect-based sentiment analysis (ABSA) has recently received considerable attention within the Natural Language Processing (NLP) community, especially for complex tasks like triplet extraction or quadruplet prediction. However, most existing studies focus on high-resource languages. In this paper, we construct a challenging benchmark dataset for Vietnamese Aspect-based Sentiment Quadruple Analysis (AbSQA), where each sentence can contain explicit and implicit aspects and opinion terms. Moreover, each sample includes at least two aspect categories with different sentiments. We release this dataset for free research purposes, believing it will push forward research in this field. In addition, we present a generative-based approach to address the AbSQA task using a multitask instruction prompt tuning framework. Specifically, we design an effective generation paradigm that leverages instruction prompts to provide more information about the task. Besides, our model leverages relational information by designing separate sub-tasks based on the quadruplet elements and fine-tunes the transformer-based pretrained generative models in a multi-task manner. The experimental results demonstrate that our approach outperforms previously established extraction-based and generative-based methods, as well as the baseline variants.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
keywords = {Aspect-based sentiment analysis, multitask prompt tuning model, Methods, Datasets, Vietnamese language}
}

@article{10.1145/3677131,
author = {Fan, Wenfei and Han, Ziyan and Xie, Min and Zhang, Guangyi},
title = {Discovering Top-k Relevant and Diversified Rules},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3677131},
doi = {10.1145/3677131},
abstract = {This paper studies the problem of discovering top-k relevant and diversified rules. Given a real-life dataset, it is to mine a set of k rules that are as close to users' interest as possible, and meanwhile, as diverse to each other as possible. It aims to reduce excessive irrelevant rules commonly returned by rule discovery. As a testbed, we consider Entity Enhancing Rules (REEs), which subsume popular data quality rules as special cases. We train a relevance model to learn users' prior knowledge, rank rules based on users' need, and propose four diversity measures to assess the diversity between rules. Based on these measures, we formulate a new discovery problem. We show that the bi-criteria discovery problem is NP-complete and hard to approximate. This said, we develop a practical algorithm for the problem, and prove its approximation bounds under certain conditions. Moreover, we develop optimization techniques to speed up the process, and parallelize the algorithm such that it guarantees to reduce runtime when given more processors. Using real-life data, we empirically verify that on average, the top-10 REEs discovered by our algorithm is able to catch 77.5\% of errors detected by the entire set Σall of REEs and achieve F_1 = 0.74 for real error detection; moreover, discovering top-ranked REEs is 62.4X faster than mining Σall .},
journal = {Proc. ACM Manag. Data},
month = sep,
articleno = {195},
numpages = {28},
keywords = {diversified, rule discovery, top-k}
}

@proceedings{10.1145/3677182,
title = {ASENS '24: Proceedings of the International Conference on Algorithms, Software Engineering, and Network Security},
year = {2024},
isbn = {9798400709784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanchang, China}
}

@proceedings{10.1145/3677333,
title = {ICPP Workshops '24: Workshop Proceedings of the 53rd International Conference on Parallel Processing},
year = {2024},
isbn = {9798400718021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Gotland, Sweden}
}

@proceedings{10.1145/3677454,
title = {ARAEML '24: Proceedings of the 2024 International Conference on Advanced Robotics, Automation Engineering and Machine Learning},
year = {2024},
isbn = {9798400717116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@proceedings{10.1145/3677525,
title = {GoodIT '24: Proceedings of the 2024 International Conference on Information Technology for Social Good},
year = {2024},
isbn = {9798400710940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bremen, Germany}
}

@proceedings{10.1145/3677619,
title = {WiPSCE '24: Proceedings of the 19th WiPSCE Conference on Primary and Secondary Computing Education Research},
year = {2024},
isbn = {9798400710056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Munich, Germany}
}

@proceedings{10.1145/3677779,
title = {CMNM '24: Proceedings of the International Conference on Modeling, Natural Language Processing and Machine Learning},
year = {2024},
isbn = {9798400709760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xi'an, China}
}

@proceedings{10.1145/3677892,
title = {DSAI '24: Proceedings of the 2024 International Conference on Digital Society and Artificial Intelligence},
year = {2024},
isbn = {9798400709838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Qingdao, China}
}

@inproceedings{10.1145/3677892.3677955,
author = {Lin, Xiushui},
title = {Exploring the Impact of AI Chatbots on Students' Entrepreneurial Intentions: An Empirical Study Using TPB and TAM},
year = {2024},
isbn = {9798400709838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677892.3677955},
doi = {10.1145/3677892.3677955},
abstract = {Artificial Intelligence (AI) chatbots, represented by ChatGPT and others, as cutting-edge examples of the advancement of AI technology, are having a profound impact on all levels of the market, industry and society. As the technology continues to evolve, these intelligences are being applied in different fields: startup environments, and are demonstrating their unique potential and value. This study proposes a comprehensive survey model aimed at exploring the factors influencing exposure to AI chatbots on students' entrepreneurial intentions (EI). Employing a quantitative analysis method, data was collected from 450 respondents who had utilized AI chatbots. Subsequently, the study utilized confirmatory factor analysis (CFA) and structural equation modeling (SEM) techniques for data analysis, facilitating the validation of the proposed hypotheses. Findings indicate that perceived ease of use (PEU) significantly impacts perceived usefulness (PU). Moreover, attitude (ATT) is significantly influenced by both PEU and PU. Furthermore, entrepreneurial intention (EI) is significantly influenced by attitude (ATT), subjective norm (SN), and perceived behavioral control (PBC), with PU having no direct impact. This study contributes to the entrepreneurship literature by addressing this emerging topic.},
booktitle = {Proceedings of the 2024 International Conference on Digital Society and Artificial Intelligence},
pages = {404–414},
numpages = {11},
location = {Qingdao, China},
series = {DSAI '24}
}

@proceedings{10.1145/3678015,
title = {APSys '24: Proceedings of the 15th ACM SIGOPS Asia-Pacific Workshop on Systems},
year = {2024},
isbn = {9798400711053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kyoto, Japan}
}

@proceedings{10.1145/3678232,
title = {PPDP '24: Proceedings of the 26th International Symposium on Principles and Practice of Declarative Programming},
year = {2024},
isbn = {9798400709692},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Milano, Italy}
}

@proceedings{10.1145/3678299,
title = {AM '24: Proceedings of the 19th International Audio Mostly Conference: Explorations in Sonic Cultures},
year = {2024},
isbn = {9798400709685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Milan, Italy}
}

@article{10.1145/3678517,
author = {Aldeen, Mohammed and Young, Jeffrey and Liao, Song and Chang, Tsu-Yao and Cheng, Long and Cai, Haipeng and Luo, Xiapu and Hu, Hongxin},
title = {End-Users Know Best: Identifying Undesired Behavior of Alexa Skills Through User Review Analysis},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3678517},
doi = {10.1145/3678517},
abstract = {The Amazon Alexa marketplace has grown rapidly in recent years due to third-party developers creating large amounts of content and publishing directly to a skills store. Despite the growth of the Amazon Alexa skills store, there have been several reported security and usability concerns, which may not be identified during the vetting phase. However, user reviews can offer valuable insights into the security \&amp; privacy, quality, and usability of the skills. To better understand the effects of these problematic skills on end-users, we introduce ReviewTracker, a tool capable of discerning and classifying semantically negative user reviews to identify likely malicious, policy violating, or malfunctioning behavior on Alexa skills. ReviewTracker employs a pre-trained FastText classifier to identify different undesired skill behaviors. We collected over 700,000 user reviews spanning 6 years with more than 200,000 negative sentiment reviews. ReviewTracker was able to identify 17,820 reviews reporting violations related to Alexa policy requirements across 2,813 skills, and 131,855 reviews highlighting different types of user frustrations associated with 9,294 skills. In addition, we developed a dynamic skill testing framework using ChatGPT to conduct two distinct types of tests on Alexa skills: one using a software-based simulation for interaction to explore the actual behaviors of skills and another through actual voice commands to understand the potential factors causing discrepancies between intended skill functionalities and user experiences. Based on the number of the undesired skill behavior reviews, we tested the top identified problematic skills and detected more than 228 skills violating at least one policy requirement. Our results demonstrate that user reviews could serve as a valuable means to identify undesired skill behaviors.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {89},
numpages = {28}
}

@article{10.1145/3678579,
author = {Lin, Yong-Han and Su, Li-Ting and Chen, Uei-Dar and Lee, Yi-Chi and Wang, Peng-Jui and Chang, Yung-Ju},
title = {Pinning, Sorting, and Categorizing Notifications: A Mixed-methods Usage and Experience Study of Mobile Notification-management Features},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3678579},
doi = {10.1145/3678579},
abstract = {As smartphone notifications increase, so does the effort required to handle them effectively. Previous research has proposed various notification management features, but empirical evidence regarding their efficacy remains sparse. In response, we developed a notification management application incorporating features derived from prior studies, including both automatic and manual sorting, categorization, and manual pinning. Utilizing a mixed-methods approach, we explored how users interact with these features in their daily routines, with the aim to identify the underlying needs driving their usage. The results indicate that pinning was the most valued feature, serving diverse purposes such as deferring notifications, ensuring quick and constant access to information, preventing accidental deletions, and providing visual reminders. Conversely, manual categorization was underutilized, with participants relying on automated categories for notification access. Moreover, participants expressed a desire for automatic features to process and organize notifications based on topic and personalize them through user input. They also expected automatic sorting to adapt more dynamically to user contexts.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {115},
numpages = {27},
keywords = {categorizing, notification management, pinning, smartphone notifications, sorting}
}

@article{10.1145/3678585,
author = {Gao, Yi and Xiao, Kaijie and Li, Fu and Xu, Weifeng and Huang, Jiaming and Dong, Wei},
title = {ChatIoT: Zero-code Generation of Trigger-action Based IoT Programs},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3678585},
doi = {10.1145/3678585},
abstract = {Trigger-Action Program (TAP) is a simple but powerful format to realize intelligent IoT applications, especially in home automation scenarios. Existing trace-driven approaches and in-situ programming approaches depend on either customized interaction commands or well-labeled datasets, resulting in limited applicable scenarios. In this paper, we propose ChatIoT, a zero-code TAP generation system based on large language models (LLMs). With a novel context-aware compressive prompting scheme, ChatIoT is able to automatically generate TAPs from user requests in a token-efficient manner and deploy them to the TAP runtime. Further, for those TAP requests including unknown sensing abilities, ChatIoT can also generate new AI models with knowledge distillation by multimodal LLMs, with a novel model customization method based on deep reinforcement learning. We implemented ChatIoT and evaluated its performance extensively. Results show that ChatIoT can reduce token consumption by 26.1-84.9\% and improve TAP generation accuracy by 4.2-65.5\% compared to state-of-the-art approaches in multiple settings. We also conducted a real user study, and ChatIoT can achieve 91.57\% TAP generation accuracy.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {103},
numpages = {29},
keywords = {Home automation, Internet of Things, LLMs, Zero-code TAP generation}
}

@article{10.1145/3678591,
author = {Fernandes, Glenn J. and Zheng, Jiayi and Pedram, Mahdi and Romano, Christopher and Shahabi, Farzad and Rothrock, Blaine and Cohen, Thomas and Zhu, Helen and Butani, Tanmeet S. and Hester, Josiah and Katsaggelos, Aggelos K. and Alshurafa, Nabil},
title = {HabitSense: A Privacy-Aware, AI-Enhanced Multimodal Wearable Platform for mHealth Applications},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3678591},
doi = {10.1145/3678591},
abstract = {Wearable cameras provide an objective method to visually confirm and automate the detection of health-risk behaviors such as smoking and overeating, which is critical for developing and testing adaptive treatment interventions. Despite the potential of wearable camera systems, adoption is hindered by inadequate clinician input in the design, user privacy concerns, and user burden. To address these barriers, we introduced HabitSense, an open-source1, multi-modal neck-worn platform developed with input from focus groups with clinicians (N=36) and user feedback from in-wild studies involving 105 participants over 35 days. Optimized for monitoring health-risk behaviors, the platform utilizes RGB, thermal, and inertial measurement unit sensors to detect eating and smoking events in real time. In a 7-day study involving 15 participants, HabitSense recorded 768 hours of footage, capturing 420.91 minutes of hand-to-mouth gestures associated with eating and smoking data crucial for training machine learning models, achieving a 92\% F1-score in gesture recognition. To address privacy concerns, the platform records only during likely health-risk behavior events using SECURE, a smart activation algorithm. Additionally, HabitSense employs on-device obfuscation algorithms that selectively obfuscate the background during recording, maintaining individual privacy while leaving gestures related to health-risk behaviors unobfuscated. Our implementation of SECURE has resulted in a 48\% reduction in storage needs and a 30\% increase in battery life. This paper highlights the critical roles of clinician feedback, extensive field testing, and privacy-enhancing algorithms in developing an unobtrusive, lightweight, and reproducible wearable system that is both feasible and acceptable for monitoring health-risk behaviors in real-world settings.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {101},
numpages = {48},
keywords = {camera, eating, machine learning, multimodal, privacy, smoking, thermal, vision transformers, wearable}
}

@proceedings{10.1145/3678719,
title = {A-TEST 2024: Proceedings of the 15th ACM International Workshop on Automating Test Case Design, Selection and Evaluation},
year = {2024},
isbn = {9798400711091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 15th ACM International Workshop on Automating Test Case Design, Selection and Evaluation (A-TEST), co-located with ECOOP and ISSTA 2024, in Vienna, on 19th of September, 2024. This year's theme of the workshop is "Using DSLs for testing and the testing of DSLs", right at intersection of the topics of the two main conferences. A-TEST'24 received six submissions, three of which were accepted for presentation. All papers were reviewed by three program committee members. Next to the regular session with papers the workshop features a hands-on session, titled "Testing DSLs with DSLs in Rascal and TESTAR", showcasing state-of-the-art automated testing techniques applied to a DSL implementation in the context of the Rascal language workbench. In particular, it highlights how a (formal) model of a DSL can function as an oracle for regular acceptance tests, and as a driver from scriptless UI testing, using tools like TESTAR.},
location = {Vienna, Austria}
}

@proceedings{10.1145/3678726,
title = {ICEMT '24: Proceedings of the 2024 8th International Conference on Education and Multimedia Technology},
year = {2024},
isbn = {9798400717611},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3678890,
title = {RAID '24: Proceedings of the 27th International Symposium on Research in Attacks, Intrusions and Defenses},
year = {2024},
isbn = {9798400709593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Padua, Italy}
}

@inproceedings{10.1145/3678890.3678930,
author = {Paladini, Tommaso and Ferro, Lara and Polino, Mario and Zanero, Stefano and Carminati, Michele},
title = {You Might Have Known It Earlier: Analyzing the Role of Underground Forums in Threat Intelligence},
year = {2024},
isbn = {9798400709593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678890.3678930},
doi = {10.1145/3678890.3678930},
abstract = {This paper analyzes 88 million hacker forum posts of a publicly available dataset and 75,000 online articles over a 20-year timespan, studying the potential of hacker forums as a proactive Cyber Threat Intelligence (CTI) source. Using a custom Natural Language Processing pipeline with fine-tuned BERT-based models, we extract named entities from forum posts and reports and cross-reference their date of occurrence over different periods. Our analysis reveals that discussions on hacker forums precede official security reports for over 60\% of the identified entities in 20 years of data. This highlights the relevance of these platforms as early indicators of cyber threats. However, our longitudinal analysis shows that such a trend has been constantly decreasing since 2012: forum discussions no longer consistently anticipate threats discussed in cybersecurity reports, possibly due to increased scrutiny or the emergence of alternative channels. This suggests that the CTI community should adapt by identifying and monitoring new platforms where threat actors congregate. Despite not being as thriving as in the first decade of 2000, underground communities are still releasing novel malware and showing interest in discussing malware employed in real cyberattacks. Our results highlight the value of hacker forums as early threat indicators and the importance of proactively monitoring them for potential cyberattack detection. This approach addresses the research gap that predominantly focuses on traditional cybersecurity reports.},
booktitle = {Proceedings of the 27th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {368–383},
numpages = {16},
keywords = {Cyber Threat Intelligence, Hacker Forums, Longitudinal Analysis, Named Entity Recognition, Natural Language Processing},
location = {Padua, Italy},
series = {RAID '24}
}

@proceedings{10.1145/3679409,
title = {ISCER '24: Proceedings of the 2024 3rd International Symposium on Control Engineering and Robotics},
year = {2024},
isbn = {9798400709951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Changsha, China}
}

@proceedings{10.1145/3681716,
title = {Mindtrek '24: Proceedings of the 27th International Academic Mindtrek Conference},
year = {2024},
isbn = {9798400718236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tampere, Finland}
}

@proceedings{10.1145/3685088,
title = {ICSCIS '24: Proceedings of the 2024 International Conference on Smart City and Information System},
year = {2024},
isbn = {9798400710155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kuala Lumpur, Malaysia}
}

@proceedings{10.1145/3685650,
title = {DocEng '24: Proceedings of the ACM Symposium on Document Engineering 2024},
year = {2024},
isbn = {9798400711695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Jose, CA, USA}
}

@proceedings{10.1145/3685767,
title = {CTCNet '24: Proceedings of the 2024 Asia Pacific Conference on Computing Technologies, Communications and Networking},
year = {2024},
isbn = {9798400709609},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chengdu, China}
}

@proceedings{10.1145/3686038,
title = {TAS '24: Proceedings of the Second International Symposium on Trustworthy Autonomous Systems},
year = {2024},
isbn = {9798400709890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@proceedings{10.1145/3686424,
title = {EDCS '24: Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Education Digitalization and Computer Science},
year = {2024},
isbn = {9798400710360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shenzhen, China}
}

@proceedings{10.1145/3686540,
title = {BDSIC '24: Proceedings of the 2024 6th International Conference on Big-data Service and Intelligent Computation},
year = {2024},
isbn = {9798400718069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hong Kong, Hong Kong}
}

@proceedings{10.1145/3686812,
title = {ICCMS '24: Proceedings of the 2024 16th International Conference on Computer Modeling and Simulation},
year = {2024},
isbn = {9798400717215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Dalian, China}
}

@proceedings{10.1145/3687311,
title = {IECT '24: Proceedings of the 2024 International Conference on Intelligent Education and Computer Technology},
year = {2024},
isbn = {9798400709920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guilin, China}
}

@proceedings{10.1145/3688351,
title = {SYSTOR '24: Proceedings of the 17th ACM International Systems and Storage Conference},
year = {2024},
isbn = {9798400711817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Like most things in Israel SYSTOR 2024 was impacted significantly by the regional conflict in the Middle East. Originally, we had hoped to still have an in-person conference. However, we eventually had to change course into a virtual conference. Fortunately, with the solidarity of our community this year actually saw a rise in submissions with a total of 38 submitted papers.},
location = {Virtual, Israel}
}

@article{10.1145/3688401,
author = {Goel, Diksha and Ward, Max and Neumann, Aneta and Neumann, Frank and Nguyen, Hung and Guo, Mingyu},
title = {Hardening Active Directory Graphs via Evolutionary Diversity Optimization based Policies},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3688401},
doi = {10.1145/3688401},
abstract = {Active Directory (AD) is the default security management system for Windows domain networks. An AD environment can be described as a cyber-attack graph, with nodes representing computers, accounts, etc., and edges indicating existing accesses or known exploits that enable attackers to move from one node to another. This paper explores a Stackelberg game model between one attacker and one defender on an AD attack graph. The attacker’s goal is to maximize their chances of successfully reaching the destination before getting detected. The defender’s aim is to block a constant number of edges to minimize the attacker’s chance of success. The paper shows that the problem is #P-hard and, therefore, intractable to solve exactly. To defend the AD graph from cyber attackers, this paper proposes two defensive approaches. In the first approach, we convert the attacker’s problem to an exponential sized Dynamic Program that is approximated by a Neural Network (NN). Once trained, the NN serves as an efficient fitness function for defender’s Evolutionary Diversity Optimization based defensive policy. The diversity emphasis on the defender’s solution provides a diverse set of training samples, improving the training accuracy of our NN for modeling the attacker. In the second approach, we propose a RL based policy to solve the attacker’s problem and Critic network assisted Evolutionary Diversity Optimization based defensive policy to solve defender’s problem. Experimental results on synthetic AD graphs show that the proposed defensive policies are scalable, highly effective, approximate attacker’s problem accurately, and generate good defensive plans.},
note = {Just Accepted},
journal = {ACM Trans. Evol. Learn. Optim.},
month = aug,
keywords = {Attack graph, active directory, reinforcement learning, evolutionary diversity optimization}
}

@proceedings{10.1145/3688574,
title = {BDE '24: Proceedings of the 2024 6th International Conference on Big Data Engineering},
year = {2024},
isbn = {9798400717857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xining, China}
}

@proceedings{10.1145/3688636,
title = {ICCBN '24: Proceedings of the 2024 12th International Conference on Communications and Broadband Networking},
year = {2024},
isbn = {9798400717109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nyingchi, China}
}

@proceedings{10.1145/3689218,
title = {PRIS '24: Proceedings of the 2024 6th International Conference on Pattern Recognition and Intelligent Systems},
year = {2024},
isbn = {9798400718250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hong Kong, Hong Kong}
}

@proceedings{10.1145/3689299,
title = {RAIIE '24: Proceedings of the 2024 3rd International Symposium on Robotics, Artificial Intelligence and Information Engineering},
year = {2024},
isbn = {9798400718311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@article{10.1145/3689719,
author = {Zhang, Chengyu and Su, Zhendong},
title = {SMT2Test: From SMT Formulas to Effective Test Cases},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689719},
doi = {10.1145/3689719},
abstract = {One of the primary challenges in software testing is generating high-quality test inputs and obtaining corresponding test oracles. This paper introduces a novel methodology to mitigate this challenge in testing program verifiers by employing SMT (Satisfiability Modulo Theories) formulas as a universal test case generator. The key idea is to transform SMT formulas into programs and link the satisfiability of the formulas with the safety property of the programs, allowing the satisfiability of the formulas to act as a test oracle for program verifiers. This method was implemented as a framework named SMT2Test, which enables the transformation of SMT formulas into Dafny and C programs. An intermediate representation was designed to augment the flexibility of this framework, streamlining the transformation for other programming languages and fostering modular transformation strategies. We evaluated the effectiveness of SMT2Test by finding defects in two program verifiers: the Dafny verifier and CPAchecker. Utilizing the SMT2Test framework with the SMT formulas from the SMT competition and SMT solver fuzzers, we discovered and reported a total of 14 previously unknown defects in these program verifiers that were not found by previous methods. After reporting, all of them have been confirmed, and 6 defects have been fixed. These findings show the effectiveness of our method and imply its potential application in testing other programming language infrastructures.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {279},
numpages = {24},
keywords = {Program Verification, SMT Solving, Software Testing}
}

@article{10.1145/3689728,
author = {Blinn, Andrew and Li, Xiang and Kim, June Hyung and Omar, Cyrus},
title = {Statically Contextualizing Large Language Models with Typed Holes},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689728},
doi = {10.1145/3689728},
abstract = {Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate code context, particularly when working with definitions that are neither in the training data nor near the cursor. This paper demonstrates that tighter integration with the type and binding structure of the programming language in use, as exposed by its language server, can help address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server is able to identify the type and typing context of the hole that the programmer is filling, with Hazel's total syntax and type error correction ensuring that a meaningful program sketch is available whenever the developer requests a completion. This allows the system to prompt the LLM with codebase-wide contextual information that is not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server, which provides error localization and error messages. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications with accompanying unit tests that have been written from scratch to avoid data contamination, and that can easily be ported to new languages because they do not have large external library dependencies. These applications serve as challenge problems due to their extensive reliance on application-specific data structures. Through an ablation study, we examine the impact of contextualization with type definitions, function headers, and errors messages, individually and in combination. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel, a low-resource language, we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource mainstream languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {288},
numpages = {31},
keywords = {Large Language Models, Program Repair, Program Synthesis}
}

@article{10.1145/3689736,
author = {Yang, Chenyuan and Deng, Yinlin and Lu, Runyu and Yao, Jiayi and Liu, Jiawei and Jabbarvand, Reyhaneh and Zhang, Lingming},
title = {WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689736},
doi = {10.1145/3689736},
abstract = {Compiler correctness is crucial, as miscompilation can falsify program behaviors, leading to serious consequences over the software supply chain. In the literature, fuzzing has been extensively studied to uncover compiler defects. However, compiler fuzzing remains challenging: Existing arts focus on black- and grey-box fuzzing, which generates test programs without sufficient understanding of internal compiler behaviors. As such, they often fail to construct test programs to exercise intricate optimizations. Meanwhile, traditional white-box techniques, such as symbolic execution, are computationally inapplicable to the giant codebase of compiler systems. Recent advances demonstrate that Large Language Models (LLMs) excel in code generation/understanding tasks and even have achieved state-of-the-art performance in black-box fuzzing. Nonetheless, guiding LLMs with compiler source-code information remains a missing piece of research in compiler testing. To this end, we propose WhiteFox, the first white-box compiler fuzzer using LLMs with source-code information to test compiler optimization, with a spotlight on detecting deep logic bugs in the emerging deep learning (DL) compilers. WhiteFox adopts a multi-agent framework: (i) an LLM-based analysis agent examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization; (ii) an LLM-based generation agent produces test programs based on the summarized requirements. Additionally, optimization-triggering tests are also used as feedback to further enhance the test generation prompt on the fly. Our evaluation on the three most popular DL compilers (i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows that WhiteFox can generate high-quality test programs to exercise deep optimizations requiring intricate conditions, practicing up to 8 times more optimizations than state-of-the-art fuzzers. To date, WhiteFox has found in total 101 bugs for the compilers under test, with 92 confirmed as previously unknown and 70 already fixed. Notably, WhiteFox has been recently acknowledged by the PyTorch team, and is in the process of being incorporated into its development workflow. Finally, beyond DL compilers, WhiteFox can also be adapted for compilers in different domains, such as LLVM, where WhiteFox has already found multiple bugs.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {296},
numpages = {27},
keywords = {Code Analysis, Fuzzing, Large Language Models, White-box Testing}
}

@article{10.1145/3689770,
author = {Ma, Cong and Ge, Zhaoyi and Lee, Edward and Zhang, Yizhou},
title = {Lexical Effect Handlers, Directly},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689770},
doi = {10.1145/3689770},
abstract = {Lexically scoping effect handlers is a language-design idea that equips algebraic effects with a modular semantics: it enables local-reasoning principles without giving up on the control-flow expressiveness that makes effect handlers powerful. However, we observe that existing implementations risk incurring costs akin to the run-time search for dynamically scoped handlers. This paper presents a compilation strategy for lexical effect handlers, adhering to the lexical scoping principle and targeting a language with low-level control over stack layout. Key aspects of this approach are formalized and proven correct. We embody the ideas in a language called Lexa: the Lexa compiler translates high-level effect handling to low-level stack switching. We evaluate the Lexa compiler on a set of benchmarks; the results suggest that it generates efficient code, reducing running-time complexity from quadratic to linear in some cases.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {330},
numpages = {29},
keywords = {Algebraic effects, Lexa, Salt, compiler correctness, continuations, effect handlers, stack switching}
}

@article{10.1145/3689789,
author = {Mariano, Benjamin and Wang, Ziteng and Pailoor, Shankara and Collberg, Christian and Dillig, I\c{s}il},
title = {Control-Flow Deobfuscation using Trace-Informed Compositional Program Synthesis},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689789},
doi = {10.1145/3689789},
abstract = {Code deobfuscation, which attempts to simplify code that has been intentionally obfuscated to prevent understanding, is a critical technique for downstream security analysis tasks like malware detection. While there has been significant prior work on code deobfuscation, most techniques either do not handle control flow obfuscations that modify control flow or they target specific classes of control flow obfuscations, making them unsuitable for handling new types of obfuscations or combinations of existing ones. In this paper, we study a new deobfuscation technique that is based on program synthesis and that can handle a broad class of control flow obfuscations. Given an obfuscated program P, our approach aims to synthesize a smallest program that is a control-flow reduction of P and that is semantically equivalent. Since our method does not assume knowledge about the types of obfuscations that have been applied to the original program, the underlying synthesis problem ends up being very challenging. To address this challenge, we propose a novel trace-informed compositional synthesis algorithm that leverages hints present in dynamic traces of the obfuscated program to decompose the synthesis problem into a set of simpler subproblems. In particular, we show how dynamic traces can be useful for inferring a suitable control-flow skeleton of the deobfuscated program and performing independent synthesis of each basic block. We have implemented this approach in a tool called Chisel and evaluate it on 546 benchmarks that have been obfuscated using combinations of six different obfuscation techniques. Our evaluation shows that our approach is effective and that it produces code that is almost identical (modulo variable renaming) to the original (non-obfuscated) program in 86\% of cases. Our evaluation also shows that Chisel significantly outperforms existing techniques.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {349},
numpages = {31},
keywords = {Deobfuscation, Obfuscation, Program Synthesis}
}

@article{10.1145/3690656,
author = {Zhao, Yuhan and Chen, Rui and Lai, Riwei and Han, Qilong and Song, Hongtao and Chen, Li},
title = {Denoising and Augmented Negative Sampling for Collaborative Filtering},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3690656},
doi = {10.1145/3690656},
abstract = {Negative sampling plays a crucial role in implicit-feedback-based collaborative filtering, where it leverages massive unlabeled data to generate negative signals for guiding supervised learning. The current state-of-the-art approaches focus on utilizing hard negative samples that contain more information to establish a better decision boundary. To strike a balance between efficiency and effectiveness, most existing methods adopt a two-pass approach: in the first pass, a fixed number of unobserved items are sampled using a simple static distribution, while, in the second pass, a more sophisticated negative sampling strategy is employed to select the final negative items. However, selecting negative samples solely from the original items in a dataset is inherently restricted due to the limited available choices, and thus may not be able to effectively contrast positive samples. In this paper, we empirically validate this observation through meticulously designed experiments and identify three major limitations of existing solutions: ambiguous trap, information discrimination, and false negative samples. Our response to such limitations is to introduce “denoised” and “augmented” negative samples that may not exist in the original dataset. This direction renders a few substantial technical challenges. First, constructing augmented negative samples may introduce excessive noise that eventually distorts the decision boundary. Second, the scarcity of supervision signals hampers the denoising process. To this end, we introduce a novel generic denoising and augmented negative sampling (DANS) paradigm and provide a concrete instantiation. First, we disentangle the hard and easy factors of negative items. Then, we regulate the augmentation of easy factors by carefully considering the direction and magnitude. Next, we propose a reverse attention mechanism to learn a user’s negative preference, which allows us to perform a dimension-level denoising procedure on hard factors. Finally, we design an advanced negative sampling strategy to identify the final negative samples, taking into account both the score function used in existing methods and a novel metric called synthesization gain. Through extensive experiments on real-world datasets, we demonstrate that our method substantially outperforms state-of-the-art baselines. Our code is publicly available at https://github.com/Asa9aoTK/ANS-Recbole.},
note = {Just Accepted},
journal = {ACM Trans. Recomm. Syst.},
month = aug,
keywords = {Collaborative filtering, augmented negative sampling, disentanglement learning}
}

@proceedings{10.1145/3690931,
title = {AIAHPC '24: Proceedings of the 2024 4th International Conference on Artificial Intelligence, Automation and High Performance Computing},
year = {2024},
isbn = {9798400710049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Zhuhai, China}
}

@proceedings{10.1145/3691016,
title = {IPICE '24: Proceedings of the 2024 International Conference on Image Processing, Intelligent Control and Computer Engineering},
year = {2024},
isbn = {9798400710285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Qingdao, China}
}

@proceedings{10.1145/3691573,
title = {SVR '24: Proceedings of the 26th Symposium on Virtual and Augmented Reality},
year = {2024},
isbn = {9798400709791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Manaus, Brazil}
}

@inproceedings{10.1145/3691573.3691581,
author = {Oliveira, Luis Ilderlandio da Silva and Nunes, Rubens Fernandes and Vidal, Creto Augusto and Cavalcante-Neto, Joaquim Bento},
title = {Generalization of Real-Time Motion Control with DRL Using Conditional Rewards and Symmetry Constraints},
year = {2024},
isbn = {9798400709791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691573.3691581},
doi = {10.1145/3691573.3691581},
abstract = {Deep Reinforcement Learning has been increasingly explored as a method for generating physics-based motions in articulated characters. However, effective control tools are still necessary to better guide the learning process and provide animators with greater control and reliability over the resulting animations. This paper proposes new control tools, including the generalization of real-time control, conditional rewards, symmetry constraints, and a user interface. Real-time control allows dynamic adjustment of chosen parameters, conditional rewards simplify the competition between rewards, symmetry constraints reduce uncoordinated movements, and the user interface facilitates training and animation parameter specification. The proposed control tools show promise in improving the quality and control of physics-based character animation.},
booktitle = {Proceedings of the 26th Symposium on Virtual and Augmented Reality},
pages = {103–112},
numpages = {10},
keywords = {Deep Reinforcement Learning, Motion Control, Physics-Based Character Animation, Real-Time Control.},
location = {Manaus, Brazil},
series = {SVR '24}
}

@article{10.1145/3698108,
author = {Kosan, Mert and Huang, Zexi and Medya, Sourav and Ranu, Sayan and Singh, Ambuj},
title = {GCFExplainer: Global Counterfactual Explainer for Graph Neural Networks},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3698108},
doi = {10.1145/3698108},
abstract = {Graph neural networks (GNNs) find applications in various domains such as computational biology, natural language processing, and computer security. Owing to their popularity, there is an increasing need to explain GNN predictions since GNNs are black-box machine learning models. One way to address this issue involves using counterfactual reasoning where the objective is to alter the GNN prediction by minimal changes in the input graph. Existing methods for counterfactual explanation of GNNs are limited to instance-specific local reasoning. This approach has two major limitations of not being able to offer global recourse policies and overloading human cognitive ability with too much information. In this work, we study the global explainability of GNNs through global counterfactual reasoning. Specifically, we want to find a small set of representative counterfactual graphs that explains all input graphs. Towards this goal, we propose GCFExplainer, a novel algorithm powered by vertex-reinforced random walks on an edit map of graphs with a greedy summary. Extensive experiments on real graph datasets show that the global explanation from GCFExplainer provides important high-level insights of the model behavior and achieves a 46.9\% gain in recourse coverage, a 9.5\% reduction in recourse cost compared to the state-of-the-art local counterfactual explainers. We also demonstrate that GCFExplainer generates explanations that are more consistent with input dataset characteristics, and is robust under adversarial attacks. In addition, K-GCFExplainer, which incorporates a graph clustering component into GCFExplainer, is introduced as a more competitive extension for datasets with a clustering structure, leading to superior performance in three out of four datasets in the experiments and better scalability.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
keywords = {Counterfactual explanation, Graph neural networks}
}

@article{10.1145/3699518,
author = {Yang, Chuanpeng and Zhu, Yao and Lu, Wang and Wang, Yidong and Chen, Qian and Gao, Chenlong and Yan, Bingjie and Chen, Yiqiang},
title = {Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3699518},
doi = {10.1145/3699518},
abstract = {Large Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs pose considerable challenges for practical deployment, particularly in environments with limited resources. The endeavor to compress language models while maintaining their accuracy has become a focal point of research. Among the various methods, knowledge distillation has emerged as an effective technique to enhance inference speed without greatly compromising performance. This paper presents a thorough survey from three aspects: method, evaluation, and application, exploring knowledge distillation techniques tailored specifically for LLMs. Specifically, we divide the methods into white-box KD and black-box KD to better illustrate their differences. Furthermore, we also explored the evaluation tasks and distillation effects between different distillation methods, and proposed directions for future research. Through in-depth understanding of the latest advancements and practical applications, this survey provides valuable resources for researchers, paving the way for sustained progress in this field.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
keywords = {Knowledge Distillation, Large Language Models, Evaluation}
}

@article{10.14778/3681954.3681973,
author = {Sun, Yushi and Xin, Hao and Sun, Kai and Xu, Yifan Ethan and Yang, Xiao and Dong, Xin Luna and Tang, Nan and Chen, Lei},
title = {Are Large Language Models a Good Replacement of Taxonomies?},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681973},
doi = {10.14778/3681954.3681973},
abstract = {Large language models (LLMs) demonstrate an impressive ability to internalize knowledge and answer natural language questions. Although previous studies validate that LLMs perform well on general knowledge while presenting poor performance on long-tail nuanced knowledge, the community is still doubtful about whether the traditional knowledge graphs should be replaced by LLMs. In this paper, we ask if the schema of knowledge graph (i.e., taxonomy) is made obsolete by LLMs. Intuitively, LLMs should perform well on common taxonomies and at taxonomy levels that are common to people. Unfortunately, there lacks a comprehensive benchmark that evaluates the LLMs over a wide range of taxonomies from common to specialized domains and at levels from root to leaf so that we can draw a confident conclusion. To narrow the research gap, we constructed a novel taxonomy hierarchical structure discovery benchmark named TaxoGlimpse to evaluate the performance of LLMs over taxonomies. TaxoGlimpse covers ten representative taxonomies from common to specialized domains with in-depth experiments of different levels of entities in this taxonomy from root to leaf. Our comprehensive experiments of eighteen LLMs under three prompting settings validate that LLMs perform miserably poorly in handling specialized taxonomies and leaf-level entities. Specifically, the QA accuracy of the best LLM drops by up to 30\% as we go from common to specialized domains and from root to leaf levels of taxonomies.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2919–2932},
numpages = {14}
}

@article{10.14778/3681954.3682014,
author = {Erfanian, Mahdi and Jagadish, H. V. and Asudeh, Abolfazl},
title = {Chameleon: Foundation Models for Fairness-Aware Multi-Modal Data Augmentation to Enhance Coverage of Minorities},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3682014},
doi = {10.14778/3681954.3682014},
abstract = {Potential harms from the under-representation of minorities in data, particularly in multi-modal settings, is a well-recognized concern. While there has been extensive effort in detecting such under-representation, resolution has remained a challenge.With recent generative AI advancements, large language and foundation models have emerged as versatile tools across various domains. In this paper, we propose Chameleon, a system that efficiently utilizes these tools to augment a dataset with minimal addition of synthetically generated tuples to enhance the coverage of the under-represented groups. Our system applies quality and outlier-detection tests to ensure the quality and semantic integrity of the generated tuples. In order to minimize the rejection chance of the generated tuples, we propose multiple strategies to provide a guide for the foundation model. Our experiment results, in addition to confirming the efficiency of our proposed algorithms, illustrate our approach's effectiveness, as the model's unfairness in a downstream task significantly dropped after data repair using Chameleon.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3470–3483},
numpages = {14}
}

@article{10.14778/3685800.3685830,
author = {Wu, Joshua and Tang, Dixin and Chalapathi, Nithin and Chambers, Tristan and Ciccolini, Julie and Phillips, Cheryl and Pickoff-White, Lisa and Parameswaran, Aditya},
title = {Dealing with Acronyms, Abbreviations, and Typos in Real-World Entity Matching},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685830},
doi = {10.14778/3685800.3685830},
abstract = {String matching is at the core of data cleaning, record matching, and information retrieval. String matching relies on a similarity measure that evaluates the similarity of two strings, regarding the two as a match if their similarity is larger than a user-defined threshold. In our collaboration with journalists and public defenders, we found that real-world datasets, such as police rosters that journalists and public defenders work with, often contain acronyms, abbreviations, and typos, thanks to errors during manual entry, into, say, a spreadsheet or a form. Unfortunately, traditional similarity measures lead to low accuracy since they do not consider all three aspects together. Some recent work proposes leveraging synonym rules to improve matching, but either requires these rules to be provided upfront, or generated prior to matching, which leads to low accuracy in our setting and similar ones. To address these limitations, we propose Smash, a simple yet effective measure to assess the similarity of two strings with acronyms, abbreviations, and typos, all without relying on synonym rules. We design a dynamic programming algorithm to efficiently compute this measure, along with two optimizations that improve accuracy. We show that compared to the best baselines, including one based on ChatGPT with GPT-4, Smash improves the max and mean F-score by 23.5\% and 110.8\%, respectively. We implement Smash in OpenRefine, a graphical data cleaning tool, to facilitate its use by journalists, public defenders, and other non-programmers for data cleaning.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4104–4116},
numpages = {13}
}

@article{10.14778/3685800.3685832,
author = {Wang, Qinlong and Lan, Tingfeng and Tang, Yinghao and Sang, Bo and Huang, Ziling and Du, Yiheng and Zhang, Haitao and Sha, Jian and Lu, Hui and Zhou, Yuanchun and Zhang, Ke and Tang, Mingjie},
title = {DLRover-RM: Resource Optimization for Deep Recommendation Models Training in the Cloud},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685832},
doi = {10.14778/3685800.3685832},
abstract = {Deep learning recommendation models (DLRM) rely on large embedding tables to manage categorical sparse features. Expanding such embedding tables can significantly enhance model performance, but at the cost of increased GPU/CPU/memory usage. Meanwhile, tech companies have built extensive cloud-based services to accelerate training DLRM models at scale. In this paper, we conduct a deep investigation of the DLRM training platforms at AntGroup and reveal two critical challenges: low resource utilization due to suboptimal configurations by users and the tendency to encounter abnormalities due to an unstable cloud environment. To overcome them, we introduce DLRover, an elastic training framework for DLRMs designed to increase resource utilization and handle the instability of a cloud environment. DLRover develops a resource-performance model by considering the unique characteristics of DLRMs and a three-stage heuristic strategy to automatically allocate and dynamically adjust resources for DLRM training jobs for higher resource utilization. Further, DLRover develops multiple mechanisms to ensure efficient and reliable execution of DLRM training jobs. Our extensive evaluation shows that DLRover reduces job completion times by 31\%, increases the job completion rate by 6\%, enhances CPU usage by 15\%, and improves memory utilization by 20\%, compared to state-of-the-art resource scheduling frameworks. DLRover has been widely deployed at AntGroup and processes thousands of DLRM training jobs on a daily basis. DLRover is open-sourced and has been adopted by 10+ companies.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4130–4144},
numpages = {15}
}

@article{10.14778/3685800.3685835,
author = {Shankar, Shreya and Li, Haotian and Asawa, Parth and Hulsebos, Madelon and Lin, Yiming and Zamfirescu-Pereira, J. D. and Chase, Harrison and Fu-Hinthorn, Will and Parameswaran, Aditya G. and Wu, Eugene},
title = {spade: Synthesizing Data Quality Assertions for Large Language Model Pipelines},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685835},
doi = {10.14778/3685800.3685835},
abstract = {Large language models (LLMs) are being increasingly deployed as part of pipelines that repeatedly process or generate data of some sort. However, a common barrier to deployment are the frequent and often unpredictable errors that plague LLMs. Acknowledging the inevitability of these errors, we propose data quality assertions to identify when LLMs may be making mistakes. We present spade, a method for automatically synthesizing data quality assertions that identify bad LLM outputs. We make the observation that developers often identify data quality issues during prototyping prior to deployment, and attempt to address them by adding instructions to the LLM prompt over time. spade therefore analyzes histories of prompt versions over time to create candidate assertion functions and then selects a minimal set that fulfills both coverage and accuracy requirements. In testing across nine different real-world LLM pipelines, spade efficiently reduces the number of assertions by 14\% and decreases false failures by 21\% when compared to simpler baselines. spade has been deployed as an offering within LangSmith, LangChain's LLM pipeline hub, and has been used to generate data quality assertions for over 2000 pipelines across a spectrum of industries.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4173–4186},
numpages = {14}
}

@article{10.14778/3685800.3685895,
author = {Zhu, Zeqi and Fan, Zeheng and Zeng, Yuxiang and Shi, Yexuan and Xu, Yi and Zhou, Mengmeng and Dong, Jin},
title = {FedSQ: A Secure System for Federated Vector Similarity Queries},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685895},
doi = {10.14778/3685800.3685895},
abstract = {Vector databases have emerged as crucial tools for managing and retrieving representation embeddings of unstructured data. Given the explosive growth of data, vector data is often distributed and stored across multiple organizations. However, privacy concerns and regulations like GDPR present new challenges in collaborative and secure queries, also known as federated queries, over those vector data distributed across various data owners. Although existing research has attempted to enable such query services for low-dimensional data, such as relational and spatial data, these solutions can be inefficient in answering vector similarity queries involving high-dimensional data. Therefore, we are motivated to develop a new prototype system called FedSQ that (1) ensures privacy protection across data owners and (2) balances query efficiency and result accuracy when processing federated vector similarity queries. To achieve these goals, FedSQ utilizes advanced secure multi-party computation techniques to prevent information leakage during query processing and incorporates indexing and sampling based optimizations to strike a proper performance balance.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4441–4444},
numpages = {4}
}

@proceedings{10.5555/3694718,
title = {JCDL '23: Proceedings of the 2023 ACM/IEEE Joint Conference on Digital Libraries},
year = {2024},
isbn = {9798350399318},
publisher = {IEEE Press},
location = {Santa Fe, New Mexico, USA}
}

