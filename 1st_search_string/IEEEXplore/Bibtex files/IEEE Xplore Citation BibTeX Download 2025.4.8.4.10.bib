@INPROCEEDINGS{9975409,
  author={Fei, Jianwei and Xia, Zhihua and Tondi, Benedetta and Barni, Mauro},
  booktitle={2022 IEEE International Workshop on Information Forensics and Security (WIFS)}, 
  title={Supervised GAN Watermarking for Intellectual Property Protection}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={We propose a watermarking method for protecting the Intellectual Property (IP) of Generative Adversarial Networks (GANs). The aim is to watermark the GAN model so that any image generated by the GAN contains an invisible watermark (signature), whose presence inside the image can be checked at a later stage for ownership verification. To achieve this goal, a pre-trained CNN watermarking decoding block is inserted at the output of the generator. The generator loss is then modified by including a watermark loss term, to ensure that the prescribed watermark can be extracted from the generated images. The watermark is embedded via fine-tuning, with reduced time complexity. Results show that our method can effectively embed an invisible watermark inside the generated images. Moreover, our method is a general one and can work with different GAN architectures, different tasks, and different resolutions of the output image. We also demonstrate the good robustness performance of the embedded watermark against several post-processing, among them, JPEG compression, noise addition, blurring, and color transformations.},
  keywords={Training;Visualization;Transform coding;Watermarking;Intellectual property;Computer architecture;Generative adversarial networks;Intellectual Property Protection;Generative Adversarial Networks;DNN Watermarking;Security of Deep Learning},
  doi={10.1109/WIFS55849.2022.9975409},
  ISSN={2157-4774},
  month={Dec}
}

@INPROCEEDINGS{10434754,
  author={Singh, Sushma and Singh, Anushka},
  booktitle={2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)}, 
  title={Intellectual Property Rights and Artificial Intelligence: Contemporary Convergence and Probable Challenges}, 
  year={2023},
  volume={10},
  number={},
  pages={1279-1286},
  abstract={Artificial intelligence (AI) has ushered in an era of unparalleled innovation, reshaping industries and societal norms. As the capabilities of AI-driven technology are advanced and expanded, a complex interplay between Intellectual Property (IP) rights and these emergent technologies has emerged, giving rise to a landscape rich with opportunities and challenges. This paper aims to comprehensively explore the intricate nexus between IP rights and the domains of artificial intelligence with special reference to patentability of the same. It will explore the evolving definitions of authorship and ownership, discuss the applicability of patent law to inventions produced by AI systems, dissect the conundrum of copyright protection for AI-generated content, analyze the role of trademarks in a digitally driven world, examine strategies for safeguarding trade secrets in the hyper-connected environment, and address the ethical considerations intrinsic to IP rights within the AI milieu. In doing so, it will provide insights into the mechanisms and adaptations necessary to uphold the tenets of IP protection while fostering innovation.},
  keywords={Industries;Technological innovation;Ethics;Intellectual property;Trademarks;Artificial intelligence;Patent law;Artificial Intelligence;Innovation;Intellectual Property Strategies;Machine Learning;Human-AI Collaborations},
  doi={10.1109/UPCON59197.2023.10434754},
  ISSN={2687-7767},
  month={Dec}
}

@INPROCEEDINGS{9577609,
  author={Ong, Ding Sheng and Seng Chan, Chee and Ng, Kam Woh and Fan, Lixin and Yang, Qiang},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Protecting Intellectual Property of Generative Adversarial Networks from Ambiguity Attacks}, 
  year={2021},
  volume={},
  number={},
  pages={3629-3638},
  abstract={Ever since Machine Learning as a Service emerges as a viable business that utilizes deep learning models to generate lucrative revenue, Intellectual Property Right (IPR) has become a major concern because these deep learning models can easily be replicated, shared, and re-distributed by any unauthorized third parties. To the best of our knowledge, one of the prominent deep learning models - Generative Adversarial Networks (GANs) which has been widely used to create photorealistic image are totally unprotected despite the existence of pioneering IPR protection methodology for Convolutional Neural Networks (CNNs). This paper therefore presents a complete protection framework in both black-box and white-box settings to enforce IPR protection on GANs. Empirically, we show that the proposed method does not compromise the original GANs performance (i.e. image generation, image super-resolution, style transfer), and at the same time, it is able to withstand both removal and ambiguity attacks against embedded watermarks. Codes are available at https://github.com/dingsheng-ong/ipr-gan.},
  keywords={Deep learning;Knowledge engineering;Computer vision;Image synthesis;Superresolution;Intellectual property;Watermarking},
  doi={10.1109/CVPR46437.2021.00363},
  ISSN={2575-7075},
  month={June}
}

@ARTICLE{10937062,
  author={Xu, Tianhua and Zhong, Sheng-hua and Zhang, Zhi and Liu, Yan},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Intellectual Property Protection for Deep Models: Pioneering Cross-Domain Fingerprinting Solutions}, 
  year={2025},
  volume={20},
  number={},
  pages={3587-3600},
  abstract={The high cost of developing high-performance deep models highlights their value as intellectual property for creators. However, it is important to consider the potential risks of theft. Although various techniques have been developed to protect the intellectual property of deep models, there is still room for improvement in terms of efficiency, comprehensiveness, and generalization. Compared with the intrusiveness of watermarking methods, fingerprinting methods do not affect the training process of the source model. Consequently, this paper proposes a fingerprinting method to address the paucity of attempts in fingerprinting methods for model protection. Our method consists of two efficient algorithms for generating fingerprinting samples, where the first one possesses the advantage of efficiency, while the second one is better in terms of robustness. The first algorithm takes a comprehensive approach to modeling the fingerprint of the deep model. The generated samples are distributed within the stable region and near the decision boundary of the model, taking into account both the duality and the conviction factors. Then, a heuristic sample perturbation algorithm is introduced, which generates a fingerprint with solid stability and generalization across multiple domains. The two algorithms proposed in this paper have been shown to be capable of withstanding attacks on intellectual property removal, detection, and evasion. They also show some advantages in terms of efficiency. In addition, the proposed method is the first to apply fingerprinting techniques in a cross-domain context.},
  keywords={Fingerprint recognition;Brain modeling;Watermarking;Protection;IP networks;Computational modeling;Context modeling;Adaptation models;Intellectual property;Training;Model protection;intellectual property;model fingerprint},
  doi={10.1109/TIFS.2025.3552175},
  ISSN={1556-6021},
  month={}
}

@INPROCEEDINGS{10545822,
  author={Yang, Nan and Jia, Wenjun and Deng, Peng and Yang, Wen},
  booktitle={2024 IEEE 2nd International Conference on Control, Electronics and Computer Technology (ICCECT)}, 
  title={Invisible Fingerprint-based Copyright Protection for Generated Medical Image}, 
  year={2024},
  volume={},
  number={},
  pages={960-964},
  abstract={In the recent years, the development of generative models has stimulated the health care progress, specially medical image generation. The synthetic medical images can be applied to several fields and have many utilization, such as data aug- mentation for model training. However, this might lead to the significant copyright issues. Firstly, the data for model training may be the privacy data belong to individuals. Secondly, owners of well-trained model may have burden computation cost. Both of these two stakeholders are not will to share model to others without any compensation or the risk of the image abuse. In order to protect the copyright of the generative model for medical image and reduce the risk of the image abuse, we propose Copyright Protection mechanism for Generated Medical Image (CP4GMI) by inducing invisible fingerprint. In other words, the generated medical image should have the unique fingerprint belong to the corresponding generative model, which can be examined when image abuse happens. Numerous experiments conducted on different medical image datasets shows that our approach can track the generative model in a high-accuracy level when the fingerprint in the generated image is mostly invisible. From the perspective of users and model inventors, we propose a copyright protection mechanism that introduces invisible fingerprints, which can be printed on the generated medical images, and can be used to protect the rights when the copyright is threatened.},
  keywords={Training;Image synthesis;Computational modeling;Medical services;Fingerprint recognition;Copyright protection;Data models;Copyright protection;medical image;image stenography},
  doi={10.1109/ICCECT60629.2024.10545822},
  ISSN={},
  month={April}
}

@INPROCEEDINGS{10890047,
  author={Lai, Qiran and Bors, Adrian G.},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Localised Frequency Latent Domain Watermarking of DDIM Generated Images}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Stable Diffusion models, relying on iterative generative latent diffusion processes, have recently achieved remarkable results in producing realistic and diverse images. Meanwhile, the widespread application of generative models raised significant concerns about the origins of image content or the infringement of intellectual property rights. Consequently, a method for identifying AI generated images and/or other information about their origins is imperatively necessary. To address these requirements we propose to embed watermarks during one of the diffusion iterative steps of the DDIM. Such watermarks are required to be recoverable while also robust to possible changes to the generated watermarked images. The watermarks are embedded in the localized regions of the latent space frequencies. The binary watermarks are detected from the generated watermarked images by means of a CNN watermark detector. The robustness of the CNN watermark detector is improved through training by considering various distortions to the watermarked images.},
  keywords={Training;Frequency-domain analysis;Diffusion processes;Watermarking;Detectors;Intellectual property;Robustness;Iterative methods;Speech processing;Protection;Image Generation;Digital Watermarking;Denoising Diffusion Implicit Model;Copyright protection},
  doi={10.1109/ICASSP49660.2025.10890047},
  ISSN={2379-190X},
  month={April}
}

@ARTICLE{10737446,
  author={Wang, Shen and Dong, Jialiang and Wu, Longfei and Guan, Zhitao},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={WEDA: Exploring Copyright Protection for Large Language Model Downstream Alignment}, 
  year={2024},
  volume={32},
  number={},
  pages={4755-4767},
  abstract={Large Language Models (LLMs) have shown incomparable representation and generalization capabilities, which have led to significant advancements in Natural Language Processing (NLP). Before deployment, the pre-trained LLMs often need to be tailored to specific downstream tasks for improved performance, which is commonly referred to as downstream alignment. This is a costly effort considering the needed manpower, training resources, and downstream-specific data. While much attention has been paid to protecting the copyright of the models themselves, the copyright protection of LLM alignment has been largely overlooked. In this paper, we present Watermark Embedding for Downstream Alignment (WEDA) scheme, which can provide effective copyright protection for two popular LLM alignment techniques parameter-efficient fine-tuning (PEFT) and in-context learning (ICL). For alignment through PEFT, we propose a Chain of Thought (CoT) based solution to embed watermarks into the PEFT weights. Furthermore, we extend this solution to safeguard alignment through ICL by utilizing the prefix-integrated CoT to watermark examples embedded within ICL prompts. We conduct an extensive experimental evaluation to demonstrate the effectiveness of our proposed scheme.},
  keywords={Watermarking;Adaptation models;Computational modeling;Training;Intellectual property;Context modeling;Cognition;Speech processing;Costs;Privacy;Copyright protection;watermark embedding;large language model;task alignment},
  doi={10.1109/TASLP.2024.3487419},
  ISSN={2329-9304},
  month={}
}

@INPROCEEDINGS{10556135,
  author={Zhang, Dawen and Xia, Boming and Liu, Yue and Xu, Xiwei and Hoang, Thong and Xing, Zhenchang and Staples, Mark and Lu, Qinghua and Zhu, Liming},
  booktitle={2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN)}, 
  title={Privacy and Copyright Protection in Generative AI: A Lifecycle Perspective}, 
  year={2024},
  volume={},
  number={},
  pages={92-97},
  abstract={The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.CCS CONCEPTS• Software and its engineering Software architectures; • Information systems World Wide Web; • Security and privacy Privacy protections; • Social and professional topics Copyrights; • Computing methodologies Machine learning.},
  keywords={Training;Data privacy;Technological innovation;Generative AI;Software architecture;Copyright protection;Software;Privacy;Copyrights;Generative AI;Data Lifecycle;Software Architecture;Software Engineering for AI},
  doi={},
  ISSN={},
  month={April}
}

@INPROCEEDINGS{10942607,
  author={Zhang, Ruisi and Koushanfar, Farinaz},
  booktitle={2024 58th Asilomar Conference on Signals, Systems, and Computers}, 
  title={Watermarking Large Language Models and the Generated Content: Opportunities and Challenges}, 
  year={2024},
  volume={},
  number={},
  pages={1779-1786},
  abstract={The widely adopted and powerful generative large language models (LLMs) have raised concerns about intellectual property rights violations and the spread of machine-generated misinformation. Watermarking serves as a promising approch to establish ownership, prevent unauthorized use, and trace the origins of LLM -generated content. This paper summarizes and shares the challenges and opportunities we found when watermarking LLMs. We begin by introducing techniques for wa-termarking LLMs themselves under different threat models and scenarios. Next, we investigate watermarking methods designed for the content generated by LLMs, assessing their effectiveness and resilience against various attacks. We also highlight the importance of watermarking domain-specific models and data, such as those used in code generation, chip design, and medical applications. Furthermore, we explore methods like hardware acceleration to improve the efficiency of the watermarking process. Finally, we discuss the limitations of current approaches and outline future research directions for the responsible use and protection of these generative AI tools.},
  keywords={Threat modeling;Generative AI;Shape;Large language models;Watermarking;Intellectual property;Medical services;Protection;Fake news;Resilience;Watermarking;Large Language Models;Hard-ware Security},
  doi={10.1109/IEEECONF60004.2024.10942607},
  ISSN={2576-2303},
  month={Oct}
}

@INPROCEEDINGS{10374953,
  author={Fei, Jianwei and Xia, Zhihua and Tondi, Benedetta and Barni, Mauro},
  booktitle={2023 IEEE International Workshop on Information Forensics and Security (WIFS)}, 
  title={Robust Retraining-free GAN Fingerprinting via Personalized Normalization}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In recent years, there has been significant growth in the commercial applications of generative models, licensed and distributed by model developers to users, who in turn use them to offer services. In this scenario, there is a need to track and identify the responsible user in the presence of a violation of the license agreement or any kind of malicious usage. Although there are methods enabling Generative Adversarial Networks (GANs) to include invisible watermarks in the images they produce, generating a model with a different watermark, referred to as a fingerprint, for each user is time- and resource-consuming due to the need to retrain the model to include the desired fingerprint. In this paper, we propose a retraining-free GAN fingerprinting method that allows model developers to easily generate model copies with the same functionality but different fingerprints. The generator is modified by inserting additional Personalized Normalization (PN) layers whose parameters (scaling and bias) are generated by two dedicated shallow networks (ParamGen Nets) taking the fingerprint as input. A watermark decoder is trained simultaneously to extract the fingerprint from the generated images. The proposed method can embed different fingerprints inside the GAN by just changing the input of the ParamGen Nets and performing a feedforward pass, without finetuning or retraining. The performance of the proposed method in terms of robustness against both model-level and image-level attacks is also superior to the state-of-the-art.},
  keywords={Forensics;Watermarking;Intellectual property;Fingerprint recognition;Licenses;Generative adversarial networks;Robustness;IPR Protection;DNN watermarking;GAN fingerprinting;Box-free Watermarking;Security of Deep Learning},
  doi={10.1109/WIFS58808.2023.10374953},
  ISSN={2157-4774},
  month={Dec}
}

@INPROCEEDINGS{10376108,
  author={Deng, Haiyu and Wang, Xu and Yu, Guangsheng and Dang, Xiaocui and Liu, Ren Ping},
  booktitle={2023 22nd International Symposium on Communications and Information Technologies (ISCIT)}, 
  title={A Novel Weights-less Watermark Embedding Method for Neural Network Models}, 
  year={2023},
  volume={},
  number={},
  pages={25-30},
  abstract={Deep learning-based Artificial Intelligence (AI) technology has been extensively used recently. AI model theft is a regular occurrence. As a result, many academics focus their efforts on safeguarding the Intellectual Property (IP) of trained Neural Network (NN) models. The majority of the most recent white-box setting watermark embedding methods rely on modifying model weights. Weights updated for the NN model during training must take into account the initial task as well as the embedding of watermarks. As a result, the accuracy of the initial task will be affected, necessitating more training time. This research proposes a novel weights-less watermark embedding method for deep neural networks to address this issue. Without actually embedding the watermark within the NN model weights, it uses a principle of code matching between the watermark and the weights. The proposed method requires less time than existing white-box setting watermark embedding methods, and the accuracy of the original task is not much diminished. Additionally, since the NN model weights are left alone, their statistical distribution will remain unchanged, giving the model increased resistance to watermark detection. The experiments in this paper demonstrate the effectiveness, efficiency, and robustness of our method.},
  keywords={Training;Resistance;Statistical distributions;Watermarking;Artificial neural networks;Intellectual property;Robustness;Watermark Embedding;Deep Learning;Neural Networks;Intellectual Property},
  doi={10.1109/ISCIT57293.2023.10376108},
  ISSN={2643-6175},
  month={Oct}
}

@INPROCEEDINGS{10548878,
  author={Li, Zongjie and Wang, Chaozheng and Ma, Pingchuan and Liu, Chaowei and Wang, Shuai and Wu, Daoyuan and Gao, Cuiyun and Liu, Yang},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)}, 
  title={On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study}, 
  year={2024},
  volume={},
  number={},
  pages={893-905},
  abstract={Recent advances in large language models (LLMs) significantly boost their usage in software engineering. However, training a well-performing LLM demands a substantial workforce for data collection and annotation. Moreover, training datasets may be proprietary or partially open, and the process often requires a costly GPU cluster. The intellectual property value of commercial LLMs makes them attractive targets for imitation attacks, but creating an imitation model with comparable parameters still incurs high costs. This motivates us to explore a practical and novel direction: slicing commercial black-box LLMs using medium-sized backbone models. In this paper, we explore the feasibility of launching imitation attacks on LLMs to extract their specialized code abilities, such as “code synthesis” and “code translation:’ We systematically investigate the effectiveness of launching code ability extraction attacks under different code-related tasks with multiple query schemes, including zero-shot, in-context, and Chain-of-Thought. We also design response checks to refine the outputs, leading to an effective imitation training process. Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs. We summarize our findings and insights to help researchers better understand the threats posed by imitation attacks, including revealing a practical attack surface for generating adversarial code examples against LLMs.},
  keywords={Training;Codes;Costs;Annotations;Graphics processing units;Closed box;Intellectual property;Large Language Models;Imitation Attacks},
  doi={10.1145/3597503.3639091},
  ISSN={1558-1225},
  month={April}
}

@INPROCEEDINGS{10735575,
  author={Jin, Heng and Zhang, Chaoyu and Shi, Shanghao and Lou, Wenjing and Hou, Y. Thomas},
  booktitle={2024 IEEE Conference on Communications and Network Security (CNS)}, 
  title={ProFLingo: A Fingerprinting-based Intellectual Property Protection Scheme for Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={Large language models (LLMs) have attracted significant attention in recent years. Due to their "Large" nature, training LLMs from scratch consumes immense computational resources. Since several major players in the artificial intelligence (AI) field have open-sourced their original LLMs, an increasing number of individuals and smaller companies are able to build derivative LLMs based on these open-sourced models at much lower costs. However, this practice opens up possibilities for unauthorized use or reproduction that may not comply with licensing agreements, and fine-tuning can change the model’s behavior, thus complicating the determination of model ownership. Current intellectual property (IP) protection schemes for LLMs are either designed for white-box settings or require additional modifications to the original model, which restricts their use in real-world settings.In this paper, we propose ProFLingo, a black-box fingerprinting-based IP protection scheme for LLMs. ProFLingo generates queries that elicit specific responses from an original model, thereby establishing unique fingerprints. Our scheme assesses the effectiveness of these queries on a suspect model to determine whether it has been derived from the original model. ProFLingo offers a non-invasive approach, which neither requires knowledge of the suspect model nor modifications to the base model or its training process. To the best of our knowledge, our method represents the first black-box fingerprinting technique for IP protection for LLMs. Our source code and generated queries are available at: https://github.com/hengvt/ProFLingo.},
  keywords={Training;Costs;Large language models;Source coding;Closed box;Intellectual property;Fingerprint recognition;Network security;Protection;Glass box},
  doi={10.1109/CNS62487.2024.10735575},
  ISSN={2994-5895},
  month={Sep.}
}

@INPROCEEDINGS{10554711,
  author={Pinto, Gustavo and de Souza, Cleidson R. B. and Neto, João Batista and de Souza, Alberto and Gotto, Tarcísio and Monteiro, Edward},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)}, 
  title={Lessons from Building StackSpot Al: A Contextualized AI Coding Assistant}, 
  year={2024},
  volume={},
  number={},
  pages={408-417},
  abstract={With their exceptional natural language processing capabilities, tools based on Large Language Models (LLMs) like ChatGPT and Co-Pilot have swiftly become indispensable resources in the software developer's toolkit. While recent studies suggest the potential productivity gains these tools can unlock, users still encounter drawbacks, such as generic or incorrect answers. Additionally, the pursuit of improved responses often leads to extensive prompt engineering efforts, diverting valuable time from writing code that delivers actual value. To address these challenges, a new breed of tools, built atop LLMs, is emerging. These tools aim to mitigate drawbacks by employing techniques like fine-tuning or enriching user prompts with contextualized information. In this paper, we delve into the lessons learned by a software development team venturing into the creation of such a contextualized LLM-based application, using retrieval-based techniques, called StackSpot Al. Over a four-month period, the team, despite lacking prior professional experience in LLM-based applications, built the product from scratch. Following the initial product release, we engaged with the development team responsible for the code generative components. Through interviews and analysis of the application's issue tracker, we uncover various intriguing challenges that teams working on LLM-based applications might encounter. For instance, we found three main group of lessons: LLM-based lessons, User-based lessons, and Technical lessons. By understanding these lessons, software development teams could become better prepared to build LLM-based applications.},
  keywords={Productivity;Codes;Writing;Chatbots;Software;Encoding;Teamwork;LLM;LLM-based applications;LLM for code;LLM4code;Code LLMs;Challenges},
  doi={10.1145/3639477.3639751},
  ISSN={2832-7659},
  month={April}
}

@ARTICLE{9724248,
  author={Liu, Gaoyang and Xu, Tianlong and Ma, Xiaoqiang and Wang, Chen},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Your Model Trains on My Data? Protecting Intellectual Property of Training Data via Membership Fingerprint Authentication}, 
  year={2022},
  volume={17},
  number={},
  pages={1024-1037},
  abstract={In recent years, data has become the new oil that fuels various machine learning (ML) applications. Just as the oil refining, providing data to an ML model is a product of massive costs and expertise efforts. However, how to protect the intellectual property (IP) of the training data in ML remains largely open. In this paper, we present MeFA, a novel framework for detecting training data IP embezzlement via Membership Fingerprint Authentication, which is able to determine whether a suspect ML model is trained on the to be protected target data or not. The key observation is that a part of data has a similar influence on the prediction behavior of different ML models. On this basis, MeFA leverages membership inference techniques to extract these data as the fingerprints of the target data and constructs an authentication model to verify the data’s ownership by identifying the obtained membership fingerprints. MeFA has several salient features. It does not assume any knowledge of the suspect model except for its black-box prediction API, through which we can merely get the prediction output of a given input, and also does not require any modification to the dataset or the training process, since it takes advantage of the inherent membership property of the data. As a by-product, MeFA can also serve as a post-protection to verify the ownership of ML models, without modifying the training process of the model. Extensive experiments on three realistic datasets and seven types of ML models validate the effectiveness of MeFA, and demonstrate that it is also robust to scenarios when the training data is partially used or preprocessed with representative membership inference defenses.},
  keywords={Data models;Predictive models;Training;Computational modeling;Training data;Watermarking;Authentication;Training data authentication;intellectual property protection;membership inference attack;membership fingerprint;machine learning model},
  doi={10.1109/TIFS.2022.3155921},
  ISSN={1556-6021},
  month={}
}

@BOOK{10769228,
  author={Kwatra, Shikhar and Kaushik, Bunny},
  booktitle={Generative AI with Amazon Bedrock: Build, scale, and secure generative AI applications using Amazon Bedrock},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Become proficient in Amazon Bedrock by taking a hands-on approach to building and scaling generative AI solutions that are robust, secure, and compliant with ethical standardsKey FeaturesLearn the foundations of Amazon Bedrock from experienced AWS Machine Learning Specialist ArchitectsMaster the core techniques to develop and deploy several AI applications at scaleGo beyond writing good prompting techniques and secure scalable frameworks by using advanced tips and tricksPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe concept of generative artificial intelligence has garnered widespread interest, with industries looking to leverage it to innovate and solve business problems. Amazon Bedrock, along with LangChain, simplifies the building and scaling of generative AI applications without needing to manage the infrastructure. Generative AI with Amazon Bedrock takes a practical approach to enabling you to accelerate the development and integration of several generative AI use cases in a seamless manner. You’ll explore techniques such as prompt engineering, retrieval augmentation, fine-tuning generative models, and orchestrating tasks using agents. The chapters take you through real-world scenarios and use cases such as text generation and summarization, image and code generation, and the creation of virtual assistants. The latter part of the book shows you how to effectively monitor and ensure security and privacy in Amazon Bedrock. By the end of this book, you’ll have gained a solid understanding of building and scaling generative AI apps using Amazon Bedrock, along with various architecture patterns and security best practices that will help you solve business problems and drive innovation in your organization.What you will learnExplore the generative AI landscape and foundation models in Amazon BedrockFine-tune generative models to improve their performanceExplore several architecture patterns for different business use casesGain insights into ethical AI practices, model governance, and risk mitigation strategiesEnhance your skills in employing agents to develop intelligence and orchestrate tasksMonitor and understand metrics and Amazon Bedrock model responseExplore various industrial use cases and architectures to solve real-world business problems using RAGStay on top of architectural best practices and industry standardsWho this book is forThis book is for generalist application engineers, solution engineers and architects, technical managers, ML advocates, data engineers, and data scientists looking to either innovate within their organization or solve business use cases using generative AI. A basic understanding of AWS APIs and core AWS services for machine learning is expected.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781804618585},
  url={https://ieeexplore.ieee.org/document/10769228}
}

@ARTICLE{10838598,
  author={Fan, Yixin and Wu, Jun},
  journal={IEEE Transactions on Cognitive Communications and Networking}, 
  title={GAI-AntiCopy: Infrequent Transformation Aided Accuracy-Consistent Copyright Protection for Generative AI Instructions in NGN}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Generative artificial intelligence (GAI) brings an unprecedented revolution to the next-generation networks (NGN) from resource allocation to network traffic monitoring. With its powerful creative content generation capabilities, GAI significantly enhances the interaction and quality of customized services in NGN. Currently, benefiting from the thriving GAI services, it is possible to build personalized GAIs through designing GAI instructions without the need for training models from scratch. Meanwhile, infringements like pirating are emerging, necessitating effective copyright protection schemes. However, current schemes suffer from an unacceptable decrease in task processing accuracy when applied to GAIs, and the success rate of watermarking is extremely low on GAI instructions. Therefore, we propose an infrequent transformation aided accuracy-consistent copyright protection scheme for GAI instructions. We first build a comprehensive GAI instruction copyright protection system for NGN, designing a complete watermarking and verification mechanism. Additionally, we integrate copyright watermark messages with the syntactic features of GAI instructions to select the embedding positions. Watermarks are embedded through emphasis and passivization, which are infrequent transformations that minimize semantic distortion. Finally, we conduct experiments on real GAI instructions datasets and compare our scheme with existing works to demonstrate that ours effectively realizes accuracy-consistent copyright protection for GAI instructions in NGN.},
  keywords={Watermarking;Copyright protection;Next generation networking;Semantics;Artificial intelligence;Steganography;Training;Protection;Linguistics;Accuracy;Copyright protection;generative AI;nextgeneration network;sentence transformation},
  doi={10.1109/TCCN.2025.3528893},
  ISSN={2332-7731},
  month={}
}

@ARTICLE{10843740,
  author={Pang, Kaiyi and Qi, Tao and Wu, Chuhan and Bai, Minhao and Jiang, Minghu and Huang, Yongfeng},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={ModelShield: Adaptive and Robust Watermark Against Model Extraction Attack}, 
  year={2025},
  volume={20},
  number={},
  pages={1767-1782},
  abstract={Large language models (LLMs) demonstrate general intelligence across a variety of machine learning tasks, thereby enhancing the commercial value of their intellectual property (IP). To protect this IP, model owners typically allow user access only in a black-box manner, however, adversaries can still utilize model extraction attacks to steal the model intelligence encoded in model generation. Watermarking technology offers a promising solution for defending against such attacks by embedding unique identifiers into the model-generated content. However, existing watermarking methods often compromise the quality of generated content due to heuristic alterations and lack robust mechanisms to counteract adversarial strategies, thus limiting their practicality in real-world scenarios. In this paper, we introduce an adaptive and robust watermarking method (named ModelShield) to protect the IP of LLMs. Our method incorporates a self-watermarking mechanism that allows LLMs to autonomously insert watermarks into their generated content to avoid the degradation of model content. We also propose a robust watermark detection mechanism capable of effectively identifying watermark signals under the interference of varying adversarial strategies. Besides, ModelShield is a plug-and-play method that does not require additional model training, enhancing its applicability in LLM deployments. Extensive evaluations on two real-world datasets and three LLMs demonstrate that our method surpasses existing methods in terms of defense effectiveness and robustness while significantly reducing the degradation of watermarking on the model-generated content.},
  keywords={Watermarking;Adaptation models;Data models;Protection;Training;Computational modeling;Intellectual property;Closed box;Robustness;Real-time systems;Large language models;model extraction attack;text watermarking;model IP protection},
  doi={10.1109/TIFS.2025.3530691},
  ISSN={1556-6021},
  month={}
}

@BOOK{10251313,
  author={Alto, Valentina},
  booktitle={Modern Generative AI with ChatGPT and OpenAI Models: Leverage the capabilities of OpenAI's LLM for productivity and innovation with GPT3 and GPT4},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Harness the power of AI with innovative, real-world applications, and unprecedented productivity boosts, powered by the latest advancements in AI technology like ChatGPT and OpenAI Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesExplore the theory behind generative AI models and the road to GPT3 and GPT4Become familiar with ChatGPT’s applications to boost everyday productivityLearn to embed OpenAI models into applications using lightweight frameworks like LangChainBook DescriptionGenerative AI models and AI language models are becoming increasingly popular due to their unparalleled capabilities. This book will provide you with insights into the inner workings of the LLMs and guide you through creating your own language models. You’ll start with an introduction to the field of generative AI, helping you understand how these models are trained to generate new data. Next, you’ll explore use cases where ChatGPT can boost productivity and enhance creativity. You’ll learn how to get the best from your ChatGPT interactions by improving your prompt design and leveraging zero, one, and few-shots learning capabilities. The use cases are divided into clusters of marketers, researchers, and developers, which will help you apply what you learn in this book to your own challenges faster. You’ll also discover enterprise-level scenarios that leverage OpenAI models’ APIs available on Azure infrastructure; both generative models like GPT-3 and embedding models like Ada. For each scenario, you’ll find an end-to-end implementation with Python, using Streamlit as the frontend and the LangChain SDK to facilitate models' integration into your applications. By the end of this book, you’ll be well equipped to use the generative AI field and start using ChatGPT and OpenAI models’ APIs in your own projects.What you will learnUnderstand generative AI concepts from basic to intermediate levelFocus on the GPT architecture for generative AI modelsMaximize ChatGPT’s value with an effective prompt designExplore applications and use cases of ChatGPTUse OpenAI models and features via API callsBuild and deploy generative AI systems with PythonLeverage Azure infrastructure for enterprise-level use casesEnsure responsible AI and ethics in generative AI systemsWho this book is forThis book is for individuals interested in boosting their daily productivity; businesspersons looking to dive deeper into real-world applications to empower their organizations; data scientists and developers trying to identify ways to boost ML models and code; marketers and researchers seeking to leverage use cases in their domain – all by using Chat GPT and OpenAI Models. A basic understanding of Python is required; however, the book provides theoretical descriptions alongside sections with code so that the reader can learn the concrete use case application without running the scripts.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781805122838},
  url={https://ieeexplore.ieee.org/document/10251313}
}

@ARTICLE{10720939,
  author={Liu, Shang and Fang, Wenji and Lu, Yao and Wang, Jing and Zhang, Qijun and Zhang, Hongce and Xie, Zhiyao},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
  title={RTLCoder: Fully Open-Source and Efficient LLM-Assisted RTL Code Generation Technique}, 
  year={2025},
  volume={44},
  number={4},
  pages={1448-1461},
  abstract={The automatic generation of RTL code (e.g., Verilog) using natural language instructions and large language models (LLMs) has attracted significant research interest recently. However, most existing approaches heavily rely on commercial LLMs, such as ChatGPT, while open-source LLMs tailored for this specific design generation task exhibit notably inferior performance. The absence of high-quality open-source solutions restricts the flexibility and data privacy of this emerging technique. In this study, we present a new customized LLM solution with a modest parameter count of only 7B, achieving better performance than GPT-3.5 on all representative benchmarks for RTL code generation. Especially, it outperforms GPT-4 in VerilogEval Machine benchmark. This remarkable balance between accuracy and efficiency is made possible by leveraging our new RTL code dataset and a customized LLM algorithm, both of which have been made fully open-source. Furthermore, we have successfully quantized our LLM to 4-bit with a total size of 4 GB, enabling it to function on a single laptop with only slight performance degradation. This efficiency allows the RTL generator to serve as a local assistant for engineers, ensuring all design privacy concerns are addressed.},
  keywords={Codes;Hardware design languages;Training;Integrated circuit modeling;Data models;Natural languages;Hardware;Data collection;Benchmark testing;Privacy;Dataset generation;hardware code generation;Verilog;large language model;preference finetuning},
  doi={10.1109/TCAD.2024.3483089},
  ISSN={1937-4151},
  month={April}
}

@INPROCEEDINGS{10386946,
  author={Bushey, Jessica},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={AI-Generated Images as an Emergent Record Format}, 
  year={2023},
  volume={},
  number={},
  pages={2020-2031},
  abstract={AI-generated Images are disrupting existing approaches to verifying the trustworthiness of visual media. The application of generative AI in fields in which images are trusted visual evidence of persons, actions and events is drawing the attention of archival scientists and AI researchers. A literature review of AI-generated images as an emergent record format, identified an absence of archival and recordkeeping knowledge. Analysis of the results revealed six thematic categories: authenticity and verifiability; manipulation and misinformation; bias and representation; attribution and intellectual property; transparency and explainability; and ethical considerations. These themes inform the development of research questions and the next phase of the study that includes the application of theory and methods of archival diplomatics and computational archival science.},
  keywords={Visualization;Ethics;Generative AI;Bibliographies;Intellectual property;Big Data;Fake news;Generative-AI;AI-generated images;Archives;Recordkeeping;Computational Archival Science;Born-digital images},
  doi={10.1109/BigData59044.2023.10386946},
  ISSN={},
  month={Dec}
}

@INPROCEEDINGS{10487593,
  author={Horne, Dwight and Pierson, Anthony and Hedary, Elvis and Freddo, Garrett and Trejo, Luis and Matis, Mark and Mask, Lonnie},
  booktitle={2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)}, 
  title={VADER-SC: A Model Agnostic Tool for Large Scale, AI Driven Automated Source Code Summarization}, 
  year={2023},
  volume={},
  number={},
  pages={2600-2607},
  abstract={Production of a natural language description for the function of a source code segment is commonly referred to as source code summarization. Useful comments in source code can facilitate more rapid onboarding of new engineers and contribute to decreased maintenance costs. Unfortunately, the documentation task can also be labor intensive. In this paper, we introduce a new model agnostic tool for AI driven automation of source code summarization at scale. The initial version of the adVanced AI Driven Enhancement to Readability for Spurce Code (VADER-SC) software offers numerous options for customization and the ability to leverage a variety of AI models to enable experimentation in resource constrained environments, while also scaling up to benefit from larger models in contexts with increased compute resources. It further supports private cloud, self-hosted, and air-gapped network configurations for environments with strict intellectual property protections or processing of sensitive or controlled data. Qualitative and quantitative results suggest model selection, fine-tuning, and multi-shot tailoring significantly impact the quality of generated comments. VADER-SC could be an enabler for practitioners to explore large-scale automation of AI driven source code summarization and researchers may find it enables studies with larger volumes of disparate data across a diversity of AI model options and target programming languages.},
  keywords={Automation;Source coding;Computational modeling;Atmospheric modeling;Production;Software;Artificial intelligence;automated source code summarization;deep learning;neural machine translation;software maintenance;program comprehension;AI driven documentation;VADER-SC},
  doi={10.1109/CSCE60160.2023.00416},
  ISSN={},
  month={July}
}

@BOOK{10769240,
  author={Bourne, Keith and Es, Shahul},
  booktitle={Unlocking Data with Generative AI and RAG: Enhance generative AI systems by integrating internal data with large language models using RAG},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Leverage cutting-edge generative AI techniques such as RAG to realize the potential of your data and drive innovation as well as gain strategic advantageKey FeaturesOptimize data retrieval and generation using vector databasesBoost decision-making and automate workflows with AI agentsOvercome common challenges in implementing real-world RAG systemsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionGenerative AI is helping organizations tap into their data in new ways, with retrieval-augmented generation (RAG) combining the strengths of large language models (LLMs) with internal data for more intelligent and relevant AI applications. The author harnesses his decade of ML experience in this book to equip you with the strategic insights and technical expertise needed when using RAG to drive transformative outcomes. The book explores RAG’s role in enhancing organizational operations by blending theoretical foundations with practical techniques. You’ll work with detailed coding examples using tools such as LangChain and Chroma’s vector database to gain hands-on experience in integrating RAG into AI systems. The chapters contain real-world case studies and sample applications that highlight RAG’s diverse use cases, from search engines to chatbots. You’ll learn proven methods for managing vector databases, optimizing data retrieval, effective prompt engineering, and quantitatively evaluating performance. The book also takes you through advanced integrations of RAG with cutting-edge AI agents and emerging non-LLM technologies. By the end of this book, you’ll be able to successfully deploy RAG in business settings, address common challenges, and push the boundaries of what’s possible with this revolutionary AI technique.What you will learnUnderstand RAG principles and their significance in generative AIIntegrate LLMs with internal data for enhanced operationsMaster vectorization, vector databases, and vector search techniquesDevelop skills in prompt engineering specific to RAG and design for precise AI responsesFamiliarize yourself with AI agents' roles in facilitating sophisticated RAG applicationsOvercome scalability, data quality, and integration issuesDiscover strategies for optimizing data retrieval and AI interpretabilityWho this book is forThis book is for AI researchers, data scientists, software developers, and business analysts looking to leverage RAG and generative AI to enhance data retrieval, improve AI accuracy, and drive innovation. It is particularly suited for anyone with a foundational understanding of AI who seeks practical, hands-on learning. The book offers real-world coding examples and strategies for implementing RAG effectively, making it accessible to both technical and non-technical audiences. A basic understanding of Python and Jupyter Notebooks is required.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835887912},
  url={https://ieeexplore.ieee.org/document/10769240}
}

@BOOK{10718332,
  author={Auffarth},
  booktitle={Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={2024 Edition – Get to grips with the LangChain framework to develop production-ready applications, including agents and personal assistants. The 2024 edition features updated code examples and an improved GitHub repository. Purchase of the print or Kindle book includes a free PDF eBook. Key FeaturesLearn how to leverage LangChain to work around LLMs’ inherent weaknessesDelve into LLMs with LangChain and explore their fundamentals, ethical dimensions, and application challengesGet better at using ChatGPT and GPT models, from heuristics and training to scalable deployment, empowering you to transform ideas into realityBook DescriptionChatGPT and the GPT models by OpenAI have brought about a revolution not only in how we write and research but also in how we can process information. This book discusses the functioning, capabilities, and limitations of LLMs underlying chat systems, including ChatGPT and Gemini. It demonstrates, in a series of practical examples, how to use the LangChain framework to build production-ready and responsive LLM applications for tasks ranging from customer support to software development assistance and data analysis – illustrating the expansive utility of LLMs in real-world applications. Unlock the full potential of LLMs within your projects as you navigate through guidance on fine-tuning, prompt engineering, and best practices for deployment and monitoring in production environments. Whether you're building creative writing tools, developing sophisticated chatbots, or crafting cutting-edge software development aids, this book will be your roadmap to mastering the transformative power of generative AI with confidence and creativity.What you will learnCreate LLM apps with LangChain, like question-answering systems and chatbotsUnderstand transformer models and attention mechanismsAutomate data analysis and visualization using pandas and PythonGrasp prompt engineering to improve performanceFine-tune LLMs and get to know the tools to unleash their powerDeploy LLMs as a service with LangChain and apply evaluation strategiesPrivately interact with documents using open-source LLMs to prevent data leaksWho this book is forThe book is for developers, researchers, and anyone interested in learning more about LangChain. Whether you are a beginner or an experienced developer, this book will serve as a valuable resource if you want to get the most out of LLMs using LangChain. Basic knowledge of Python is a prerequisite, while prior exposure to machine learning will help you follow along more easily.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835088364},
  url={https://ieeexplore.ieee.org/document/10718332}
}

@ARTICLE{10195236,
  author={Zhang, Youzhi and Chen, Dongkai and Jajodia, Sushil and Pugliese, Andrea and Subrahmanian, V. S. and Xiong, Yanhai},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={GAIT: A Game-Theoretic Defense Against Intellectual Property Theft}, 
  year={2024},
  volume={21},
  number={4},
  pages={1967-1980},
  abstract={Months may pass before the victim of IP theft even knows they have been compromised. During this time, the attacker can exfiltrate large amounts of data. Recent work has proposed the idea of injecting a set of believable fake versions of a real document into a network so that the attacker has to expend time and effort to identify the real document from a sea of similar documents. In this paper, we consider the problem of an attacker who is smart and breaks a technical document down into small, bit-sized “units” and inspects them one by one so as to defeat the fake document defense. If a unit in a document is determined to be fake, the adversary does not need to look further at the same document. He can also immediately identify as fake, any other document that contains the same unit. In this paper, we consider the problem of a smart attacker using this strategy. Our proposed defensive algorithm, called ${\sf GAIT}$GAIT, is shown to be successful in mitigating such attacks. ${\sf GAIT}$GAIT can work in conjunction with any NLP-based generative method to create fake technical documents.},
  keywords={Games;Nash equilibrium;IP networks;History;Public key;Intellectual property;Heuristic algorithms;H.2.0.a security;integrity and protection < H.2.0 general < H.2 database management < H information technology and systems},
  doi={10.1109/TDSC.2023.3299225},
  ISSN={1941-0018},
  month={July}
}

@INPROCEEDINGS{10764959,
  author={Hu, Chao and Chai, Yitian and Zhou, Hao and Meng, Fandong and Zhou, Jie and Gu, Xiaodong},
  booktitle={2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={How Effectively Do Code Language Models Understand Poor-Readability Code?}, 
  year={2024},
  volume={},
  number={},
  pages={795-806},
  abstract={Code language models such as CodeT5 and CodeLlama have demonstrated substantial achievement in code comprehension. While the majority of research efforts have focused on improving model architectures and training processes, we find that the current benchmarks used for evaluating code comprehension models are confined to high-readability code, regardless of the popularity of low-readability code in reality. As such, they are inadequate to demonstrate the full spectrum of the model’s ability, particularly the robustness to varying readability degrees. In this paper, we analyze the robustness of code summarization models to code with varying readability, including seven obfuscated datasets derived from existing benchmarks. Our findings indicate that current code summarization models are vulnerable to code with poor readability. In particular, their performance predominantly depends on semantic cues within the code, often neglecting the syntactic aspects. Existing benchmarks are biased toward evaluating semantic features, thereby overlooking the models’ ability to understand non-sensitive syntactic features. Based on the findings, we present PoorCodeSumEval, a new evaluation benchmark on code summarization tasks. PoorCodeSumEval innovatively introduces readability into the testing process, considering semantic, syntactic, and their cross-obfuscation, thereby providing a more comprehensive and rigorous evaluation of code summarization models. Our studies also provide more insightful suggestions for future research, such as constructing multi-readability benchmarks to evaluate the robustness of models on poor-readability code, proposing readability-awareness metrics, and automatic methods for code data cleaning and normalization.},
  keywords={Training;Codes;Sensitivity;Semantics;Benchmark testing;Syntactics;Robustness;Data models;Cleaning;Software engineering;Code language models;Code summarization;Code readability},
  doi={},
  ISSN={2643-1572},
  month={Oct}
}

@INPROCEEDINGS{10810490,
  author={Heng, Panha and Yongsiriwit, Karn and Chaisiriprasert, Parkpoom},
  booktitle={2024 8th International Conference on Information Technology (InCIT)}, 
  title={Comparing the Effectiveness of Generative AI for Learning and Developing Flutter Application}, 
  year={2024},
  volume={},
  number={},
  pages={746-751},
  abstract={The rapid growth of business demands modern technological advancements, leading to an increased need for accelerated learning and development of tools. Generative AI has become a key player in enhancing these areas by aiding in code generation for application development. Specifically, Generative AI can produce functional code for various programming languages, aiding in the setup of UI components, navigation, and complex state management. This study evaluates the effectiveness of three widely-used Generative AI tools— ChatGPT, Copilot, and Codeium—chosen for their popularity and diverse approaches to code generation. Standardized prompts were used to generate Flutter code for beginner, intermediate, and advanced tasks. The results show that ChatGPT outperformed other tools, consistently generating runnable and comprehensive code, while Copilot and Codeium exhibited some limitations in handling complex tasks. These findings suggest that integrating Generative AI into Flutter development can significantly accelerate the coding process and enhance application quality.},
  keywords={Computer languages;Codes;Generative AI;Navigation;Chatbots;Encoding;Information technology;Business;Generative AI;Code generation;Flutter;Prompt engineering;Cross-platform},
  doi={10.1109/InCIT63192.2024.10810490},
  ISSN={},
  month={Nov}
}

@BOOK{10410239,
  author={Barrett, Clark and Boyd, Brad and Bursztein, Elie and Carlini, Nicholas and Chen, Brad and Choi, Jihye and Roy Chowdhury, Amrita and Christodorescu, Mihai and Datta, Anupam and Feizi, Soheil and Fisher, Kathleen and Hashimoto, Tatsunori and Hendrycks, Dan and Jha, Somesh and Kang, Daniel and Kerschbaum, Florian and Mitchell, Eric and Mitchell, John and Ramzan, Zulfikar and Shams, Khawaja and Song, Dawn and Taly, Ankur and Yang, Diyi},
  booktitle={Identifying and Mitigating the Security Risks of Generative AI},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Every major technical invention resurfaces the dual-use dilemma — the new technology has the potential to be used for good as well as for harm. Generative AI (GenAI) techniques, such as large language models (LLMs) and diffusion models, have shown remarkable capabilities (e.g., in-context learning, code-completion, and text-to-image generation and editing). However, GenAI can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks. This monograph reports the findings of a workshop held at Google (co-organized by Stanford University and the University of Wisconsin-Madison) on the dual-use dilemma posed by GenAI. This work is not meant to be comprehensive, but is rather an attempt to synthesize some of the interesting findings from the workshop. Short-term and long-term goals for the community on this topic are discussed. This work should provide both a launching point for a discussion on this important topic, as well as interesting problems that the research community can work to address.},
  keywords={Privacy and Security},
  doi={},
  ISSN={},
  publisher={now},
  isbn={9781638283133},
  url={https://ieeexplore.ieee.org/document/10410239}
}

@ARTICLE{9782131,
  author={Ou, Weihan and Ding, Steven H. H. and Tian, Yuan and Song, Leo},
  journal={IEEE Transactions on Software Engineering}, 
  title={SCS-Gan: Learning Functionality-Agnostic Stylometric Representations for Source Code Authorship Verification}, 
  year={2023},
  volume={49},
  number={4},
  pages={1426-1442},
  abstract={In recent years, the number of anonymous script-based fileless malware attacks and software copyright disputes has increased rapidly. In the literature, automated Code Authorship Analysis (CAA) techniques have been proposed to reduce the manual effort in identifying those attacks and issues. Most CAA techniques aim to solve the task of Authorship Attribution (AA), i.e., identifying the actual author of a source code fragment from a given set of candidate authors. However, in many real-world scenarios, investigators do not have a predefined set of authors containing the actual author at the time of investigation, i.e., contradicting AA's assumption. Additionally, existing AA techniques ignore the influence of code functionality when identifying the authorship, which leads to biased matching simply based on code functionality. Different from AA, the task of (extreme) Authorship Verification (AV) is to decide if two texts were written by the same person or not. AV techniques do not need a predefined author set and thus could be applied in more code authorship-related applications than AA. To our knowledge, there is no previous work attempting to solve the AV problem for the source code. To fill the gap, we propose a novel adversarial neural network, namely SCS-Gan, that can learn a stylometric representation of code for automated AV. With the multi-head attention mechanism, SCS-Gan focuses on the code parts that are most informative regarding personal styles and generates functionality-agnostic stylometric representations through adversarial training. We benchmark SCS-Gan and two state-of-the-art code representation models on four out-of-sample datasets collected from a real-world programming competition. Our experiment results show that SCS-Gan outperforms the baselines on all four out-of-sample datasets.},
  keywords={Codes;Task analysis;Training;Encoding;Feature extraction;Malware;Python;Cyber threat intelligence;representation learning;adversarial learning;authorship analysis;code authorship verification},
  doi={10.1109/TSE.2022.3177228},
  ISSN={1939-3520},
  month={April}
}

@INPROCEEDINGS{10925007,
  author={Ugbomah, Christopher and Kumi, Sandra and Lomotey, Richard K. and Deters, Ralph},
  booktitle={2024 IEEE Smart World Congress (SWC)}, 
  title={Overcoming the Data Availability Paradox with Managed Digital Twin Instances}, 
  year={2024},
  volume={},
  number={},
  pages={2025-2032},
  abstract={Digital transformation, IoT, and cloud storage have led to vast data collections that tend to exist in isolated silos, thus limiting their utilization. This leads to the data availability paradox, in which individuals, groups, and organizations possess vast amounts of data but struggle to utilize or share it effectively. The struggle is due to privacy, security, and intellectual property concerns. When this happens, individuals and institutions are unable to take full advantage of the capability of Industry 4.0 and digitalization. This paper proposes using managed Digital Twin Instances (DTI) to overcome this paradox. The fully managed digital twin instances provide well-defined and fully controlled access to models derived from the underlying data sets. Digital twin instances, therefore serve as virtual replicas of real-world systems, entities, or processes, enabling controlled access. A key issue in this approach is the effective hosting and managing large numbers of digital twin instances. The paper introduces a management and hosting framework and its evaluation in the MS Azure cloud environment.},
  keywords={Atomic measurements;Cloud computing;Runtime;Intellectual property;Containers;Throughput;Data models;HTTP;Digital twins;Standards;digital twin;data availability paradox;data sharing},
  doi={10.1109/SWC62898.2024.00311},
  ISSN={2993-396X},
  month={Dec}
}

@INPROCEEDINGS{10707489,
  author={Ludwig, Heiko and Zhou, Yi and Zawad, Syed and Ong, Yuya and Li, Pengyuan and Butler, Eric and Zahid, Eelaaf},
  booktitle={2024 IEEE International Conference on Web Services (ICWS)}, 
  title={Towards Collecting Royalties for Copyrighted Data for Generative Models}, 
  year={2024},
  volume={},
  number={},
  pages={20-26},
  abstract={Addressing issues of copyrighted data in the context of generative models has become an important issue for content creators, publishers, organizations training generative models, and those who deploy generative models for particular applications. Copyright holders want to ensure that they are fairly compensated for their work and users of training data and models do not want to expose themselves to litigation. However, traditional models of bulk-licensing data fit only poorly the context of model training. In this paper, we want to discuss why a traditional data license is not always a good fit, how data is used in the life-cycle of generative models and which impact data has on model output. This can be used as a foundation for a pay-per-(model)use compensation based how data contributes to a model’s output. Having a way to compensate copyright holders in this way reduces risk for model trainers, avoids large investments upfront, and encourage a lively data ecosystem in which the creation and distribution of original work is encouraged and fairly compensated.},
  keywords={Training;Web services;Biological system modeling;Large language models;Ecosystems;Training data;Licenses;Data models;Recording;Context modeling;Generative AI;large language models;copyright;royalty;licensing;data},
  doi={10.1109/ICWS62655.2024.00020},
  ISSN={2836-3868},
  month={July}
}

@ARTICLE{10720776,
  author={Wu, Houmin and Lim, Sangguk and Xiao, Bin},
  journal={IEEE Access}, 
  title={Animated Avatar Generation Technology Research Based on Deep Convolutional Generative Adversarial Network Integrated With Self-Attention and Spectral Normalization}, 
  year={2024},
  volume={12},
  number={},
  pages={154614-154630},
  abstract={The burgeoning field of large language models (LLMs), exemplified by DALL-E and Stable Diffusion, has made image generation a reality. However, the computationally intensive GPU training these models necessitate incurs substantial financial burdens. Moreover, while a plethora of image datasets are accessible, specialized anime avatar datasets remain elusive and are often entangled in copyright disputes. This scarcity presents a significant research opportunity: developing a cost-effective, user-friendly anime avatar generation technique that circumvents these challenges. This paper introduces a novel method for creating animated avatars, leveraging the deep convolutional generative adversarial network(DCGAN) architecture and enhanced with Self-Attention (SA) and Spectral Normalization (SN), termed the SA+SN-DCGAN. The integration of the SA mechanism into the generator significantly elevates the quality of the output. Meanwhile, the application of SN to the discriminator effectively combats the notorious vanishing or exploding gradients, and thereby diminishing the likelihood of over-fitting. Our methodology involved sourcing anime avatars from reputable public domains and standardizing them using OpenCV. A meticulous grid search was employed to fine-tune model hyper-parameters. After 300 epochs of rigorous training, the generator and discriminator achieved stable error rates, with the synthesized images closely mirroring the fidelity of authentic avatars. Comparative evaluations against prevailing models underscore the SA+SN_DCGAN method’s superiority in producing highly realistic anime avatars, affirming its exceptional overall performance. This study not only contributes a novel solution to the domain of anime avatar generation but also paves the way for future research in the field.},
  keywords={Avatars;Generative adversarial networks;Generators;Training;Noise measurement;Computer architecture;Visualization;Unsupervised learning;Speech coding;Semantics;Generative adversarial network;anime avatar;self-attention model;spectral normalization},
  doi={10.1109/ACCESS.2024.3482989},
  ISSN={2169-3536},
  month={}
}

@ARTICLE{10735776,
  author={Yang, Zhou and Zhao, Zhipeng and Wang, Chenyu and Shi, Jieke and Kim, Dongsun and Han, DongGyun and Lo, David},
  journal={IEEE Transactions on Software Engineering}, 
  title={Gotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in Code Models}, 
  year={2024},
  volume={50},
  number={12},
  pages={3290-3306},
  abstract={Leveraging large-scale datasets from open-source projects and advances in large language models, recent progress has led to sophisticated code models for key software engineering tasks, such as program repair and code completion. These models are trained on data from various sources, including public open-source projects like GitHub and private, confidential code from companies, raising significant privacy concerns. This paper investigates a crucial but unexplored question: What is the risk of membership information leakage in code models? Membership leakage refers to the vulnerability where an attacker can infer whether a specific data point was part of the training dataset. We present Gotcha, a novel membership inference attack method designed for code models, and evaluate its effectiveness on Java-based datasets. Gotcha simultaneously considers three key factors: model input, model output, and ground truth. Our ablation study confirms that each factor significantly enhances attack performance. Our ablation study confirms that each factor significantly enhances attack performance. Our investigation reveals a troubling finding: membership leakage risk is significantly elevated. While previous methods had accuracy close to random guessing, Gotcha achieves high precision, with a true positive rate of 0.95 and a low false positive rate of 0.10. We also demonstrate that the attacker's knowledge of the victim model (e.g., model architecture and pre-training data) affects attack success. Additionally, modifying decoding strategies can help reduce membership leakage risks. This research highlights the urgent need to better understand the privacy vulnerabilities of code models and develop strong countermeasures against these threats.},
  keywords={Codes;Data models;Training data;Training;Information leakage;Software development management;Data privacy;Privacy;Decoding;Source coding;Membership inference attack;privacy;large language models for code;code completion},
  doi={10.1109/TSE.2024.3482719},
  ISSN={1939-3520},
  month={Dec}
}

@INPROCEEDINGS{10734434,
  author={Rasnayaka, Sanka and Wang, Guanlin and Shariffdeen, Ridwan and Iyer, Ganesh Neelakanta},
  booktitle={2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code)}, 
  title={An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project}, 
  year={2024},
  volume={},
  number={},
  pages={111-118},
  abstract={Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s). Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain. However, the usefulness of LLMs in an academic software engineering project has not been fully explored yet. In this study, we explore the usefulness of LLMs for 214 students working in teams consisting of up to six members. Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs.In this paper, we analyze the AI-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base. We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of LLM from a computer science student’s perspective. Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration.CCS CONCEPTS• Software and its engineering → Software development techniques; • Applied computing → Education.},
  keywords={Productivity;Codes;Large language models;Conferences;Education;Debugging;Syntactics;Software;Software engineering;Software development management;LLM for Code Generation;Software Engineering},
  doi={},
  ISSN={},
  month={April}
}

@INPROCEEDINGS{10673464,
  author={Mhatre, Madhura and Pandey, Ananya and Rane, Harsh and Sahu, Sunita},
  booktitle={2024 Asia Pacific Conference on Innovation in Technology (APCIT)}, 
  title={A Novel Approach for Creating Flowcharts using Generative AI}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Flowcharts have been widely used to visualize and communicate complex processes in a wide variety of domains such as education, technology, science, medicine, and manufacturing. In this paper we propose a methodology to automate the process of flowchart creation using generative AI, aiming to streamline the process and enhance the efficiency of flowchart creation. By using a large dataset of labeled flowchart-text pairs, the model learns to understand the semantics and relationships within the input text and converts them into a visually coherent and accurate flowchart representation. The methodology ensures the efficiency of the model by performing data preprocessing, model training, and performance evaluation. Automating the flowchart creation process will result in cost-effectiveness as well as consistency and accuracy. We aim to harness the power of Generative AI to automate flowchart creation, making it a valuable tool for professionals seeking an efficient and reliable means of visualizing complex processes.},
  keywords={Training;Industries;Flowcharts;Visualization;Accuracy;Generative AI;Data preprocessing;Flowchart;Generative AI;Model Training;Performance Evaluation;Process Visualization},
  doi={10.1109/APCIT62007.2024.10673464},
  ISSN={},
  month={July}
}

@INPROCEEDINGS{10734633,
  author={Ramler, Rudolf and Moser, Michael and Fischer, Lukas and Nissl, Markus and Heinzl, René},
  booktitle={2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code)}, 
  title={Industrial Experience Report on AI-Assisted Coding in Professional Software Development}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={AI-based tools for software development are widely discussed in academic literature. They promise to boost software development performance, especially in code creation. This paper collects insights from practitioners about the use and implications of AI assistance in industrial software development, with a focus on SMEs. Through interviews with five developers from three software development organization, we gathered and analyzed the experiences made in industrial practice, and we identified lessons learned and open challenges. ChatGPT and Copilot are used in industry projects. While they are considered useful for many code-related development activities, their integration in the development workflow remains mostly shallow. Contradicting observations about speed-ups due to AI support in development are reported. Legal issues are of minor concern although awareness exists.CCS CONCEPTS#x2022; Software and its engineering → Automatic programming.},
  keywords={Industries;Codes;Law;Large language models;Conferences;Organizations;Software;Encoding;Interviews;Software development management;AI-assisted development;code generation;ChatGPT;Copilot},
  doi={},
  ISSN={},
  month={April}
}

@INPROCEEDINGS{10655907,
  author={Seo, Juwon and Lee, Sung-Hoon and Lee, Tae-Young and Moon, Seungjun and Park, Gyeong-Moon},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Generative Unlearning for Any Identity}, 
  year={2024},
  volume={},
  number={},
  pages={9151-9161},
  abstract={Recent advances in generative models trained on large-scale datasets have made it possible to synthesize highquality samples across various domains. Moreover, the emergence of strong inversion networks enables not only a reconstruction of real-world images but also the modification of attributes through various editing methods. However, in certain domains related to privacy issues, e.g., human faces, advanced generative models along with strong inversion methods can lead to potential misuses. In this paper, we propose an essential yet under-explored task called generative identity unlearning, which steers the model not to generate an image of specific identity. In the generative identity unlearning, we target the following objectives: (i) preventing the generation of images with a certain identity, and (ii) preserving the overall quality of the generative model. To satisfy these goals, we propose a novel framework, Generative Unlearning for Any IDEntity (GUIDE), which prevents the reconstruction of a specific identity by unlearning the generator with only a single image. GUIDE consists of two parts: (i) finding a target point for optimization that unidentifies the source latent code and (ii) novel loss functions that facilitate the unlearning procedure while less affecting the learned distribution. Our extensive experiments demonstrate that our proposed method achieves state-of-the-art performance in the generative machine unlearning task. The code is available at https://github.com/KHU-AGI/GUIDE.},
  keywords={Industries;Privacy;Extrapolation;Codes;Generative adversarial networks;Generators;Pattern recognition;machine unlearning;GAN},
  doi={10.1109/CVPR52733.2024.00874},
  ISSN={2575-7075},
  month={June}
}

@INPROCEEDINGS{10549662,
  author={Yang, Zhou and Zhao, Zhipeng and Wang, Chenyu and Shi, Jieke and Kim, Dongsun and Han, Donggyun and Lo, David},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)}, 
  title={Unveiling Memorization in Code Models}, 
  year={2024},
  volume={},
  number={},
  pages={867-879},
  abstract={The availability of large-scale datasets, advanced architectures, and powerful computational resources have led to effective code models that automate diverse software engineering activities. The datasets usually consist of billions of lines of code from both open-source and private repositories. A code model memorizes and produces source code verbatim, which potentially contains vulnerabilities, sensitive information, or code with strict licenses, leading to potential security and privacy issues. This paper investigates an important problem: to what extent do code models memorize their training data? We conduct an empirical study to explore memorization in large pre-trained code models. Our study highlights that simply extracting 20,000 outputs (each having 512 tokens) from a code model can produce over 40,125 code snippets that are memorized from the training data. To provide a better understanding, we build a taxonomy of memorized contents with 3 categories and 14 subcategories. The results show that the prompts sent to the code models affect the distribution of memorized contents. We identify several key factors of memorization. Specifically, given the same architecture, larger models suffer more from memorization problem. A code model produces more memorization when it is allowed to generate longer outputs. We also find a strong positive correlation between the number of an output's occurrences in the training data and that in the generated outputs, which indicates that a potential way to reduce memorization is to remove duplicates in the training data. We then identify effective metrics that infer whether an output contains memorization accurately. We also make suggestions to deal with memorization.},
  keywords={Codes;Computational modeling;Source coding;Taxonomy;Training data;Computer architecture;Data models;Software and its engineering → Software development techniques;Computing methodologies → Artificial intelligence;Security and privacy;Open-Source Software;Memorization;Code Generation},
  doi={10.1145/3597503.3639074},
  ISSN={1558-1225},
  month={April}
}

@ARTICLE{9627776,
  author={Sun, Hui and Zhu, Tianqing and Zhang, Zhiqiu and Jin, Dawei and Xiong, Ping and Zhou, Wanlei},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Adversarial Attacks Against Deep Generative Models on Data: A Survey}, 
  year={2023},
  volume={35},
  number={4},
  pages={3367-3388},
  abstract={Deep generative models have gained much attention given their ability to generate data for applications as varied as healthcare to financial technology to surveillance, and many more - the most popular models being generative adversarial networks (GANs) and variational auto-encoders (VAEs). Yet, as with all machine learning models, ever is the concern over security breaches and privacy leaks and deep generative models are no exception. In fact, these models have advanced so rapidly in recent years that work on their security is still in its infancy. In an attempt to audit the current and future threats against these models, and to provide a roadmap for defense preparations in the short term, we prepared this comprehensive and specialized survey on the security and privacy preservation of GANs and VAEs. Our focus is on the inner connection between attacks and model architectures and, more specifically, on five components of deep generative models: the training data, the latent code, the generators/decoders of GANs/VAEs, the discriminators/encoders of GANs/VAEs, and the generated data. For each model, component and attack, we review the current research progress and identify the key challenges. The paper concludes with a discussion of possible future attacks and research directions in the field.},
  keywords={Training;Generators;Data models;Codes;Biological system modeling;Security;Privacy;Deep generative models;deep learning;membership inference attack;evasion attack;model defense},
  doi={10.1109/TKDE.2021.3130903},
  ISSN={1558-2191},
  month={April}
}

@INPROCEEDINGS{10646683,
  author={Yang, Borui and Li, Wei and Xiang, Liyao and Li, Bo},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)}, 
  title={SrcMarker: Dual-Channel Source Code Watermarking via Scalable Code Transformations}, 
  year={2024},
  volume={},
  number={},
  pages={4088-4106},
  abstract={The expansion of the open source community and the rise of large language models have raised ethical and security concerns on the distribution of source code, such as misconduct on copyrighted code, distributions without proper licenses, or misuse of the code for malicious purposes. Hence it is important to track the ownership of source code, in which watermarking is a major technique. Yet, drastically different from natural languages, source code watermarking requires far stricter and more complicated rules to ensure the readability as well as the functionality of the source code. Hence we introduce SrcMarker, a watermarking system to unobtrusively encode ID bitstrings into source code, without affecting the usage and semantics of the code. To this end, SrcMarker performs transformations on an AST-based intermediate representation that enables unified transformations across different programming languages. The core of the system utilizes learning-based embedding and extraction modules to select rule-based transformations for watermarking. In addition, a novel feature-approximation technique is designed to tackle the inherent non-differentiability of rule selection, thus seamlessly integrating the rule-based transformations and learning-based networks into an interconnected system to enable end-to-end training. Extensive experiments demonstrate the superiority of SrcMarker over existing methods in various watermarking requirements.},
  keywords={Training;Computer languages;Privacy;Codes;Source coding;Semantics;Pipelines},
  doi={10.1109/SP54263.2024.00097},
  ISSN={2375-1207},
  month={May}
}

@INPROCEEDINGS{10394237,
  author={Khoury, Raphaël and Avila, Anderson R. and Brunelle, Jacob and Camara, Baba Mamadou},
  booktitle={2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={How Secure is Code Generated by ChatGPT?}, 
  year={2023},
  volume={},
  number={},
  pages={2445-2451},
  abstract={In recent years, large language models have been responsible for great advances in the field of artificial intelligence (AI). ChatGPT in particular, an AI chatbot developed and recently released by OpenAI, has taken the field to the next level. The conversational model is able not only to process human-like text, but also to translate natural language into code. However, the safety of programs generated by ChatGPT should not be overlooked. In this paper, we perform an experiment to address this issue. Specifically, we ask ChatGPT to generate a number of computer programs in order to evaluate the security of the resulting source code. We further investigate whether ChatGPT can be prodded to improve code security by appropriate prompts, and discuss the ethical aspects of using AI to generate code. Results suggest that ChatGPT is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.},
  keywords={Codes;Source coding;Chatbots;Safety;Artificial intelligence;Standards;Programming profession;Large language models;ChatGPT;code security;automatic code generation},
  doi={10.1109/SMC53992.2023.10394237},
  ISSN={2577-1655},
  month={Oct}
}

@BOOK{10803972,
  author={Hall, Dr. Vincent Austin},
  booktitle={Coding with ChatGPT and Other LLMs: Navigate LLMs for effective coding, debugging, and AI-driven development},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Leverage LLM (large language models) for developing unmatched coding skills, solving complex problems faster, and implementing AI responsiblyKey FeaturesUnderstand the strengths and weaknesses of LLM-powered software for enhancing performance while minimizing potential issuesGrasp the ethical considerations, biases, and legal aspects of LLM-generated code for responsible AI usageBoost your coding speed and improve quality with IDE integrationPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionKeeping up with the AI revolution and its application in coding can be challenging, but with guidance from AI and ML expert Dr. Vincent Hall—who holds a PhD in machine learning and has extensive experience in licensed software development—this book helps both new and experienced coders to quickly adopt best practices and stay relevant in the field. You’ll learn how to use LLMs such as ChatGPT and Gemini to produce efficient, explainable, and shareable code and discover techniques to maximize the potential of LLMs. The book focuses on integrated development environments (IDEs) and provides tips to avoid pitfalls, such as bias and unexplainable code, to accelerate your coding speed. You’ll master advanced coding applications with LLMs, including refactoring, debugging, and optimization, while examining ethical considerations, biases, and legal implications. You’ll also use cutting-edge tools for code generation, architecting, description, and testing to avoid legal hassles while advancing your career. By the end of this book, you’ll be well-prepared for future innovations in AI-driven software development, with the ability to anticipate emerging LLM technologies and generate ideas that shape the future of development.What you will learnUtilize LLMs for advanced coding tasks, such as refactoring and optimizationUnderstand how IDEs and LLM tools help coding productivityMaster advanced debugging to resolve complex coding issuesIdentify and avoid common pitfalls in LLM-generated codeExplore advanced strategies for code generation, testing, and descriptionDevelop practical skills to advance your coding career with LLMsWho this book is forThis book is for experienced coders and new developers aiming to master LLMs, data scientists and machine learning engineers looking for advanced techniques for coding with LLMs, and AI enthusiasts exploring ethical and legal implications. Tech professionals will find practical insights for innovation and career growth in this book, while AI consultants and tech hobbyists will discover new methods for training and personal projects.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781805127963},
  url={https://ieeexplore.ieee.org/document/10803972}
}

@INPROCEEDINGS{9773864,
  author={O’Hara, Christopher and Menu, Jonathan and Van Den Brand, Mark},
  booktitle={2022 IEEE International Systems Conference (SysCon)}, 
  title={COGENT: A Concurrent Engineering and Generative Engineering Tooling Platform}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={System architecture design is a complex and complicated process. Systems, subsystems, and components must undergo a strict evaluation process detailing trade-offs, risks, benefits, and feasibility at the fringes of what is technologically possible. Poor architecture design leads to poor product performance, wasted resources, and in worst-case scenarios–fatalities caused by mission/product failure. Two upcoming domains seek to improve the generation, evaluation, and selection of system architecture configurations. These domains are generative engineering and concurrent engineering. Generative engineering allows for the automatic generation and evaluation of thousands of architecture configurations. Concurrent engineering is a methodology of subsystem design teams working collaboratively and simultaneously to create and select system architecture configurations. However, what had yet to be established was the value of combining the two domains. We sought to combine generative engineering and concurrent engineering to identify this value by creating the Concurrent Generative Engineering Tooling (COGENT) platform. COGENT is a plugin solution architecture that enables cross-functional teams in automated system architecture generation in concurrent design facilities. A conceptual FireSat case study was explored, demonstrating COGENT capabilities such as enabling concurrent users, synchronized tool usage, centralized object storage, and connectivity to third-party software and/or user-defined features for space systems. COGENT is modular, extensible, and easy to integrate into any system development lifecycle. With COGENT, system designers can focus on their primary concerns, goals, and constraints. Using COGENT will allow system engineers, system architects, and subsystem designers to identify optimal system architecture configurations at a fraction of the time and cost.},
  keywords={Costs;System performance;Systems architecture;Computer architecture;Metadata;Concurrent engineering;Software;Systems Architecture;Systems Engineering;Generative Engineering;Concurrent Engineering;Space Engineering},
  doi={10.1109/SysCon53536.2022.9773864},
  ISSN={2472-9647},
  month={April}
}

@INPROCEEDINGS{10646865,
  author={Aghakhani, Hojjat and Dai, Wei and Manoel, Andre and Fernandes, Xavier and Kharkar, Anant and Kruegel, Christopher and Vigna, Giovanni and Evans, David and Zorn, Ben and Sim, Robert},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)}, 
  title={TrojanPuzzle: Covertly Poisoning Code-Suggestion Models}, 
  year={2024},
  volume={},
  number={},
  pages={1122-1140},
  abstract={With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model’s training by injecting malicious data. Poisoning attacks could be designed to influence the model’s suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, Covert and TrojanPuzzle, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TrojanPuzzle, goes one step further in generating less suspicious poison data by never explicitly including certain (suspicious) parts of the payload in the poison data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TrojanPuzzle robust against signature-based dataset-cleansing methods that can filter out suspicious sequences from the training data. Our evaluation against models of two sizes demonstrates that both Covert and TrojanPuzzle have significant implications for practitioners when selecting code used to train or tune code-suggestion models.},
  keywords={Training;Codes;Toxicology;Training data;Static analysis;Transformers;Data models;Large Language Models;Generative AI;Code Generation;Data Poisoning;Trustworthy AI},
  doi={10.1109/SP54263.2024.00140},
  ISSN={2375-1207},
  month={May}
}

@INPROCEEDINGS{10795058,
  author={Shang, Xiuwei and Cheng, Shaoyin and Chen, Guoqiang and Zhang, Yanming and Hu, Li and Yu, Xiao and Li, Gangyang and Zhang, Weiming and Yu, Nenghai},
  booktitle={2024 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={How Far Have We Gone in Binary Code Understanding Using Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-12},
  abstract={Binary code analysis plays a pivotal role in various software security applications, such as software maintenance, malware detection, software vulnerability discovery, patch analysis, etc. However, unlike source code, understanding binary code is challenging for reverse engineers due to the absence of semantic information. Therefore, automated tools are needed to assist human players in interpreting binary code. In recent years, two groups of technologies have shown promising prospects: (1) Deep learning-based technologies have demonstrated competitive results in tasks related to binary code understanding, furthermore, (2) Large Language Models (LLMs) have been extensively pre-trained at the source-code level for tasks such as code understanding and generation. This makes participants wonder about the ability of LLMs in binary code understanding. In this work, we propose a benchmark to evaluate the effectiveness of LLMs in real-world reverse engineering scenarios. The benchmark covers two key binary code understanding tasks, including function name recovery and binary code summarization. We gain valuable insights into their capabilities and limitations through extensive evaluations of popular LLMs using our benchmark. Our evaluations reveal that existing LLMs can understand binary code to a certain extent, thereby improving the efficiency of binary code analysis. Our results highlight the great potential of the LLMs in advancing the field of binary code understanding.},
  keywords={Software maintenance;Large language models;Source coding;Semantics;Reverse engineering;Binary codes;Benchmark testing;Malware;Security;Software engineering;Reverse Engineering;Binary Code Understanding;Program Comprehension;Large Language Models},
  doi={10.1109/ICSME58944.2024.00012},
  ISSN={2576-3148},
  month={Oct}
}

@INPROCEEDINGS{10696569,
  author={Tan, Yue Hern and Chua, Hui Na and Low, Yeh-Ching and Jasser, Muhammed Basheer},
  booktitle={2024 IEEE 14th International Conference on Control System, Computing and Engineering (ICCSCE)}, 
  title={Current Landscape of Generative AI: Models, Applications, Regulations and Challenges}, 
  year={2024},
  volume={},
  number={},
  pages={168-173},
  abstract={Generative AI models have witnessed remarkable advancements, blurring the lines between human creativity and machine generation. This paper concisely reviews the current Generative AI landscape, exploring its diverse applications across various domains. We delve into the capabilities of these models, from creating images and music to generating creative text formats. Furthermore, the paper examines the real-world applications of Generative AI, highlighting its potential to revolutionize industries like design, marketing, education, and scientific discovery. However, while existing research extensively explores specific aspects of Generative AI, an analysis of the technology’s landscape, encompassing its capabilities, applications in content creation, and regulatory considerations, remains limited. This paper strives to bridge this gap by delivering a more holistic landscape of GenAI. Our analysis of the GenAI landscape pinpointed user behavior research and responsible development practices as key to user-centric AI creation. Through this study, we aim to stimulate discussion and collaboration between researchers, developers, and policymakers to ensure this powerful technology is harnessed responsibly for the benefit of industry and society.},
  keywords={Industries;Privacy;Generative AI;Reviews;Law;Explainable AI;Prevention and mitigation;Education;Regulation;Creativity;Generative AI;Deep Learning;Applications;Regulations;Ethics;Content Creation},
  doi={10.1109/ICCSCE61582.2024.10696569},
  ISSN={},
  month={Aug}
}

@ARTICLE{10742321,
  author={Omran Almagrabi, Alaa and Khan, Rafiq Ahmad},
  journal={IEEE Access}, 
  title={Optimizing Secure AI Lifecycle Model Management With Innovative Generative AI Strategies}, 
  year={2025},
  volume={13},
  number={},
  pages={12889-12920},
  abstract={Generative AI (GAI) is one of the significant components that can efficiently improve and augment the AI cycle model’s robustness when it comes to different threats, weaknesses, and abnormalities detection. When applied in this field, GAI is very useful in emulating the various forms of security violations in actual adversarial settings. These scenarios are important when different aspects of an AI system are tested on how robust they are and thus permit the developers to amend any vulnerability that may be induced before the time it could be utilized in practice. Data and model manipulation, data theft, and adversarial attacks as well as model inference threats which we do a systematic analysis to disrupt the integrity, confidentiality as well as availability of AI models. Considering the current weaknesses and threats related to GAI we provide a systematic approach to how safety concerns that are currently relevant can be integrated with every stage of Artificial Intelligence (AI) lifecycle management: from continuous monitoring to the application of cybersecurity trends and practices, etc. In our approach, the emphasis is placed on the multi-level security management strategy that incorporates the improvement of coding practices, validation and testing, and the implementation of advanced intrusion detection systems. Before proceeding to further analysis and discussion of the given topic, it is also critical to mention the aspect of regulation and ethical concern as the major drivers of GAI usage. Additionally, organizations can involve GAI in the lifecycle to address security needs, during the development, acquisition, deployment, updating, maintenance, and decommissioning of the AI system, making them reliable, safe, and secure all through their lifecycle. Toward these ends, the goal of this work is to present a set of canonical recommendations for the many scientists, engineers, managers, technologists, and policymakers who will play a key role in constructing a sound and secure AI future.},
  keywords={Artificial intelligence;Data models;Security;Training;Organizations;Law;Generative AI;Ethics;Synthetic data;Generative adversarial networks;Generative artificial intelligence;AI lifecycle model;security threats and practices;systematic mapping study},
  doi={10.1109/ACCESS.2024.3491373},
  ISSN={2169-3536},
  month={}
}

@ARTICLE{10598190,
  author={Carvalko, Joseph R.},
  journal={IEEE Transactions on Technology and Society}, 
  title={Generative AI, Ingenuity, and Law}, 
  year={2024},
  volume={5},
  number={2},
  pages={169-182},
  abstract={This paper discusses generative pre-trained transformer technology and its intersection with forms of creativity and law. It highlights the potential of generative AI to change considerable elements of society, including modes of creative endeavors, problem-solving, employment, education, justice, medicine, and governance. The author emphasizes the need for policymakers and experts to join in regulating against the potential risks and implications of this technology. The European Commission has taken steps to address the risks of AI through the European AI Act (EIA), which categorizes AI uses based on their potential harm. The legislation aims to ensure scrutiny and control in extreme cases like autonomous weapons or medical devices. However, the author criticizes the lack of meaningful AI oversight in the United States and argues that time has come for government to step in and offer meaningful regulation given the technology’s (1) rate of diffusion (2) virtually uncountable product permutations, the purposes, extent and depths to which it is anticipated to penetrate institutional and daily life.},
  keywords={Generative AI;Artificial intelligence;Ethics;Internet;Chatbots;Deep learning;Neural networks;Regulation;Problem-solving;Employment;Education;Europe;Creativity;Social implications of technology;Risk management;Artificial intelligence;computation and language;deep learning neural networks;NLP;LLM;OpenAI;ChatGPT;generative AI;generative pretrained transformer;transformer-based AI;European AI Act;EIA;technology ethics},
  doi={10.1109/TTS.2024.3413591},
  ISSN={2637-6415},
  month={June}
}

@INPROCEEDINGS{9664064,
  author={Ji, Yong},
  booktitle={2021 7th International Conference on Systems and Informatics (ICSAI)}, 
  title={Intelligent Patent Text Summarization Analysis Method}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Patent mining and patent analysis of patented technologies will help protect the interests of intellectual property rights and provide enterprises with correct scientific research directions. In order to study the profitable patents of pharmaceutical companies, this paper proposes an Abstractive RL-LSTM neural network method based on patent texts. The reinforcement learning method is introduced into LSTM. The purpose is to rely on Q-learning to learn the relationship between the main layers. The two parallel layers share the weight of attention from the Q value, and realize the hierarchical control between the LSTM structure of the patent document and the LSTM structure of the sentence. The experimental results show that compared with other methods, the method proposed in this paper can further improve the ROUGE index and alleviate the dependence of the decoder on the input.},
  keywords={Patents;Q-learning;Neural networks;Companies;Intellectual property;Decoding;Computational efficiency;Patent analysis;Text summarization;RL-LSTM;Machine learning},
  doi={10.1109/ICSAI53574.2021.9664064},
  ISSN={},
  month={Nov}
}

@INPROCEEDINGS{10599552,
  author={Katzy, Jonathan and Popescu, Răzvan-Mihai and Van Deursen, Arie and Izadi, Maliheh},
  booktitle={2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge) Conference Acronym:}, 
  title={An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets}, 
  year={2024},
  volume={},
  number={},
  pages={74-85},
  abstract={Does the training of large language models potentially infringe upon code licenses? Furthermore, are there any datasets available that can be safely used for training these models without violating such licenses? In our study, we assess the current trends in the field and the importance of incorporating code into the training of large language models. Additionally, we examine publicly available datasets to see whether these models can be trained on them without the risk of legal issues in the future. To accomplish this, we compiled a list of 53 large language models trained on file-level code. We then extracted their datasets and analyzed how much they overlap with a dataset we created, consisting exclusively of strong copyleft code. Our analysis revealed that every dataset we examined contained license inconsistencies, despite being selected based on their associated repository licenses. We analyzed a total of 514 million code files, discovering 38 million exact duplicates present in our strong copyleft dataset. Additionally, we examined 171 million file-leading comments, identifying 16 million with strong copyleft licenses and another 11 million comments that discouraged copying without explicitly mentioning a license. Based on the findings of our study, which highlights the pervasive issue of license inconsistencies in large language models trained on code, our recommendation for both researchers and the community is to prioritize the development and adoption of best practices for dataset creation and management.},
  keywords={Training;Surveys;Codes;Terminology;Law;Large language models;Licenses;Large Language Models;Foundation Models;Code Licensing;Soft-ware Engineering;ML4SE;Machine Learning;Datasets},
  doi={10.1145/3650105.3652298},
  ISSN={},
  month={April}
}

@INPROCEEDINGS{8624727,
  author={Hoque, Tamzidul and Cruz, Jonathan and Chakraborty, Prabuddha and Bhunia, Swarup},
  booktitle={2018 IEEE International Test Conference (ITC)}, 
  title={Hardware IP Trust Validation: Learn (the Untrustworthy), and Verify}, 
  year={2018},
  volume={},
  number={},
  pages={1-10},
  abstract={Increasing reliance on hardware Intellectual Property (IP) cores in modern system-on-chip (SoC) design flow, often obtained from untrusted vendors distributed across the globe, can significantly compromise the security of SoCs. While the design could be verified for a specified functionality using existing tools, it is extremely hard to verify its trustworthiness to guarantee that no hidden, and possibly malicious function exists in the form of a hardware Trojan. Conventional verification process and tools fail to verify the trust of a third-party IP, primarily due to the lack of trusted reference design or golden models. In this paper, for the first time to our knowledge, we introduce a systematic framework to apply machine learning based classification for hardware IP trust verification. A supervised classifier could be trained for identifying Trojan nets within a suspect IP, but the detection coverage and accuracy are extremely sensitive to the quality of training set available. Furthermore, reliance on a static training database limits the classifier’s ability in detecting new Trojans and facilitates adversarial learning. The proposed framework includes a Trojan insertion tool that dynamically generates a large number of diverse implementations of Trojan classes for creating a robust training set. It is significantly more difficult for an adversary to evade our classifier using known Trojan classes since the tool dynamically samples the entire Trojan population. To further improve the efficiency of the system, we combined three machine learning models into an average probability Voting Ensemble. Our results for two broad classes of Trojan show excellent classification accuracy of 99.69% and 99.88% with F-score of 86.69% and 88.37% for sequential and combinational Trojans, respectively.},
  keywords={Trojan horses;Hardware;IP networks;Training;Tools;Feature extraction;Payloads},
  doi={10.1109/TEST.2018.8624727},
  ISSN={2378-2250},
  month={Oct}
}

@ARTICLE{10440330,
  author={Sai, Siva and Gaur, Aanchal and Sai, Revant and Chamola, Vinay and Guizani, Mohsen and Rodrigues, Joel J. P. C.},
  journal={IEEE Access}, 
  title={Generative AI for Transformative Healthcare: A Comprehensive Study of Emerging Models, Applications, Case Studies, and Limitations}, 
  year={2024},
  volume={12},
  number={},
  pages={31078-31106},
  abstract={Generative artificial intelligence (GAI) can be broadly described as an artificial intelligence system capable of generating images, text, and other media types with human prompts. GAI models like ChatGPT, DALL-E, and Bard have recently caught the attention of industry and academia equally. GAI applications span various industries like art, gaming, fashion, and healthcare. In healthcare, GAI shows promise in medical research, diagnosis, treatment, and patient care and is already making strides in real-world deployments. There has yet to be any detailed study concerning the applications and scope of GAI in healthcare. Addressing this research gap, we explore several applications, real-world scenarios, and limitations of GAI in healthcare. We examine how GAI models like ChatGPT and DALL-E can be leveraged to aid in the applications of medical imaging, drug discovery, personalized patient treatment, medical simulation and training, clinical trial optimization, mental health support, healthcare operations and research, medical chatbots, human movement simulation, and a few more applications. Along with applications, we cover four real-world healthcare scenarios that employ GAI: visual snow syndrome diagnosis, molecular drug optimization, medical education, and dentistry. We also provide an elaborate discussion on seven healthcare-customized LLMs like Med-PaLM, BioGPT, DeepHealth, etc.,Since GAI is still evolving, it poses challenges like the lack of professional expertise in decision making, risk of patient data privacy, issues in integrating with existing healthcare systems, and the problem of data bias which are elaborated on in this work along with several other challenges. We also put forward multiple directions for future research in GAI for healthcare.},
  keywords={Medical services;Chatbots;Medical diagnostic imaging;Data models;Generative AI;Training;Solid modeling;Generative adversarial networks;Artificial intelligence;Drugs;Visualization;Generative AI;ChatGPT;healthcare;LLMs;applications},
  doi={10.1109/ACCESS.2024.3367715},
  ISSN={2169-3536},
  month={}
}

@ARTICLE{10506333,
  author={Wang, Yangang and Sun, Wenqian and Rao, Ruting},
  journal={IEEE Sensors Journal}, 
  title={Accurate and Real-Time Variant Hand Pose Estimation Based on Gray Code Bounding Box Representation}, 
  year={2024},
  volume={24},
  number={11},
  pages={18043-18053},
  abstract={Effective hand gestures are crucial for human-machine interactions, and recent advancements in neural networks offer promising avenues for efficient hand pose estimation. However, existing methods still face challenges in detecting hand poses of different scales within a single RGB image sensor. This article introduces a novel approach, drawing inspiration from modulus conversion, to enhance the efficiency of hand pose estimation from a single RGB image sensor. The method involves transforming the floating-point values of hand regions into binary codes, ensuring continuous numerical space without a significant computational overhead. This approach significantly improves accuracy for hands of varying sizes in both detection and pose estimation. Furthermore, this article addresses the challenge of datasets lacking hand keypoints annotations by introducing a novel loss computation for labeled keypoints during network training. To assess the effectiveness of the proposed method, a new benchmark for variant hand scales is presented, facilitating evaluation across different hand sizes. The proposed approach undergoes testing on diverse datasets, with experimental results demonstrating comparable performance to state-of-the-art methods, thereby validating its efficacy. Additionally, the study conducts several ablation studies, exploring aspects such as the choice of Gray code, code length, effectiveness across different hand scales, and training with labeled keypoints to affirm the efficiency and effectiveness of the proposed method.},
  keywords={Pose estimation;Training;Annotations;Three-dimensional displays;Sensors;Color;Reflective binary codes;Real-time systems;Bounding box representation;gray code;hand pose estimation;real-time},
  doi={10.1109/JSEN.2024.3389055},
  ISSN={1558-1748},
  month={June}
}

@BOOK{10540158,
  author={Gheorghiu, Andrei},
  booktitle={Building Data-Driven Applications with LlamaIndex: A practical guide to retrieval-augmented generation (RAG) to enhance LLM applications},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Solve real-world problems easily with artificial intelligence (AI) using the LlamaIndex data framework to enhance your LLM-based Python applications Key FeaturesExamine text chunking effects on RAG workflows and understand security in RAG app developmentDiscover chatbots and agents and learn how to build complex conversation enginesBuild as you learn by applying the knowledge you gain to a hands-on projectBook DescriptionDiscover the immense potential of Generative AI and Large Language Models (LLMs) with this comprehensive guide. Learn to overcome LLM limitations, such as contextual memory constraints, prompt size issues, real-time data gaps, and occasional ‘hallucinations’. Follow practical examples to personalize and launch your LlamaIndex projects, mastering skills in ingesting, indexing, querying, and connecting dynamic knowledge bases. From fundamental LLM concepts to LlamaIndex deployment and customization, this book provides a holistic grasp of LlamaIndex's capabilities and applications. By the end, you'll be able to resolve LLM challenges and build interactive AI-driven applications using best practices in prompt engineering and troubleshooting Generative AI projects.What you will learnUnderstand the LlamaIndex ecosystem and common use casesMaster techniques to ingest and parse data from various sources into LlamaIndexDiscover how to create optimized indexes tailored to your use casesUnderstand how to query LlamaIndex effectively and interpret responsesBuild an end-to-end interactive web application with LlamaIndex, Python, and StreamlitCustomize a LlamaIndex configuration based on your project needsPredict costs and deal with potential privacy issuesDeploy LlamaIndex applications that others can useWho this book is forThis book is for Python developers with basic knowledge of natural language processing (NLP) and LLMs looking to build interactive LLM applications. Experienced developers and conversational AI developers will also benefit from the advanced techniques covered in the book to fully unleash the capabilities of the framework.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781805124405},
  url={https://ieeexplore.ieee.org/document/10540158}
}

@ARTICLE{10636196,
  author={Fei, Jianwei and Xia, Zhihua and Tondi, Benedetta and Barni, Mauro},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs}, 
  year={2024},
  volume={19},
  number={},
  pages={8322-8337},
  abstract={We propose a novel multi-bit box-free watermarking method for the protection of Intellectual Property Rights (IPR) of GANs with improved robustness against white-box model-level attacks like fine-tuning, pruning, quantization, and surrogate model attacks. The watermark is embedded by adding an extra watermarking loss term during GAN training, ensuring that the images generated by the GAN contain an invisible watermark that can be retrieved by a pre-trained watermark decoder. In order to improve the robustness against white-box model-level attacks, we make sure that the model converges to a wide flat minimum of the watermarking loss term, in such a way that any modification of the model parameters does not erase the watermark. To do so, we add random noise vectors to the parameters of the generator and require that the watermarking loss term is as invariant as possible with respect to the presence of noise. This procedure forces the generator to converge to a wide flat minimum of the watermarking loss. The proposed method is architecture- and dataset-agnostic, thus being applicable to many different generation tasks and models, as well as to CNN-based image processing architectures. We present the results of extensive experiments showing that the presence of the watermark has a negligible impact on the quality of the generated images, and proving the superior robustness of the watermark against model modification and surrogate model attacks.},
  keywords={Watermarking;Robustness;Glass box;Generative adversarial networks;Closed box;Decoding;Generators;GAN watermarking;DNN watermarking;ownership verification;watermark robustness;deep learning security},
  doi={10.1109/TIFS.2024.3443650},
  ISSN={1556-6021},
  month={}
}

@ARTICLE{9146274,
  author={Hua, Wei and Sui, Yulei and Wan, Yao and Liu, Guangzhong and Xu, Guandong},
  journal={IEEE Transactions on Reliability}, 
  title={FCCA: Hybrid Code Representation for Functional Clone Detection Using Attention Networks}, 
  year={2021},
  volume={70},
  number={1},
  pages={304-318},
  abstract={Code cloning, which reuses a fragment of source code via copy-and-paste with or without modifications, is a common way for code reuse and software prototyping. However, the duplicated code fragments often affect software quality, resulting in high maintenance cost. The existing clone detectors using shallow textual or syntactical features to identify code similarity are still ineffective in accurately finding sophisticated functional code clones in real-world code bases. This article proposes functional code clone detector using attention (FCCA), a deep-learning-based code clone detection approach on top of a hybrid code representation by preserving multiple code features, including unstructured (code in the form of sequential tokens) and structured (code in the form of abstract syntax trees and control-flow graphs) information. Multiple code features are fused into a hybrid representation, which is equipped with an attention mechanism that pays attention to important code parts and features that contribute to the final detection accuracy. We have implemented and evaluated FCCA using 275 777 real-world code clone pairs written in Java. The experimental results show that FCCA outperforms several state-of-the-art approaches for detecting functional code clones in terms of accuracy, recall, and F1 score.},
  keywords={Cloning;Feature extraction;Detectors;Syntactics;Software quality;Maintenance engineering;Attention mechanism;code clone detection;code representation;deep neural network (DNN)},
  doi={10.1109/TR.2020.3001918},
  ISSN={1558-1721},
  month={March}
}

@INPROCEEDINGS{10764998,
  author={Chen, Zhi and Jiang, Lingxiao},
  booktitle={2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Promise and Peril of Collaborative Code Generation Models: Balancing Effectiveness and Memorization}, 
  year={2024},
  volume={},
  number={},
  pages={493-505},
  abstract={In the rapidly evolving field of machine learning, training models with datasets from various locations and organizations presents significant challenges due to privacy and legal concerns. The exploration of effective collaborative training settings, which are capable of leveraging valuable knowledge from distributed and isolated datasets, is increasingly crucial.This study investigates key factors that impact the effectiveness of collaborative training methods in code next-token prediction, as well as the correctness and utility of the generated code, showing the promise of such methods. Additionally, we evaluate the memorization of different participant training data across various collaborative training settings, including centralized, federated, and incremental training, showing their potential risks in leaking data.Our findings indicate that the size and diversity of code datasets are pivotal factors influencing the success of collaborative trained code models. We demonstrate that federated learning achieves competitive performance compared to centralized training while offering better data protection, as evidenced by lower memorization ratios in the generated code. However, federated learning can still produce verbatim code snippets from hidden training data, potentially violating data privacy or copyright. Our study further explores the patterns of effectiveness and memorization in incremental learning, emphasizing the importance of the sequence in which individual participant datasets are introduced. Also, we identify the memorization phenomenon of cross-organizational clones as a prevalent challenge in both centralized and federated learning scenarios. Our findings highlight the persistent risk of data leakage during inference, even when training data remains unseen. We conclude with strategic recommendations for practitioners and researchers to optimize the use of multisource datasets, thereby propelling the cross-organizational collaboration forward.CCS Concepts• Software and its engineering → Collaboration in software development; • Computing methodologies → Simulation evaluation; • Security and privacy;},
  keywords={Training;Uniform resource locators;Codes;Incremental learning;Federated learning;Computational modeling;Collaboration;Training data;Cloning;Software engineering;Collaborative Training;Memorization;Large Language Model;Code Generation},
  doi={},
  ISSN={2643-1572},
  month={Oct}
}

@ARTICLE{10184464,
  author={Luo, Yuanjing and Zhou, Tongqing and Cui, Shenglan and Ye, Yunfan and Liu, Fang and Cai, Zhiping},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Fixing the Double Agent Vulnerability of Deep Watermarking: A Patch-Level Solution Against Artwork Plagiarism}, 
  year={2024},
  volume={34},
  number={3},
  pages={1670-1683},
  abstract={Increasing artwork plagiarism incidents stresses the urgent need for proper copyright protection on behalf of the creators. The latest development in this context focuses on embedding watermarks via deep encoder-decoder networks. However, we find that deep watermarking has a serious vulnerability on its robustness when facing deliberate plagiarism. To manifest it, we construct an attack that misuses watermarking encoder as a plagiarism lookout for bypassing copyright detection. As a remedy, we propose a patch-level deep watermarking framework (DIPW) to retain copyright evidence in essential patches with plagiarism resistance, inspired by a user study observation that subject elements in artworks are the principal plagiarism entities. Technically, DIPW adaptively finds the embedding patches by identifying a subset of non-overlapping and feature-rich objects; and tailors the model with dual-distortion losses and adversarial plagiarism noise injection for robustness. Experimental results demonstrate the superiority of DIPW in facilitating better robustness, secrecy, and imperceptibility with acceptable time burden.},
  keywords={Watermarking;Plagiarism;Training;Robustness;Decoding;Distortion;Copyright protection;Deep watermarking;artwork copyright protection;plagiarism resistance;convolutional neural networks},
  doi={10.1109/TCSVT.2023.3295895},
  ISSN={1558-2205},
  month={March}
}

@ARTICLE{8963921,
  author={Tseng, Bo-Wei and Wu, Pei-Yuan},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Compressive Privacy Generative Adversarial Network}, 
  year={2020},
  volume={15},
  number={},
  pages={2499-2513},
  abstract={Machine learning as a service (MLaaS) has brought much convenience to our daily lives recently. However, the fact that the service is provided through cloud raises privacy leakage issues. In this work we propose the compressive privacy generative adversarial network (CPGAN), a data-driven adversarial learning framework for generating compressing representations that retain utility comparable to state-of-the-art, with the additional feature of defending against reconstruction attack. This is achieved by applying adversarial learning scheme to the design of compression network (privatizer), whose utility/privacy performances are evaluated by the utility classifier and the adversary reconstructor, respectively. Experimental results demonstrate that CPGAN achieves better utility/privacy trade-off in comparison with the previous work, and is applicable to real-world large datasets.},
  keywords={Data privacy;Privatization;Privacy;Data models;Generative adversarial networks;Stochastic processes;Feature extraction;Compressive privacy;cyber security;privacy preserving machine learning;adversarial learning;generative adversarial networks;machine learning as a service},
  doi={10.1109/TIFS.2020.2968188},
  ISSN={1556-6021},
  month={}
}

@INPROCEEDINGS{9691515,
  author={Moorthy, Vaishnavi and Venkataraman, Revathi},
  booktitle={2021 IEEE 18th India Council International Conference (INDICON)}, 
  title={Generative Adversarial Analysis using U-LSB Based Audio Steganography}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Audio steganography is the technique of hiding data within a carrier audio file, where concealed data is imperceptible to humans. The existing deep-learning-based approaches depend on human handcraft for the generation and steganalysis of the steganographic audio. Generative Adversarial Network (GAN) based models are used nowadays for the generation of audio data from random latent space have proven to be efficient. This can be further utilized to strengthen the existing audio steganography methods. The proposed framework is a GAN model consisting of the U-Net-based generator, LSB-based embedder, and the discriminator. The method relies on an unsupervised adversarial training algorithm for embedding secret audio within the carrier audio in the temporal domain, making it imperceptible to humans. The experimental results on the 1-second Speech Command dataset show that the model can effectively produce steganographic audio using embedding probabilities. The steganographic method has proven to produce high-fidelity audio files capable of resisting steganalysis as compared to the audios generated using conventional methods. This technique can be effectively used for secured communication of the audio files transmission in midst of cyberspace hacking tricks.},
  keywords={Training;Industries;Steganography;Neural networks;Information security;Detectors;Generative adversarial networks;audio steganography;audio file;generative adversarial network;carrier audio;unsupervised adversarial training},
  doi={10.1109/INDICON52576.2021.9691515},
  ISSN={2325-9418},
  month={Dec}
}

@INPROCEEDINGS{10631139,
  author={Ran, Zhuoheng and Abdelgawad, Muhammad A.A. and Zhang, Zekai and Cheung, Ray C.C. and Yan, Hong},
  booktitle={2024 IEEE 35th International Conference on Application-specific Systems, Architectures and Processors (ASAP)}, 
  title={RO-SVD: A Reconfigurable Hardware Copyright Protection Framework for AIGC Applications}, 
  year={2024},
  volume={},
  number={},
  pages={135-142},
  abstract={The dramatic surge in the utilisation of generative artificial intelligence (GenAI) underscores the need for a secure and efficient mechanism to responsibly manage, use and disseminate multidimensional data generated by artificial intelligence (AI). In this paper, we propose a blockchain-based copyright traceability framework called ring oscillator-singular value decomposition (RO-SVD), which introduces decomposition computing to approximate low-rank matrices generated from hardware entropy sources and establishes an AI-generated content (AIGC) copyright traceability mechanism at the device level. By leveraging the parallelism and reconfigurability of field-programmable gate arrays (FPGAs), our framework can be easily constructed on existing AI -accelerated devices and provide a low-cost solution to emerging copyright issues of AIGC. We developed a hardware-software (HW /SW) co-design prototype based on comprehensive analysis and on-board experiments with multiple AI-applicable FPGAs. Using AI-generated images as a case study, oyr framework demonstrated effectiveness and emphasised customisation, unpredictability, efficiency, manage-ment and reconfigurability. To the best of our knowledge, this is the first practical hardware study discussing and implementing copyright traceability specifically for AI -generated conten t.},
  keywords={Program processors;Generative AI;Systems architecture;Surge protection;Prototypes;Parallel processing;Hardware;AI-generated content;copyright protection;singular value decomposition (SVD);blockchain;nanofabrication;AI security;low-power AI},
  doi={10.1109/ASAP61560.2024.00037},
  ISSN={2160-052X},
  month={July}
}

@ARTICLE{10221755,
  author={Wang, Yuntao and Pan, Yanghe and Yan, Miao and Su, Zhou and Luan, Tom H.},
  journal={IEEE Open Journal of the Computer Society}, 
  title={A Survey on ChatGPT: AI–Generated Contents, Challenges, and Solutions}, 
  year={2023},
  volume={4},
  number={},
  pages={280-302},
  abstract={With the widespread use of large artificial intelligence (AI) models such as ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is leading a paradigm shift in content creation and knowledge representation. AIGC uses generative large AI algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts. Despite the recent significant progress in AIGC, security, privacy, ethical, and legal challenges still need to be addressed. This paper presents an in-depth survey of working principles, security and privacy threats, state-of-the-art solutions, and future challenges of the AIGC paradigm. Specifically, we first explore the enabling technologies, general architecture of AIGC, and discuss its working modes and key characteristics. Then, we investigate the taxonomy of security and privacy threats to AIGC and highlight the ethical and societal implications of GPT and AIGC technologies. Furthermore, we review the state-of-the-art AIGC watermarking approaches for regulatable AIGC paradigms regarding the AIGC model and its produced content. Finally, we identify future challenges and open research directions related to AIGC.},
  keywords={Artificial intelligence;Chatbots;Surveys;Security;Computational modeling;Privacy;Training;AIGC;generative AI;ChatGPT;security;and privacy},
  doi={10.1109/OJCS.2023.3300321},
  ISSN={2644-1268},
  month={}
}

@INPROCEEDINGS{10133375,
  author={Olney, Brooks and Karam, Robert},
  booktitle={2023 IEEE International Symposium on Hardware Oriented Security and Trust (HOST)}, 
  title={Bits to BNNs: Reconstructing FPGA ML-IP with Joint Bitstream and Side-Channel Analysis}, 
  year={2023},
  volume={},
  number={},
  pages={238-248},
  abstract={Energy-efficient hardware acceleration platforms for edge deployment of artificial intelligence (AI) and machine learning (ML) applications has been an ongoing research endeavor. Many efforts have focused on optimizing the algorithms and compute structures for use in resource-constrained hardware such as field-programmable gate arrays (FPGAs). Indeed, the difficult nature of crafting the best model makes the ML model itself a valuable intellectual property (IP) asset. This can be problematic, as the IP can now be exposed to an attacker through physical interfaces, enabling threats from side-channel analysis (SCA) attacks. One of the more devastating attacks is the model extraction attack, which threatens piracy and cloning of the valuable IP. While the problem of SCA-based model extraction on FPGA-deployed neural networks has been well-studied, it does not capture the full picture of what vulnerabilities may be present in those platforms. In this paper, we demonstrate how bitstream analysis can be used to obtain neural network parameters and connectivity information from block RAMs (BRAMs). We leverage the knowledge gleaned from the bitstream to mount a power SCA attack to further refine the network reconstruction effort. This is the first method that has approached the problem of ML-IP theft from the angle of FPGA bitstream analysis and suggests that further work is needed to improve security assurance for edge intelligence.},
  keywords={Knowledge engineering;Machine learning algorithms;Computational modeling;Neural networks;Random access memory;Machine learning;Intellectual property},
  doi={10.1109/HOST55118.2023.10133375},
  ISSN={2765-8406},
  month={May}
}

@INPROCEEDINGS{10616568,
  author={Asharani, R and Vidyalakshmi, K},
  booktitle={2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS)}, 
  title={A Comparative Analysis on Exploration of Stegosploits across Various Media Formats}, 
  year={2024},
  volume={1},
  number={},
  pages={1-8},
  abstract={The terms “steganography” and “exploits” are combined to form “stegosploits,” a clever and covert method of hiding hostile payloads inside innocent media files. The investigation of stegosploits in a variety of media types is presented in-depth in this study. Stegosploits are a serious danger to cybersecurity because they may elude detection methods that are conventional, giving attackers the ability to covertly implant and distribute malicious payloads over a variety of communication routes. In order to improve knowledge and awareness of this new cyber threat scenario, this article will look at stegosploit tactics in photos, audios, videos, and other multimedia formats.},
  keywords={Surveys;Deep learning;Steganography;Reviews;Media;Streaming media;Generative adversarial networks;Adversarial Generative Network;Deep Learning;Image Features;Steganography;Steganalysis;Multimedia Security;and Malware Hiding},
  doi={10.1109/ICKECS61492.2024.10616568},
  ISSN={},
  month={April}
}

@INPROCEEDINGS{10578869,
  author={Palacios-Alonso, Daniel and Urquiza-Fuentes, Jaime and Velázquez-Iturbide, J. Ángel and Guillén-García, Julio},
  booktitle={2024 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Experiences and Proposals of Use of Generative AI in Advanced Software Courses}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={The last year, we have witnessed the popularization of generative artificial intelligence. Its output includes text, code, image, audio, speech, voice, music, and video. Therefore, it impacts education courses where students are required to elaborate on any of these artifacts. In particular, the generation of code affects informatics courses, where assignments usually ask students to develop and deliver programming code. The impact of generative artificial intelligence on informatics courses has been mainly studied for introductory programming courses. These studies have shown that generative artificial intelligence is able to produce highly sophisticated programs, but also that its results and rationale can be inaccurate. Moreover, the impact of generative artificial intelligence has not been studied for other informatics subjects. In this paper, we present our preliminary experience and proposals on three advanced software courses, namely video games, advanced algorithms and language processors. For the video games course, we present the opportunities of use of generative artificial intelligence and the results of a survey conducted with students on their use to obtain different media products. For the algorithms course, we present the result of a session driven by the instructor on different design techniques, showing the merits and demerits of the answers generated. For the language processors course, a proposal of use of generative artificial intelligence is presented, broken down into the parts of a typical language processor. The paper concludes with some suggestions for instructors.},
  keywords={Surveys;Video games;Program processors;Codes;Generative AI;Software algorithms;Software;informatics education;generative artificial intelligence;video games;advanced algorithms;language processors},
  doi={10.1109/EDUCON60312.2024.10578869},
  ISSN={2165-9567},
  month={May}
}

@INPROCEEDINGS{10825085,
  author={Wood, David and Lublinsky, Boris and Roytman, Alexy and Singh, Shivdeep and Adam, Constantin and Adebayo, Abdulhamid and An, Sungeun and Chang, Yuan Chi and Dang, Xuan-Hong and Desai, Nirmit and Dolfi, Michele and Emami-Gohari, Hajar and Eres, Revital and Goto, Takuya and Joshi, Dhiraj and Koyfman, Yan and Nassar, Mohammad and Patel, Hima and Selvam, Paramesvaran and Shah, Yousaf and Surendran, Saptha and Tsuzuku, Daiki and Zerfos, Petros and Daijavad, Shahrokh},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Data-Prep-Kit: getting your data ready for LLM application development}, 
  year={2024},
  volume={},
  number={},
  pages={2234-2243},
  abstract={Data preparation is the first and a very important step towards any Large Language Model (LLM) development. This paper introduces an easy-to-use, extensible, and scale-flexible open-source data preparation toolkit called Data Prep Kit (DPK). DPK is architected and designed to enable users to scale their data preparation to their needs. With DPK they can prepare data on a local machine or effortlessly scale to run on a cluster with thousands of CPU Cores. DPK comes with a highly scalable, yet extensible set of modules that transform natural language and code data. If the user needs additional transforms, they can be easily developed using extensive DPK support for transform creation. These modules can be used independently or pipelined to perform a series of operations. In this paper, we describe DPK architecture and show its performance from a small scale to a very large number of CPUs. The modules from DPK have been used for the preparation of Granite Models [1] [2]. We believe DPK is a valuable contribution to the AI community to easily prepare data to enhance the performance of their LLM models or to fine-tune models with Retrieval-Augmented Generation (RAG).},
  keywords={Automation;Codes;Runtime;Large language models;Retrieval augmented generation;Natural languages;Transforms;Writing;Data models;Sparks;LLM;Generative AI;Data Preparation;Data Processing;Toolkit;Open-source;Ray;Spark;KFP;RAG},
  doi={10.1109/BigData62323.2024.10825085},
  ISSN={2573-2978},
  month={Dec}
}

@INPROCEEDINGS{8478502,
  author={Wu, Feifei and Mi, Lan and Li, Xin and Huang, Lucheng and Tong, Yiming},
  booktitle={2018 IEEE International Symposium on Innovation and Entrepreneurship (TEMS-ISIE)}, 
  title={Identifying Potential Standard Essential Patents Based on Text Mining and Generative Topographic Mapping}, 
  year={2018},
  volume={},
  number={},
  pages={1-9},
  abstract={The identification of the potential standards essential patents (SEPs) can make great contributions to not only the technology management theory, but also to the real practices of establishment and development of enterprise competitiveness and standardization strategy. However, despite the importance of identifying potential SEPs, the approaches to identify potential SEPs lack of adequate mining of existing technical standard text and validation of identification results based on standard updates. Therefore, in this paper, we contribute to resolving this issue by proposing a research model that integrates text mining and the generative topographic mapping (GTM) to effectively identify and verity the potential SEPs based on existing and updated standards. The universal terrestrial radio access (UTRA) technology is selected as a case study. In this case, the TF-IDF algorithm and the Latent Dirichlet Allocation (LDA) method are applied to analyze the keywords and technical theme of standard and patent documents, and GTM is used to construct standard and patent map, then the two maps are mapped by the improved similarity algorithm we proposed. Finally, 39 potential SEPs of the technology are identified, 24 of which have been verified, and the other 15 are likely to be included in subsequent standard version. This paper will contribute to the identification of SEPs methodology, and will be of interest to UTRA technology research and development experts.},
  keywords={Standards;Patents;Vacuum technology;Technological innovation;Text mining;Forecasting;Technical standard;Standard essential patents;Patent identification;Generative Topographic Mapping;Text mining},
  doi={10.1109/TEMS-ISIE.2018.8478502},
  ISSN={},
  month={March}
}

@ARTICLE{10925518,
  author={Singh, Himanshu Kumar and Baranwal, Naman and Singh, Kedar Nath and Singh, Amit Kumar},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Multilevel Ownership Protection via Watermarking and Encryption}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={Nowadays, the ownership of shared social images has attracted increasingly serious privacy violation concerns, thus the protection of these images is particularly important. Further, due to the increasing value of deep learning (DL) models in social media platforms, there is an urgent demand to protect their copyright and prevent privacy leakage. Currently, relatively limited research has been carried out in the field of ownership protection for deep neural network (DNN) and, at the same time, related social images. This article presents a robust copyright protection system and method for the DNN model and related social images to verify ownership. Initially, split-way training of our model was established, based on cover and encoded secret images, to reduce or eradicate privacy leakage. Then, the sender sends only encoded vector information instead of raw data to adversarial-based embedding and extraction networks. Next, we embed a secret mark in DL embedding and extraction models, using interpolation-based watermarking to verify the ownership of suspicious models if any piracy or infringements occur. Last, the intended receiver extracts the hidden information using the extraction networks. Extensive experiments show that the proposed system is more robust and secure against attacks than state-of-the-art methods, making it beneficial for social media and other practical applications. The results demonstrate that the proposed method outperforms state-of-the-art methods in terms of average peak signal-to-noise noise ratio, normalized correlation, number of pixel change rate, and unified average change intensity, with improvements of 47.25%, 43.25%, 17.89%, and 14.23%, respectively.},
  keywords={Watermarking;Encryption;Privacy;Convolutional neural networks;Robustness;Feature extraction;Copyright protection;Convolutional codes;Biological system modeling;Training;Attacks;copyright protection;deep learning;digital watermarking;encryption;split learning},
  doi={10.1109/TCSS.2025.3544766},
  ISSN={2329-924X},
  month={}
}

@INPROCEEDINGS{8952475,
  author={Kang, Hong Jin and Bissyandé, Tegawendé F. and Lo, David},
  booktitle={2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Assessing the Generalizability of Code2vec Token Embeddings}, 
  year={2019},
  volume={},
  number={},
  pages={1-12},
  abstract={Many Natural Language Processing (NLP) tasks, such as sentiment analysis or syntactic parsing, have benefited from the development of word embedding models. In particular, regardless of the training algorithms, the learned embeddings have often been shown to be generalizable to different NLP tasks. In contrast, despite recent momentum on word embeddings for source code, the literature lacks evidence of their generalizability beyond the example task they have been trained for. In this experience paper, we identify 3 potential downstream tasks, namely code comments generation, code authorship identification, and code clones detection, that source code token embedding models can be applied to. We empirically assess a recently proposed code token embedding model, namely code2vec's token embeddings. Code2vec was trained on the task of predicting method names, and while there is potential for using the vectors it learns on other tasks, it has not been explored in literature. Therefore, we fill this gap by focusing on its generalizability for the tasks we have identified. Eventually, we show that source code token embeddings cannot be readily leveraged for the downstream tasks. Our experiments even show that our attempts to use them do not result in any improvements over less sophisticated methods. We call for more research into effective and general use of code embeddings.},
  keywords={Task analysis;Software engineering;Natural language processing;Training;Cloning;Vocabulary;Code Embeddings;Distributed Representations;Big Code},
  doi={10.1109/ASE.2019.00011},
  ISSN={2643-1572},
  month={Nov}
}

@INPROCEEDINGS{10884501,
  author={Lai, Kai-Huang and Xi, Wu-Dong and Xing, Xing-Xing and Wan, Wei and Wang, Chang-Dong and Chen, Min and Guizani, Mohsen},
  booktitle={2024 IEEE International Conference on Data Mining (ICDM)}, 
  title={RecCoder: Reformulating Sequential Recommendation as Large Language Model-Based Code Completion}, 
  year={2024},
  volume={},
  number={},
  pages={191-200},
  abstract={In the evolving landscape of sequential recommendation systems, the application of Large Language Models (LLMs) is increasingly prominent. However, current attempts typically utilize general-purpose LLMs, which present a mismatch in capability and a large semantic gap relative to the specialized needs of recommendation tasks. To tackle these issues, we introduce RecCoder, an innovative model that reformulates sequential recommendation as a code completion task. This approach leverages the superior reasoning capability of code LLMs as a backbone, aligning well with the requirements of recommendation systems. To bridge the semantic gap, RecCoder creates extra tokens for each item and employs item content to initialize token embeddings. Furthermore, we have developed a suite of Semantic Adaptation Fine-tuning tasks, tailored to enhance the model's acquisition of both content and collaborative semantic information, thus aligning the model's intrinsic capabilities with the unique demands of recommendation tasks. Through extensive testing on three public datasets, RecCoder has shown remarkable improvements over existing models in terms of recommendation accuracy and efficiency. This success highlights the substantial yet previously underexplored potential of code LLMs in improving recommendation accuracy and efficiency, suggesting a promising new direction for future research in this area. The implementation code is accessible at https://github.com/AllminerLab/Code-for-RecCoder-master.},
  keywords={Adaptation models;Codes;Accuracy;Large language models;Semantics;Cognition;Data models;Data mining;Recommender systems;Testing;sequential recommendation;large language model},
  doi={10.1109/ICDM59182.2024.00026},
  ISSN={2374-8486},
  month={Dec}
}

@BOOK{10769301,
  author={Irani, Behram and Sonawane, Rahul},
  booktitle={Generative AI-Powered Assistant for Developers: Accelerate software development with Amazon Q Developer},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Leverage Amazon Q Developer to boost productivity and maximize efficiency by accelerating software development life cycle tasksKey FeaturesFirst book on the market to thoroughly explore all of Amazon Q Developer’s featuresGain an understanding of Amazon Q Developer's capabilities across the software development life cycle through real-world examplesBuild apps with Amazon Q Developer by auto-generating code in various languages within supported IDEsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionMany developers face the challenge of managing repetitive tasks and maintaining productivity. This book will help you tackle both these challenges with Amazon Q Developer, a generative AI-powered assistant designed to optimize coding and streamline workflows. This book takes you through the setup and customization of Amazon Q Developer, demonstrating how to leverage its capabilities for auto-code generation, code explanation, and transformation across multiple IDEs and programming languages. You'll learn to use Amazon Q Developer to enhance coding experiences, generate accurate code references, and ensure security by scanning for vulnerabilities. The book also shows you how to use Amazon Q Developer for AWS-related tasks, including solution building, applying architecture best practices, and troubleshooting errors. Each chapter provides practical insights and step-by-step guidance to help you fully integrate this powerful tool into your development process. You’ll get to grips with effortless code implementation, explanation, transformation, and documentation, helping you create applications faster and improve your development experience. By the end of this book, you’ll have mastered Amazon Q Developer to accelerate your software development lifecycle, improve code quality, and build applications faster and more efficiently.What you will learnUnderstand the importance of generative AI-powered assistants in developers' daily workEnable Amazon Q Developer for IDEs and with AWS services to leverage code suggestionsCustomize Amazon Q Developer to align with organizational coding standardsUtilize Amazon Q Developer for code explanation, transformation, and feature developmentUnderstand code references and scan for code security issues using Amazon Q DeveloperAccelerate building solutions and troubleshooting errors on AWSWho this book is forThis book is for coders, software developers, application builders, data engineers, and technical resources using AWS services looking to leverage Amazon Q Developer's features to enhance productivity and accelerate business outcomes. Basic coding skills are needed to understand the concepts covered in this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835081204},
  url={https://ieeexplore.ieee.org/document/10769301}
}

@INPROCEEDINGS{10764906,
  author={Wang, Jian and Liu, Shangqing and Xie, Xiaofei and Li, Yi},
  booktitle={2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={An Empirical Study to Evaluate AIGC Detectors on Code Content}, 
  year={2024},
  volume={},
  number={},
  pages={844-856},
  abstract={Artificial Intelligence Generated Content (AIGC) has garnered considerable attention for its impressive performance, with Large Language Models (LLMs), like ChatGPT, emerging as a leading AIGC model that produces high-quality responses across various applications, including software development and maintenance. Despite its potential, the misuse of LLMs, especially in security and safety-critical domains, such as academic integrity and answering questions on Stack Overflow, poses significant concerns. Numerous AIGC detectors have been developed and evaluated on natural language data. However, their performance on code-related content generated by LLMs remains unexplored.To fill this gap, in this paper, we present an empirical study evaluating existing AIGC detectors in the software domain. We select three state-of-the-art LLMs, i.e., GPT-3.5, WizardCoder and CodeLlama, for machine-content generation. We further created a comprehensive dataset including 2.23M samples comprising code-related content for each model, encompassing popular software activities like Q&A (150K), code summarization (1M), and code generation (1.1M). We evaluated thirteen AIGC detectors, comprising six commercial and seven open-source solutions, assessing their performance on this dataset. Our results indicate that AIGC detectors perform less on code-related data than natural language data. Fine-tuning can enhance detector performance, especially for content within the same domain; but generalization remains a challenge.CCS CONCEPTS• General and reference → Empirical studies; • Security and privacy → Social aspects of security and privacy; • Computing methodologies → Natural language generation.},
  keywords={Privacy;Codes;Natural languages;Natural language generation;Detectors;Software;Maintenance;Security;Software engineering;Software development management;AIGC Detection;Code Generation;Large Language Model},
  doi={},
  ISSN={2643-1572},
  month={Oct}
}

@INPROCEEDINGS{8758633,
  author={Salmani, Hassan and Hoque, Tamzidul and Bhunia, Swarup and Yasin, Muhammad and Rajendran, Jeyavijayan JV and Karimi, Naghmeh},
  booktitle={2019 IEEE 37th VLSI Test Symposium (VTS)}, 
  title={Special Session: Countering IP Security threats in Supply chain}, 
  year={2019},
  volume={},
  number={},
  pages={1-9},
  abstract={The continuing decrease in feature size of integrated circuits, and the increase of the complexity and cost of design and fabrication has led to outsourcing the design and fabrication of integrated circuits to third parties across the globe, and in turn has introduced several security vulnerabilities. The adversaries in the supply chain can pirate integrated circuits, overproduce these circuits, perform reverse engineering, and/or insert hardware Trojans in these circuits. Developing countermeasures against such security threats is highly crucial. Accordingly, this paper first develops a learning-based trust verification framework to detect hardware Trojans. To tackle Trojan insertion, IP piracy and overproduction, logic locking schemes and in particular stripped functionality logic locking is discussed and its resiliency against the state-of-the-art attacks is investigated.},
  keywords={Integrated circuits;Training;Data models;Monitoring;Tagging;Valves;Machine learning},
  doi={10.1109/VTS.2019.8758633},
  ISSN={2375-1053},
  month={April}
}

@BOOK{10522552,
  author={Bodungen, Clint and Crow, Aaron},
  booktitle={ChatGPT for Cybersecurity Cookbook: Learn practical generative AI recipes to supercharge your cybersecurity skills},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Master ChatGPT and the OpenAI API and harness the power of cutting-edge generative AI and large language models to revolutionize the way you perform penetration testing, threat detection, and risk assessment.Key FeaturesEnhance your skills by leveraging ChatGPT to generate complex commands, write code, and create toolsAutomate penetration testing, risk assessment, and threat detection tasks using the OpenAI API and Python programmingRevolutionize your approach to cybersecurity with an AI-powered toolkitPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionAre you ready to unleash the potential of AI-driven cybersecurity? This cookbook takes you on a journey toward enhancing your cybersecurity skills, whether you’re a novice or a seasoned professional. By leveraging cutting-edge generative AI and large language models such as ChatGPT, you'll gain a competitive advantage in the ever-evolving cybersecurity landscape. ChatGPT for Cybersecurity Cookbook shows you how to automate and optimize various cybersecurity tasks, including penetration testing, vulnerability assessments, risk assessment, and threat detection. Each recipe demonstrates step by step how to utilize ChatGPT and the OpenAI API to generate complex commands, write code, and even create complete tools. You’ll discover how AI-powered cybersecurity can revolutionize your approach to security, providing you with new strategies and techniques for tackling challenges. As you progress, you’ll dive into detailed recipes covering attack vector automation, vulnerability scanning, GPT-assisted code analysis, and more. By learning to harness the power of generative AI, you'll not only expand your skillset but also increase your efficiency. By the end of this cybersecurity book, you’ll have the confidence and knowledge you need to stay ahead of the curve, mastering the latest generative AI tools and techniques in cybersecurity.What you will learnMaster ChatGPT prompt engineering for complex cybersecurity tasksUse the OpenAI API to enhance and automate penetration testingImplement artificial intelligence-driven vulnerability assessments and risk analysesAutomate threat detection with the OpenAI APIDevelop custom AI-enhanced cybersecurity tools and scriptsPerform AI-powered cybersecurity training and exercisesOptimize cybersecurity workflows using generative AI-powered techniquesWho this book is forThis book is for cybersecurity professionals, IT experts, and enthusiasts looking to harness the power of ChatGPT and the OpenAI API in their cybersecurity operations. Whether you're a red teamer, blue teamer, or security researcher, this book will help you revolutionize your approach to cybersecurity with generative AI-powered techniques. A basic understanding of cybersecurity concepts along with familiarity in Python programming is expected. Experience with command-line tools and basic knowledge of networking concepts and web technologies is also required.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781805125112},
  url={https://ieeexplore.ieee.org/document/10522552}
}

@BOOK{10769392,
  author={Rodriguez, Carlos and Shaikh, Samira},
  booktitle={Generative AI Foundations in Python: Discover key techniques and navigate modern challenges in LLMs},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Begin your generative AI journey with Python as you explore large language models, understand responsible generative AI practices, and apply your knowledge to real-world applications through guided tutorialsKey FeaturesGain expertise in prompt engineering, LLM fine-tuning, and domain adaptationUse transformers-based LLMs and diffusion models to implement AI applicationsDiscover strategies to optimize model performance, address ethical considerations, and build trust in AI systemsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe intricacies and breadth of generative AI (GenAI) and large language models can sometimes eclipse their practical application. It is pivotal to understand the foundational concepts needed to implement generative AI. This guide explains the core concepts behind -of-the-art generative models by combining theory and hands-on application. Generative AI Foundations in Python begins by laying a foundational understanding, presenting the fundamentals of generative LLMs and their historical evolution, while also setting the stage for deeper exploration. You’ll also understand how to apply generative LLMs in real-world applications. The book cuts through the complexity and offers actionable guidance on deploying and fine-tuning pre-trained language models with Python. Later, you’ll delve into topics such as task-specific fine-tuning, domain adaptation, prompt engineering, quantitative evaluation, and responsible AI, focusing on how to effectively and responsibly use generative LLMs. By the end of this book, you’ll be well-versed in applying generative AI capabilities to real-world problems, confidently navigating its enormous potential ethically and responsibly.What you will learnDiscover the fundamentals of GenAI and its foundations in NLPDissect foundational generative architectures including GANs, transformers, and diffusion modelsFind out how to fine-tune LLMs for specific NLP tasksUnderstand transfer learning and fine-tuning to facilitate domain adaptation, including fields such as financeExplore prompt engineering, including in-context learning, templatization, and rationalization through chain-of-thought and RAGImplement responsible practices with generative LLMs to minimize bias, toxicity, and other harmful outputsWho this book is forThis book is for developers, data scientists, and machine learning engineers embarking on projects driven by generative AI. A general understanding of machine learning and deep learning, as well as some proficiency with Python, is expected.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835464915},
  url={https://ieeexplore.ieee.org/document/10769392}
}

@ARTICLE{10416322,
  author={de Souza, Cleidson Ronald Botelho and Rodríguez-Pérez, Gema and Basha, Manaal and Yoon, Dongwook and Beschastnikh, Ivan},
  journal={IEEE Software}, 
  title={The Fine Balance Between Helping With Your Job and Taking It: AI Code Assistants Come to the Fore}, 
  year={2024},
  volume={41},
  number={6},
  pages={111-118},
  abstract={AI code generation tools are reshaping the software engineering landscape. We provide recommendations for practitioners interested in these tools based on narratives we have collected regarding two AI code generation tools, GitHub Copilot and Tabnine.},
  keywords={Codes;Blogs;Artificial intelligence;Social networking (online);Software engineering;Software development management;Productivity;Generative AI;Software tools;Software development management;Law;Licenses},
  doi={10.1109/MS.2024.3357787},
  ISSN={1937-4194},
  month={Nov}
}

@ARTICLE{10758657,
  author={Kim, Tae-Seok and John Ignacio, Marvin and Yu, Seunghee and Jin, Hulin and Kim, Yong-Guk},
  journal={IEEE Access}, 
  title={UI/UX for Generative AI: Taxonomy, Trend, and Challenge}, 
  year={2024},
  volume={12},
  number={},
  pages={179891-179911},
  abstract={Current technological advancements in Information Technology are closely linked to Generative Artificial Intelligence, enabling the automation of complex tasks such as generating documents, images, videos, audio, and actions. As such tasks can save much human labor and resources, diverse industries are trying to adopt this technology. However, developing a product utilizing Generative AI is a challenging task, partly because it is a new technology and many users are not familiar with it yet. This paper’s primary goal is to find a better way to design Generative AI systems, especially from the Human-Computer Interaction perspective. To begin, we propose a taxonomy for Generative AI systems based on their modality, such as text-based, image-based, audio-based, and multi-modal-based systems, and then evaluate them in terms of their usability, because their functionalities should be aligned with the User Interface (UI), leading to a better User Experience (UX). We survey important trends in this area and introduce future applications by touching upon the issue of explainable AI. Although Generative AI has a bright future, it faces formidable challenges in our industries and society. It is hoped that the taxonomy and research findings presented here will be a useful framework for future research in Generative AI systems and their UI/UX.},
  keywords={Generative AI;Artificial intelligence;Chatbots;Taxonomy;User experience;Internet;Encoding;Bidirectional control;Generators;Decoding;Audio;explainable AI;generative AI;image;multimodal-GPT;text;UI/UX},
  doi={10.1109/ACCESS.2024.3502628},
  ISSN={2169-3536},
  month={}
}

@INPROCEEDINGS{10317307,
  author={Zhang, Manman and Ma, Yuchen and Luo, Ge and Li, Sheng and Qian, Zhenxing and Zhang, Xinpeng},
  booktitle={2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)}, 
  title={Identifying the Style of Chatting}, 
  year={2023},
  volume={},
  number={},
  pages={1085-1092},
  abstract={Group chat brings great convenience to people’s online social interaction. When identifying users in online social networks, individual speaking style of the member in group chat particularly plays an important role. In this paper, we propose a novel and efficient group chat hashing framework termed Group Chat Style Hashing(GCSH), which is the first work to utilize personal hashes in group chats to identify the speakers. GCSH combines a supervised VAE and a discriminator to extract accurate identity-related features and applies a new sample aggregation hash algorithm to generate an exclusive identity style hash for each one in the group chat. Our model fuses more context information to directly get the representative hashes, achieving an end-to-end identification model and sharply reducing the matching cost compared with previous text hashing methods. We conduct extensive experiments to prove the effectiveness and the generalization ability of our method across datasets.},
  keywords={Costs;Social networking (online);Fuses;Asia;Information processing;Feature extraction;Context modeling},
  doi={10.1109/APSIPAASC58517.2023.10317307},
  ISSN={2640-0103},
  month={Oct}
}

@INPROCEEDINGS{10179439,
  author={Patrick-Evans, James and Dannehl, Moritz and Kinder, Johannes},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)}, 
  title={XFL: Naming Functions in Binaries with Extreme Multi-label Learning}, 
  year={2023},
  volume={},
  number={},
  pages={2375-2390},
  abstract={Reverse engineers benefit from the presence of identifiers such as function names in a binary, but usually these are removed for release. Training a machine learning model to predict function names automatically is promising but fundamentally hard: unlike words in natural language, most function names occur only once. In this paper, we address this problem by introducing eXtreme Function Labeling (XFL), an extreme multi-label learning approach to selecting appropriate labels for binary functions. XFL splits function names into tokens, treating each as an informative label akin to the problem of tagging texts in natural language. We relate the semantics of binary code to labels through Dexter, a novel function embedding that combines static analysis-based features with local context from the call graph and global context from the entire binary. We demonstrate that XFL/Dexter outperforms the state of the art in function labeling on a dataset of 10,047 binaries from the Debian project, achieving a precision of 83.5%. We also study combinations of XFL with alternative binary embeddings from the literature and show that Dexter consistently performs best for this task. As a result, we demonstrate that binary function labeling can be effectively phrased in terms of multi-label learning, and that binary function embeddings benefit from including explicit semantic features.},
  keywords={Training;Semantics;Natural languages;XML;Binary codes;Static analysis;Predictive models;Binary-Analysis;Reverse-Engineering;Representation-Learning;Extreme-Multi-label-Learning},
  doi={10.1109/SP46215.2023.10179439},
  ISSN={2375-1207},
  month={May}
}

@INPROCEEDINGS{10655884,
  author={Wu, Xiaoyu and Hua, Yang and Liang, Chumeng and Zhang, Jiaru and Wang, Hao and Song, Tao and Guan, Haibing},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion}, 
  year={2024},
  volume={},
  number={},
  pages={10812-10821},
  abstract={Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dream-booth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.},
  keywords={Training;Fabrication;Visualization;Monte Carlo methods;Law;Image synthesis;Authentication;Diffusion models;trustworthy AI;copyright authentication;AI for social good},
  doi={10.1109/CVPR52733.2024.01028},
  ISSN={2575-7075},
  month={June}
}

@INBOOK{10897050,
  author={Islam, Mohammad Rubyet},
  booktitle={Generative AI, Cybersecurity, and Ethics}, 
  title={Foundations of Ethics in GenAI}, 
  year={2025},
  volume={},
  number={},
  pages={111-162},
  abstract={Summary <p>This chapter explores the ethical considerations essential for the responsible development and deployment of generative artificial intelligence (GenAI). It underscores the importance of guiding principles such as fairness, accountability, and transparency to mitigate biases and ensure privacy. The chapter traces the evolution of ethics in technology from ancient philosophies to contemporary artificial intelligence (AI) guidelines, highlighting pivotal regulatory frameworks like the EU's Ethics Guidelines for Trustworthy AI. It also emphasizes the need for adaptive and internationally converged regulatory approaches to address the unique challenges posed by GenAI, ensuring its alignment with societal values and ethical norms.</p>},
  keywords={Ethics;Artificial intelligence;Organizations;Intellectual property;Guidelines;Stakeholders;Nuclear weapons;Navigation;Decision making;Data protection},
  doi={10.1002/9781394279326.ch5},
  ISSN={},
  publisher={Wiley},
  isbn={9781394279319},
  url={https://ieeexplore.ieee.org/document/10897050}
}

@ARTICLE{10711270,
  author={Zheng, Yue and Chang, Chip-Hong and Huang, Shih-Hsu and Chen, Pin-Yu and Picek, Stjepan},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems}, 
  title={An Overview of Trustworthy AI: Advances in IP Protection, Privacy-Preserving Federated Learning, Security Verification, and GAI Safety Alignment}, 
  year={2024},
  volume={14},
  number={4},
  pages={582-607},
  abstract={AI has undergone a remarkable evolution journey marked by groundbreaking milestones. Like any powerful tool, it can be turned into a weapon for devastation in the wrong hands. Understanding that no model is perfect, trustworthy AI is initiated with an intuitive aim to mitigate the harm it can inflict on people and society by prioritizing socially responsible AI ideation, design, development, and deployment towards effecting positive changes. The scope of trustworthy AI is encompassing, covering qualities such as safety, security, privacy, transparency, explainability, fairness, impartiality, robustness, reliability, and accountability. This overview paper anchors on recent advances in four research hotspots of trustworthy AI with compelling and challenging security, privacy, and safety issues. The topics discussed include the intellectual property protection of deep learning and generative models, the trustworthiness of federated learning, verification and testing tools of AI systems, and the safety alignment of generative AI systems. Through this comprehensive review, we aim to provide readers with an overview of the most up-to-date research problems and solutions. By presenting the rapidly evolving factors and constraints that motivate the emerging attack and defense strategies throughout the AI life-cycle, we hope to inspire more research effort into guiding AI technologies towards beneficial purposes with greater robustness against malicious use intent.},
  keywords={Artificial intelligence;Security;Data models;Protection;Integrated circuit modeling;Training;Circuits and systems;Privacy;Training data;Mathematical models;Federated learning;deep neural network;generative AI;large language model;formal verification;safety alignment;trustworthy AI;model poisoning;data poisoning;backdoor;watermarking;fingerprinting;security;privacy-preservation},
  doi={10.1109/JETCAS.2024.3477348},
  ISSN={2156-3365},
  month={Dec}
}

@ARTICLE{10915587,
  author={Luo, Huixin and Li, Li and Zhang, Xinpeng},
  journal={IEEE Transactions on Multimedia}, 
  title={Secure Neural Network Watermarking Protocol Against Evidence Exposure Attack}, 
  year={2025},
  volume={},
  number={},
  pages={1-13},
  abstract={Trigger-based backdoor watermarking is an extensively utilized and effective method to safeguard the copyright of deep neural networks (DNNs), in which the trigger set could be taken as the key of the watermark. However, during the verification stage, there is a risk that the trigger set could be leaked and exposed to adversaries. If this occurs, the adversaries might apply this leaked trigger set to claim ownership of the model, posing significant copyright issues for the watermarked DNN. To address such an evidence exposure problem, a secure neural network watermarking protocol is put forward in this paper. In the proposed protocol, the trigger set is not fixed, once the trigger is utilized for verification, it is invalid and cannot be used for verification in the future. As a result, even if the trigger set is leaked during the verification process and obtained by the attacker, they cannot use it for copyright verification since it is invalid. To assist the protocol, a trigger set generation method is designed, in which the auxiliary classifier generative adversarial network (ACGAN) and the target classification model are trained together. The special logits distribution and the labels of the generated trigger samples can be ensured and verified effectively in this way. The performance of the trigger generation methods regarding effectiveness, fidelity, and robustness is verified by experiments, and the security analysis of the designed watermarking protocol is conducted.},
  keywords={Watermarking;Training;Protocols;Noise;Generative adversarial networks;Closed box;Artificial neural networks;Robustness;Training data;Generators;Deep neural network;copyright protection;backdoor watermarking;automatic trigger sample generation},
  doi={10.1109/TMM.2025.3542975},
  ISSN={1941-0077},
  month={}
}

@BOOK{10522548,
  author={Singh, Paul and Karuparti, Anurag and Maeda, John},
  booktitle={Generative AI for Cloud Solutions: Architect modern AI LLMs in secure, scalable, and ethical cloud environments},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Explore Generative AI, the engine behind ChatGPT, and delve into topics like LLM-infused frameworks, autonomous agents, and responsible innovation, to gain valuable insights into the future of AIKey FeaturesGain foundational GenAI knowledge and understand how to scale GenAI/ChatGPT in the cloudUnderstand advanced techniques for customizing LLMs for organizations via fine-tuning, prompt engineering, and responsible AIPeek into the future to explore emerging trends like multimodal AI and autonomous agentsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionGenerative artificial intelligence technologies and services, including ChatGPT, are transforming our work, life, and communication landscapes. To thrive in this new era, harnessing the full potential of these technologies is crucial. Generative AI for Cloud Solutions is a comprehensive guide to understanding and using Generative AI within cloud platforms. This book covers the basics of cloud computing and Generative AI/ChatGPT, addressing scaling strategies and security concerns. With its help, you’ll be able to apply responsible AI practices and other methods such as fine-tuning, RAG, autonomous agents, LLMOps, and Assistants APIs. As you progress, you’ll learn how to design and implement secure and scalable ChatGPT solutions on the cloud, while also gaining insights into the foundations of building conversational AI, such as chatbots. This process will help you customize your AI applications to suit your specific requirements. By the end of this book, you’ll have gained a solid understanding of the capabilities of Generative AI and cloud computing, empowering you to develop efficient and ethical AI solutions for a variety of applications and services.What you will learnGet started with the essentials of generative AI, LLMs, and ChatGPT, and understand how they function togetherUnderstand how we started applying NLP to concepts like transformersGrasp the process of fine-tuning and developing apps based on RAGExplore effective prompt engineering strategiesAcquire insights into the app development frameworks and lifecycles of LLMs, including important aspects of LLMOps, autonomous agents, and Assistants APIsDiscover how to scale and secure GenAI systems, while understanding the principles of responsible AIWho this book is forThis artificial intelligence book is for aspiring cloud architects, data analysts, cloud developers, data scientists, AI researchers, technical business leaders, and technology evangelists looking to understanding the interplay between GenAI and cloud computing. Some chapters provide a broad overview of GenAI, which are suitable for readers with basic to no prior AI experience, aspiring to harness AI's potential. Other chapters delve into technical concepts that require intermediate data and AI skills. A basic understanding of a cloud ecosystem is required to get the most out of this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835080160},
  url={https://ieeexplore.ieee.org/document/10522548}
}

@BOOK{10540154,
  author={Alto, Valentina},
  booktitle={Building LLM Powered Applications: Create intelligent apps and agents with large language models},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Get hands-on with GPT 3.5, GPT 4, LangChain, Llama 2, Falcon LLM and more, to build LLM-powered sophisticated AI applicationsKey FeaturesEmbed LLMs into real-world applicationsUse LangChain to orchestrate LLMs and their components within applicationsGrasp basic and advanced techniques of prompt engineeringBook DescriptionBuilding LLM Powered Applications delves into the fundamental concepts, cutting-edge technologies, and practical applications that LLMs offer, ultimately paving the way for the emergence of large foundation models (LFMs) that extend the boundaries of AI capabilities. The book begins with an in-depth introduction to LLMs. We then explore various mainstream architectural frameworks, including both proprietary models (GPT 3.5/4) and open-source models (Falcon LLM), and analyze their unique strengths and differences. Moving ahead, with a focus on the Python-based, lightweight framework called LangChain, we guide you through the process of creating intelligent agents capable of retrieving information from unstructured data and engaging with structured data using LLMs and powerful toolkits. Furthermore, the book ventures into the realm of LFMs, which transcend language modeling to encompass various AI tasks and modalities, such as vision and audio. Whether you are a seasoned AI expert or a newcomer to the field, this book is your roadmap to unlock the full potential of LLMs and forge a new era of intelligent machines.What you will learnExplore the core components of LLM architecture, including encoder-decoder blocks and embeddingsUnderstand the unique features of LLMs like GPT-3.5/4, Llama 2, and Falcon LLMUse AI orchestrators like LangChain, with Streamlit for the frontendGet familiar with LLM components such as memory, prompts, and toolsLearn how to use non-parametric knowledge and vector databasesUnderstand the implications of LFMs for AI research and industry applicationsCustomize your LLMs with fine tuningLearn about the ethical implications of LLM-powered applicationsWho this book is for Software engineers and data scientists who want hands-on guidance for applying LLMs to build applications. The book will also appeal to technical leaders, students, and researchers interested in applied LLM topics. We don’t assume previous experience with LLM specifically. But readers should have core ML/software engineering fundamentals to understand and apply the content.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835462638},
  url={https://ieeexplore.ieee.org/document/10540154}
}

@BOOK{10163291,
  author={Babcock, Joseph and Bali, Raghav},
  booktitle={Generative AI with Python and TensorFlow 2: Create images, text, and music with VAEs, GANs, LSTMs, Transformer models},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Fun and exciting projects to learn what artificial minds can createKey FeaturesCode examples are in TensorFlow 2, which make it easy for PyTorch users to follow alongLook inside the most famous deep generative models, from GPT to MuseGANLearn to build and adapt your own models in TensorFlow 2.xExplore exciting, cutting-edge use cases for deep generative AIBook DescriptionMachines are excelling at creative human skills such as painting, writing, and composing music. Could you be more creative than generative AI? In this book, you’ll explore the evolution of generative models, from restricted Boltzmann machines and deep belief networks to VAEs and GANs. You’ll learn how to implement models yourself in TensorFlow and get to grips with the latest research on deep neural networks. There’s been an explosion in potential use cases for generative models. You’ll look at Open AI’s news generator, deepfakes, and training deep learning agents to navigate a simulated environment. Recreate the code that’s under the hood and uncover surprising links between text, image, and music generation.What you will learnExport the code from GitHub into Google Colab to see how everything works for yourselfCompose music using LSTM models, simple GANs, and MuseGANCreate deepfakes using facial landmarks, autoencoders, and pix2pix GANLearn how attention and transformers have changed NLPBuild several text generation pipelines based on LSTMs, BERT, and GPT-2Implement paired and unpaired style transfer with networks like StyleGANDiscover emerging applications of generative AI like folding proteins and creating videos from imagesWho this book is forThis is a book for Python programmers who are keen to create and have some fun using generative models. To make the most out of this book, you should have a basic familiarity with math and statistics for machine learning.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781800208506},
  url={https://ieeexplore.ieee.org/document/10163291}
}

@ARTICLE{10304637,
  author={Iatraki, Georgia and Mikropoulos, Tassos A.},
  journal={Presence}, 
  title={Augmented Reality in Physics Education: Students with Intellectual Disabilities Inquire the Structure of Matter}, 
  year={2022},
  volume={31},
  number={},
  pages={89-106},
  abstract={Immersive technologies support educational activities and provide motivating contexts which are increasingly implemented in special education settings. Augmented Reality (AR) seems to improve the level of engagement in teaching and learning processes for all students, including students with Intellectual Disabilities (ID). However, there is a lack of research that investigates AR learning environments where students with ID can be involved in inquiry-based activities and acquire academic content linked to real situations. The purpose of this study was to implement a single-subject design and evaluate the effects of an AR system on students’ performance on the microscopic level of the structure of matter and especially the phase-states of water. A functional relationship was found between students’ correct responses during probe sessions and the AR inquiry-based intervention. In addition, a social validity assessment indicated that the AR glasses helped students with ID to acquire physics concepts, as well as inquiry skills in a vivid experience. The students also reported satisfaction from using the AR glasses. Suggestions for future research include the design of AR-based interventions for other science concepts for students with ID as well as other special educational needs.},
  keywords={},
  doi={10.1162/pres_a_00374},
  ISSN={1054-7460},
  month={Dec}
}

@ARTICLE{10909100,
  author={Ankalaki, Shilpa and Atmakuri, Aparna Rajesh and Pallavi, M. and Hukkeri, Geetabai S and Jan, Tony and Naik, Ganesh R.},
  journal={IEEE Access}, 
  title={Cyber Attack Prediction: From Traditional Machine Learning to Generative Artificial Intelligence}, 
  year={2025},
  volume={13},
  number={},
  pages={44662-44706},
  abstract={The escalating sophistication of cyber threats poses significant risks to individuals, organizations, and nations. Cybercrime, encompassing activities like hacking and data breaches, has severe economic and societal consequences. In today’s interconnected world, robust cybersecurity measures are paramount to mitigate these risks and protect sensitive information. However, traditional security solutions struggle to keep pace with the evolving threat landscape. Artificial Intelligence (AI) offers a powerful arsenal of techniques to address these challenges. This paper explores the application of AI methods, including Machine Learning (ML), Deep Learning (DL), Natural Language Processing (NLP), Explainable AI (XAI), and Generative AI, in solving various cybersecurity problems. This paper presents a comprehensive analysis of AI techniques for enhancing cybersecurity. Key contributions include: 1) comparative study of ML and DL methods: Evaluating their accuracy, applicability, and suitability for various cybersecurity challenges; 2) investigation into XAI approaches: Enhancing the transparency and interpretability of AI-powered security solutions, particularly in anomaly detection; 3) exploration of emerging trends in Generative AI (Gen-AI) and NLP: Examining their potential to simulate and mitigate cyber threats through advanced techniques like threat intelligence generation and attack simulations; 4) application of GenAI in cybersecurity and real-world products of GenAI for cyber security. This research aims to advance the state-of-the-art in AI-driven cybersecurity by providing insights into effective and reliable solutions for mitigating cyber risks and improving the overall security posture.},
  keywords={Artificial intelligence;Computer security;Security;Cyberattack;Explainable AI;Generative AI;Ransomware;Deep learning;Chatbots;Accuracy;Cybersecurity;cyber-attack prediction;machine learning;deep learning;explainable AI;generative AI},
  doi={10.1109/ACCESS.2025.3547433},
  ISSN={2169-3536},
  month={}
}

@ARTICLE{10177044,
  author={Vaidya, Jaideep and Asif, Hafiz},
  journal={IEEE Spectrum}, 
  title={A Critical Look at AI-Generate Software: Coding with the New AI Tools is Both Irresistible and Dangerous}, 
  year={2023},
  volume={60},
  number={7},
  pages={34-39},
  abstract={In many ways, we live in the world of The Matrix. If Neo were to help us peel back the layers, we would find code all around us. Indeed, modern society runs on code: Whether you buy something online or in a store, check out a book at the library, fill a prescription, file your taxes, or drive your car, you are most probably interacting with a system that is powered by software. And the ubiquity, scale, and complexity of all that code just keeps increasing, with billions of lines of code being written every year. The programmers who hammer out that code tend to be overburdened, and their first attempt at constructing the needed software is almost always fragile or buggy- and so is their second and sometimes even the finalversion. It may fail unexpectedly, have unanticipated consequences, or be vulnerable to attack, sometimes resulting in immense damage.},
  keywords={Codes;Software engineering;Complexity theory;Artificial intelligence;Software performance;Programming;Computer security},
  doi={10.1109/MSPEC.2023.10177044},
  ISSN={1939-9340},
  month={July}
}

@INPROCEEDINGS{10764930,
  author={Dipongkor, Atish Kumar},
  booktitle={2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Can Large Language Models Comprehend Code Stylometry?}, 
  year={2024},
  volume={},
  number={},
  pages={2429-2431},
  abstract={Code Authorship Attribution (CAA) has several applications such as copyright disputes, plagiarism detection and criminal prosecution. Existing studies mainly focused on CAA by proposing machine learning (ML) and Deep Learning (DL) based techniques. The main limitations of ML-based techniques are (a) manual feature engineering is required to train these models and (b) they are vulnerable to adversarial attack. In this study, we initially fine-tune five Large Language Models (LLMs) for CAA and evaluate their performance. Our results show that LLMs are robust and less vulnerable compared to existing techniques in CAA task.},
  keywords={Deep learning;Codes;Large language models;Plagiarism;Manuals;Software engineering},
  doi={},
  ISSN={2643-1572},
  month={Oct}
}

@BOOK{10769311,
  author={Rothman, Denis},
  booktitle={RAG-Driven Generative AI: Build custom retrieval augmented generation pipelines with LlamaIndex, Deep Lake, and Pinecone},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Minimize AI hallucinations and build accurate, custom generative AI pipelines with RAG using embedded vector databases and integrated human feedback Purchase of the print or Kindle book includes a free eBook in PDF formatKey FeaturesImplement RAG’s traceable outputs, linking each response to its source document to build reliable multimodal conversational agentsDeliver accurate generative AI models in pipelines integrating RAG, real-time human feedback improvements, and knowledge graphsBalance cost and performance between dynamic retrieval datasets and fine-tuning static dataBook DescriptionRAG-Driven Generative AI provides a roadmap for building effective LLM, computer vision, and generative AI systems that balance performance and costs. This book offers a detailed exploration of RAG and how to design, manage, and control multimodal AI pipelines. By connecting outputs to traceable source documents, RAG improves output accuracy and contextual relevance, offering a dynamic approach to managing large volumes of information. This AI book shows you how to build a RAG framework, providing practical knowledge on vector stores, chunking, indexing, and ranking. You’ll discover techniques to optimize your project’s performance and better understand your data, including using adaptive RAG and human feedback to refine retrieval accuracy, balancing RAG with fine-tuning, implementing dynamic RAG to enhance real-time decision-making, and visualizing complex data with knowledge graphs. You’ll be exposed to a hands-on blend of frameworks like LlamaIndex and Deep Lake, vector databases such as Pinecone and Chroma, and models from Hugging Face and OpenAI. By the end of this book, you will have acquired the skills to implement intelligent solutions, keeping you competitive in fields from production to customer service across any project.What you will learnScale RAG pipelines to handle large datasets efficientlyEmploy techniques that minimize hallucinations and ensure accurate responsesImplement indexing techniques to improve AI accuracy with traceable and transparent outputsCustomize and scale RAG-driven generative AI systems across domainsFind out how to use Deep Lake and Pinecone for efficient and fast data retrievalControl and build robust generative AI systems grounded in real-world dataCombine text and image data for richer, more informative AI responsesWho this book is forThis book is ideal for data scientists, AI engineers, machine learning engineers, and MLOps engineers. If you are a solutions architect, software developer, product manager, or project manager looking to enhance the decision-making process of building RAG applications, then you’ll find this book useful.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781836200901},
  url={https://ieeexplore.ieee.org/document/10769311}
}

@INPROCEEDINGS{10556211,
  author={Tanaka, Hiroshi and Ide, Masaru and Yajima, Jun and Onodera, Sachiko and Munakata, Kazuki and Yoshioka, Nobukazu},
  booktitle={2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN)}, 
  title={Taxonomy of Generative AI Applications for Risk Assessment}, 
  year={2024},
  volume={},
  number={},
  pages={288-289},
  abstract={The superior functionality and versatility of generative AI have raised expectations for the improvement of human society and concerns about the ethical and social risks associated with the use of generative AI. Many previous studies have presented risk issues as concerns associated with the use of generative AI, but since most of these concerns are from the user's perspective, they are difficult to lead to specific countermeasures. In this study, the risk issues presented by the previous studies were broken down into more detailed elements, and risk factors and impacts were identified. In this way, we presented information that leads to countermeasure proposals for generative AI risks.CCS CONCEPTS• General and reference→Evaluation; Surveys and overviews, • Human-centered computing→HCI theory, concepts and models; • Social and professional topics→Computing / technology policy.},
  keywords={Surveys;Ethics;Generative AI;Computational modeling;Atmospheric modeling;Taxonomy;Risk management;language models;responsible innovation;technology risks;responsible AI;risk assessment},
  doi={},
  ISSN={},
  month={April}
}

@INPROCEEDINGS{9793971,
  author={Sun, Zhensu and Li, Li and Liu, Yan and Du, Xiaoning and Li, Li},
  booktitle={2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)}, 
  title={On the Importance of Building High-quality Training Datasets for Neural Code Search}, 
  year={2022},
  volume={},
  number={},
  pages={1609-1620},
  abstract={The performance of neural code search is significantly influenced by the quality of the training data from which the neural models are derived. A large corpus of high-quality query and code pairs is demanded to establish a precise mapping from the natural language to the programming language. Due to the limited availability, most widely-used code search datasets are established with compromise, such as using code comments as a replacement of queries. Our empirical study on a famous code search dataset reveals that over one-third of its queries contain noises that make them deviate from natural user queries. Models trained through noisy data are faced with severe performance degradation when applied in real-world scenarios. To improve the dataset quality and make the queries of its samples semantically identical to real user queries is critical for the practical usability of neural code search. In this paper, we propose a data cleaning framework consisting of two subsequent filters: a rule-based syntactic filter and a model-based semantic filter. This is the first framework that applies semantic query cleaning to code search datasets. Experimentally, we evaluated the effectiveness of our framework on two widely-used code search models and three manually-annotated code retrieval benchmarks. Training the popular DeepCS model with the filtered dataset from our framework improves its performance by 19.2% MRR and 21.3% Answer@l, on average with the three validation benchmarks.},
  keywords={Training;Codes;Computational modeling;Semantics;Training data;Benchmark testing;Data models;Code search;dataset;data cleaning;deep learning},
  doi={10.1145/3510003.3510160},
  ISSN={1558-1225},
  month={May}
}

@INPROCEEDINGS{10805696,
  author={Hussain, Mumtaz and Soomro, Tariq Rahim},
  booktitle={2024 Global Conference on Wireless and Optical Technologies (GCWOT)}, 
  title={The Avalanche of Artificial Intelligence and its Ethical Implications on Multicultural Diverse Global Village}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={The history of AI began in 1938 with the development of the Turing bombe by Alan Turing, followed by the Turing Test. Turing's work raised the question of whether machines can think, sparking extensive research. The progression of AI continued with the introduction of LISP in 1958 and Expert Systems in the 1960s. Technological advancements, such as, computing power, networking, and the rise of machine learning led to AI's rapid development. Today, AI is widely applied in various fields. This paper comprises an introduction, a literature review, a proposal for an Ethical Framework for AI development, and a conclusion. The introduction provides a concise history of AI, advance of AI, and delves into terms ethics and culture. The literature review examines different areas of applications of ethical AI and AI Ethics. Subsequently, a unique framework for ethical considerations in AI is suggested, which concludes the paper.},
  keywords={Wireless communication;Ethics;Bibliographies;Machine learning;History;Proposals;Artificial intelligence;Expert systems;Artificial Intelligence;Ethics;Global Village;Ethical Implication of AI},
  doi={10.1109/GCWOT63882.2024.10805696},
  ISSN={},
  month={Sep.}
}

@INPROCEEDINGS{10298504,
  author={Xiong, Jiaqi and Chen, Guoqiang and Chen, Kejiang and Gao, Han and Cheng, Shaoyin and Zhang, Weiming},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={HexT5: Unified Pre-Training for Stripped Binary Code Information Inference}, 
  year={2023},
  volume={},
  number={},
  pages={774-786},
  abstract={Decompilation is a widely used process for reverse engineers to significantly enhance code readability by lifting assembly code to a higher-level C-like language, pseudo-code. Nevertheless, the process of compilation and stripping irreversibly discards high-level semantic information that is crucial to code comprehension, such as comments, identifier names, and types. Existing approaches typically recover only one type of information, making them suboptimal for semantic inference. In this paper, we treat pseudo-code as a special programming language, then present a unified pre-trained model, HexT5, that is trained on vast amounts of natural language comments, source identifiers, and pseudo-code using novel pseudo-code-based pre-training objectives. We fine-tune HexT5 on various downstream tasks, including code summarization, variable name recovery, function name recovery, and similarity detection. Comprehensive experiments show that HexT5 achieves state-of-the-art performance on four downstream tasks, and it demonstrates the robust effectiveness and generalizability of HexT5 for binary-related tasks.},
  keywords={Computer languages;Semantics;Natural languages;Binary codes;Object recognition;Data mining;Task analysis;Reverse Engineering;Deep Learning;Binary Diffing;Information Inference;Programming Language Model},
  doi={10.1109/ASE56229.2023.00099},
  ISSN={2643-1572},
  month={Sep.}
}

@ARTICLE{10398474,
  author={Xu, Minrui and Du, Hongyang and Niyato, Dusit and Kang, Jiawen and Xiong, Zehui and Mao, Shiwen and Han, Zhu and Jamalipour, Abbas and Kim, Dong In and Shen, Xuemin and Leung, Victor C. M. and Poor, H. Vincent},
  journal={IEEE Communications Surveys & Tutorials}, 
  title={Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services}, 
  year={2024},
  volume={26},
  number={2},
  pages={1127-1170},
  abstract={Artificial Intelligence-Generated Content (AIGC) is an automated method for generating, manipulating, and modifying valuable and diverse data using AI algorithms creatively. This survey paper focuses on the deployment of AIGC applications, e.g., ChatGPT and Dall-E, at mobile edge networks, namely mobile AIGC networks, that provide personalized and customized AIGC services in real time while maintaining user privacy. We begin by introducing the background and fundamentals of generative models and the lifecycle of AIGC services at mobile AIGC networks, which includes data collection, training, fine-tuning, inference, and product management. We then discuss the collaborative cloud-edge-mobile infrastructure and technologies required to support AIGC services and enable users to access AIGC at mobile edge networks. Furthermore, we explore AIGC-driven creative applications and use cases for mobile AIGC networks. Additionally, we discuss the implementation, security, and privacy challenges of deploying mobile AIGC networks. Finally, we highlight some future research directions and open issues for the full realization of mobile AIGC networks.},
  keywords={Computational modeling;Servers;Biological system modeling;Artificial intelligence;Generative AI;Surveys;Mobile handsets;AIGC;generative AI;mobile edge networks;communication and networking;AI training and inference;Internet technology},
  doi={10.1109/COMST.2024.3353265},
  ISSN={1553-877X},
  month={Secondquarter}
}

@INPROCEEDINGS{9617416,
  author={Al Hasan, Md Mahfuz and Vashistha, Nidish and Taheri, Shayan and Tehranipoor, Mark and Asadizanjani, Navid},
  booktitle={2021 IEEE International Symposium on the Physical and Failure Analysis of Integrated Circuits (IPFA)}, 
  title={Generative Adversarial Network for Integrated Circuits Physical Assurance Using Scanning Electron Microscopy}, 
  year={2021},
  volume={},
  number={},
  pages={1-12},
  abstract={Recent advancements in Artificial Intelligence (AI) and Computer Vision (CV) provide the researchers in Failure Analysis and Reliability (FAR) as well as Hardware Security (HS) with new opportunities to design novel systems to locate security failures or malicious modifications. Such developments in automation and verification modes are extremely helpful in particular for government agencies who must physically assure chips with billions of transistors within critical applications. AI based techniques such as deep learning can provide a high-performance detection and recognition of elements from Scanning Electron Microscopic (SEM) images acquired from Integrated Circuits (ICs) and understand unseen images if they are trained well. However, they require a large and diverse set of images for building their knowledge. Possessing a large number of manufactured designs as well as the high cost and execution time associated with the image acquisition process are the major bottlenecks for creating a sufficient dataset. Alternatively, conventional data augmentation techniques such as intensity change, noise injection, rotation, and translation are not always able to project the variations of images acquired by SEM with different acquisition parameters. Furthermore, augmentations like rotation, translation, and shear might generate unacceptable augmented cell structures. This paper proposes a unique approach to detect logic cells on SEM images and use the extracted samples to generate diversified synthetic logic cell images by a Generative Adversarial Network (GAN) to address insufficient data problems. We introduce an image quality assessment metric for the synthetic dataset in order to study the qualification of generated samples for recognition computations.},
  keywords={Integrated circuits;Scanning electron microscopy;Image recognition;Microscopy;Generative adversarial networks;Real-time systems;Hardware;Artificial intelligence;Computer Vision;Failure Analysis and Reliability;Generative Adversarial Network;Jensen-Shannon Divergence;Hardware Security;Image Analytics;and SEM Microscopy},
  doi={10.1109/IPFA53173.2021.9617416},
  ISSN={1946-1550},
  month={Sep.}
}

@ARTICLE{10414101,
  author={Khan, Faiza Babar and Durad, Muhammad Hanif and Khan, Asifullah and Khan, Farrukh Aslam and Rizwan, Muhammad and Ali, Aftab},
  journal={IEEE Access}, 
  title={Design and Performance Analysis of an Anti-Malware System Based on Generative Adversarial Network Framework}, 
  year={2024},
  volume={12},
  number={},
  pages={27683-27708},
  abstract={The cyber realm is overwhelmed with dynamic malware that promptly penetrates all defense mechanisms, operates unapprehended to the user, and covertly causes damage to sensitive data. The current generation of cyber users is being victimized by the interpolation of malware each day due to the pervasive progression of Internet connectivity. Malware is dispersed to infiltrate the security, privacy, and integrity of the system. Conventional malware detection systems do not have the potential to detect novel malware without the accessibility of their signatures, which gives rise to a high False Negative Rate (FNR). Previously, there were numerous attempts to address the issue of malware detection, but none of them effectively combined the capabilities of signature-based and machine learning-based detection engines. To address this issue, we have developed an integrated Anti-Malware System (AMS) architecture that incorporates both conventional signature-based detection and AI-based detection modules. Our approach employs a Generative Adversarial Network (GAN) based Malware Classifier Optimizer (MCOGAN) framework, which can optimize a malware classifier. This framework utilizes GANs to generate fabricated benign files that can be used to train external discriminators for optimization purposes. We describe our proposed framework and anti-malware system in detail to provide a better understanding of how a malware detection system works. We evaluate our approach using the Figshare dataset and state-of-the-art models as discriminators. Our results showcase enhanced malware detection performance, yielding a 10% performance boost, thus affirming the efficacy of our approach compared to existing models.},
  keywords={Malware;Generative adversarial networks;Support vector machines;Machine learning;Generators;Terminology;Training;Performance evaluation;Anti-malware system;generative adversarial networks;malware sandboxes;malware;unpacker;performance},
  doi={10.1109/ACCESS.2024.3358454},
  ISSN={2169-3536},
  month={}
}

@BOOK{10522580,
  author={Ping, David},
  booktitle={The Machine Learning Solutions Architect Handbook: Practical strategies and best practices on the ML lifecycle, system design, MLOps, and generative AI},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Design, build, and secure scalable machine learning (ML) systems to solve real-world business problems with Python and AWS Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesGo in-depth into the ML lifecycle, from ideation and data management to deployment and scalingApply risk management techniques in the ML lifecycle and design architectural patterns for various ML platforms and solutionsUnderstand the generative AI lifecycle, its core technologies, and implementation risksBook DescriptionDavid Ping, Head of GenAI and ML Solution Architecture for global industries at AWS, provides expert insights and practical examples to help you become a proficient ML solutions architect, linking technical architecture to business-related skills. You'll learn about ML algorithms, cloud infrastructure, system design, MLOps , and how to apply ML to solve real-world business problems. David explains the generative AI project lifecycle and examines Retrieval Augmented Generation (RAG), an effective architecture pattern for generative AI applications. You’ll also learn about open-source technologies, such as Kubernetes/Kubeflow, for building a data science environment and ML pipelines before building an enterprise ML architecture using AWS. As well as ML risk management and the different stages of AI/ML adoption, the biggest new addition to the handbook is the deep exploration of generative AI. By the end of this book , you’ll have gained a comprehensive understanding of AI/ML across all key aspects, including business use cases, data science, real-world solution architecture, risk management, and governance. You’ll possess the skills to design and construct ML solutions that effectively cater to common use cases and follow established ML architecture patterns, enabling you to excel as a true professional in the field.What you will learnApply ML methodologies to solve business problems across industriesDesign a practical enterprise ML platform architectureGain an understanding of AI risk management frameworks and techniquesBuild an end-to-end data management architecture using AWSTrain large-scale ML models and optimize model inference latencyCreate a business application using artificial intelligence services and custom modelsDive into generative AI with use cases, architecture patterns, and RAGWho this book is forThis book is for solutions architects working on ML projects, ML engineers transitioning to ML solution architect roles, and MLOps engineers. Additionally, data scientists and analysts who want to enhance their practical knowledge of ML systems engineering, as well as AI/ML product managers and risk officers who want to gain an understanding of ML solutions and AI risk management, will also find this book useful. A basic knowledge of Python, AWS, linear algebra, probability, and cloud infrastructure is required before you get started with this handbook.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781805124825},
  url={https://ieeexplore.ieee.org/document/10522580}
}

@INPROCEEDINGS{10581129,
  author={Dhamiwal, Anjali and Shubham and Mahajan, Shilpa},
  booktitle={2024 International Conference on Intelligent Systems for Cybersecurity (ISCS)}, 
  title={Adversarial Machine Learning for Blockchain Security}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper explores the intersection of adversarial machine learning (AML) and blockchain security, presenting a comprehensive analysis of the challenges and solutions in leveraging AML techniques to protect blockchain networks. The decentralized and non-regular nature of blockchain technology exposes it to various adversarial threats, including 51% attacks, double spending, and smart contract vulnerabilities. To address these challenges, AML offers a range of techniques, such as adversarial training, generative adversarial networks (GANs), feature engineering, and robust ML models. These techniques enhance the resilience of blockchain systems against adversarial attacks and safeguard sensitive data. However, implementing ML models in blockchain environments presents challenges, including scalability and cross-chain compatibility. The paper discusses these challenges and proposes solutions to overcome them, emphasizing the importance of integrating AML into blockchain security frameworks. Overall, the paper highlights the potential of AML in enhancing the security of blockchain networks and mitigating the evolving threats posed by malicious actors.},
  keywords={Training;Systematics;Scalability;Smart contracts;Generative adversarial networks;Adversarial machine learning;Blockchains;Adversarial Machine Learning;Blockchain Security;Artificial Intelligence;Distributed Ledger Technology},
  doi={10.1109/ISCS61804.2024.10581129},
  ISSN={},
  month={May}
}
