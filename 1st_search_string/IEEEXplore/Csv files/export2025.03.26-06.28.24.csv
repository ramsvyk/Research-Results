"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Supervised GAN Watermarking for Intellectual Property Protection","J. Fei; Z. Xia; B. Tondi; M. Barni","Nanjing University of Information Science and Technology, Nanjing, China; Jinan University Guangzhou, China; University of Siena Siena, Italy; University of Siena Siena, Italy",2022 IEEE International Workshop on Information Forensics and Security (WIFS),"14 Dec 2022","2022","","","1","6","We propose a watermarking method for protecting the Intellectual Property (IP) of Generative Adversarial Networks (GANs). The aim is to watermark the GAN model so that any image generated by the GAN contains an invisible watermark (signature), whose presence inside the image can be checked at a later stage for ownership verification. To achieve this goal, a pre-trained CNN watermarking decoding block is inserted at the output of the generator. The generator loss is then modified by including a watermark loss term, to ensure that the prescribed watermark can be extracted from the generated images. The watermark is embedded via fine-tuning, with reduced time complexity. Results show that our method can effectively embed an invisible watermark inside the generated images. Moreover, our method is a general one and can work with different GAN architectures, different tasks, and different resolutions of the output image. We also demonstrate the good robustness performance of the embedded watermark against several post-processing, among them, JPEG compression, noise addition, blurring, and color transformations.","2157-4774","979-8-3503-0967-6","10.1109/WIFS55849.2022.9975409","Research and Development; National Natural Science Foundation of China; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9975409","Intellectual Property Protection;Generative Adversarial Networks;DNN Watermarking;Security of Deep Learning","Training;Visualization;Transform coding;Watermarking;Intellectual property;Computer architecture;Generative adversarial networks","","21","","26","IEEE","14 Dec 2022","","","IEEE","IEEE Conferences"
"Intellectual Property Rights and Artificial Intelligence: Contemporary Convergence and Probable Challenges","S. Singh; A. Singh","School of Law, Sharda University, Noida, U.P, India; School of Law, Sharda University, Noida, U.P, India","2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)","26 Feb 2024","2023","10","","1279","1286","Artificial intelligence (AI) has ushered in an era of unparalleled innovation, reshaping industries and societal norms. As the capabilities of AI-driven technology are advanced and expanded, a complex interplay between Intellectual Property (IP) rights and these emergent technologies has emerged, giving rise to a landscape rich with opportunities and challenges. This paper aims to comprehensively explore the intricate nexus between IP rights and the domains of artificial intelligence with special reference to patentability of the same. It will explore the evolving definitions of authorship and ownership, discuss the applicability of patent law to inventions produced by AI systems, dissect the conundrum of copyright protection for AI-generated content, analyze the role of trademarks in a digitally driven world, examine strategies for safeguarding trade secrets in the hyper-connected environment, and address the ethical considerations intrinsic to IP rights within the AI milieu. In doing so, it will provide insights into the mechanisms and adaptations necessary to uphold the tenets of IP protection while fostering innovation.","2687-7767","979-8-3503-8247-1","10.1109/UPCON59197.2023.10434754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434754","Artificial Intelligence;Innovation;Intellectual Property Strategies;Machine Learning;Human-AI Collaborations","Industries;Technological innovation;Ethics;Intellectual property;Trademarks;Artificial intelligence;Patent law","","","","35","IEEE","26 Feb 2024","","","IEEE","IEEE Conferences"
"Protecting Intellectual Property of Generative Adversarial Networks from Ambiguity Attacks","D. S. Ong; C. Seng Chan; K. W. Ng; L. Fan; Q. Yang",University of Malaya; University of Malaya; WeBank AI Lab; WeBank AI Lab; Hong Kong University of Science and Technology,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"2 Nov 2021","2021","","","3629","3638","Ever since Machine Learning as a Service emerges as a viable business that utilizes deep learning models to generate lucrative revenue, Intellectual Property Right (IPR) has become a major concern because these deep learning models can easily be replicated, shared, and re-distributed by any unauthorized third parties. To the best of our knowledge, one of the prominent deep learning models - Generative Adversarial Networks (GANs) which has been widely used to create photorealistic image are totally unprotected despite the existence of pioneering IPR protection methodology for Convolutional Neural Networks (CNNs). This paper therefore presents a complete protection framework in both black-box and white-box settings to enforce IPR protection on GANs. Empirically, we show that the proposed method does not compromise the original GANs performance (i.e. image generation, image super-resolution, style transfer), and at the same time, it is able to withstand both removal and ambiguity attacks against embedded watermarks. Codes are available at https://github.com/dingsheng-ong/ipr-gan.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577609","","Deep learning;Knowledge engineering;Computer vision;Image synthesis;Superresolution;Intellectual property;Watermarking","","33","","33","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Intellectual Property Protection for Deep Models: Pioneering Cross-Domain Fingerprinting Solutions","T. Xu; S. -h. Zhong; Z. Zhang; Y. Liu","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China",IEEE Transactions on Information Forensics and Security,"","2025","PP","99","1","1","The high cost of developing high-performance deep models highlights their value as intellectual property for creators. However, it is important to consider the potential risks of theft. Although various techniques have been developed to protect the intellectual property of deep models, there is still room for improvement in terms of efficiency, comprehensiveness, and generalization. Compared with the intrusiveness of watermarking methods, fingerprinting methods do not affect the training process of the source model. Consequently, this paper proposes a fingerprinting method to address the paucity of attempts in fingerprinting methods for model protection. Our method consists of two efficient algorithms for generating fingerprinting samples, where the first one possesses the advantage of efficiency, while the second one is better in terms of robustness. The first algorithm takes a comprehensive approach to modeling the fingerprint of the deep model. The generated samples are distributed within the stable region and near the decision boundary of the model, taking into account both the duality and the conviction factors. Then, a heuristic sample perturbation algorithm is introduced, which generates a fingerprint with solid stability and generalization across multiple domains. The two algorithms proposed in this paper have been shown to be capable of withstanding attacks on intellectual property removal, detection, and evasion. They also show some advantages in terms of efficiency. In addition, the proposed method is the first to apply fingerprinting techniques in a cross-domain context.","1556-6021","","10.1109/TIFS.2025.3552175","National Natural Science Foundation of China(grant numbers:62472291); Natural Science Foundation of Guangdong Province(grant numbers:2023A1515012685,2025A1515012154); Open Fund of National Engineering Laboratory for Big Data System Computing Technology(grant numbers:SZU-BDSC-OF2024-14); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10937062","Model Protection;Intellectual Property;Model Fingerprint","Fingerprint recognition;Brain modeling;Watermarking;Protection;IP networks;Computational modeling;Context modeling;Adaptation models;Intellectual property;Training","","","","","IEEE","21 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Invisible Fingerprint-based Copyright Protection for Generated Medical Image","N. Yang; W. Jia; P. Deng; W. Yang","School of Computer Science and Software Engineering, Southwest Petroleum University, Nanchong, China; School of Computer Science and Software Engineering, Southwest Petroleum University, Nanchong, China; School of Computer Science and Software Engineering, Southwest Petroleum University, Nanchong, China; School of Computer Science and Software Engineering, Southwest Petroleum University, Nanchong, China","2024 IEEE 2nd International Conference on Control, Electronics and Computer Technology (ICCECT)","7 Jun 2024","2024","","","960","964","In the recent years, the development of generative models has stimulated the health care progress, specially medical image generation. The synthetic medical images can be applied to several fields and have many utilization, such as data aug- mentation for model training. However, this might lead to the significant copyright issues. Firstly, the data for model training may be the privacy data belong to individuals. Secondly, owners of well-trained model may have burden computation cost. Both of these two stakeholders are not will to share model to others without any compensation or the risk of the image abuse. In order to protect the copyright of the generative model for medical image and reduce the risk of the image abuse, we propose Copyright Protection mechanism for Generated Medical Image (CP4GMI) by inducing invisible fingerprint. In other words, the generated medical image should have the unique fingerprint belong to the corresponding generative model, which can be examined when image abuse happens. Numerous experiments conducted on different medical image datasets shows that our approach can track the generative model in a high-accuracy level when the fingerprint in the generated image is mostly invisible. From the perspective of users and model inventors, we propose a copyright protection mechanism that introduces invisible fingerprints, which can be printed on the generated medical images, and can be used to protect the rights when the copyright is threatened.","","979-8-3503-8095-8","10.1109/ICCECT60629.2024.10545822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10545822","Copyright protection;medical image;image stenography","Training;Image synthesis;Computational modeling;Medical services;Fingerprint recognition;Copyright protection;Data models","","","","30","IEEE","7 Jun 2024","","","IEEE","IEEE Conferences"
"Localised Frequency Latent Domain Watermarking of DDIM Generated Images","Q. Lai; A. G. Bors","Department of Computer Science, University of York, York, UK; Department of Computer Science, University of York, York, UK","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Stable Diffusion models, relying on iterative generative latent diffusion processes, have recently achieved remarkable results in producing realistic and diverse images. Meanwhile, the widespread application of generative models raised significant concerns about the origins of image content or the infringement of intellectual property rights. Consequently, a method for identifying AI generated images and/or other information about their origins is imperatively necessary. To address these requirements we propose to embed watermarks during one of the diffusion iterative steps of the DDIM. Such watermarks are required to be recoverable while also robust to possible changes to the generated watermarked images. The watermarks are embedded in the localized regions of the latent space frequencies. The binary watermarks are detected from the generated watermarked images by means of a CNN watermark detector. The robustness of the CNN watermark detector is improved through training by considering various distortions to the watermarked images.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890047","Image Generation;Digital Watermarking;Denoising Diffusion Implicit Model;Copyright protection","Training;Frequency-domain analysis;Diffusion processes;Watermarking;Detectors;Intellectual property;Robustness;Iterative methods;Speech processing;Protection","","","","20","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"WEDA: Exploring Copyright Protection for Large Language Model Downstream Alignment","S. Wang; J. Dong; L. Wu; Z. Guan","School of Control and Computer Engineering, North China Electric Power University, Beijing, China; School of Systems and Computing, The University of New South Wales, Canberra, ACT, Australia; Department of Mathematics and Computer Science, Fayetteville State University, Fayetteville, NC, USA; School of Control and Computer Engineering, North China Electric Power University, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","8 Nov 2024","2024","32","","4755","4767","Large Language Models (LLMs) have shown incomparable representation and generalization capabilities, which have led to significant advancements in Natural Language Processing (NLP). Before deployment, the pre-trained LLMs often need to be tailored to specific downstream tasks for improved performance, which is commonly referred to as downstream alignment. This is a costly effort considering the needed manpower, training resources, and downstream-specific data. While much attention has been paid to protecting the copyright of the models themselves, the copyright protection of LLM alignment has been largely overlooked. In this paper, we present Watermark Embedding for Downstream Alignment (WEDA) scheme, which can provide effective copyright protection for two popular LLM alignment techniques parameter-efficient fine-tuning (PEFT) and in-context learning (ICL). For alignment through PEFT, we propose a Chain of Thought (CoT) based solution to embed watermarks into the PEFT weights. Furthermore, we extend this solution to safeguard alignment through ICL by utilizing the prefix-integrated CoT to watermark examples embedded within ICL prompts. We conduct an extensive experimental evaluation to demonstrate the effectiveness of our proposed scheme.","2329-9304","","10.1109/TASLP.2024.3487419","National Natural Science Foundation of China(grant numbers:62372173); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10737446","Copyright protection;watermark embedding;large language model;task alignment","Watermarking;Adaptation models;Computational modeling;Training;Intellectual property;Context modeling;Cognition;Speech processing;Costs;Privacy","","","","39","IEEE","29 Oct 2024","","","IEEE","IEEE Journals"
"Privacy and Copyright Protection in Generative AI: A Lifecycle Perspective","D. Zhang; B. Xia; Y. Liu; X. Xu; T. Hoang; Z. Xing; M. Staples; Q. Lu; L. Zhu","CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia",2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN),"18 Jun 2024","2024","","","92","97","The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.CCS CONCEPTS• Software and its engineering Software architectures; • Information systems World Wide Web; • Security and privacy Privacy protections; • Social and professional topics Copyrights; • Computing methodologies Machine learning.","","979-8-4007-0591-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556135","Privacy;Copyrights;Generative AI;Data Lifecycle;Software Architecture;Software Engineering for AI","Training;Data privacy;Technological innovation;Generative AI;Software architecture;Copyright protection;Software","","2","","29","","18 Jun 2024","","","IEEE","IEEE Conferences"
"Robust Retraining-free GAN Fingerprinting via Personalized Normalization","J. Fei; Z. Xia; B. Tondi; M. Barni","Jinan University, Guangzhou, China; Jinan University, Guangzhou, China; University of Siena, Siena, Italy; University of Siena, Siena, Italy",2023 IEEE International Workshop on Information Forensics and Security (WIFS),"1 Jan 2024","2023","","","1","6","In recent years, there has been significant growth in the commercial applications of generative models, licensed and distributed by model developers to users, who in turn use them to offer services. In this scenario, there is a need to track and identify the responsible user in the presence of a violation of the license agreement or any kind of malicious usage. Although there are methods enabling Generative Adversarial Networks (GANs) to include invisible watermarks in the images they produce, generating a model with a different watermark, referred to as a fingerprint, for each user is time- and resource-consuming due to the need to retrain the model to include the desired fingerprint. In this paper, we propose a retraining-free GAN fingerprinting method that allows model developers to easily generate model copies with the same functionality but different fingerprints. The generator is modified by inserting additional Personalized Normalization (PN) layers whose parameters (scaling and bias) are generated by two dedicated shallow networks (ParamGen Nets) taking the fingerprint as input. A watermark decoder is trained simultaneously to extract the fingerprint from the generated images. The proposed method can embed different fingerprints inside the GAN by just changing the input of the ParamGen Nets and performing a feedforward pass, without finetuning or retraining. The performance of the proposed method in terms of robustness against both model-level and image-level attacks is also superior to the state-of-the-art.","2157-4774","979-8-3503-2491-4","10.1109/WIFS58808.2023.10374953","National Key R&D Program of China(grant numbers:2022YFB3103100); National Natural Science Foundation of China(grant numbers:62122032,62172233,62102189); China Scholarship Council (CSC)(grant numbers:202109040029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10374953","IPR Protection;DNN watermarking;GAN fingerprinting;Box-free Watermarking;Security of Deep Learning","Forensics;Watermarking;Intellectual property;Fingerprint recognition;Licenses;Generative adversarial networks;Robustness","","2","","17","IEEE","1 Jan 2024","","","IEEE","IEEE Conferences"
"A Novel Weights-less Watermark Embedding Method for Neural Network Models","H. Deng; X. Wang; G. Yu; X. Dang; R. P. Liu","Global Big Data Technologies Centre, University of Technology Sydney, Australia; Global Big Data Technologies Centre, University of Technology Sydney, Australia; Data61 CSIRO, Sydney, Australia; Global Big Data Technologies Centre, University of Technology Sydney, Australia; Global Big Data Technologies Centre, University of Technology Sydney, Australia",2023 22nd International Symposium on Communications and Information Technologies (ISCIT),"3 Jan 2024","2023","","","25","30","Deep learning-based Artificial Intelligence (AI) technology has been extensively used recently. AI model theft is a regular occurrence. As a result, many academics focus their efforts on safeguarding the Intellectual Property (IP) of trained Neural Network (NN) models. The majority of the most recent white-box setting watermark embedding methods rely on modifying model weights. Weights updated for the NN model during training must take into account the initial task as well as the embedding of watermarks. As a result, the accuracy of the initial task will be affected, necessitating more training time. This research proposes a novel weights-less watermark embedding method for deep neural networks to address this issue. Without actually embedding the watermark within the NN model weights, it uses a principle of code matching between the watermark and the weights. The proposed method requires less time than existing white-box setting watermark embedding methods, and the accuracy of the original task is not much diminished. Additionally, since the NN model weights are left alone, their statistical distribution will remain unchanged, giving the model increased resistance to watermark detection. The experiments in this paper demonstrate the effectiveness, efficiency, and robustness of our method.","2643-6175","978-1-6654-5731-6","10.1109/ISCIT57293.2023.10376108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10376108","Watermark Embedding;Deep Learning;Neural Networks;Intellectual Property","Training;Resistance;Statistical distributions;Watermarking;Artificial neural networks;Intellectual property;Robustness","","1","","17","IEEE","3 Jan 2024","","","IEEE","IEEE Conferences"
"On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study","Z. Li; C. Wang; P. Ma; C. Liu; S. Wang; D. Wu; C. Gao; Y. Liu","The Hong Kong University of Science and Technology, Hong Kong SAR, China; The Chinese University of Hong Kong, Hong Kong SAR, China; The Hong Kong University of Science and Technology, Hong Kong SAR, China; National University of Singapore, Singapore, Singapore; The Hong Kong University of Science and Technology, Hong Kong SAR, China; Nanyang Technological University, Singapore, Singapore; Harbin Institute of Technology, Shenzhen, China; Nanyang Technological University, Singapore, Singapore",2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE),"14 Jun 2024","2024","","","893","905","Recent advances in large language models (LLMs) significantly boost their usage in software engineering. However, training a well-performing LLM demands a substantial workforce for data collection and annotation. Moreover, training datasets may be proprietary or partially open, and the process often requires a costly GPU cluster. The intellectual property value of commercial LLMs makes them attractive targets for imitation attacks, but creating an imitation model with comparable parameters still incurs high costs. This motivates us to explore a practical and novel direction: slicing commercial black-box LLMs using medium-sized backbone models. In this paper, we explore the feasibility of launching imitation attacks on LLMs to extract their specialized code abilities, such as “code synthesis” and “code translation:’ We systematically investigate the effectiveness of launching code ability extraction attacks under different code-related tasks with multiple query schemes, including zero-shot, in-context, and Chain-of-Thought. We also design response checks to refine the outputs, leading to an effective imitation training process. Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs. We summarize our findings and insights to help researchers better understand the threats posed by imitation attacks, including revealing a practical attack surface for generating adversarial code examples against LLMs.","1558-1225","979-8-4007-0217-4","10.1145/3597503.3639091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548878","Large Language Models;Imitation Attacks","Training;Codes;Costs;Annotations;Graphics processing units;Closed box;Intellectual property","","1","","91","","14 Jun 2024","","","IEEE","IEEE Conferences"
"Lessons from Building StackSpot Al: A Contextualized AI Coding Assistant","G. Pinto; C. R. B. de Souza; J. B. Neto; A. de Souza; T. Gotto; E. Monteiro","Zup Innovation & UFPA, Belèm, PA, Brazil; UFPA, Belèm, PA, Brazil; Zup Innovation, São Paulo, SP, Brazil; Zup Innovation, São Paulo, SP, Brazil; Zup Innovation, São Paulo, SP, Brazil; StackSpot, São Paulo, SP, Brazil",2024 IEEE/ACM 46th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),"18 Jun 2024","2024","","","408","417","With their exceptional natural language processing capabilities, tools based on Large Language Models (LLMs) like ChatGPT and Co-Pilot have swiftly become indispensable resources in the software developer's toolkit. While recent studies suggest the potential productivity gains these tools can unlock, users still encounter drawbacks, such as generic or incorrect answers. Additionally, the pursuit of improved responses often leads to extensive prompt engineering efforts, diverting valuable time from writing code that delivers actual value. To address these challenges, a new breed of tools, built atop LLMs, is emerging. These tools aim to mitigate drawbacks by employing techniques like fine-tuning or enriching user prompts with contextualized information. In this paper, we delve into the lessons learned by a software development team venturing into the creation of such a contextualized LLM-based application, using retrieval-based techniques, called StackSpot Al. Over a four-month period, the team, despite lacking prior professional experience in LLM-based applications, built the product from scratch. Following the initial product release, we engaged with the development team responsible for the code generative components. Through interviews and analysis of the application's issue tracker, we uncover various intriguing challenges that teams working on LLM-based applications might encounter. For instance, we found three main group of lessons: LLM-based lessons, User-based lessons, and Technical lessons. By understanding these lessons, software development teams could become better prepared to build LLM-based applications.","2832-7659","979-8-4007-0501-4","10.1145/3639477.3639751","INES; CNPq(grant numbers:420406/2023-9,442779/2023-2,465614/2014-0,308623/2022-3); FAPESPA(grant numbers:053/2021); FACEPE(grant numbers:APQ-0399-1.03/17,PRONEX APQ/0388-1.03/14); CAPES(grant numbers:88887.136410/2017-00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554711","LLM;LLM-based applications;LLM for code;LLM4code;Code LLMs;Challenges","Productivity;Codes;Writing;Chatbots;Software;Encoding;Teamwork","","1","","34","","18 Jun 2024","","","IEEE","IEEE Conferences"
"ProFLingo: A Fingerprinting-based Intellectual Property Protection Scheme for Large Language Models","H. Jin; C. Zhang; S. Shi; W. Lou; Y. T. Hou","Virginia Tech, Arlington, VA, USA; Virginia Tech, Arlington, VA, USA; Virginia Tech, Arlington, VA, USA; Virginia Tech, Arlington, VA, USA; Virginia Tech, Arlington, VA, USA",2024 IEEE Conference on Communications and Network Security (CNS),"31 Oct 2024","2024","","","1","9","Large language models (LLMs) have attracted significant attention in recent years. Due to their ""Large"" nature, training LLMs from scratch consumes immense computational resources. Since several major players in the artificial intelligence (AI) field have open-sourced their original LLMs, an increasing number of individuals and smaller companies are able to build derivative LLMs based on these open-sourced models at much lower costs. However, this practice opens up possibilities for unauthorized use or reproduction that may not comply with licensing agreements, and fine-tuning can change the model’s behavior, thus complicating the determination of model ownership. Current intellectual property (IP) protection schemes for LLMs are either designed for white-box settings or require additional modifications to the original model, which restricts their use in real-world settings.In this paper, we propose ProFLingo, a black-box fingerprinting-based IP protection scheme for LLMs. ProFLingo generates queries that elicit specific responses from an original model, thereby establishing unique fingerprints. Our scheme assesses the effectiveness of these queries on a suspect model to determine whether it has been derived from the original model. ProFLingo offers a non-invasive approach, which neither requires knowledge of the suspect model nor modifications to the base model or its training process. To the best of our knowledge, our method represents the first black-box fingerprinting technique for IP protection for LLMs. Our source code and generated queries are available at: https://github.com/hengvt/ProFLingo.","2994-5895","979-8-3503-7596-1","10.1109/CNS62487.2024.10735575","Office of Naval Research; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10735575","","Training;Costs;Large language models;Source coding;Closed box;Intellectual property;Fingerprint recognition;Network security;Protection;Glass box","","","","34","IEEE","31 Oct 2024","","","IEEE","IEEE Conferences"
"Your Model Trains on My Data? Protecting Intellectual Property of Training Data via Membership Fingerprint Authentication","G. Liu; T. Xu; X. Ma; C. Wang","Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China",IEEE Transactions on Information Forensics and Security,"15 Mar 2022","2022","17","","1024","1037","In recent years, data has become the new oil that fuels various machine learning (ML) applications. Just as the oil refining, providing data to an ML model is a product of massive costs and expertise efforts. However, how to protect the intellectual property (IP) of the training data in ML remains largely open. In this paper, we present MeFA, a novel framework for detecting training data IP embezzlement via Membership Fingerprint Authentication, which is able to determine whether a suspect ML model is trained on the to be protected target data or not. The key observation is that a part of data has a similar influence on the prediction behavior of different ML models. On this basis, MeFA leverages membership inference techniques to extract these data as the fingerprints of the target data and constructs an authentication model to verify the data’s ownership by identifying the obtained membership fingerprints. MeFA has several salient features. It does not assume any knowledge of the suspect model except for its black-box prediction API, through which we can merely get the prediction output of a given input, and also does not require any modification to the dataset or the training process, since it takes advantage of the inherent membership property of the data. As a by-product, MeFA can also serve as a post-protection to verify the ownership of ML models, without modifying the training process of the model. Extensive experiments on three realistic datasets and seven types of ML models validate the effectiveness of MeFA, and demonstrate that it is also robust to scenarios when the training data is partially used or preprocessed with representative membership inference defenses.","1556-6021","","10.1109/TIFS.2022.3155921","National Natural Science Foundation of China(grant numbers:61872416,62171189,62002104,62071192); Fundamental Research Funds for the Central Universities of China(grant numbers:2019kfyXJJS017); Key Research and Development Program of Hubei Province(grant numbers:2020BAB120); Wuhan Yellow Crane Talents (Excellent Young Scholar); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9724248","Training data authentication;intellectual property protection;membership inference attack;membership fingerprint;machine learning model","Data models;Predictive models;Training;Computational modeling;Training data;Watermarking;Authentication","","10","","53","IEEE","2 Mar 2022","","","IEEE","IEEE Journals"
"Generative AI with Amazon Bedrock: Build, scale, and secure generative AI applications using Amazon Bedrock","S. Kwatra; B. Kaushik",NA; NA,"Generative AI with Amazon Bedrock: Build, scale, and secure generative AI applications using Amazon Bedrock","","2024","","","","","Become proficient in Amazon Bedrock by taking a hands-on approach to building and scaling generative AI solutions that are robust, secure, and compliant with ethical standardsKey FeaturesLearn the foundations of Amazon Bedrock from experienced AWS Machine Learning Specialist ArchitectsMaster the core techniques to develop and deploy several AI applications at scaleGo beyond writing good prompting techniques and secure scalable frameworks by using advanced tips and tricksPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe concept of generative artificial intelligence has garnered widespread interest, with industries looking to leverage it to innovate and solve business problems. Amazon Bedrock, along with LangChain, simplifies the building and scaling of generative AI applications without needing to manage the infrastructure. Generative AI with Amazon Bedrock takes a practical approach to enabling you to accelerate the development and integration of several generative AI use cases in a seamless manner. You’ll explore techniques such as prompt engineering, retrieval augmentation, fine-tuning generative models, and orchestrating tasks using agents. The chapters take you through real-world scenarios and use cases such as text generation and summarization, image and code generation, and the creation of virtual assistants. The latter part of the book shows you how to effectively monitor and ensure security and privacy in Amazon Bedrock. By the end of this book, you’ll have gained a solid understanding of building and scaling generative AI apps using Amazon Bedrock, along with various architecture patterns and security best practices that will help you solve business problems and drive innovation in your organization.What you will learnExplore the generative AI landscape and foundation models in Amazon BedrockFine-tune generative models to improve their performanceExplore several architecture patterns for different business use casesGain insights into ethical AI practices, model governance, and risk mitigation strategiesEnhance your skills in employing agents to develop intelligence and orchestrate tasksMonitor and understand metrics and Amazon Bedrock model responseExplore various industrial use cases and architectures to solve real-world business problems using RAGStay on top of architectural best practices and industry standardsWho this book is forThis book is for generalist application engineers, solution engineers and architects, technical managers, ML advocates, data engineers, and data scientists looking to either innovate within their organization or solve business use cases using generative AI. A basic understanding of AWS APIs and core AWS services for machine learning is expected.","","9781804618585","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769229.pdf&bkn=10769228&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"ModelShield: Adaptive and Robust Watermark Against Model Extraction Attack","K. Pang; T. Qi; C. Wu; M. Bai; M. Jiang; Y. Huang","School of Humanities, Tsinghua University, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Huawei Technologies Company Ltd., Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; School of Humanities, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China",IEEE Transactions on Information Forensics and Security,"13 Feb 2025","2025","20","","1767","1782","Large language models (LLMs) demonstrate general intelligence across a variety of machine learning tasks, thereby enhancing the commercial value of their intellectual property (IP). To protect this IP, model owners typically allow user access only in a black-box manner, however, adversaries can still utilize model extraction attacks to steal the model intelligence encoded in model generation. Watermarking technology offers a promising solution for defending against such attacks by embedding unique identifiers into the model-generated content. However, existing watermarking methods often compromise the quality of generated content due to heuristic alterations and lack robust mechanisms to counteract adversarial strategies, thus limiting their practicality in real-world scenarios. In this paper, we introduce an adaptive and robust watermarking method (named ModelShield) to protect the IP of LLMs. Our method incorporates a self-watermarking mechanism that allows LLMs to autonomously insert watermarks into their generated content to avoid the degradation of model content. We also propose a robust watermark detection mechanism capable of effectively identifying watermark signals under the interference of varying adversarial strategies. Besides, ModelShield is a plug-and-play method that does not require additional model training, enhancing its applicability in LLM deployments. Extensive evaluations on two real-world datasets and three LLMs demonstrate that our method surpasses existing methods in terms of defense effectiveness and robustness while significantly reducing the degradation of watermarking on the model-generated content.","1556-6021","","10.1109/TIFS.2025.3530691","CCF-Sangfor ‘Yuanwang’ Research Fund(grant numbers:20240202); National Natural Science Foundation of China(grant numbers:U2336208,62036001,62262002,82090053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10843740","Large language models;model extraction attack;text watermarking;model IP protection","Watermarking;Adaptation models;Data models;Protection;Training;Computational modeling;Intellectual property;Closed box;Robustness;Real-time systems","","","","63","IEEE","16 Jan 2025","","","IEEE","IEEE Journals"
"Modern Generative AI with ChatGPT and OpenAI Models: Leverage the capabilities of OpenAI's LLM for productivity and innovation with GPT3 and GPT4","V. Alto",NA,Modern Generative AI with ChatGPT and OpenAI Models: Leverage the capabilities of OpenAI's LLM for productivity and innovation with GPT3 and GPT4,"","2023","","","","","Harness the power of AI with innovative, real-world applications, and unprecedented productivity boosts, powered by the latest advancements in AI technology like ChatGPT and OpenAI Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesExplore the theory behind generative AI models and the road to GPT3 and GPT4Become familiar with ChatGPT’s applications to boost everyday productivityLearn to embed OpenAI models into applications using lightweight frameworks like LangChainBook DescriptionGenerative AI models and AI language models are becoming increasingly popular due to their unparalleled capabilities. This book will provide you with insights into the inner workings of the LLMs and guide you through creating your own language models. You’ll start with an introduction to the field of generative AI, helping you understand how these models are trained to generate new data. Next, you’ll explore use cases where ChatGPT can boost productivity and enhance creativity. You’ll learn how to get the best from your ChatGPT interactions by improving your prompt design and leveraging zero, one, and few-shots learning capabilities. The use cases are divided into clusters of marketers, researchers, and developers, which will help you apply what you learn in this book to your own challenges faster. You’ll also discover enterprise-level scenarios that leverage OpenAI models’ APIs available on Azure infrastructure; both generative models like GPT-3 and embedding models like Ada. For each scenario, you’ll find an end-to-end implementation with Python, using Streamlit as the frontend and the LangChain SDK to facilitate models' integration into your applications. By the end of this book, you’ll be well equipped to use the generative AI field and start using ChatGPT and OpenAI models’ APIs in your own projects.What you will learnUnderstand generative AI concepts from basic to intermediate levelFocus on the GPT architecture for generative AI modelsMaximize ChatGPT’s value with an effective prompt designExplore applications and use cases of ChatGPTUse OpenAI models and features via API callsBuild and deploy generative AI systems with PythonLeverage Azure infrastructure for enterprise-level use casesEnsure responsible AI and ethics in generative AI systemsWho this book is forThis book is for individuals interested in boosting their daily productivity; businesspersons looking to dive deeper into real-world applications to empower their organizations; data scientists and developers trying to identify ways to boost ML models and code; marketers and researchers seeking to leverage use cases in their domain – all by using Chat GPT and OpenAI Models. A basic understanding of Python is required; however, the book provides theoretical descriptions alongside sections with code so that the reader can learn the concrete use case application without running the scripts.","","9781805122838","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251314.pdf&bkn=10251313&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"RTLCoder: Fully Open-Source and Efficient LLM-Assisted RTL Code Generation Technique","S. Liu; W. Fang; Y. Lu; J. Wang; Q. Zhang; H. Zhang; Z. Xie","Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR, China",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,"20 Mar 2025","2025","44","4","1448","1461","The automatic generation of RTL code (e.g., Verilog) using natural language instructions and large language models (LLMs) has attracted significant research interest recently. However, most existing approaches heavily rely on commercial LLMs, such as ChatGPT, while open-source LLMs tailored for this specific design generation task exhibit notably inferior performance. The absence of high-quality open-source solutions restricts the flexibility and data privacy of this emerging technique. In this study, we present a new customized LLM solution with a modest parameter count of only 7B, achieving better performance than GPT-3.5 on all representative benchmarks for RTL code generation. Especially, it outperforms GPT-4 in VerilogEval Machine benchmark. This remarkable balance between accuracy and efficiency is made possible by leveraging our new RTL code dataset and a customized LLM algorithm, both of which have been made fully open-source. Furthermore, we have successfully quantized our LLM to 4-bit with a total size of 4 GB, enabling it to function on a single laptop with only slight performance degradation. This efficiency allows the RTL generator to serve as a local assistant for engineers, ensuring all design privacy concerns are addressed.","1937-4151","","10.1109/TCAD.2024.3483089","Hong Kong Research Grants Council (RGC) ECS(grant numbers:26208723); National Natural Science Foundation of China(grant numbers:62304192,92364102); ACCESS—AI Chip Center for Emerging Smart Systems, sponsored by InnoHK funding, Hong Kong, SAR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10720939","Dataset generation;hardware code generation;Verilog;large language model;preference finetuning","Codes;Hardware design languages;Training;Integrated circuit modeling;Data models;Natural languages;Hardware;Data collection;Benchmark testing;Privacy","","","","41","IEEE","17 Oct 2024","","","IEEE","IEEE Journals"
"AI-Generated Images as an Emergent Record Format","J. Bushey","School of Information, San José State University, San José, USA",2023 IEEE International Conference on Big Data (BigData),"22 Jan 2024","2023","","","2020","2031","AI-generated Images are disrupting existing approaches to verifying the trustworthiness of visual media. The application of generative AI in fields in which images are trusted visual evidence of persons, actions and events is drawing the attention of archival scientists and AI researchers. A literature review of AI-generated images as an emergent record format, identified an absence of archival and recordkeeping knowledge. Analysis of the results revealed six thematic categories: authenticity and verifiability; manipulation and misinformation; bias and representation; attribution and intellectual property; transparency and explainability; and ethical considerations. These themes inform the development of research questions and the next phase of the study that includes the application of theory and methods of archival diplomatics and computational archival science.","","979-8-3503-2445-7","10.1109/BigData59044.2023.10386946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386946","Generative-AI;AI-generated images;Archives;Recordkeeping;Computational Archival Science;Born-digital images","Visualization;Ethics;Generative AI;Bibliographies;Intellectual property;Big Data;Fake news","","3","","86","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"VADER-SC: A Model Agnostic Tool for Large Scale, AI Driven Automated Source Code Summarization","D. Horne; A. Pierson; E. Hedary; G. Freddo; L. Trejo; M. Matis; L. Mask","Agile Dev. Engineering, L3Harris Technologies, Greenville, TX, USA; Deptartment of Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Deptartment of Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Deptartment of Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Deptartment of Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Deptartment of Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Agile Dev. Engineering, L3Harris Technologies, Greenville, TX, USA","2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)","9 Apr 2024","2023","","","2600","2607","Production of a natural language description for the function of a source code segment is commonly referred to as source code summarization. Useful comments in source code can facilitate more rapid onboarding of new engineers and contribute to decreased maintenance costs. Unfortunately, the documentation task can also be labor intensive. In this paper, we introduce a new model agnostic tool for AI driven automation of source code summarization at scale. The initial version of the adVanced AI Driven Enhancement to Readability for Spurce Code (VADER-SC) software offers numerous options for customization and the ability to leverage a variety of AI models to enable experimentation in resource constrained environments, while also scaling up to benefit from larger models in contexts with increased compute resources. It further supports private cloud, self-hosted, and air-gapped network configurations for environments with strict intellectual property protections or processing of sensitive or controlled data. Qualitative and quantitative results suggest model selection, fine-tuning, and multi-shot tailoring significantly impact the quality of generated comments. VADER-SC could be an enabler for practitioners to explore large-scale automation of AI driven source code summarization and researchers may find it enables studies with larger volumes of disparate data across a diversity of AI model options and target programming languages.","","979-8-3503-2759-5","10.1109/CSCE60160.2023.00416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10487593","automated source code summarization;deep learning;neural machine translation;software maintenance;program comprehension;AI driven documentation;VADER-SC","Automation;Source coding;Computational modeling;Atmospheric modeling;Production;Software;Artificial intelligence","","","","38","IEEE","9 Apr 2024","","","IEEE","IEEE Conferences"
"Unlocking Data with Generative AI and RAG: Enhance generative AI systems by integrating internal data with large language models using RAG","K. Bourne; S. Es",NA; NA,Unlocking Data with Generative AI and RAG: Enhance generative AI systems by integrating internal data with large language models using RAG,"","2024","","","","","Leverage cutting-edge generative AI techniques such as RAG to realize the potential of your data and drive innovation as well as gain strategic advantageKey FeaturesOptimize data retrieval and generation using vector databasesBoost decision-making and automate workflows with AI agentsOvercome common challenges in implementing real-world RAG systemsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionGenerative AI is helping organizations tap into their data in new ways, with retrieval-augmented generation (RAG) combining the strengths of large language models (LLMs) with internal data for more intelligent and relevant AI applications. The author harnesses his decade of ML experience in this book to equip you with the strategic insights and technical expertise needed when using RAG to drive transformative outcomes. The book explores RAG’s role in enhancing organizational operations by blending theoretical foundations with practical techniques. You’ll work with detailed coding examples using tools such as LangChain and Chroma’s vector database to gain hands-on experience in integrating RAG into AI systems. The chapters contain real-world case studies and sample applications that highlight RAG’s diverse use cases, from search engines to chatbots. You’ll learn proven methods for managing vector databases, optimizing data retrieval, effective prompt engineering, and quantitatively evaluating performance. The book also takes you through advanced integrations of RAG with cutting-edge AI agents and emerging non-LLM technologies. By the end of this book, you’ll be able to successfully deploy RAG in business settings, address common challenges, and push the boundaries of what’s possible with this revolutionary AI technique.What you will learnUnderstand RAG principles and their significance in generative AIIntegrate LLMs with internal data for enhanced operationsMaster vectorization, vector databases, and vector search techniquesDevelop skills in prompt engineering specific to RAG and design for precise AI responsesFamiliarize yourself with AI agents' roles in facilitating sophisticated RAG applicationsOvercome scalability, data quality, and integration issuesDiscover strategies for optimizing data retrieval and AI interpretabilityWho this book is forThis book is for AI researchers, data scientists, software developers, and business analysts looking to leverage RAG and generative AI to enhance data retrieval, improve AI accuracy, and drive innovation. It is particularly suited for anyone with a foundational understanding of AI who seeks practical, hands-on learning. The book offers real-world coding examples and strategies for implementing RAG effectively, making it accessible to both technical and non-technical audiences. A basic understanding of Python and Jupyter Notebooks is required.","","9781835887912","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769241.pdf&bkn=10769240&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs","Auffarth",NA,"Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs","","2023","","","","","2024 Edition – Get to grips with the LangChain framework to develop production-ready applications, including agents and personal assistants. The 2024 edition features updated code examples and an improved GitHub repository. Purchase of the print or Kindle book includes a free PDF eBook. Key FeaturesLearn how to leverage LangChain to work around LLMs’ inherent weaknessesDelve into LLMs with LangChain and explore their fundamentals, ethical dimensions, and application challengesGet better at using ChatGPT and GPT models, from heuristics and training to scalable deployment, empowering you to transform ideas into realityBook DescriptionChatGPT and the GPT models by OpenAI have brought about a revolution not only in how we write and research but also in how we can process information. This book discusses the functioning, capabilities, and limitations of LLMs underlying chat systems, including ChatGPT and Gemini. It demonstrates, in a series of practical examples, how to use the LangChain framework to build production-ready and responsive LLM applications for tasks ranging from customer support to software development assistance and data analysis – illustrating the expansive utility of LLMs in real-world applications. Unlock the full potential of LLMs within your projects as you navigate through guidance on fine-tuning, prompt engineering, and best practices for deployment and monitoring in production environments. Whether you're building creative writing tools, developing sophisticated chatbots, or crafting cutting-edge software development aids, this book will be your roadmap to mastering the transformative power of generative AI with confidence and creativity.What you will learnCreate LLM apps with LangChain, like question-answering systems and chatbotsUnderstand transformer models and attention mechanismsAutomate data analysis and visualization using pandas and PythonGrasp prompt engineering to improve performanceFine-tune LLMs and get to know the tools to unleash their powerDeploy LLMs as a service with LangChain and apply evaluation strategiesPrivately interact with documents using open-source LLMs to prevent data leaksWho this book is forThe book is for developers, researchers, and anyone interested in learning more about LangChain. Whether you are a beginner or an experienced developer, this book will serve as a valuable resource if you want to get the most out of LLMs using LangChain. Basic knowledge of Python is a prerequisite, while prior exposure to machine learning will help you follow along more easily.","","9781835088364","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10718333.pdf&bkn=10718332&pdfType=book","","","","","","","","15 Oct 2024","","","Packt Publishing","Packt Publishing eBooks"
"GAIT: A Game-Theoretic Defense Against Intellectual Property Theft","Y. Zhang; D. Chen; S. Jajodia; A. Pugliese; V. S. Subrahmanian; Y. Xiong","Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, Hong Kong SAR, China; Department of Computer Science, Dartmouth College, Hanover, NH, USA; Center for Secure Information Systems, George Mason University, Fairfax, VA, USA; University of Calabria, Rende, Italy; Department of Computer Science, and with the Buffett Institute for Global Affairs, Northwestern University, Evanston, IL, USA; Department of Data Science, William & Mary, Williamsburg, VA, USA",IEEE Transactions on Dependable and Secure Computing,"10 Jul 2024","2024","21","4","1967","1980","Months may pass before the victim of IP theft even knows they have been compromised. During this time, the attacker can exfiltrate large amounts of data. Recent work has proposed the idea of injecting a set of believable fake versions of a real document into a network so that the attacker has to expend time and effort to identify the real document from a sea of similar documents. In this paper, we consider the problem of an attacker who is smart and breaks a technical document down into small, bit-sized “units” and inspects them one by one so as to defeat the fake document defense. If a unit in a document is determined to be fake, the adversary does not need to look further at the same document. He can also immediately identify as fake, any other document that contains the same unit. In this paper, we consider the problem of a smart attacker using this strategy. Our proposed defensive algorithm, called ${\sf GAIT}$GAIT, is shown to be successful in mitigating such attacks. ${\sf GAIT}$GAIT can work in conjunction with any NLP-based generative method to create fake technical documents.","1941-0018","","10.1109/TDSC.2023.3299225","ONR(grant numbers:N00014-18-1-2670,N00014-20-1-2407); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10195236","H.2.0.a security;integrity and protection < H.2.0 general < H.2 database management < H information technology and systems","Games;Nash equilibrium;IP networks;History;Public key;Intellectual property;Heuristic algorithms","","","","32","IEEE","26 Jul 2023","","","IEEE","IEEE Journals"
"How Effectively Do Code Language Models Understand Poor-Readability Code?","C. Hu; Y. Chai; H. Zhou; F. Meng; J. Zhou; X. Gu","Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China; WeChat AI, Tencent, China; WeChat AI, Tencent, China; Tencent Inc., China; Shanghai Jiao Tong University, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","795","806","Code language models such as CodeT5 and CodeLlama have demonstrated substantial achievement in code comprehension. While the majority of research efforts have focused on improving model architectures and training processes, we find that the current benchmarks used for evaluating code comprehension models are confined to high-readability code, regardless of the popularity of low-readability code in reality. As such, they are inadequate to demonstrate the full spectrum of the model’s ability, particularly the robustness to varying readability degrees. In this paper, we analyze the robustness of code summarization models to code with varying readability, including seven obfuscated datasets derived from existing benchmarks. Our findings indicate that current code summarization models are vulnerable to code with poor readability. In particular, their performance predominantly depends on semantic cues within the code, often neglecting the syntactic aspects. Existing benchmarks are biased toward evaluating semantic features, thereby overlooking the models’ ability to understand non-sensitive syntactic features. Based on the findings, we present PoorCodeSumEval, a new evaluation benchmark on code summarization tasks. PoorCodeSumEval innovatively introduces readability into the testing process, considering semantic, syntactic, and their cross-obfuscation, thereby providing a more comprehensive and rigorous evaluation of code summarization models. Our studies also provide more insightful suggestions for future research, such as constructing multi-readability benchmarks to evaluate the robustness of models on poor-readability code, proposing readability-awareness metrics, and automatic methods for code data cleaning and normalization.","2643-1572","979-8-4007-1248-7","","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764959","Code language models;Code summarization;Code readability","Training;Codes;Sensitivity;Semantics;Benchmark testing;Syntactics;Robustness;Data models;Cleaning;Software engineering","","","","46","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Comparing the Effectiveness of Generative AI for Learning and Developing Flutter Application","P. Heng; K. Yongsiriwit; P. Chaisiriprasert","College of Digital Innovation Technology, Rangsit University, Thailand; College of Digital Innovation Technology, Rangsit University, Thailand; College of Digital Innovation Technology, Rangsit University, Thailand",2024 8th International Conference on Information Technology (InCIT),"30 Dec 2024","2024","","","746","751","The rapid growth of business demands modern technological advancements, leading to an increased need for accelerated learning and development of tools. Generative AI has become a key player in enhancing these areas by aiding in code generation for application development. Specifically, Generative AI can produce functional code for various programming languages, aiding in the setup of UI components, navigation, and complex state management. This study evaluates the effectiveness of three widely-used Generative AI tools— ChatGPT, Copilot, and Codeium—chosen for their popularity and diverse approaches to code generation. Standardized prompts were used to generate Flutter code for beginner, intermediate, and advanced tasks. The results show that ChatGPT outperformed other tools, consistently generating runnable and comprehensive code, while Copilot and Codeium exhibited some limitations in handling complex tasks. These findings suggest that integrating Generative AI into Flutter development can significantly accelerate the coding process and enhance application quality.","","979-8-3503-6630-3","10.1109/InCIT63192.2024.10810490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10810490","Generative AI;Code generation;Flutter;Prompt engineering;Cross-platform","Computer languages;Codes;Generative AI;Navigation;Chatbots;Encoding;Information technology;Business","","","","11","IEEE","30 Dec 2024","","","IEEE","IEEE Conferences"
"Identifying and Mitigating the Security Risks of Generative AI","C. Barrett; B. Boyd; E. Bursztein; N. Carlini; B. Chen; J. Choi; A. Roy Chowdhury; M. Christodorescu; A. Datta; S. Feizi; K. Fisher; T. Hashimoto; D. Hendrycks; S. Jha; D. Kang; F. Kerschbaum; E. Mitchell; J. Mitchell; Z. Ramzan; K. Shams; D. Song; A. Taly; D. Yang",NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA,Identifying and Mitigating the Security Risks of Generative AI,"","2024","","","","","Every major technical invention resurfaces the dual-use dilemma — the new technology has the potential to be used for good as well as for harm. Generative AI (GenAI) techniques, such as large language models (LLMs) and diffusion models, have shown remarkable capabilities (e.g., in-context learning, code-completion, and text-to-image generation and editing). However, GenAI can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks. This monograph reports the findings of a workshop held at Google (co-organized by Stanford University and the University of Wisconsin-Madison) on the dual-use dilemma posed by GenAI. This work is not meant to be comprehensive, but is rather an attempt to synthesize some of the interesting findings from the workshop. Short-term and long-term goals for the community on this topic are discussed. This work should provide both a launching point for a discussion on this important topic, as well as interesting problems that the research community can work to address.","","9781638283133","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10410240.pdf&bkn=10410239&pdfType=book","Privacy and Security","","","","","","","22 Jan 2024","","","now","Now Foundations and Trends Books"
"LKAN: LLM-Based Knowledge-Aware Attention Network for Clinical Staging of Liver Cancer","Y. Li; X. Zheng; J. Li; Q. Dai; C. -D. Wang; M. Chen","School of Electronics and Information, Guangdong Polytechnic Normal University, Guangzhou, Guangdong, China; School of Electronics and Information, Guangdong Polytechnic Normal University, Guangzhou, Guangdong, China; Department of Interventional Oncology, the First Affiliated Hospital, Sun Yat-sen University, Guangzhou, China; School of Electronics and Information, Guangdong Polytechnic Normal University, Guangzhou, Guangdong, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China",IEEE Journal of Biomedical and Health Informatics,"","2024","PP","99","1","14","Clinical staging of liver cancer (CSoLC), an important indicator for evaluating the degree of deterioration of primary liver cancer cells (PLCCs), is key in the diagnosis, treatment, and rehabilitation of liver cancer. In China, the current CSoLC adopts the China liver cancer (CNLC) staging, which is usually evaluated by clinicians based on the patient's radiology reports. Therefore, inferring clinical information from unstructured radiology reports can provide auxiliary decision support for clinicians. The key to solving the challenging task is to guide the model to pay attention to the staging-related words or sentences, and the following issues may occur: 1) Imbalanced categories: The symptoms of liver cancer in the early- or mid-stage are not obvious, resulting in more data in the end-stage. 2) Domain sensitivity of liver cancer data: The liver cancer dataset contains a large amount of domain knowledge, and the conventional methods can exacerbate out-of-vocabulary, which greatly affects the accuracy of classification. 3) Free-text and lengthy report: The radiology report of liver cancer sparsely describes various lesions with domain-specific terms, which poses difficulties in mining key information related to staging. To tackle these challenges, this article proposes a large language model (LLM)-based Knowledge-aware Attention Network (LKAN) for CSoLC. First, for maintaining semantic consistency, LLM and a rule-based algorithm are integrated to generate more diverse and reasonable data. Second, unlabeled radiology corpus of liver cancer are pre-trained to introduce domain knowledge for subsequent representation learning. Third, attention is improved by incorporating both global and local features, which can provide professional guidance for the classifier to focus on the important information. Compared with the baseline models, the classification accuracy of LKAN has achieved the best results with 90.3% Accuracy, 90.0% Macro_F1 score, and 90.0% Macro_Recall. The code is available at https://github.com/xczhh/Supplemental-Material.","2168-2208","","10.1109/JBHI.2024.3478809","National Key Research and Development Program of China(grant numbers:2021YFF1201200); NSFC(grant numbers:62276277); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10713996","Clinical staging of liver cancer (CSoLC);Chinese radiology reports;natural language processing;domain knowledge","Radiology;Liver cancer;Cancer;Accuracy;Liver;Lesions;Bioinformatics;Sensitivity;Magnetic resonance imaging;Vectors","","","","","IEEE","11 Oct 2024","","","IEEE","IEEE Early Access Articles"
"SCS-Gan: Learning Functionality-Agnostic Stylometric Representations for Source Code Authorship Verification","W. Ou; S. H. H. Ding; Y. Tian; L. Song","School of Computing, Queen's University, Ontario, Canada; School of Computing, Queen's University, Ontario, Canada; School of Computing, Queen's University, Ontario, Canada; School of Computing, Queen's University, Ontario, Canada",IEEE Transactions on Software Engineering,"18 Apr 2023","2023","49","4","1426","1442","In recent years, the number of anonymous script-based fileless malware attacks and software copyright disputes has increased rapidly. In the literature, automated Code Authorship Analysis (CAA) techniques have been proposed to reduce the manual effort in identifying those attacks and issues. Most CAA techniques aim to solve the task of Authorship Attribution (AA), i.e., identifying the actual author of a source code fragment from a given set of candidate authors. However, in many real-world scenarios, investigators do not have a predefined set of authors containing the actual author at the time of investigation, i.e., contradicting AA's assumption. Additionally, existing AA techniques ignore the influence of code functionality when identifying the authorship, which leads to biased matching simply based on code functionality. Different from AA, the task of (extreme) Authorship Verification (AV) is to decide if two texts were written by the same person or not. AV techniques do not need a predefined author set and thus could be applied in more code authorship-related applications than AA. To our knowledge, there is no previous work attempting to solve the AV problem for the source code. To fill the gap, we propose a novel adversarial neural network, namely SCS-Gan, that can learn a stylometric representation of code for automated AV. With the multi-head attention mechanism, SCS-Gan focuses on the code parts that are most informative regarding personal styles and generates functionality-agnostic stylometric representations through adversarial training. We benchmark SCS-Gan and two state-of-the-art code representation models on four out-of-sample datasets collected from a real-world programming competition. Our experiment results show that SCS-Gan outperforms the baselines on all four out-of-sample datasets.","1939-3520","","10.1109/TSE.2022.3177228","Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN-2020-06962); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782131","Cyber threat intelligence;representation learning;adversarial learning;authorship analysis;code authorship verification","Codes;Task analysis;Training;Encoding;Feature extraction;Malware;Python","","","","52","CCBYNCND","25 May 2022","","","IEEE","IEEE Journals"
"Overcoming the Data Availability Paradox with Managed Digital Twin Instances","C. Ugbomah; S. Kumi; R. K. Lomotey; R. Deters","Department of Computer Science, University of Saskatchewan, SK, Canada; Department of Computer Science, University of Saskatchewan, SK, Canada; Information Sciences and Technology, The Pennsylvania State University, PA, USA; Department of Computer Science, University of Saskatchewan, SK, Canada",2024 IEEE Smart World Congress (SWC),"24 Mar 2025","2024","","","2025","2032","Digital transformation, IoT, and cloud storage have led to vast data collections that tend to exist in isolated silos, thus limiting their utilization. This leads to the data availability paradox, in which individuals, groups, and organizations possess vast amounts of data but struggle to utilize or share it effectively. The struggle is due to privacy, security, and intellectual property concerns. When this happens, individuals and institutions are unable to take full advantage of the capability of Industry 4.0 and digitalization. This paper proposes using managed Digital Twin Instances (DTI) to overcome this paradox. The fully managed digital twin instances provide well-defined and fully controlled access to models derived from the underlying data sets. Digital twin instances, therefore serve as virtual replicas of real-world systems, entities, or processes, enabling controlled access. A key issue in this approach is the effective hosting and managing large numbers of digital twin instances. The paper introduces a management and hosting framework and its evaluation in the MS Azure cloud environment.","2993-396X","979-8-3315-2086-1","10.1109/SWC62898.2024.00311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10925007","digital twin;data availability paradox;data sharing","Atomic measurements;Cloud computing;Runtime;Intellectual property;Containers;Throughput;Data models;HTTP;Digital twins;Standards","","","","22","IEEE","24 Mar 2025","","","IEEE","IEEE Conferences"
"Towards Collecting Royalties for Copyrighted Data for Generative Models","H. Ludwig; Y. Zhou; S. Zawad; Y. Ong; P. Li; E. Butler; E. Zahid","IBM Research, San Jose, CA, United States; IBM Research, San Jose, CA, United States; IBM Research, San Jose, CA, United States; IBM Research, San Jose, CA, United States; IBM Research, San Jose, CA, United States; IBM Research, San Jose, CA, United States; IBM Research, Cambridge, MA, United States",2024 IEEE International Conference on Web Services (ICWS),"15 Oct 2024","2024","","","20","26","Addressing issues of copyrighted data in the context of generative models has become an important issue for content creators, publishers, organizations training generative models, and those who deploy generative models for particular applications. Copyright holders want to ensure that they are fairly compensated for their work and users of training data and models do not want to expose themselves to litigation. However, traditional models of bulk-licensing data fit only poorly the context of model training. In this paper, we want to discuss why a traditional data license is not always a good fit, how data is used in the life-cycle of generative models and which impact data has on model output. This can be used as a foundation for a pay-per-(model)use compensation based how data contributes to a model’s output. Having a way to compensate copyright holders in this way reduces risk for model trainers, avoids large investments upfront, and encourage a lively data ecosystem in which the creation and distribution of original work is encouraged and fairly compensated.","2836-3868","979-8-3503-6855-0","10.1109/ICWS62655.2024.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10707489","Generative AI;large language models;copyright;royalty;licensing;data","Training;Web services;Biological system modeling;Large language models;Ecosystems;Training data;Licenses;Data models;Recording;Context modeling","","","","31","IEEE","15 Oct 2024","","","IEEE","IEEE Conferences"
"GAI-AntiCopy: Infrequent Transformation Aided Accuracy-Consistent Copyright Protection for Generative AI Instructions in NGN","Y. Fan; J. Wu","Production and Systems, Graduate School of Information, Waseda University, Fukuoka, Japan; Production and Systems, Graduate School of Information, Waseda University, Fukuoka, Japan",IEEE Transactions on Cognitive Communications and Networking,"","2025","PP","99","1","1","Generative artificial intelligence (GAI) brings an unprecedented revolution to the next-generation networks (NGN) from resource allocation to network traffic monitoring. With its powerful creative content generation capabilities, GAI significantly enhances the interaction and quality of customized services in NGN. Currently, benefiting from the thriving GAI services, it is possible to build personalized GAIs through designing GAI instructions without the need for training models from scratch. Meanwhile, infringements like pirating are emerging, necessitating effective copyright protection schemes. However, current schemes suffer from an unacceptable decrease in task processing accuracy when applied to GAIs, and the success rate of watermarking is extremely low on GAI instructions. Therefore, we propose an infrequent transformation aided accuracy-consistent copyright protection scheme for GAI instructions. We first build a comprehensive GAI instruction copyright protection system for NGN, designing a complete watermarking and verification mechanism. Additionally, we integrate copyright watermark messages with the syntactic features of GAI instructions to select the embedding positions. Watermarks are embedded through emphasis and passivization, which are infrequent transformations that minimize semantic distortion. Finally, we conduct experiments on real GAI instructions datasets and compare our scheme with existing works to demonstrate that ours effectively realizes accuracy-consistent copyright protection for GAI instructions in NGN.","2332-7731","","10.1109/TCCN.2025.3528893","Japan Society for the Promotion of Science (JSPS) KAKENHI(grant numbers:23K11072); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10838598","Copyright protection;generative AI;nextgeneration network;sentence transformation","Watermarking;Copyright protection;Next generation networking;Semantics;Artificial intelligence;Steganography;Training;Protection;Linguistics;Accuracy","","","","","IEEE","13 Jan 2025","","","IEEE","IEEE Early Access Articles"
"Animated Avatar Generation Technology Research Based on Deep Convolutional Generative Adversarial Network Integrated With Self-Attention and Spectral Normalization","H. Wu; S. Lim; B. Xiao","School of Information Engineering, Guangzhou Vocational College of Technology and Business, Guangzhou, China; Department of Computer and Information Engineering, Youngsan University, Yangsan-si, Republic of Korea; School of Information Engineering, Guangzhou Vocational College of Technology and Business, Guangzhou, China",IEEE Access,"28 Oct 2024","2024","12","","154614","154630","The burgeoning field of large language models (LLMs), exemplified by DALL-E and Stable Diffusion, has made image generation a reality. However, the computationally intensive GPU training these models necessitate incurs substantial financial burdens. Moreover, while a plethora of image datasets are accessible, specialized anime avatar datasets remain elusive and are often entangled in copyright disputes. This scarcity presents a significant research opportunity: developing a cost-effective, user-friendly anime avatar generation technique that circumvents these challenges. This paper introduces a novel method for creating animated avatars, leveraging the deep convolutional generative adversarial network(DCGAN) architecture and enhanced with Self-Attention (SA) and Spectral Normalization (SN), termed the SA+SN-DCGAN. The integration of the SA mechanism into the generator significantly elevates the quality of the output. Meanwhile, the application of SN to the discriminator effectively combats the notorious vanishing or exploding gradients, and thereby diminishing the likelihood of over-fitting. Our methodology involved sourcing anime avatars from reputable public domains and standardizing them using OpenCV. A meticulous grid search was employed to fine-tune model hyper-parameters. After 300 epochs of rigorous training, the generator and discriminator achieved stable error rates, with the synthesized images closely mirroring the fidelity of authentic avatars. Comparative evaluations against prevailing models underscore the SA+SN_DCGAN method’s superiority in producing highly realistic anime avatars, affirming its exceptional overall performance. This study not only contributes a novel solution to the domain of anime avatar generation but also paves the way for future research in the field.","2169-3536","","10.1109/ACCESS.2024.3482989","Guangdong Provincial Innovation Team Project for General Colleges and Universities(grant numbers:2023KCXTDO75); Guangdong Higher Education Scientific Research Platform and Project(grant numbers:2022KTSCX301); Key Project of Guangzhou Key Laboratory of Intelligent Interactive Technology and the Application Laboratory Open Research Topics(grant numbers:2022SYS04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10720776","Generative adversarial network;anime avatar;self-attention model;spectral normalization","Avatars;Generative adversarial networks;Generators;Training;Noise measurement;Computer architecture;Visualization;Unsupervised learning;Speech coding;Semantics","","","","55","CCBYNCND","17 Oct 2024","","","IEEE","IEEE Journals"
"Gotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in Code Models","Z. Yang; Z. Zhao; C. Wang; J. Shi; D. Kim; D. Han; D. Lo","School of Computing and Information Systems, Singapore Management University, Singapore; University of Copenhagen, Copenhagen, Denmark; School of Computing and Information Systems, Singapore Management University, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore; Korea University, Seoul, South Korea; Royal Holloway, University of London, Egham, U.K.; School of Computing and Information Systems, Singapore Management University, Singapore",IEEE Transactions on Software Engineering,"11 Dec 2024","2024","50","12","3290","3306","Leveraging large-scale datasets from open-source projects and advances in large language models, recent progress has led to sophisticated code models for key software engineering tasks, such as program repair and code completion. These models are trained on data from various sources, including public open-source projects like GitHub and private, confidential code from companies, raising significant privacy concerns. This paper investigates a crucial but unexplored question: What is the risk of membership information leakage in code models? Membership leakage refers to the vulnerability where an attacker can infer whether a specific data point was part of the training dataset. We present Gotcha, a novel membership inference attack method designed for code models, and evaluate its effectiveness on Java-based datasets. Gotcha simultaneously considers three key factors: model input, model output, and ground truth. Our ablation study confirms that each factor significantly enhances attack performance. Our ablation study confirms that each factor significantly enhances attack performance. Our investigation reveals a troubling finding: membership leakage risk is significantly elevated. While previous methods had accuracy close to random guessing, Gotcha achieves high precision, with a true positive rate of 0.95 and a low false positive rate of 0.10. We also demonstrate that the attacker's knowledge of the victim model (e.g., model architecture and pre-training data) affects attack success. Additionally, modifying decoding strategies can help reduce membership leakage risks. This research highlights the urgent need to better understand the privacy vulnerabilities of code models and develop strong countermeasures against these threats.","1939-3520","","10.1109/TSE.2024.3482719","National Research Foundation(grant numbers:NRF-NRFI08-2022-0002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10735776","Membership inference attack;privacy;large language models for code;code completion","Codes;Data models;Training data;Training;Information leakage;Software development management;Data privacy;Privacy;Decoding;Source coding","","1","","92","IEEE","25 Oct 2024","","","IEEE","IEEE Journals"
"An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project","S. Rasnayaka; G. Wang; R. Shariffdeen; G. N. Iyer","School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore",2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"30 Oct 2024","2024","","","111","118","Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s). Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain. However, the usefulness of LLMs in an academic software engineering project has not been fully explored yet. In this study, we explore the usefulness of LLMs for 214 students working in teams consisting of up to six members. Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs.In this paper, we analyze the AI-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base. We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of LLM from a computer science student’s perspective. Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration.CCS CONCEPTS• Software and its engineering → Software development techniques; • Applied computing → Education.","","979-8-4007-0579-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734434","LLM for Code Generation;Software Engineering","Productivity;Codes;Large language models;Conferences;Education;Debugging;Syntactics;Software;Software engineering;Software development management","","","","23","CCBY","30 Oct 2024","","","IEEE","IEEE Conferences"
"A Novel Approach for Creating Flowcharts using Generative AI","M. Mhatre; A. Pandey; H. Rane; S. Sahu","Department of Computer Engineering, Vivekanand Education Society’s Institute of Technology, Mumbai, India; Department of Computer Engineering, Vivekanand Education Society’s Institute of Technology, Mumbai, India; Department of Computer Engineering, Vivekanand Education Society’s Institute of Technology, Mumbai, India; Department of Computer Engineering, Vivekanand Education Society’s Institute of Technology, Mumbai, India",2024 Asia Pacific Conference on Innovation in Technology (APCIT),"18 Sep 2024","2024","","","1","7","Flowcharts have been widely used to visualize and communicate complex processes in a wide variety of domains such as education, technology, science, medicine, and manufacturing. In this paper we propose a methodology to automate the process of flowchart creation using generative AI, aiming to streamline the process and enhance the efficiency of flowchart creation. By using a large dataset of labeled flowchart-text pairs, the model learns to understand the semantics and relationships within the input text and converts them into a visually coherent and accurate flowchart representation. The methodology ensures the efficiency of the model by performing data preprocessing, model training, and performance evaluation. Automating the flowchart creation process will result in cost-effectiveness as well as consistency and accuracy. We aim to harness the power of Generative AI to automate flowchart creation, making it a valuable tool for professionals seeking an efficient and reliable means of visualizing complex processes.","","979-8-3503-6153-7","10.1109/APCIT62007.2024.10673464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10673464","Flowchart;Generative AI;Model Training;Performance Evaluation;Process Visualization","Training;Industries;Flowcharts;Visualization;Accuracy;Generative AI;Data preprocessing","","","","17","IEEE","18 Sep 2024","","","IEEE","IEEE Conferences"
"Industrial Experience Report on AI-Assisted Coding in Professional Software Development","R. Ramler; M. Moser; L. Fischer; M. Nissl; R. Heinzl","Software Competence Center, Hagenberg GmbH (SCCH), Hagenberg, Austria; Software Competence Center, Hagenberg GmbH (SCCH), Hagenberg, Austria; Software Competence Center, Hagenberg GmbH (SCCH), Hagenberg, Austria; Database and Artificial Intelligence TU Wien, Vienna, Austria; Building Digital Solutions 421 GmbH, Vienna, Austria",2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"30 Oct 2024","2024","","","1","7","AI-based tools for software development are widely discussed in academic literature. They promise to boost software development performance, especially in code creation. This paper collects insights from practitioners about the use and implications of AI assistance in industrial software development, with a focus on SMEs. Through interviews with five developers from three software development organization, we gathered and analyzed the experiences made in industrial practice, and we identified lessons learned and open challenges. ChatGPT and Copilot are used in industry projects. While they are considered useful for many code-related development activities, their integration in the development workflow remains mostly shallow. Contradicting observations about speed-ups due to AI support in development are reported. Legal issues are of minor concern although awareness exists.CCS CONCEPTS#x2022; Software and its engineering → Automatic programming.","","979-8-4007-0579-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734633","AI-assisted development;code generation;ChatGPT;Copilot","Industries;Codes;Law;Large language models;Conferences;Organizations;Software;Encoding;Interviews;Software development management","","","","9","","30 Oct 2024","","","IEEE","IEEE Conferences"
"Generative Unlearning for Any Identity","J. Seo; S. -H. Lee; T. -Y. Lee; S. Moon; G. -M. Park","Kyung Hee University, Yongin, Republic of Korea; Kyung Hee University, Yongin, Republic of Korea; Kyung Hee University, Yongin, Republic of Korea; KLleon Tech., Seoul, Republic of Korea; Kyung Hee University, Yongin, Republic of Korea",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","9151","9161","Recent advances in generative models trained on large-scale datasets have made it possible to synthesize highquality samples across various domains. Moreover, the emergence of strong inversion networks enables not only a reconstruction of real-world images but also the modification of attributes through various editing methods. However, in certain domains related to privacy issues, e.g., human faces, advanced generative models along with strong inversion methods can lead to potential misuses. In this paper, we propose an essential yet under-explored task called generative identity unlearning, which steers the model not to generate an image of specific identity. In the generative identity unlearning, we target the following objectives: (i) preventing the generation of images with a certain identity, and (ii) preserving the overall quality of the generative model. To satisfy these goals, we propose a novel framework, Generative Unlearning for Any IDEntity (GUIDE), which prevents the reconstruction of a specific identity by unlearning the generator with only a single image. GUIDE consists of two parts: (i) finding a target point for optimization that unidentifies the source latent code and (ii) novel loss functions that facilitate the unlearning procedure while less affecting the learned distribution. Our extensive experiments demonstrate that our proposed method achieves state-of-the-art performance in the generative machine unlearning task. The code is available at https://github.com/KHU-AGI/GUIDE.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.00874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10655907","machine unlearning;GAN","Industries;Privacy;Extrapolation;Codes;Generative adversarial networks;Generators;Pattern recognition","","2","","56","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"Unveiling Memorization in Code Models","Z. Yang; Z. Zhao; C. Wang; J. Shi; D. Kim; D. Han; D. Lo","School of Computing and Information Systems, Singapore Management University, Singapore; Department of Computer Science, University of Copenhagen, Copenhagen, Denmark; School of Computing and Information Systems, Singapore Management University, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore; School of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Department of Computer Science, Royal Holloway, University of London, London, UK; School of Computing and Information Systems, Singapore Management University, Singapore",2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE),"14 Jun 2024","2024","","","867","879","The availability of large-scale datasets, advanced architectures, and powerful computational resources have led to effective code models that automate diverse software engineering activities. The datasets usually consist of billions of lines of code from both open-source and private repositories. A code model memorizes and produces source code verbatim, which potentially contains vulnerabilities, sensitive information, or code with strict licenses, leading to potential security and privacy issues. This paper investigates an important problem: to what extent do code models memorize their training data? We conduct an empirical study to explore memorization in large pre-trained code models. Our study highlights that simply extracting 20,000 outputs (each having 512 tokens) from a code model can produce over 40,125 code snippets that are memorized from the training data. To provide a better understanding, we build a taxonomy of memorized contents with 3 categories and 14 subcategories. The results show that the prompts sent to the code models affect the distribution of memorized contents. We identify several key factors of memorization. Specifically, given the same architecture, larger models suffer more from memorization problem. A code model produces more memorization when it is allowed to generate longer outputs. We also find a strong positive correlation between the number of an output's occurrences in the training data and that in the generated outputs, which indicates that a potential way to reduce memorization is to remove duplicates in the training data. We then identify effective metrics that infer whether an output contains memorization accurately. We also make suggestions to deal with memorization.","1558-1225","979-8-4007-0217-4","10.1145/3597503.3639074","National Research Foundation(grant numbers:NRF-NRFI08-2022-0002); National Research Foundation, Singapore; National Research Foundation of Korea (NRF)(grant numbers:2021R1A5A1021944,2021R1I1A3048013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10549662","Software and its engineering → Software development techniques;Computing methodologies → Artificial intelligence;Security and privacy;Open-Source Software;Memorization;Code Generation","Codes;Computational modeling;Source coding;Taxonomy;Training data;Computer architecture;Data models","","5","","74","CCBY","14 Jun 2024","","","IEEE","IEEE Conferences"
"Adversarial Attacks Against Deep Generative Models on Data: A Survey","H. Sun; T. Zhu; Z. Zhang; D. Jin; P. Xiong; W. Zhou","China University of Geosciences, Wuhan, Hubei, China; School of Computer Science, China University of Geosciences, Wuhan, China; China University of Geosciences, Wuhan, Hubei, China; Zhongnan University of Economy and Law, Hubei, China; Zhongnan University of Economy and Law, Hubei, China; City University of Macau, Macau, China",IEEE Transactions on Knowledge and Data Engineering,"7 Mar 2023","2023","35","4","3367","3388","Deep generative models have gained much attention given their ability to generate data for applications as varied as healthcare to financial technology to surveillance, and many more - the most popular models being generative adversarial networks (GANs) and variational auto-encoders (VAEs). Yet, as with all machine learning models, ever is the concern over security breaches and privacy leaks and deep generative models are no exception. In fact, these models have advanced so rapidly in recent years that work on their security is still in its infancy. In an attempt to audit the current and future threats against these models, and to provide a roadmap for defense preparations in the short term, we prepared this comprehensive and specialized survey on the security and privacy preservation of GANs and VAEs. Our focus is on the inner connection between attacks and model architectures and, more specifically, on five components of deep generative models: the training data, the latent code, the generators/decoders of GANs/VAEs, the discriminators/encoders of GANs/VAEs, and the generated data. For each model, component and attack, we review the current research progress and identify the key challenges. The paper concludes with a discussion of possible future attacks and research directions in the field.","1558-2191","","10.1109/TKDE.2021.3130903","National Natural Science Foundation of China(grant numbers:61972366); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627776","Deep generative models;deep learning;membership inference attack;evasion attack;model defense","Training;Generators;Data models;Codes;Biological system modeling;Security;Privacy","","24","","123","CCBY","26 Nov 2021","","","IEEE","IEEE Journals"
"SrcMarker: Dual-Channel Source Code Watermarking via Scalable Code Transformations","B. Yang; W. Li; L. Xiang; B. Li",Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Hong Kong University of Science and Technology,2024 IEEE Symposium on Security and Privacy (SP),"5 Sep 2024","2024","","","4088","4106","The expansion of the open source community and the rise of large language models have raised ethical and security concerns on the distribution of source code, such as misconduct on copyrighted code, distributions without proper licenses, or misuse of the code for malicious purposes. Hence it is important to track the ownership of source code, in which watermarking is a major technique. Yet, drastically different from natural languages, source code watermarking requires far stricter and more complicated rules to ensure the readability as well as the functionality of the source code. Hence we introduce SrcMarker, a watermarking system to unobtrusively encode ID bitstrings into source code, without affecting the usage and semantics of the code. To this end, SrcMarker performs transformations on an AST-based intermediate representation that enables unified transformations across different programming languages. The core of the system utilizes learning-based embedding and extraction modules to select rule-based transformations for watermarking. In addition, a novel feature-approximation technique is designed to tackle the inherent non-differentiability of rule selection, thus seamlessly integrating the rule-based transformations and learning-based networks into an interconnected system to enable end-to-end training. Extensive experiments demonstrate the superiority of SrcMarker over existing methods in various watermarking requirements.","2375-1207","979-8-3503-3130-1","10.1109/SP54263.2024.00097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646683","","Training;Computer languages;Privacy;Codes;Source coding;Semantics;Pipelines","","","","68","IEEE","5 Sep 2024","","","IEEE","IEEE Conferences"
"How Secure is Code Generated by ChatGPT?","R. Khoury; A. R. Avila; J. Brunelle; B. M. Camara","Université du Québec en Outaouais, Gatineau, Canada; Institut National de Recherche Scientifique, Gatineau, Canada; Université du Québec en Outaouais, Gatineau, Canada; Université du Québec en Outaouais, Gatineau, Canada","2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","29 Jan 2024","2023","","","2445","2451","In recent years, large language models have been responsible for great advances in the field of artificial intelligence (AI). ChatGPT in particular, an AI chatbot developed and recently released by OpenAI, has taken the field to the next level. The conversational model is able not only to process human-like text, but also to translate natural language into code. However, the safety of programs generated by ChatGPT should not be overlooked. In this paper, we perform an experiment to address this issue. Specifically, we ask ChatGPT to generate a number of computer programs in order to evaluate the security of the resulting source code. We further investigate whether ChatGPT can be prodded to improve code security by appropriate prompts, and discuss the ethical aspects of using AI to generate code. Results suggest that ChatGPT is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.","2577-1655","979-8-3503-3702-0","10.1109/SMC53992.2023.10394237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10394237","Large language models;ChatGPT;code security;automatic code generation","Codes;Source coding;Chatbots;Safety;Artificial intelligence;Standards;Programming profession","","32","","22","IEEE","29 Jan 2024","","","IEEE","IEEE Conferences"
"Coding with ChatGPT and Other LLMs: Navigate LLMs for effective coding, debugging, and AI-driven development","D. V. A. Hall",NA,"Coding with ChatGPT and Other LLMs: Navigate LLMs for effective coding, debugging, and AI-driven development","","2024","","","","","Leverage LLM (large language models) for developing unmatched coding skills, solving complex problems faster, and implementing AI responsiblyKey FeaturesUnderstand the strengths and weaknesses of LLM-powered software for enhancing performance while minimizing potential issuesGrasp the ethical considerations, biases, and legal aspects of LLM-generated code for responsible AI usageBoost your coding speed and improve quality with IDE integrationPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionKeeping up with the AI revolution and its application in coding can be challenging, but with guidance from AI and ML expert Dr. Vincent Hall—who holds a PhD in machine learning and has extensive experience in licensed software development—this book helps both new and experienced coders to quickly adopt best practices and stay relevant in the field. You’ll learn how to use LLMs such as ChatGPT and Gemini to produce efficient, explainable, and shareable code and discover techniques to maximize the potential of LLMs. The book focuses on integrated development environments (IDEs) and provides tips to avoid pitfalls, such as bias and unexplainable code, to accelerate your coding speed. You’ll master advanced coding applications with LLMs, including refactoring, debugging, and optimization, while examining ethical considerations, biases, and legal implications. You’ll also use cutting-edge tools for code generation, architecting, description, and testing to avoid legal hassles while advancing your career. By the end of this book, you’ll be well-prepared for future innovations in AI-driven software development, with the ability to anticipate emerging LLM technologies and generate ideas that shape the future of development.What you will learnUtilize LLMs for advanced coding tasks, such as refactoring and optimizationUnderstand how IDEs and LLM tools help coding productivityMaster advanced debugging to resolve complex coding issuesIdentify and avoid common pitfalls in LLM-generated codeExplore advanced strategies for code generation, testing, and descriptionDevelop practical skills to advance your coding career with LLMsWho this book is forThis book is for experienced coders and new developers aiming to master LLMs, data scientists and machine learning engineers looking for advanced techniques for coding with LLMs, and AI enthusiasts exploring ethical and legal implications. Tech professionals will find practical insights for innovation and career growth in this book, while AI consultants and tech hobbyists will discover new methods for training and personal projects.","","9781805127963","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10803973.pdf&bkn=10803972&pdfType=book","","","","","","","","16 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"COGENT: A Concurrent Engineering and Generative Engineering Tooling Platform","C. O’Hara; J. Menu; M. Van Den Brand","Department of Software Technology, Eindhoven University of Technology, Eindhoven, NL; Simulation and Test Solutions, Siemens Digital Industries Software, Leuven, Belgium; Department of Software Technology, Eindhoven University of Technology, Eindhoven, NL",2022 IEEE International Systems Conference (SysCon),"16 May 2022","2022","","","1","8","System architecture design is a complex and complicated process. Systems, subsystems, and components must undergo a strict evaluation process detailing trade-offs, risks, benefits, and feasibility at the fringes of what is technologically possible. Poor architecture design leads to poor product performance, wasted resources, and in worst-case scenarios–fatalities caused by mission/product failure. Two upcoming domains seek to improve the generation, evaluation, and selection of system architecture configurations. These domains are generative engineering and concurrent engineering. Generative engineering allows for the automatic generation and evaluation of thousands of architecture configurations. Concurrent engineering is a methodology of subsystem design teams working collaboratively and simultaneously to create and select system architecture configurations. However, what had yet to be established was the value of combining the two domains. We sought to combine generative engineering and concurrent engineering to identify this value by creating the Concurrent Generative Engineering Tooling (COGENT) platform. COGENT is a plugin solution architecture that enables cross-functional teams in automated system architecture generation in concurrent design facilities. A conceptual FireSat case study was explored, demonstrating COGENT capabilities such as enabling concurrent users, synchronized tool usage, centralized object storage, and connectivity to third-party software and/or user-defined features for space systems. COGENT is modular, extensible, and easy to integrate into any system development lifecycle. With COGENT, system designers can focus on their primary concerns, goals, and constraints. Using COGENT will allow system engineers, system architects, and subsystem designers to identify optimal system architecture configurations at a fraction of the time and cost.","2472-9647","978-1-6654-3992-3","10.1109/SysCon53536.2022.9773864","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9773864","Systems Architecture;Systems Engineering;Generative Engineering;Concurrent Engineering;Space Engineering","Costs;System performance;Systems architecture;Computer architecture;Metadata;Concurrent engineering;Software","","2","","17","IEEE","16 May 2022","","","IEEE","IEEE Conferences"
"TrojanPuzzle: Covertly Poisoning Code-Suggestion Models","H. Aghakhani; W. Dai; A. Manoel; X. Fernandes; A. Kharkar; C. Kruegel; G. Vigna; D. Evans; B. Zorn; R. Sim","University of California, Santa Barbara; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; University of California, Santa Barbara; University of California, Santa Barbara; University of Virginia; Microsoft Corporation; Microsoft Corporation",2024 IEEE Symposium on Security and Privacy (SP),"5 Sep 2024","2024","","","1122","1140","With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model’s training by injecting malicious data. Poisoning attacks could be designed to influence the model’s suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, Covert and TrojanPuzzle, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TrojanPuzzle, goes one step further in generating less suspicious poison data by never explicitly including certain (suspicious) parts of the payload in the poison data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TrojanPuzzle robust against signature-based dataset-cleansing methods that can filter out suspicious sequences from the training data. Our evaluation against models of two sizes demonstrates that both Covert and TrojanPuzzle have significant implications for practitioners when selecting code used to train or tune code-suggestion models.","2375-1207","979-8-3503-3130-1","10.1109/SP54263.2024.00140","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646865","Large Language Models;Generative AI;Code Generation;Data Poisoning;Trustworthy AI","Training;Codes;Toxicology;Training data;Static analysis;Transformers;Data models","","4","","54","IEEE","5 Sep 2024","","","IEEE","IEEE Conferences"
"How Far Have We Gone in Binary Code Understanding Using Large Language Models","X. Shang; S. Cheng; G. Chen; Y. Zhang; L. Hu; X. Yu; G. Li; W. Zhang; N. Yu","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China",2024 IEEE International Conference on Software Maintenance and Evolution (ICSME),"19 Dec 2024","2024","","","1","12","Binary code analysis plays a pivotal role in various software security applications, such as software maintenance, malware detection, software vulnerability discovery, patch analysis, etc. However, unlike source code, understanding binary code is challenging for reverse engineers due to the absence of semantic information. Therefore, automated tools are needed to assist human players in interpreting binary code. In recent years, two groups of technologies have shown promising prospects: (1) Deep learning-based technologies have demonstrated competitive results in tasks related to binary code understanding, furthermore, (2) Large Language Models (LLMs) have been extensively pre-trained at the source-code level for tasks such as code understanding and generation. This makes participants wonder about the ability of LLMs in binary code understanding. In this work, we propose a benchmark to evaluate the effectiveness of LLMs in real-world reverse engineering scenarios. The benchmark covers two key binary code understanding tasks, including function name recovery and binary code summarization. We gain valuable insights into their capabilities and limitations through extensive evaluations of popular LLMs using our benchmark. Our evaluations reveal that existing LLMs can understand binary code to a certain extent, thereby improving the efficiency of binary code analysis. Our results highlight the great potential of the LLMs in advancing the field of binary code understanding.","2576-3148","979-8-3503-9568-6","10.1109/ICSME58944.2024.00012","Natural Science Foundation of China(grant numbers:U20B2047,62072421,62002334,62102386,62121002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795058","Reverse Engineering;Binary Code Understanding;Program Comprehension;Large Language Models","Software maintenance;Large language models;Source coding;Semantics;Reverse engineering;Binary codes;Benchmark testing;Malware;Security;Software engineering","","","","67","IEEE","19 Dec 2024","","","IEEE","IEEE Conferences"
"Current Landscape of Generative AI: Models, Applications, Regulations and Challenges","Y. H. Tan; H. N. Chua; Y. -C. Low; M. B. Jasser","School of Engineering and Technology, Sunway University, Bandar Sunway, Selangor Darul Ehsan, Malaysia; School of Engineering and Technology, Sunway University, Bandar Sunway, Selangor Darul Ehsan, Malaysia; School of Engineering and Technology, Sunway University, Bandar Sunway, Selangor Darul Ehsan, Malaysia; School of Engineering and Technology, Sunway University, Bandar Sunway, Selangor Darul Ehsan, Malaysia","2024 IEEE 14th International Conference on Control System, Computing and Engineering (ICCSCE)","2 Oct 2024","2024","","","168","173","Generative AI models have witnessed remarkable advancements, blurring the lines between human creativity and machine generation. This paper concisely reviews the current Generative AI landscape, exploring its diverse applications across various domains. We delve into the capabilities of these models, from creating images and music to generating creative text formats. Furthermore, the paper examines the real-world applications of Generative AI, highlighting its potential to revolutionize industries like design, marketing, education, and scientific discovery. However, while existing research extensively explores specific aspects of Generative AI, an analysis of the technology’s landscape, encompassing its capabilities, applications in content creation, and regulatory considerations, remains limited. This paper strives to bridge this gap by delivering a more holistic landscape of GenAI. Our analysis of the GenAI landscape pinpointed user behavior research and responsible development practices as key to user-centric AI creation. Through this study, we aim to stimulate discussion and collaboration between researchers, developers, and policymakers to ensure this powerful technology is harnessed responsibly for the benefit of industry and society.","","979-8-3503-6450-7","10.1109/ICCSCE61582.2024.10696569","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10696569","Generative AI;Deep Learning;Applications;Regulations;Ethics;Content Creation","Industries;Privacy;Generative AI;Reviews;Law;Explainable AI;Prevention and mitigation;Education;Regulation;Creativity","","","","45","IEEE","2 Oct 2024","","","IEEE","IEEE Conferences"
"Optimizing Secure AI Lifecycle Model Management With Innovative Generative AI Strategies","A. Omran Almagrabi; R. A. Khan","Department of Information Systems, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Computer Science and IT, Software Engineering Research Group, University of Malakand, Chakdara, Pakistan",IEEE Access,"24 Jan 2025","2025","13","","12889","12920","Generative AI (GAI) is one of the significant components that can efficiently improve and augment the AI cycle model’s robustness when it comes to different threats, weaknesses, and abnormalities detection. When applied in this field, GAI is very useful in emulating the various forms of security violations in actual adversarial settings. These scenarios are important when different aspects of an AI system are tested on how robust they are and thus permit the developers to amend any vulnerability that may be induced before the time it could be utilized in practice. Data and model manipulation, data theft, and adversarial attacks as well as model inference threats which we do a systematic analysis to disrupt the integrity, confidentiality as well as availability of AI models. Considering the current weaknesses and threats related to GAI we provide a systematic approach to how safety concerns that are currently relevant can be integrated with every stage of Artificial Intelligence (AI) lifecycle management: from continuous monitoring to the application of cybersecurity trends and practices, etc. In our approach, the emphasis is placed on the multi-level security management strategy that incorporates the improvement of coding practices, validation and testing, and the implementation of advanced intrusion detection systems. Before proceeding to further analysis and discussion of the given topic, it is also critical to mention the aspect of regulation and ethical concern as the major drivers of GAI usage. Additionally, organizations can involve GAI in the lifecycle to address security needs, during the development, acquisition, deployment, updating, maintenance, and decommissioning of the AI system, making them reliable, safe, and secure all through their lifecycle. Toward these ends, the goal of this work is to present a set of canonical recommendations for the many scientists, engineers, managers, technologists, and policymakers who will play a key role in constructing a sound and secure AI future.","2169-3536","","10.1109/ACCESS.2024.3491373","Department of Information Systems, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Deanship of Scientific Research (DSR) at King Abdulaziz University(grant numbers:GPIP: 1278-611-2024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10742321","Generative artificial intelligence;AI lifecycle model;security threats and practices;systematic mapping study","Artificial intelligence;Data models;Security;Training;Organizations;Law;Generative AI;Ethics;Synthetic data;Generative adversarial networks","","1","","70","CCBYNCND","4 Nov 2024","","","IEEE","IEEE Journals"
"Generative AI, Ingenuity, and Law","J. R. Carvalko","Technology and Ethics Research Working Group, Interdisciplinary Center for Bioethics, Institution for Social and Policy Studies, Yale University, New Haven, CT, USA",IEEE Transactions on Technology and Society,"9 Aug 2024","2024","5","2","169","182","This paper discusses generative pre-trained transformer technology and its intersection with forms of creativity and law. It highlights the potential of generative AI to change considerable elements of society, including modes of creative endeavors, problem-solving, employment, education, justice, medicine, and governance. The author emphasizes the need for policymakers and experts to join in regulating against the potential risks and implications of this technology. The European Commission has taken steps to address the risks of AI through the European AI Act (EIA), which categorizes AI uses based on their potential harm. The legislation aims to ensure scrutiny and control in extreme cases like autonomous weapons or medical devices. However, the author criticizes the lack of meaningful AI oversight in the United States and argues that time has come for government to step in and offer meaningful regulation given the technology’s (1) rate of diffusion (2) virtually uncountable product permutations, the purposes, extent and depths to which it is anticipated to penetrate institutional and daily life.","2637-6415","","10.1109/TTS.2024.3413591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10598190","Artificial intelligence;computation and language;deep learning neural networks;NLP;LLM;OpenAI;ChatGPT;generative AI;generative pretrained transformer;transformer-based AI;European AI Act;EIA;technology ethics","Generative AI;Artificial intelligence;Ethics;Internet;Chatbots;Deep learning;Neural networks;Regulation;Problem-solving;Employment;Education;Europe;Creativity;Social implications of technology;Risk management","","1","","69","IEEE","15 Jul 2024","","","IEEE","IEEE Journals"
"Intelligent Patent Text Summarization Analysis Method","Y. Ji","The High School Affiliated to Renmin University of China, Beijing, China",2021 7th International Conference on Systems and Informatics (ICSAI),"10 Jan 2022","2021","","","1","6","Patent mining and patent analysis of patented technologies will help protect the interests of intellectual property rights and provide enterprises with correct scientific research directions. In order to study the profitable patents of pharmaceutical companies, this paper proposes an Abstractive RL-LSTM neural network method based on patent texts. The reinforcement learning method is introduced into LSTM. The purpose is to rely on Q-learning to learn the relationship between the main layers. The two parallel layers share the weight of attention from the Q value, and realize the hierarchical control between the LSTM structure of the patent document and the LSTM structure of the sentence. The experimental results show that compared with other methods, the method proposed in this paper can further improve the ROUGE index and alleviate the dependence of the decoder on the input.","","978-1-6654-2624-4","10.1109/ICSAI53574.2021.9664064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664064","Patent analysis;Text summarization;RL-LSTM;Machine learning","Patents;Q-learning;Neural networks;Companies;Intellectual property;Decoding;Computational efficiency","","","","19","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets","J. Katzy; R. -M. Popescu; A. Van Deursen; M. Izadi","Delft University of Technology, Delft, Netherlands; Delft University of Technology, Delft, Netherlands; Delft University of Technology, Delft, Netherlands; Delft University of Technology, Delft, Netherlands",2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge) Conference Acronym:,"30 Jul 2024","2024","","","74","85","Does the training of large language models potentially infringe upon code licenses? Furthermore, are there any datasets available that can be safely used for training these models without violating such licenses? In our study, we assess the current trends in the field and the importance of incorporating code into the training of large language models. Additionally, we examine publicly available datasets to see whether these models can be trained on them without the risk of legal issues in the future. To accomplish this, we compiled a list of 53 large language models trained on file-level code. We then extracted their datasets and analyzed how much they overlap with a dataset we created, consisting exclusively of strong copyleft code. Our analysis revealed that every dataset we examined contained license inconsistencies, despite being selected based on their associated repository licenses. We analyzed a total of 514 million code files, discovering 38 million exact duplicates present in our strong copyleft dataset. Additionally, we examined 171 million file-leading comments, identifying 16 million with strong copyleft licenses and another 11 million comments that discouraged copying without explicitly mentioning a license. Based on the findings of our study, which highlights the pervasive issue of license inconsistencies in large language models trained on code, our recommendation for both researchers and the community is to prioritize the development and adoption of best practices for dataset creation and management.","","979-8-4007-0536-6","10.1145/3650105.3652298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10599552","Large Language Models;Foundation Models;Code Licensing;Soft-ware Engineering;ML4SE;Machine Learning;Datasets","Training;Surveys;Codes;Terminology;Law;Large language models;Licenses","","","","41","","30 Jul 2024","","","IEEE","IEEE Conferences"
"Hardware IP Trust Validation: Learn (the Untrustworthy), and Verify","T. Hoque; J. Cruz; P. Chakraborty; S. Bhunia","Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL",2018 IEEE International Test Conference (ITC),"24 Jan 2019","2018","","","1","10","Increasing reliance on hardware Intellectual Property (IP) cores in modern system-on-chip (SoC) design flow, often obtained from untrusted vendors distributed across the globe, can significantly compromise the security of SoCs. While the design could be verified for a specified functionality using existing tools, it is extremely hard to verify its trustworthiness to guarantee that no hidden, and possibly malicious function exists in the form of a hardware Trojan. Conventional verification process and tools fail to verify the trust of a third-party IP, primarily due to the lack of trusted reference design or golden models. In this paper, for the first time to our knowledge, we introduce a systematic framework to apply machine learning based classification for hardware IP trust verification. A supervised classifier could be trained for identifying Trojan nets within a suspect IP, but the detection coverage and accuracy are extremely sensitive to the quality of training set available. Furthermore, reliance on a static training database limits the classifier’s ability in detecting new Trojans and facilitates adversarial learning. The proposed framework includes a Trojan insertion tool that dynamically generates a large number of diverse implementations of Trojan classes for creating a robust training set. It is significantly more difficult for an adversary to evade our classifier using known Trojan classes since the tool dynamically samples the entire Trojan population. To further improve the efficiency of the system, we combined three machine learning models into an average probability Voting Ensemble. Our results for two broad classes of Trojan show excellent classification accuracy of 99.69% and 99.88% with F-score of 86.69% and 88.37% for sequential and combinational Trojans, respectively.","2378-2250","978-1-5386-8382-8","10.1109/TEST.2018.8624727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624727","","Trojan horses;Hardware;IP networks;Training;Tools;Feature extraction;Payloads","","39","","18","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Generative AI for Transformative Healthcare: A Comprehensive Study of Emerging Models, Applications, Case Studies, and Limitations","S. Sai; A. Gaur; R. Sai; V. Chamola; M. Guizani; J. J. P. C. Rodrigues","Department of Electrical and Electronics Engineering and APPCAIR, Birla Institute of Technology and Science (BITS), Pilani Campus, Pilani, Rajasthan, India; Department of Electrical and Communication Engineering, Maharaja Agrasen Institute of Technology, Delhi, New Delhi, India; Department of Computer Science and Information Systems, Birla Institute of Technology and Science (BITS), Pilani Campus, Pilani, Rajasthan, India; Department of Electrical and Electronics Engineering and APPCAIR, Birla Institute of Technology and Science (BITS), Pilani Campus, Pilani, Rajasthan, India; Department of Machine Learning, Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, United Arab Emirates; Department of Computer Engineering and Information Systems, Lusófona University, Lisbon, Portugal",IEEE Access,"1 Mar 2024","2024","12","","31078","31106","Generative artificial intelligence (GAI) can be broadly described as an artificial intelligence system capable of generating images, text, and other media types with human prompts. GAI models like ChatGPT, DALL-E, and Bard have recently caught the attention of industry and academia equally. GAI applications span various industries like art, gaming, fashion, and healthcare. In healthcare, GAI shows promise in medical research, diagnosis, treatment, and patient care and is already making strides in real-world deployments. There has yet to be any detailed study concerning the applications and scope of GAI in healthcare. Addressing this research gap, we explore several applications, real-world scenarios, and limitations of GAI in healthcare. We examine how GAI models like ChatGPT and DALL-E can be leveraged to aid in the applications of medical imaging, drug discovery, personalized patient treatment, medical simulation and training, clinical trial optimization, mental health support, healthcare operations and research, medical chatbots, human movement simulation, and a few more applications. Along with applications, we cover four real-world healthcare scenarios that employ GAI: visual snow syndrome diagnosis, molecular drug optimization, medical education, and dentistry. We also provide an elaborate discussion on seven healthcare-customized LLMs like Med-PaLM, BioGPT, DeepHealth, etc.,Since GAI is still evolving, it poses challenges like the lack of professional expertise in decision making, risk of patient data privacy, issues in integrating with existing healthcare systems, and the problem of data bias which are elaborated on in this work along with several other challenges. We also put forward multiple directions for future research in GAI for healthcare.","2169-3536","","10.1109/ACCESS.2024.3367715","Fund provided by the Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)(grant numbers:8481000021); Chanakya Fellowship Program of the Technology Innovation Hub (TIH) Foundation for Internet of Things (IoT) Internet of Everything (IoE)(grant numbers:CFP/2022/027); Brazilian National Council for Scientific and Technological Development—CNPq via(grant numbers:306607/2023-9); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440330","Generative AI;ChatGPT;healthcare;LLMs;applications","Medical services;Chatbots;Medical diagnostic imaging;Data models;Generative AI;Training;Solid modeling;Generative adversarial networks;Artificial intelligence;Drugs;Visualization","","28","","88","CCBYNCND","20 Feb 2024","","","IEEE","IEEE Journals"
"Accurate and Real-Time Variant Hand Pose Estimation Based on Gray Code Bounding Box Representation","Y. Wang; W. Sun; R. Rao","Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, School of Automation, Southeast University, Nanjing, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, School of Automation, Southeast University, Nanjing, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, School of Automation, Southeast University, Nanjing, China",IEEE Sensors Journal,"3 Jun 2024","2024","24","11","18043","18053","Effective hand gestures are crucial for human-machine interactions, and recent advancements in neural networks offer promising avenues for efficient hand pose estimation. However, existing methods still face challenges in detecting hand poses of different scales within a single RGB image sensor. This article introduces a novel approach, drawing inspiration from modulus conversion, to enhance the efficiency of hand pose estimation from a single RGB image sensor. The method involves transforming the floating-point values of hand regions into binary codes, ensuring continuous numerical space without a significant computational overhead. This approach significantly improves accuracy for hands of varying sizes in both detection and pose estimation. Furthermore, this article addresses the challenge of datasets lacking hand keypoints annotations by introducing a novel loss computation for labeled keypoints during network training. To assess the effectiveness of the proposed method, a new benchmark for variant hand scales is presented, facilitating evaluation across different hand sizes. The proposed approach undergoes testing on diverse datasets, with experimental results demonstrating comparable performance to state-of-the-art methods, thereby validating its efficacy. Additionally, the study conducts several ablation studies, exploring aspects such as the choice of Gray code, code length, effectiveness across different hand scales, and training with labeled keypoints to affirm the efficiency and effectiveness of the proposed method.","1558-1748","","10.1109/JSEN.2024.3389055","National Natural Science Foundation of China(grant numbers:62076061); Natural Science Foundation of Jiangsu Province(grant numbers:BK20220127); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506333","Bounding box representation;gray code;hand pose estimation;real-time","Pose estimation;Training;Annotations;Three-dimensional displays;Sensors;Color;Reflective binary codes;Real-time systems","","","","49","IEEE","22 Apr 2024","","","IEEE","IEEE Journals"
"Building Data-Driven Applications with LlamaIndex: A practical guide to retrieval-augmented generation (RAG) to enhance LLM applications","A. Gheorghiu",NA,Building Data-Driven Applications with LlamaIndex: A practical guide to retrieval-augmented generation (RAG) to enhance LLM applications,"","2024","","","","","Solve real-world problems easily with artificial intelligence (AI) using the LlamaIndex data framework to enhance your LLM-based Python applications Key FeaturesExamine text chunking effects on RAG workflows and understand security in RAG app developmentDiscover chatbots and agents and learn how to build complex conversation enginesBuild as you learn by applying the knowledge you gain to a hands-on projectBook DescriptionDiscover the immense potential of Generative AI and Large Language Models (LLMs) with this comprehensive guide. Learn to overcome LLM limitations, such as contextual memory constraints, prompt size issues, real-time data gaps, and occasional ‘hallucinations’. Follow practical examples to personalize and launch your LlamaIndex projects, mastering skills in ingesting, indexing, querying, and connecting dynamic knowledge bases. From fundamental LLM concepts to LlamaIndex deployment and customization, this book provides a holistic grasp of LlamaIndex's capabilities and applications. By the end, you'll be able to resolve LLM challenges and build interactive AI-driven applications using best practices in prompt engineering and troubleshooting Generative AI projects.What you will learnUnderstand the LlamaIndex ecosystem and common use casesMaster techniques to ingest and parse data from various sources into LlamaIndexDiscover how to create optimized indexes tailored to your use casesUnderstand how to query LlamaIndex effectively and interpret responsesBuild an end-to-end interactive web application with LlamaIndex, Python, and StreamlitCustomize a LlamaIndex configuration based on your project needsPredict costs and deal with potential privacy issuesDeploy LlamaIndex applications that others can useWho this book is forThis book is for Python developers with basic knowledge of natural language processing (NLP) and LLMs looking to build interactive LLM applications. Experienced developers and conversational AI developers will also benefit from the advanced techniques covered in the book to fully unleash the capabilities of the framework.","","9781805124405","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10540159.pdf&bkn=10540158&pdfType=book","","","","","","","","28 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs","J. Fei; Z. Xia; B. Tondi; M. Barni","College of Cyber Security, Engineering Research Center of Trustworthy AI, Ministry of Education, Jinan University, Guangzhou, China; College of Cyber Security, Engineering Research Center of Trustworthy AI, Ministry of Education, Jinan University, Guangzhou, China; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy",IEEE Transactions on Information Forensics and Security,"19 Sep 2024","2024","19","","8322","8337","We propose a novel multi-bit box-free watermarking method for the protection of Intellectual Property Rights (IPR) of GANs with improved robustness against white-box model-level attacks like fine-tuning, pruning, quantization, and surrogate model attacks. The watermark is embedded by adding an extra watermarking loss term during GAN training, ensuring that the images generated by the GAN contain an invisible watermark that can be retrieved by a pre-trained watermark decoder. In order to improve the robustness against white-box model-level attacks, we make sure that the model converges to a wide flat minimum of the watermarking loss term, in such a way that any modification of the model parameters does not erase the watermark. To do so, we add random noise vectors to the parameters of the generator and require that the watermarking loss term is as invariant as possible with respect to the presence of noise. This procedure forces the generator to converge to a wide flat minimum of the watermarking loss. The proposed method is architecture- and dataset-agnostic, thus being applicable to many different generation tasks and models, as well as to CNN-based image processing architectures. We present the results of extensive experiments showing that the presence of the watermark has a negligible impact on the quality of the generated images, and proving the superior robustness of the watermark against model modification and surrogate model attacks.","1556-6021","","10.1109/TIFS.2024.3443650","National Key Research and Development Program of China(grant numbers:2022YFB3103100); National Natural Science Foundation of China(grant numbers:U23B2023,62122032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10636196","GAN watermarking;DNN watermarking;ownership verification;watermark robustness;deep learning security","Watermarking;Robustness;Glass box;Generative adversarial networks;Closed box;Decoding;Generators","","2","","71","IEEE","14 Aug 2024","","","IEEE","IEEE Journals"
"FCCA: Hybrid Code Representation for Functional Clone Detection Using Attention Networks","W. Hua; Y. Sui; Y. Wan; G. Liu; G. Xu","College of Information Engineering, Shanghai Maritime University, Shanghai, China; University of Technology, Sydney, NSW, Australia; Zhejiang University, Hangzhou, China; College of Information Engineering, Shanghai Maritime University, Shanghai, China; University of Technology, Sydney, NSW, Australia",IEEE Transactions on Reliability,"2 Mar 2021","2021","70","1","304","318","Code cloning, which reuses a fragment of source code via copy-and-paste with or without modifications, is a common way for code reuse and software prototyping. However, the duplicated code fragments often affect software quality, resulting in high maintenance cost. The existing clone detectors using shallow textual or syntactical features to identify code similarity are still ineffective in accurately finding sophisticated functional code clones in real-world code bases. This article proposes functional code clone detector using attention (FCCA), a deep-learning-based code clone detection approach on top of a hybrid code representation by preserving multiple code features, including unstructured (code in the form of sequential tokens) and structured (code in the form of abstract syntax trees and control-flow graphs) information. Multiple code features are fused into a hybrid representation, which is equipped with an attention mechanism that pays attention to important code parts and features that contribute to the final detection accuracy. We have implemented and evaluated FCCA using 275 777 real-world code clone pairs written in Java. The experimental results show that FCCA outperforms several state-of-the-art approaches for detecting functional code clones in terms of accuracy, recall, and F1 score.","1558-1721","","10.1109/TR.2020.3001918","National Natural Science Foundation of China(grant numbers:61672338,61373028); Australian Research Council(grant numbers:DP200101374,LP170100891,DP200101328); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146274","Attention mechanism;code clone detection;code representation;deep neural network (DNN)","Cloning;Feature extraction;Detectors;Syntactics;Software quality;Maintenance engineering","","57","","67","IEEE","22 Jul 2020","","","IEEE","IEEE Journals"
"Promise and Peril of Collaborative Code Generation Models: Balancing Effectiveness and Memorization","Z. Chen; L. Jiang","Centre for Research on Intelligent Software Engineering, School of Computing and Information Systems, Singapore Management University, Singapore; Centre for Research on Intelligent Software Engineering, School of Computing and Information Systems, Singapore Management University, Singapore",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","493","505","In the rapidly evolving field of machine learning, training models with datasets from various locations and organizations presents significant challenges due to privacy and legal concerns. The exploration of effective collaborative training settings, which are capable of leveraging valuable knowledge from distributed and isolated datasets, is increasingly crucial.This study investigates key factors that impact the effectiveness of collaborative training methods in code next-token prediction, as well as the correctness and utility of the generated code, showing the promise of such methods. Additionally, we evaluate the memorization of different participant training data across various collaborative training settings, including centralized, federated, and incremental training, showing their potential risks in leaking data.Our findings indicate that the size and diversity of code datasets are pivotal factors influencing the success of collaborative trained code models. We demonstrate that federated learning achieves competitive performance compared to centralized training while offering better data protection, as evidenced by lower memorization ratios in the generated code. However, federated learning can still produce verbatim code snippets from hidden training data, potentially violating data privacy or copyright. Our study further explores the patterns of effectiveness and memorization in incremental learning, emphasizing the importance of the sequence in which individual participant datasets are introduced. Also, we identify the memorization phenomenon of cross-organizational clones as a prevalent challenge in both centralized and federated learning scenarios. Our findings highlight the persistent risk of data leakage during inference, even when training data remains unseen. We conclude with strategic recommendations for practitioners and researchers to optimize the use of multisource datasets, thereby propelling the cross-organizational collaboration forward.CCS Concepts• Software and its engineering → Collaboration in software development; • Computing methodologies → Simulation evaluation; • Security and privacy;","2643-1572","979-8-4007-1248-7","","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764998","Collaborative Training;Memorization;Large Language Model;Code Generation","Training;Uniform resource locators;Codes;Incremental learning;Federated learning;Computational modeling;Collaboration;Training data;Cloning;Software engineering","","","","60","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Fixing the Double Agent Vulnerability of Deep Watermarking: A Patch-Level Solution Against Artwork Plagiarism","Y. Luo; T. Zhou; S. Cui; Y. Ye; F. Liu; Z. Cai","College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; School of Design, Hunan University, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; School of Design, Hunan University, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China",IEEE Transactions on Circuits and Systems for Video Technology,"6 Mar 2024","2024","34","3","1670","1683","Increasing artwork plagiarism incidents stresses the urgent need for proper copyright protection on behalf of the creators. The latest development in this context focuses on embedding watermarks via deep encoder-decoder networks. However, we find that deep watermarking has a serious vulnerability on its robustness when facing deliberate plagiarism. To manifest it, we construct an attack that misuses watermarking encoder as a plagiarism lookout for bypassing copyright detection. As a remedy, we propose a patch-level deep watermarking framework (DIPW) to retain copyright evidence in essential patches with plagiarism resistance, inspired by a user study observation that subject elements in artworks are the principal plagiarism entities. Technically, DIPW adaptively finds the embedding patches by identifying a subset of non-overlapping and feature-rich objects; and tailors the model with dual-distortion losses and adversarial plagiarism noise injection for robustness. Experimental results demonstrate the superiority of DIPW in facilitating better robustness, secrecy, and imperceptibility with acceptable time burden.","1558-2205","","10.1109/TCSVT.2023.3295895","National Natural Science Foundation of China(grant numbers:62102425); Science and Technology Innovation Program of Hunan Province(grant numbers:2022RC3061,2021RC2071); Postgraduate Research and Innovation Project of Hunan Province(grant numbers:CX20220049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10184464","Deep watermarking;artwork copyright protection;plagiarism resistance;convolutional neural networks","Watermarking;Plagiarism;Training;Robustness;Decoding;Distortion;Copyright protection","","1","","60","IEEE","17 Jul 2023","","","IEEE","IEEE Journals"
"Compressive Privacy Generative Adversarial Network","B. -W. Tseng; P. -Y. Wu","Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan",IEEE Transactions on Information Forensics and Security,"7 Feb 2020","2020","15","","2499","2513","Machine learning as a service (MLaaS) has brought much convenience to our daily lives recently. However, the fact that the service is provided through cloud raises privacy leakage issues. In this work we propose the compressive privacy generative adversarial network (CPGAN), a data-driven adversarial learning framework for generating compressing representations that retain utility comparable to state-of-the-art, with the additional feature of defending against reconstruction attack. This is achieved by applying adversarial learning scheme to the design of compression network (privatizer), whose utility/privacy performances are evaluated by the utility classifier and the adversary reconstructor, respectively. Experimental results demonstrate that CPGAN achieves better utility/privacy trade-off in comparison with the previous work, and is applicable to real-world large datasets.","1556-6021","","10.1109/TIFS.2020.2968188","Ministry of Science and Technology, Taiwan(grant numbers:MOST-107-2634-F-002-008-,MOST-108-2634-F-002-005-); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8963921","Compressive privacy;cyber security;privacy preserving machine learning;adversarial learning;generative adversarial networks;machine learning as a service","Data privacy;Privatization;Privacy;Data models;Generative adversarial networks;Stochastic processes;Feature extraction","","28","","71","IEEE","20 Jan 2020","","","IEEE","IEEE Journals"
"Generative Adversarial Analysis using U-LSB Based Audio Steganography","V. Moorthy; R. Venkataraman","Dept of Networking and Communications, School of Computing, SRMIST, Kattankulathur, TN, India; Dept of Networking and Communications, School of Computing, SRMIST, Kattankulathur, TN, India",2021 IEEE 18th India Council International Conference (INDICON),"1 Feb 2022","2021","","","1","6","Audio steganography is the technique of hiding data within a carrier audio file, where concealed data is imperceptible to humans. The existing deep-learning-based approaches depend on human handcraft for the generation and steganalysis of the steganographic audio. Generative Adversarial Network (GAN) based models are used nowadays for the generation of audio data from random latent space have proven to be efficient. This can be further utilized to strengthen the existing audio steganography methods. The proposed framework is a GAN model consisting of the U-Net-based generator, LSB-based embedder, and the discriminator. The method relies on an unsupervised adversarial training algorithm for embedding secret audio within the carrier audio in the temporal domain, making it imperceptible to humans. The experimental results on the 1-second Speech Command dataset show that the model can effectively produce steganographic audio using embedding probabilities. The steganographic method has proven to produce high-fidelity audio files capable of resisting steganalysis as compared to the audios generated using conventional methods. This technique can be effectively used for secured communication of the audio files transmission in midst of cyberspace hacking tricks.","2325-9418","978-1-6654-4175-9","10.1109/INDICON52576.2021.9691515","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9691515","audio steganography;audio file;generative adversarial network;carrier audio;unsupervised adversarial training","Training;Industries;Steganography;Neural networks;Information security;Detectors;Generative adversarial networks","","3","","14","IEEE","1 Feb 2022","","","IEEE","IEEE Conferences"
"RO-SVD: A Reconfigurable Hardware Copyright Protection Framework for AIGC Applications","Z. Ran; M. A. A. Abdelgawad; Z. Zhang; R. C. C. Cheung; H. Yan","Department of Electrical Engineering, Centre for Intelligent Multidimensional Data Analysis, City University of Hong Kong, Hong Kong SAR, China; Department of Electrical Engineering, Centre for Intelligent Multidimensional Data Analysis, City University of Hong Kong, Hong Kong SAR, China; Department of Electrical Engineering, Centre for Intelligent Multidimensional Data Analysis, City University of Hong Kong, Hong Kong SAR, China; Department of Electrical Engineering, Centre for Intelligent Multidimensional Data Analysis, City University of Hong Kong, Hong Kong SAR, China; Department of Electrical Engineering, Centre for Intelligent Multidimensional Data Analysis, City University of Hong Kong, Hong Kong SAR, China","2024 IEEE 35th International Conference on Application-specific Systems, Architectures and Processors (ASAP)","22 Aug 2024","2024","","","135","142","The dramatic surge in the utilisation of generative artificial intelligence (GenAI) underscores the need for a secure and efficient mechanism to responsibly manage, use and disseminate multidimensional data generated by artificial intelligence (AI). In this paper, we propose a blockchain-based copyright traceability framework called ring oscillator-singular value decomposition (RO-SVD), which introduces decomposition computing to approximate low-rank matrices generated from hardware entropy sources and establishes an AI-generated content (AIGC) copyright traceability mechanism at the device level. By leveraging the parallelism and reconfigurability of field-programmable gate arrays (FPGAs), our framework can be easily constructed on existing AI -accelerated devices and provide a low-cost solution to emerging copyright issues of AIGC. We developed a hardware-software (HW /SW) co-design prototype based on comprehensive analysis and on-board experiments with multiple AI-applicable FPGAs. Using AI-generated images as a case study, oyr framework demonstrated effectiveness and emphasised customisation, unpredictability, efficiency, manage-ment and reconfigurability. To the best of our knowledge, this is the first practical hardware study discussing and implementing copyright traceability specifically for AI -generated conten t.","2160-052X","979-8-3503-4963-4","10.1109/ASAP61560.2024.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10631139","AI-generated content;copyright protection;singular value decomposition (SVD);blockchain;nanofabrication;AI security;low-power AI","Program processors;Generative AI;Systems architecture;Surge protection;Prototypes;Parallel processing;Hardware","","","","36","IEEE","22 Aug 2024","","","IEEE","IEEE Conferences"
"A Survey on ChatGPT: AI–Generated Contents, Challenges, and Solutions","Y. Wang; Y. Pan; M. Yan; Z. Su; T. H. Luan","School of Cyber Science, Engineering, Xi'an Jiaotong University, Xi'an, China; School of Cyber Science, Engineering, Xi'an Jiaotong University, Xi'an, China; School of Cyber Science, Engineering, Xi'an Jiaotong University, Xi'an, China; School of Cyber Science, Engineering, Xi'an Jiaotong University, Xi'an, China; School of Cyber Science, Engineering, Xi'an Jiaotong University, Xi'an, China",IEEE Open Journal of the Computer Society,"26 Oct 2023","2023","4","","280","302","With the widespread use of large artificial intelligence (AI) models such as ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is leading a paradigm shift in content creation and knowledge representation. AIGC uses generative large AI algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts. Despite the recent significant progress in AIGC, security, privacy, ethical, and legal challenges still need to be addressed. This paper presents an in-depth survey of working principles, security and privacy threats, state-of-the-art solutions, and future challenges of the AIGC paradigm. Specifically, we first explore the enabling technologies, general architecture of AIGC, and discuss its working modes and key characteristics. Then, we investigate the taxonomy of security and privacy threats to AIGC and highlight the ethical and societal implications of GPT and AIGC technologies. Furthermore, we review the state-of-the-art AIGC watermarking approaches for regulatable AIGC paradigms regarding the AIGC model and its produced content. Finally, we identify future challenges and open research directions related to AIGC.","2644-1268","","10.1109/OJCS.2023.3300321","National Natural Science Foundation of China(grant numbers:U22A2029,U20A20175); Fundamental Research Funds for the Central Universities; China Postdoctoral Science Foundation(grant numbers:2023M732820); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10221755","AIGC;generative AI;ChatGPT;security;and privacy","Artificial intelligence;Chatbots;Surveys;Security;Computational modeling;Privacy;Training","","86","","116","CCBY","16 Aug 2023","","","IEEE","IEEE Journals"
"Bits to BNNs: Reconstructing FPGA ML-IP with Joint Bitstream and Side-Channel Analysis","B. Olney; R. Karam","Department of Computer Science and Engineering, University of South Florida; Department of Computer Science and Engineering, University of South Florida",2023 IEEE International Symposium on Hardware Oriented Security and Trust (HOST),"25 May 2023","2023","","","238","248","Energy-efficient hardware acceleration platforms for edge deployment of artificial intelligence (AI) and machine learning (ML) applications has been an ongoing research endeavor. Many efforts have focused on optimizing the algorithms and compute structures for use in resource-constrained hardware such as field-programmable gate arrays (FPGAs). Indeed, the difficult nature of crafting the best model makes the ML model itself a valuable intellectual property (IP) asset. This can be problematic, as the IP can now be exposed to an attacker through physical interfaces, enabling threats from side-channel analysis (SCA) attacks. One of the more devastating attacks is the model extraction attack, which threatens piracy and cloning of the valuable IP. While the problem of SCA-based model extraction on FPGA-deployed neural networks has been well-studied, it does not capture the full picture of what vulnerabilities may be present in those platforms. In this paper, we demonstrate how bitstream analysis can be used to obtain neural network parameters and connectivity information from block RAMs (BRAMs). We leverage the knowledge gleaned from the bitstream to mount a power SCA attack to further refine the network reconstruction effort. This is the first method that has approached the problem of ML-IP theft from the angle of FPGA bitstream analysis and suggests that further work is needed to improve security assurance for edge intelligence.","2765-8406","979-8-3503-0062-8","10.1109/HOST55118.2023.10133375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10133375","","Knowledge engineering;Machine learning algorithms;Computational modeling;Neural networks;Random access memory;Machine learning;Intellectual property","","2","","40","IEEE","25 May 2023","","","IEEE","IEEE Conferences"
"A Comparative Analysis on Exploration of Stegosploits across Various Media Formats","R. Asharani; K. Vidyalakshmi","Dept. of CSE, Sri Siddhartha Institute of Technology, Sri Siddhartha Academy of Higher Education, Tumkur, India; Department of CSE(DS), Sri Siddhartha Institute of Technology, Sri Siddhartha Academy of Higher Education, Tumkur, India",2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS),"7 Aug 2024","2024","1","","1","8","The terms “steganography” and “exploits” are combined to form “stegosploits,” a clever and covert method of hiding hostile payloads inside innocent media files. The investigation of stegosploits in a variety of media types is presented in-depth in this study. Stegosploits are a serious danger to cybersecurity because they may elude detection methods that are conventional, giving attackers the ability to covertly implant and distribute malicious payloads over a variety of communication routes. In order to improve knowledge and awareness of this new cyber threat scenario, this article will look at stegosploit tactics in photos, audios, videos, and other multimedia formats.","","979-8-3503-5968-8","10.1109/ICKECS61492.2024.10616568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10616568","Adversarial Generative Network;Deep Learning;Image Features;Steganography;Steganalysis;Multimedia Security;and Malware Hiding","Surveys;Deep learning;Steganography;Reviews;Media;Streaming media;Generative adversarial networks","","","","34","IEEE","7 Aug 2024","","","IEEE","IEEE Conferences"
"Experiences and Proposals of Use of Generative AI in Advanced Software Courses","D. Palacios-Alonso; J. Urquiza-Fuentes; J. Á. Velázquez-Iturbide; J. Guillén-García","Department of Computing and Statistics, Universidad Rey Juan, Carlos, Madrid, Spain; Department of Computing and Statistics, Universidad Rey Juan, Carlos, Madrid, Spain; Department of Computing and Statistics, Universidad Rey Juan, Carlos, Madrid, Spain; Department of Computing and Statistics, Universidad Rey Juan, Carlos, Madrid, Spain",2024 IEEE Global Engineering Education Conference (EDUCON),"8 Jul 2024","2024","","","1","10","The last year, we have witnessed the popularization of generative artificial intelligence. Its output includes text, code, image, audio, speech, voice, music, and video. Therefore, it impacts education courses where students are required to elaborate on any of these artifacts. In particular, the generation of code affects informatics courses, where assignments usually ask students to develop and deliver programming code. The impact of generative artificial intelligence on informatics courses has been mainly studied for introductory programming courses. These studies have shown that generative artificial intelligence is able to produce highly sophisticated programs, but also that its results and rationale can be inaccurate. Moreover, the impact of generative artificial intelligence has not been studied for other informatics subjects. In this paper, we present our preliminary experience and proposals on three advanced software courses, namely video games, advanced algorithms and language processors. For the video games course, we present the opportunities of use of generative artificial intelligence and the results of a survey conducted with students on their use to obtain different media products. For the algorithms course, we present the result of a session driven by the instructor on different design techniques, showing the merits and demerits of the answers generated. For the language processors course, a proposal of use of generative artificial intelligence is presented, broken down into the parts of a typical language processor. The paper concludes with some suggestions for instructors.","2165-9567","979-8-3503-9402-3","10.1109/EDUCON60312.2024.10578869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578869","informatics education;generative artificial intelligence;video games;advanced algorithms;language processors","Surveys;Video games;Program processors;Codes;Generative AI;Software algorithms;Software","","","","30","IEEE","8 Jul 2024","","","IEEE","IEEE Conferences"
"Data-Prep-Kit: getting your data ready for LLM application development","D. Wood; B. Lublinsky; A. Roytman; S. Singh; C. Adam; A. Adebayo; S. An; Y. C. Chang; X. -H. Dang; N. Desai; M. Dolfi; H. Emami-Gohari; R. Eres; T. Goto; D. Joshi; Y. Koyfman; M. Nassar; H. Patel; P. Selvam; Y. Shah; S. Surendran; D. Tsuzuku; P. Zerfos; S. Daijavad","IBM Research, New York, USA; IBM Research, Mulhaddart, Ireland; IBM Research Israel, Haifa, Israel; IBM Research India, Bangalore, India; IBM Research, New York, USA; IBM Research, New York, USA; Almaden Research Center IBM, San Jose, CA, USA; IBM Research, New York, USA; IBM Research, New York, USA; IBM Research, New York, USA; IBM Research, Zurich, Switzerland; IBM Research, New York, USA; IBM Research Israel, Haifa, Israel; IBM Software, Tokyo, Japan; IBM Research, New York, USA; IBM Research, New York, USA; IBM Research Israel, Haifa, Israel; IBM Research India, Bangalore, India; IBM Research India, Bangalore, India; IBM Research, New York, USA; IBM Research India, Bangalore, India; IBM Software, Tokyo, Japan; IBM Research, New York, USA; Almaden Research Center IBM, San Jose, CA, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","2234","2243","Data preparation is the first and a very important step towards any Large Language Model (LLM) development. This paper introduces an easy-to-use, extensible, and scale-flexible open-source data preparation toolkit called Data Prep Kit (DPK). DPK is architected and designed to enable users to scale their data preparation to their needs. With DPK they can prepare data on a local machine or effortlessly scale to run on a cluster with thousands of CPU Cores. DPK comes with a highly scalable, yet extensible set of modules that transform natural language and code data. If the user needs additional transforms, they can be easily developed using extensive DPK support for transform creation. These modules can be used independently or pipelined to perform a series of operations. In this paper, we describe DPK architecture and show its performance from a small scale to a very large number of CPUs. The modules from DPK have been used for the preparation of Granite Models [1] [2]. We believe DPK is a valuable contribution to the AI community to easily prepare data to enhance the performance of their LLM models or to fine-tune models with Retrieval-Augmented Generation (RAG).","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825085","LLM;Generative AI;Data Preparation;Data Processing;Toolkit;Open-source;Ray;Spark;KFP;RAG","Automation;Codes;Runtime;Large language models;Retrieval augmented generation;Natural languages;Transforms;Writing;Data models;Sparks","","","","29","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Identifying Potential Standard Essential Patents Based on Text Mining and Generative Topographic Mapping","F. Wu; L. Mi; X. Li; L. Huang; Y. Tong","College of Economics and Management, Beijing University of Technology, Beijing, China; College of Economics and Management, Beijing University of Technology, Beijing, China; College of Economics and Management, Beijing University of Technology, Beijing, China; College of Economics and Management, Beijing University of Technology, Beijing, China; College of Economics and Management, Beijing University of Technology, Beijing, China",2018 IEEE International Symposium on Innovation and Entrepreneurship (TEMS-ISIE),"4 Oct 2018","2018","","","1","9","The identification of the potential standards essential patents (SEPs) can make great contributions to not only the technology management theory, but also to the real practices of establishment and development of enterprise competitiveness and standardization strategy. However, despite the importance of identifying potential SEPs, the approaches to identify potential SEPs lack of adequate mining of existing technical standard text and validation of identification results based on standard updates. Therefore, in this paper, we contribute to resolving this issue by proposing a research model that integrates text mining and the generative topographic mapping (GTM) to effectively identify and verity the potential SEPs based on existing and updated standards. The universal terrestrial radio access (UTRA) technology is selected as a case study. In this case, the TF-IDF algorithm and the Latent Dirichlet Allocation (LDA) method are applied to analyze the keywords and technical theme of standard and patent documents, and GTM is used to construct standard and patent map, then the two maps are mapped by the improved similarity algorithm we proposed. Finally, 39 potential SEPs of the technology are identified, 24 of which have been verified, and the other 15 are likely to be included in subsequent standard version. This paper will contribute to the identification of SEPs methodology, and will be of interest to UTRA technology research and development experts.","","978-1-5386-4475-1","10.1109/TEMS-ISIE.2018.8478502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478502","Technical standard;Standard essential patents;Patent identification;Generative Topographic Mapping;Text mining","Standards;Patents;Vacuum technology;Technological innovation;Text mining;Forecasting","","6","","34","IEEE","4 Oct 2018","","","IEEE","IEEE Conferences"
"Assessing the Generalizability of Code2vec Token Embeddings","H. J. Kang; T. F. Bissyandé; D. Lo","Singapore Management University; University of Luxembourg, Luxembourg, Luxembourg; Singapore Management University",2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),"9 Jan 2020","2019","","","1","12","Many Natural Language Processing (NLP) tasks, such as sentiment analysis or syntactic parsing, have benefited from the development of word embedding models. In particular, regardless of the training algorithms, the learned embeddings have often been shown to be generalizable to different NLP tasks. In contrast, despite recent momentum on word embeddings for source code, the literature lacks evidence of their generalizability beyond the example task they have been trained for. In this experience paper, we identify 3 potential downstream tasks, namely code comments generation, code authorship identification, and code clones detection, that source code token embedding models can be applied to. We empirically assess a recently proposed code token embedding model, namely code2vec's token embeddings. Code2vec was trained on the task of predicting method names, and while there is potential for using the vectors it learns on other tasks, it has not been explored in literature. Therefore, we fill this gap by focusing on its generalizability for the tasks we have identified. Eventually, we show that source code token embeddings cannot be readily leveraged for the downstream tasks. Our experiments even show that our attempts to use them do not result in any improvements over less sophisticated methods. We call for more research into effective and general use of code embeddings.","2643-1572","978-1-7281-2508-4","10.1109/ASE.2019.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952475","Code Embeddings;Distributed Representations;Big Code","Task analysis;Software engineering;Natural language processing;Training;Cloning;Vocabulary","","54","","65","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Multilevel Ownership Protection via Watermarking and Encryption","H. K. Singh; N. Baranwal; K. N. Singh; A. K. Singh","Department of CSE, National Institute of Technology Patna, Patna, India; Department of CSE, Indian Institute of Technology Kanpur, Kanpur, India; Department of CSE/IT, Jaypee Institute of Information Technology (JIIT), Noida, India; Department of CSE, National Institute of Technology Patna, Patna, India",IEEE Transactions on Computational Social Systems,"","2025","PP","99","1","10","Nowadays, the ownership of shared social images has attracted increasingly serious privacy violation concerns, thus the protection of these images is particularly important. Further, due to the increasing value of deep learning (DL) models in social media platforms, there is an urgent demand to protect their copyright and prevent privacy leakage. Currently, relatively limited research has been carried out in the field of ownership protection for deep neural network (DNN) and, at the same time, related social images. This article presents a robust copyright protection system and method for the DNN model and related social images to verify ownership. Initially, split-way training of our model was established, based on cover and encoded secret images, to reduce or eradicate privacy leakage. Then, the sender sends only encoded vector information instead of raw data to adversarial-based embedding and extraction networks. Next, we embed a secret mark in DL embedding and extraction models, using interpolation-based watermarking to verify the ownership of suspicious models if any piracy or infringements occur. Last, the intended receiver extracts the hidden information using the extraction networks. Extensive experiments show that the proposed system is more robust and secure against attacks than state-of-the-art methods, making it beneficial for social media and other practical applications. The results demonstrate that the proposed method outperforms state-of-the-art methods in terms of average peak signal-to-noise noise ratio, normalized correlation, number of pixel change rate, and unified average change intensity, with improvements of 47.25%, 43.25%, 17.89%, and 14.23%, respectively.","2329-924X","","10.1109/TCSS.2025.3544766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10925518","Attacks;copyright protection;deep learning;digital watermarking;encryption;split learning","Watermarking;Encryption;Privacy;Convolutional neural networks;Robustness;Feature extraction;Copyright protection;Convolutional codes;Biological system modeling;Training","","","","","IEEE","13 Mar 2025","","","IEEE","IEEE Early Access Articles"
"An Empirical Study to Evaluate AIGC Detectors on Code Content","J. Wang; S. Liu; X. Xie; Y. Li","Singapore Management University, Singapore; Singapore Management University, Singapore; Singapore Management University, Singapore; Singapore Management University, Singapore",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","844","856","Artificial Intelligence Generated Content (AIGC) has garnered considerable attention for its impressive performance, with Large Language Models (LLMs), like ChatGPT, emerging as a leading AIGC model that produces high-quality responses across various applications, including software development and maintenance. Despite its potential, the misuse of LLMs, especially in security and safety-critical domains, such as academic integrity and answering questions on Stack Overflow, poses significant concerns. Numerous AIGC detectors have been developed and evaluated on natural language data. However, their performance on code-related content generated by LLMs remains unexplored.To fill this gap, in this paper, we present an empirical study evaluating existing AIGC detectors in the software domain. We select three state-of-the-art LLMs, i.e., GPT-3.5, WizardCoder and CodeLlama, for machine-content generation. We further created a comprehensive dataset including 2.23M samples comprising code-related content for each model, encompassing popular software activities like Q&A (150K), code summarization (1M), and code generation (1.1M). We evaluated thirteen AIGC detectors, comprising six commercial and seven open-source solutions, assessing their performance on this dataset. Our results indicate that AIGC detectors perform less on code-related data than natural language data. Fine-tuning can enhance detector performance, especially for content within the same domain; but generalization remains a challenge.CCS CONCEPTS• General and reference → Empirical studies; • Security and privacy → Social aspects of security and privacy; • Computing methodologies → Natural language generation.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764906","AIGC Detection;Code Generation;Large Language Model","Privacy;Codes;Natural languages;Natural language generation;Detectors;Software;Maintenance;Security;Software engineering;Software development management","","","","73","","29 Nov 2024","","","IEEE","IEEE Conferences"
"RecCoder: Reformulating Sequential Recommendation as Large Language Model-Based Code Completion","K. -H. Lai; W. -D. Xi; X. -X. Xing; W. Wan; C. -D. Wang; M. Chen; M. Guizani","School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; UX Center, NetEase Games, Guangzhou, China; UX Center, NetEase Games, Guangzhou, China; UX Center, NetEase Games, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Machine Learning Department, Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE",2024 IEEE International Conference on Data Mining (ICDM),"21 Feb 2025","2024","","","191","200","In the evolving landscape of sequential recommendation systems, the application of Large Language Models (LLMs) is increasingly prominent. However, current attempts typically utilize general-purpose LLMs, which present a mismatch in capability and a large semantic gap relative to the specialized needs of recommendation tasks. To tackle these issues, we introduce RecCoder, an innovative model that reformulates sequential recommendation as a code completion task. This approach leverages the superior reasoning capability of code LLMs as a backbone, aligning well with the requirements of recommendation systems. To bridge the semantic gap, RecCoder creates extra tokens for each item and employs item content to initialize token embeddings. Furthermore, we have developed a suite of Semantic Adaptation Fine-tuning tasks, tailored to enhance the model's acquisition of both content and collaborative semantic information, thus aligning the model's intrinsic capabilities with the unique demands of recommendation tasks. Through extensive testing on three public datasets, RecCoder has shown remarkable improvements over existing models in terms of recommendation accuracy and efficiency. This success highlights the substantial yet previously underexplored potential of code LLMs in improving recommendation accuracy and efficiency, suggesting a promising new direction for future research in this area. The implementation code is accessible at https://github.com/AllminerLab/Code-for-RecCoder-master.","2374-8486","979-8-3315-0668-1","10.1109/ICDM59182.2024.00026","NSFC(grant numbers:62276277); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10884501","sequential recommendation;large language model","Adaptation models;Codes;Accuracy;Large language models;Semantics;Cognition;Data models;Data mining;Recommender systems;Testing","","","","40","IEEE","21 Feb 2025","","","IEEE","IEEE Conferences"
"Generative AI-Powered Assistant for Developers: Accelerate software development with Amazon Q Developer","B. Irani; R. Sonawane",NA; NA,Generative AI-Powered Assistant for Developers: Accelerate software development with Amazon Q Developer,"","2024","","","","","Leverage Amazon Q Developer to boost productivity and maximize efficiency by accelerating software development life cycle tasksKey FeaturesFirst book on the market to thoroughly explore all of Amazon Q Developer’s featuresGain an understanding of Amazon Q Developer's capabilities across the software development life cycle through real-world examplesBuild apps with Amazon Q Developer by auto-generating code in various languages within supported IDEsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionMany developers face the challenge of managing repetitive tasks and maintaining productivity. This book will help you tackle both these challenges with Amazon Q Developer, a generative AI-powered assistant designed to optimize coding and streamline workflows. This book takes you through the setup and customization of Amazon Q Developer, demonstrating how to leverage its capabilities for auto-code generation, code explanation, and transformation across multiple IDEs and programming languages. You'll learn to use Amazon Q Developer to enhance coding experiences, generate accurate code references, and ensure security by scanning for vulnerabilities. The book also shows you how to use Amazon Q Developer for AWS-related tasks, including solution building, applying architecture best practices, and troubleshooting errors. Each chapter provides practical insights and step-by-step guidance to help you fully integrate this powerful tool into your development process. You’ll get to grips with effortless code implementation, explanation, transformation, and documentation, helping you create applications faster and improve your development experience. By the end of this book, you’ll have mastered Amazon Q Developer to accelerate your software development lifecycle, improve code quality, and build applications faster and more efficiently.What you will learnUnderstand the importance of generative AI-powered assistants in developers' daily workEnable Amazon Q Developer for IDEs and with AWS services to leverage code suggestionsCustomize Amazon Q Developer to align with organizational coding standardsUtilize Amazon Q Developer for code explanation, transformation, and feature developmentUnderstand code references and scan for code security issues using Amazon Q DeveloperAccelerate building solutions and troubleshooting errors on AWSWho this book is forThis book is for coders, software developers, application builders, data engineers, and technical resources using AWS services looking to leverage Amazon Q Developer's features to enhance productivity and accelerate business outcomes. Basic coding skills are needed to understand the concepts covered in this book.","","9781835081204","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769302.pdf&bkn=10769301&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Special Session: Countering IP Security threats in Supply chain","H. Salmani; T. Hoque; S. Bhunia; M. Yasin; J. J. Rajendran; N. Karimi","EECS Departmente, Howard University; ECE Department, Florida University; ECE Department, Florida University; ECE Department, Texas A&M University; ECE Department, Texas A&M University; CSEE Department, University of Maryland, Baltimore County",2019 IEEE 37th VLSI Test Symposium (VTS),"11 Jul 2019","2019","","","1","9","The continuing decrease in feature size of integrated circuits, and the increase of the complexity and cost of design and fabrication has led to outsourcing the design and fabrication of integrated circuits to third parties across the globe, and in turn has introduced several security vulnerabilities. The adversaries in the supply chain can pirate integrated circuits, overproduce these circuits, perform reverse engineering, and/or insert hardware Trojans in these circuits. Developing countermeasures against such security threats is highly crucial. Accordingly, this paper first develops a learning-based trust verification framework to detect hardware Trojans. To tackle Trojan insertion, IP piracy and overproduction, logic locking schemes and in particular stripped functionality logic locking is discussed and its resiliency against the state-of-the-art attacks is investigated.","2375-1053","978-1-7281-1170-4","10.1109/VTS.2019.8758633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758633","","Integrated circuits;Training;Data models;Monitoring;Tagging;Valves;Machine learning","","1","","63","IEEE","11 Jul 2019","","","IEEE","IEEE Conferences"
"ChatGPT for Cybersecurity Cookbook: Learn practical generative AI recipes to supercharge your cybersecurity skills","C. Bodungen; A. Crow",NA; NA,ChatGPT for Cybersecurity Cookbook: Learn practical generative AI recipes to supercharge your cybersecurity skills,"","2024","","","","","Master ChatGPT and the OpenAI API and harness the power of cutting-edge generative AI and large language models to revolutionize the way you perform penetration testing, threat detection, and risk assessment.Key FeaturesEnhance your skills by leveraging ChatGPT to generate complex commands, write code, and create toolsAutomate penetration testing, risk assessment, and threat detection tasks using the OpenAI API and Python programmingRevolutionize your approach to cybersecurity with an AI-powered toolkitPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionAre you ready to unleash the potential of AI-driven cybersecurity? This cookbook takes you on a journey toward enhancing your cybersecurity skills, whether you’re a novice or a seasoned professional. By leveraging cutting-edge generative AI and large language models such as ChatGPT, you'll gain a competitive advantage in the ever-evolving cybersecurity landscape. ChatGPT for Cybersecurity Cookbook shows you how to automate and optimize various cybersecurity tasks, including penetration testing, vulnerability assessments, risk assessment, and threat detection. Each recipe demonstrates step by step how to utilize ChatGPT and the OpenAI API to generate complex commands, write code, and even create complete tools. You’ll discover how AI-powered cybersecurity can revolutionize your approach to security, providing you with new strategies and techniques for tackling challenges. As you progress, you’ll dive into detailed recipes covering attack vector automation, vulnerability scanning, GPT-assisted code analysis, and more. By learning to harness the power of generative AI, you'll not only expand your skillset but also increase your efficiency. By the end of this cybersecurity book, you’ll have the confidence and knowledge you need to stay ahead of the curve, mastering the latest generative AI tools and techniques in cybersecurity.What you will learnMaster ChatGPT prompt engineering for complex cybersecurity tasksUse the OpenAI API to enhance and automate penetration testingImplement artificial intelligence-driven vulnerability assessments and risk analysesAutomate threat detection with the OpenAI APIDevelop custom AI-enhanced cybersecurity tools and scriptsPerform AI-powered cybersecurity training and exercisesOptimize cybersecurity workflows using generative AI-powered techniquesWho this book is forThis book is for cybersecurity professionals, IT experts, and enthusiasts looking to harness the power of ChatGPT and the OpenAI API in their cybersecurity operations. Whether you're a red teamer, blue teamer, or security researcher, this book will help you revolutionize your approach to cybersecurity with generative AI-powered techniques. A basic understanding of cybersecurity concepts along with familiarity in Python programming is expected. Experience with command-line tools and basic knowledge of networking concepts and web technologies is also required.","","9781805125112","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10522553.pdf&bkn=10522552&pdfType=book","","","","","","","","8 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"Generative AI Foundations in Python: Discover key techniques and navigate modern challenges in LLMs","C. Rodriguez; S. Shaikh",NA; NA,Generative AI Foundations in Python: Discover key techniques and navigate modern challenges in LLMs,"","2024","","","","","Begin your generative AI journey with Python as you explore large language models, understand responsible generative AI practices, and apply your knowledge to real-world applications through guided tutorialsKey FeaturesGain expertise in prompt engineering, LLM fine-tuning, and domain adaptationUse transformers-based LLMs and diffusion models to implement AI applicationsDiscover strategies to optimize model performance, address ethical considerations, and build trust in AI systemsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe intricacies and breadth of generative AI (GenAI) and large language models can sometimes eclipse their practical application. It is pivotal to understand the foundational concepts needed to implement generative AI. This guide explains the core concepts behind -of-the-art generative models by combining theory and hands-on application. Generative AI Foundations in Python begins by laying a foundational understanding, presenting the fundamentals of generative LLMs and their historical evolution, while also setting the stage for deeper exploration. You’ll also understand how to apply generative LLMs in real-world applications. The book cuts through the complexity and offers actionable guidance on deploying and fine-tuning pre-trained language models with Python. Later, you’ll delve into topics such as task-specific fine-tuning, domain adaptation, prompt engineering, quantitative evaluation, and responsible AI, focusing on how to effectively and responsibly use generative LLMs. By the end of this book, you’ll be well-versed in applying generative AI capabilities to real-world problems, confidently navigating its enormous potential ethically and responsibly.What you will learnDiscover the fundamentals of GenAI and its foundations in NLPDissect foundational generative architectures including GANs, transformers, and diffusion modelsFind out how to fine-tune LLMs for specific NLP tasksUnderstand transfer learning and fine-tuning to facilitate domain adaptation, including fields such as financeExplore prompt engineering, including in-context learning, templatization, and rationalization through chain-of-thought and RAGImplement responsible practices with generative LLMs to minimize bias, toxicity, and other harmful outputsWho this book is forThis book is for developers, data scientists, and machine learning engineers embarking on projects driven by generative AI. A general understanding of machine learning and deep learning, as well as some proficiency with Python, is expected.","","9781835464915","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769393.pdf&bkn=10769392&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"The Fine Balance Between Helping With Your Job and Taking It: AI Code Assistants Come to the Fore","C. R. B. de Souza; G. Rodríguez-Pérez; M. Basha; D. Yoon; I. Beschastnikh","Universidade Federal do Pará, Belém, Brazil; University of British Columbia, Kelowna, BC, Canada; University of British Columbia, Kelowna, BC, Canada; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada",IEEE Software,"4 Oct 2024","2024","41","6","111","118","AI code generation tools are reshaping the software engineering landscape. We provide recommendations for practitioners interested in these tools based on narratives we have collected regarding two AI code generation tools, GitHub Copilot and Tabnine.","1937-4194","","10.1109/MS.2024.3357787","Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN-2014-04870,GR023171); National Council for Scientific and Technological Development(grant numbers:420406/2023-9,442779/2023-2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10416322","","Codes;Blogs;Artificial intelligence;Social networking (online);Software engineering;Software development management;Productivity;Generative AI;Software tools;Software development management;Law;Licenses","","2","","13","IEEE","29 Jan 2024","","","IEEE","IEEE Magazines"
"UI/UX for Generative AI: Taxonomy, Trend, and Challenge","T. -S. Kim; M. John Ignacio; S. Yu; H. Jin; Y. -G. Kim","Department of Computer Engineering, Sejong University, Seoul, Republic of Korea; Department of Computer Engineering, Sejong University, Seoul, Republic of Korea; Department of Business Administration, Sejong University, Seoul, Republic of Korea; School of Computer Science and Technology, Anhui University, Hefei, China; Department of Computer Engineering, Sejong University, Seoul, Republic of Korea",IEEE Access,"5 Dec 2024","2024","12","","179891","179911","Current technological advancements in Information Technology are closely linked to Generative Artificial Intelligence, enabling the automation of complex tasks such as generating documents, images, videos, audio, and actions. As such tasks can save much human labor and resources, diverse industries are trying to adopt this technology. However, developing a product utilizing Generative AI is a challenging task, partly because it is a new technology and many users are not familiar with it yet. This paper’s primary goal is to find a better way to design Generative AI systems, especially from the Human-Computer Interaction perspective. To begin, we propose a taxonomy for Generative AI systems based on their modality, such as text-based, image-based, audio-based, and multi-modal-based systems, and then evaluate them in terms of their usability, because their functionalities should be aligned with the User Interface (UI), leading to a better User Experience (UX). We survey important trends in this area and introduce future applications by touching upon the issue of explainable AI. Although Generative AI has a bright future, it faces formidable challenges in our industries and society. It is hoped that the taxonomy and research findings presented here will be a useful framework for future research in Generative AI systems and their UI/UX.","2169-3536","","10.1109/ACCESS.2024.3502628","Information Technology Research Center Support Program(grant numbers:IITP-2022-RS-2022-00156354); Korean Government [Ministry of Science and Information Technology (MSIT)] from the Institute of Information and Communications Technology Planning and Evaluation (IITP)(grant numbers:RS-2019-II190231); Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education(grant numbers:2020R1A6A1A03038540); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10758657","Audio;explainable AI;generative AI;image;multimodal-GPT;text;UI/UX","Generative AI;Artificial intelligence;Chatbots;Taxonomy;User experience;Internet;Encoding;Bidirectional control;Generators;Decoding","","","","120","CCBY","20 Nov 2024","","","IEEE","IEEE Journals"
"Identifying the Style of Chatting","M. Zhang; Y. Ma; G. Luo; S. Li; Z. Qian; X. Zhang","School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University",2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),"20 Nov 2023","2023","","","1085","1092","Group chat brings great convenience to people’s online social interaction. When identifying users in online social networks, individual speaking style of the member in group chat particularly plays an important role. In this paper, we propose a novel and efficient group chat hashing framework termed Group Chat Style Hashing(GCSH), which is the first work to utilize personal hashes in group chats to identify the speakers. GCSH combines a supervised VAE and a discriminator to extract accurate identity-related features and applies a new sample aggregation hash algorithm to generate an exclusive identity style hash for each one in the group chat. Our model fuses more context information to directly get the representative hashes, achieving an end-to-end identification model and sharply reducing the matching cost compared with previous text hashing methods. We conduct extensive experiments to prove the effectiveness and the generalization ability of our method across datasets.","2640-0103","979-8-3503-0067-3","10.1109/APSIPAASC58517.2023.10317307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10317307","","Costs;Social networking (online);Fuses;Asia;Information processing;Feature extraction;Context modeling","","","","25","IEEE","20 Nov 2023","","","IEEE","IEEE Conferences"
"XFL: Naming Functions in Binaries with Extreme Multi-label Learning","J. Patrick-Evans; M. Dannehl; J. Kinder","Research Institute CODE, Bundeswehr University, Munich, Germany; Research Institute CODE, Bundeswehr University, Munich, Germany; Research Institute CODE, Bundeswehr University, Munich, Germany",2023 IEEE Symposium on Security and Privacy (SP),"21 Jul 2023","2023","","","2375","2390","Reverse engineers benefit from the presence of identifiers such as function names in a binary, but usually these are removed for release. Training a machine learning model to predict function names automatically is promising but fundamentally hard: unlike words in natural language, most function names occur only once. In this paper, we address this problem by introducing eXtreme Function Labeling (XFL), an extreme multi-label learning approach to selecting appropriate labels for binary functions. XFL splits function names into tokens, treating each as an informative label akin to the problem of tagging texts in natural language. We relate the semantics of binary code to labels through Dexter, a novel function embedding that combines static analysis-based features with local context from the call graph and global context from the entire binary. We demonstrate that XFL/Dexter outperforms the state of the art in function labeling on a dataset of 10,047 binaries from the Debian project, achieving a precision of 83.5%. We also study combinations of XFL with alternative binary embeddings from the literature and show that Dexter consistently performs best for this task. As a result, we demonstrate that binary function labeling can be effectively phrased in terms of multi-label learning, and that binary function embeddings benefit from including explicit semantic features.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179439","Binary-Analysis;Reverse-Engineering;Representation-Learning;Extreme-Multi-label-Learning","Training;Semantics;Natural languages;XML;Binary codes;Static analysis;Predictive models","","3","","69","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion","X. Wu; Y. Hua; C. Liang; J. Zhang; H. Wang; T. Song; H. Guan",Shanghai Jiao Tong University; Queen's University Belfast; University of Southern California; Shanghai Jiao Tong University; Louisiana State University; Shanghai Jiao Tong University; Shanghai Jiao Tong University,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","10812","10821","Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dream-booth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.01028","Intel Corporation(grant numbers:12679); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10655884","Diffusion models;trustworthy AI;copyright authentication;AI for social good","Training;Fabrication;Visualization;Monte Carlo methods;Law;Image synthesis;Authentication","","","","40","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"An Overview of Trustworthy AI: Advances in IP Protection, Privacy-Preserving Federated Learning, Security Verification, and GAI Safety Alignment","Y. Zheng; C. -H. Chang; S. -H. Huang; P. -Y. Chen; S. Picek","School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore; Department of Electronic Engineering, Chung Yuan Christian University, Taoyuan, Taiwan; IBM Research, Yorktown Heights, NY, USA; Institute for Computing and Information Sciences, Radboud University, Nijmegen, The Netherlands",IEEE Journal on Emerging and Selected Topics in Circuits and Systems,"13 Dec 2024","2024","14","4","582","607","AI has undergone a remarkable evolution journey marked by groundbreaking milestones. Like any powerful tool, it can be turned into a weapon for devastation in the wrong hands. Understanding that no model is perfect, trustworthy AI is initiated with an intuitive aim to mitigate the harm it can inflict on people and society by prioritizing socially responsible AI ideation, design, development, and deployment towards effecting positive changes. The scope of trustworthy AI is encompassing, covering qualities such as safety, security, privacy, transparency, explainability, fairness, impartiality, robustness, reliability, and accountability. This overview paper anchors on recent advances in four research hotspots of trustworthy AI with compelling and challenging security, privacy, and safety issues. The topics discussed include the intellectual property protection of deep learning and generative models, the trustworthiness of federated learning, verification and testing tools of AI systems, and the safety alignment of generative AI systems. Through this comprehensive review, we aim to provide readers with an overview of the most up-to-date research problems and solutions. By presenting the rapidly evolving factors and constraints that motivate the emerging attack and defense strategies throughout the AI life-cycle, we hope to inspire more research effort into guiding AI technologies towards beneficial purposes with greater robustness against malicious use intent.","2156-3365","","10.1109/JETCAS.2024.3477348","National Natural Science Foundation of China(grant numbers:62404192); University Development Fund of The Chinese University of Hong Kong, Shenzhen(grant numbers:UDF01003337); Imperial College/Nanyang Technological University (NTU) CYber Protection for HEalthcaRe (IN-CYPHER) Program supported by the National Research Foundation, Prime Minister’s Office, Singapore, under its Campus for Research Excellence and Technological Enterprise (CREATE) Program; National Science and Technology Council (NSTC), Taiwan(grant numbers:112-2221-E-033-050-MY3,113-2640-E-008-001); Chief Digital and Artificial Intelligence Office(grant numbers:W519TC-23-9-2037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711270","Federated learning;deep neural network;generative AI;large language model;formal verification;safety alignment;trustworthy AI;model poisoning;data poisoning;backdoor;watermarking;fingerprinting;security;privacy-preservation","Artificial intelligence;Security;Data models;Protection;Integrated circuit modeling;Training;Circuits and systems;Privacy;Training data;Mathematical models","","","","278","CCBYNCND","9 Oct 2024","","","IEEE","IEEE Journals"
"Foundations of Ethics in GenAI","M. R. Islam","Geroge Mason Univeristy, Fairfax, Virginia, United States","Generative AI, Cybersecurity, and Ethics","","2025","","","111","162","Summary <p>This chapter explores the ethical considerations essential for the responsible development and deployment of generative artificial intelligence (GenAI). It underscores the importance of guiding principles such as fairness, accountability, and transparency to mitigate biases and ensure privacy. The chapter traces the evolution of ethics in technology from ancient philosophies to contemporary artificial intelligence (AI) guidelines, highlighting pivotal regulatory frameworks like the EU's Ethics Guidelines for Trustworthy AI. It also emphasizes the need for adaptive and internationally converged regulatory approaches to address the unique challenges posed by GenAI, ensuring its alignment with societal values and ethical norms.</p>","","9781394279319","10.1002/9781394279326.ch5","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10897050.pdf&bkn=10896969&pdfType=chapter","","Ethics;Artificial intelligence;Organizations;Intellectual property;Guidelines;Stakeholders;Nuclear weapons;Navigation;Decision making;Data protection","","","","","","20 Feb 2025","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"Cyber Attack Prediction: From Traditional Machine Learning to Generative Artificial Intelligence","S. Ankalaki; A. R. Atmakuri; M. Pallavi; G. S. Hukkeri; T. Jan; G. R. Naik","Department of Computer Science and Engineering, Manipal Institute of Technology Bengaluru, Manipal Academy of Higher Education, Manipal, Karnataka, India; Department of CSE, SoET, Centurion University of Technology and Management, Bhubaneswar, Odisha, India; School of Computer Science and Engineering, Presidency University, Bengaluru, India; Department of Computer Science and Engineering, Manipal Institute of Technology Bengaluru, Manipal Academy of Higher Education, Manipal, Karnataka, India; Centre for Artificial Intelligence Research and Optimization (AIRO), Design and Creative Technology Vertical, Torrens University, Ultimo, NSW, Australia; Centre for Artificial Intelligence Research and Optimization (AIRO), Design and Creative Technology Vertical, Torrens University, Ultimo, NSW, Australia",IEEE Access,"14 Mar 2025","2025","13","","44662","44706","The escalating sophistication of cyber threats poses significant risks to individuals, organizations, and nations. Cybercrime, encompassing activities like hacking and data breaches, has severe economic and societal consequences. In today’s interconnected world, robust cybersecurity measures are paramount to mitigate these risks and protect sensitive information. However, traditional security solutions struggle to keep pace with the evolving threat landscape. Artificial Intelligence (AI) offers a powerful arsenal of techniques to address these challenges. This paper explores the application of AI methods, including Machine Learning (ML), Deep Learning (DL), Natural Language Processing (NLP), Explainable AI (XAI), and Generative AI, in solving various cybersecurity problems. This paper presents a comprehensive analysis of AI techniques for enhancing cybersecurity. Key contributions include: 1) comparative study of ML and DL methods: Evaluating their accuracy, applicability, and suitability for various cybersecurity challenges; 2) investigation into XAI approaches: Enhancing the transparency and interpretability of AI-powered security solutions, particularly in anomaly detection; 3) exploration of emerging trends in Generative AI (Gen-AI) and NLP: Examining their potential to simulate and mitigate cyber threats through advanced techniques like threat intelligence generation and attack simulations; 4) application of GenAI in cybersecurity and real-world products of GenAI for cyber security. This research aims to advance the state-of-the-art in AI-driven cybersecurity by providing insights into effective and reliable solutions for mitigating cyber risks and improving the overall security posture.","2169-3536","","10.1109/ACCESS.2025.3547433","Manipal Academy of Higher Education (Open Access Funding); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10909100","Cybersecurity;cyber-attack prediction;machine learning;deep learning;explainable AI;generative AI","Artificial intelligence;Computer security;Security;Cyberattack;Explainable AI;Generative AI;Ransomware;Deep learning;Chatbots;Accuracy","","","","247","CCBY","3 Mar 2025","","","IEEE","IEEE Journals"
"Generative AI for Cloud Solutions: Architect modern AI LLMs in secure, scalable, and ethical cloud environments","P. Singh; A. Karuparti; J. Maeda",NA; NA; NA,"Generative AI for Cloud Solutions: Architect modern AI LLMs in secure, scalable, and ethical cloud environments","","2024","","","","","Explore Generative AI, the engine behind ChatGPT, and delve into topics like LLM-infused frameworks, autonomous agents, and responsible innovation, to gain valuable insights into the future of AIKey FeaturesGain foundational GenAI knowledge and understand how to scale GenAI/ChatGPT in the cloudUnderstand advanced techniques for customizing LLMs for organizations via fine-tuning, prompt engineering, and responsible AIPeek into the future to explore emerging trends like multimodal AI and autonomous agentsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionGenerative artificial intelligence technologies and services, including ChatGPT, are transforming our work, life, and communication landscapes. To thrive in this new era, harnessing the full potential of these technologies is crucial. Generative AI for Cloud Solutions is a comprehensive guide to understanding and using Generative AI within cloud platforms. This book covers the basics of cloud computing and Generative AI/ChatGPT, addressing scaling strategies and security concerns. With its help, you’ll be able to apply responsible AI practices and other methods such as fine-tuning, RAG, autonomous agents, LLMOps, and Assistants APIs. As you progress, you’ll learn how to design and implement secure and scalable ChatGPT solutions on the cloud, while also gaining insights into the foundations of building conversational AI, such as chatbots. This process will help you customize your AI applications to suit your specific requirements. By the end of this book, you’ll have gained a solid understanding of the capabilities of Generative AI and cloud computing, empowering you to develop efficient and ethical AI solutions for a variety of applications and services.What you will learnGet started with the essentials of generative AI, LLMs, and ChatGPT, and understand how they function togetherUnderstand how we started applying NLP to concepts like transformersGrasp the process of fine-tuning and developing apps based on RAGExplore effective prompt engineering strategiesAcquire insights into the app development frameworks and lifecycles of LLMs, including important aspects of LLMOps, autonomous agents, and Assistants APIsDiscover how to scale and secure GenAI systems, while understanding the principles of responsible AIWho this book is forThis artificial intelligence book is for aspiring cloud architects, data analysts, cloud developers, data scientists, AI researchers, technical business leaders, and technology evangelists looking to understanding the interplay between GenAI and cloud computing. Some chapters provide a broad overview of GenAI, which are suitable for readers with basic to no prior AI experience, aspiring to harness AI's potential. Other chapters delve into technical concepts that require intermediate data and AI skills. A basic understanding of a cloud ecosystem is required to get the most out of this book.","","9781835080160","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10522549.pdf&bkn=10522548&pdfType=book","","","","","","","","8 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"Building LLM Powered Applications: Create intelligent apps and agents with large language models","V. Alto",NA,Building LLM Powered Applications: Create intelligent apps and agents with large language models,"","2024","","","","","Get hands-on with GPT 3.5, GPT 4, LangChain, Llama 2, Falcon LLM and more, to build LLM-powered sophisticated AI applicationsKey FeaturesEmbed LLMs into real-world applicationsUse LangChain to orchestrate LLMs and their components within applicationsGrasp basic and advanced techniques of prompt engineeringBook DescriptionBuilding LLM Powered Applications delves into the fundamental concepts, cutting-edge technologies, and practical applications that LLMs offer, ultimately paving the way for the emergence of large foundation models (LFMs) that extend the boundaries of AI capabilities. The book begins with an in-depth introduction to LLMs. We then explore various mainstream architectural frameworks, including both proprietary models (GPT 3.5/4) and open-source models (Falcon LLM), and analyze their unique strengths and differences. Moving ahead, with a focus on the Python-based, lightweight framework called LangChain, we guide you through the process of creating intelligent agents capable of retrieving information from unstructured data and engaging with structured data using LLMs and powerful toolkits. Furthermore, the book ventures into the realm of LFMs, which transcend language modeling to encompass various AI tasks and modalities, such as vision and audio. Whether you are a seasoned AI expert or a newcomer to the field, this book is your roadmap to unlock the full potential of LLMs and forge a new era of intelligent machines.What you will learnExplore the core components of LLM architecture, including encoder-decoder blocks and embeddingsUnderstand the unique features of LLMs like GPT-3.5/4, Llama 2, and Falcon LLMUse AI orchestrators like LangChain, with Streamlit for the frontendGet familiar with LLM components such as memory, prompts, and toolsLearn how to use non-parametric knowledge and vector databasesUnderstand the implications of LFMs for AI research and industry applicationsCustomize your LLMs with fine tuningLearn about the ethical implications of LLM-powered applicationsWho this book is for Software engineers and data scientists who want hands-on guidance for applying LLMs to build applications. The book will also appeal to technical leaders, students, and researchers interested in applied LLM topics. We don’t assume previous experience with LLM specifically. But readers should have core ML/software engineering fundamentals to understand and apply the content.","","9781835462638","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10540155.pdf&bkn=10540154&pdfType=book","","","","","","","","28 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"Augmented Reality in Physics Education: Students with Intellectual Disabilities Inquire the Structure of Matter","G. Iatraki; T. A. Mikropoulos","Department of Primary Education University of Ioannina, Greece; Department of Primary Education University of Ioannina, Greece",Presence,"1 Nov 2023","2022","31","","89","106","Immersive technologies support educational activities and provide motivating contexts which are increasingly implemented in special education settings. Augmented Reality (AR) seems to improve the level of engagement in teaching and learning processes for all students, including students with Intellectual Disabilities (ID). However, there is a lack of research that investigates AR learning environments where students with ID can be involved in inquiry-based activities and acquire academic content linked to real situations. The purpose of this study was to implement a single-subject design and evaluate the effects of an AR system on students’ performance on the microscopic level of the structure of matter and especially the phase-states of water. A functional relationship was found between students’ correct responses during probe sessions and the AR inquiry-based intervention. In addition, a social validity assessment indicated that the AR glasses helped students with ID to acquire physics concepts, as well as inquiry skills in a vivid experience. The students also reported satisfaction from using the AR glasses. Suggestions for future research include the design of AR-based interventions for other science concepts for students with ID as well as other special educational needs.","1054-7460","","10.1162/pres_a_00374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304637","","","","","","","","1 Nov 2023","","","MIT Press","MIT Press Journals"
"A Critical Look at AI-Generate Software: Coding with the New AI Tools is Both Irresistible and Dangerous","J. Vaidya; H. Asif",NA; NA,IEEE Spectrum,"10 Jul 2023","2023","60","7","34","39","In many ways, we live in the world of The Matrix. If Neo were to help us peel back the layers, we would find code all around us. Indeed, modern society runs on code: Whether you buy something online or in a store, check out a book at the library, fill a prescription, file your taxes, or drive your car, you are most probably interacting with a system that is powered by software. And the ubiquity, scale, and complexity of all that code just keeps increasing, with billions of lines of code being written every year. The programmers who hammer out that code tend to be overburdened, and their first attempt at constructing the needed software is almost always fragile or buggy- and so is their second and sometimes even the finalversion. It may fail unexpectedly, have unanticipated consequences, or be vulnerable to attack, sometimes resulting in immense damage.","1939-9340","","10.1109/MSPEC.2023.10177044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177044","","Codes;Software engineering;Complexity theory;Artificial intelligence;Software performance;Programming;Computer security","","4","","0","IEEE","10 Jul 2023","","","IEEE","IEEE Magazines"
"Generative AI with Python and TensorFlow 2: Create images, text, and music with VAEs, GANs, LSTMs, Transformer models","J. Babcock; R. Bali",NA; NA,"Generative AI with Python and TensorFlow 2: Create images, text, and music with VAEs, GANs, LSTMs, Transformer models","","2021","","","","","Fun and exciting projects to learn what artificial minds can createKey FeaturesCode examples are in TensorFlow 2, which make it easy for PyTorch users to follow alongLook inside the most famous deep generative models, from GPT to MuseGANLearn to build and adapt your own models in TensorFlow 2.xExplore exciting, cutting-edge use cases for deep generative AIBook DescriptionMachines are excelling at creative human skills such as painting, writing, and composing music. Could you be more creative than generative AI? In this book, you’ll explore the evolution of generative models, from restricted Boltzmann machines and deep belief networks to VAEs and GANs. You’ll learn how to implement models yourself in TensorFlow and get to grips with the latest research on deep neural networks. There’s been an explosion in potential use cases for generative models. You’ll look at Open AI’s news generator, deepfakes, and training deep learning agents to navigate a simulated environment. Recreate the code that’s under the hood and uncover surprising links between text, image, and music generation.What you will learnExport the code from GitHub into Google Colab to see how everything works for yourselfCompose music using LSTM models, simple GANs, and MuseGANCreate deepfakes using facial landmarks, autoencoders, and pix2pix GANLearn how attention and transformers have changed NLPBuild several text generation pipelines based on LSTMs, BERT, and GPT-2Implement paired and unpaired style transfer with networks like StyleGANDiscover emerging applications of generative AI like folding proteins and creating videos from imagesWho this book is forThis is a book for Python programmers who are keen to create and have some fun using generative models. To make the most out of this book, you should have a basic familiarity with math and statistics for machine learning.","","9781800208506","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163292.pdf&bkn=10163291&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Can Large Language Models Comprehend Code Stylometry?","A. K. Dipongkor","University of Central, Florida, USA",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","2429","2431","Code Authorship Attribution (CAA) has several applications such as copyright disputes, plagiarism detection and criminal prosecution. Existing studies mainly focused on CAA by proposing machine learning (ML) and Deep Learning (DL) based techniques. The main limitations of ML-based techniques are (a) manual feature engineering is required to train these models and (b) they are vulnerable to adversarial attack. In this study, we initially fine-tune five Large Language Models (LLMs) for CAA and evaluate their performance. Our results show that LLMs are robust and less vulnerable compared to existing techniques in CAA task.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764930","","Deep learning;Codes;Large language models;Plagiarism;Manuals;Software engineering","","","","26","","29 Nov 2024","","","IEEE","IEEE Conferences"
"RAG-Driven Generative AI: Build custom retrieval augmented generation pipelines with LlamaIndex, Deep Lake, and Pinecone","D. Rothman",NA,"RAG-Driven Generative AI: Build custom retrieval augmented generation pipelines with LlamaIndex, Deep Lake, and Pinecone","","2024","","","","","Minimize AI hallucinations and build accurate, custom generative AI pipelines with RAG using embedded vector databases and integrated human feedback Purchase of the print or Kindle book includes a free eBook in PDF formatKey FeaturesImplement RAG’s traceable outputs, linking each response to its source document to build reliable multimodal conversational agentsDeliver accurate generative AI models in pipelines integrating RAG, real-time human feedback improvements, and knowledge graphsBalance cost and performance between dynamic retrieval datasets and fine-tuning static dataBook DescriptionRAG-Driven Generative AI provides a roadmap for building effective LLM, computer vision, and generative AI systems that balance performance and costs. This book offers a detailed exploration of RAG and how to design, manage, and control multimodal AI pipelines. By connecting outputs to traceable source documents, RAG improves output accuracy and contextual relevance, offering a dynamic approach to managing large volumes of information. This AI book shows you how to build a RAG framework, providing practical knowledge on vector stores, chunking, indexing, and ranking. You’ll discover techniques to optimize your project’s performance and better understand your data, including using adaptive RAG and human feedback to refine retrieval accuracy, balancing RAG with fine-tuning, implementing dynamic RAG to enhance real-time decision-making, and visualizing complex data with knowledge graphs. You’ll be exposed to a hands-on blend of frameworks like LlamaIndex and Deep Lake, vector databases such as Pinecone and Chroma, and models from Hugging Face and OpenAI. By the end of this book, you will have acquired the skills to implement intelligent solutions, keeping you competitive in fields from production to customer service across any project.What you will learnScale RAG pipelines to handle large datasets efficientlyEmploy techniques that minimize hallucinations and ensure accurate responsesImplement indexing techniques to improve AI accuracy with traceable and transparent outputsCustomize and scale RAG-driven generative AI systems across domainsFind out how to use Deep Lake and Pinecone for efficient and fast data retrievalControl and build robust generative AI systems grounded in real-world dataCombine text and image data for richer, more informative AI responsesWho this book is forThis book is ideal for data scientists, AI engineers, machine learning engineers, and MLOps engineers. If you are a solutions architect, software developer, product manager, or project manager looking to enhance the decision-making process of building RAG applications, then you’ll find this book useful.","","9781836200901","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769312.pdf&bkn=10769311&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Taxonomy of Generative AI Applications for Risk Assessment","H. Tanaka; M. Ide; J. Yajima; S. Onodera; K. Munakata; N. Yoshioka","Fujitsu Limited, Kawasaki, Japan; Fujitsu Limited, Kawasaki, Japan; Fujitsu Limited, Kawasaki, Japan; Fujitsu Limited, Kawasaki, Japan; Fujitsu Limited, Kawasaki, Japan; Waseda University, Tokyo, Japan",2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN),"18 Jun 2024","2024","","","288","289","The superior functionality and versatility of generative AI have raised expectations for the improvement of human society and concerns about the ethical and social risks associated with the use of generative AI. Many previous studies have presented risk issues as concerns associated with the use of generative AI, but since most of these concerns are from the user's perspective, they are difficult to lead to specific countermeasures. In this study, the risk issues presented by the previous studies were broken down into more detailed elements, and risk factors and impacts were identified. In this way, we presented information that leads to countermeasure proposals for generative AI risks.CCS CONCEPTS• General and reference→Evaluation; Surveys and overviews, • Human-centered computing→HCI theory, concepts and models; • Social and professional topics→Computing / technology policy.","","979-8-4007-0591-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556211","language models;responsible innovation;technology risks;responsible AI;risk assessment","Surveys;Ethics;Generative AI;Computational modeling;Atmospheric modeling;Taxonomy;Risk management","","","","7","","18 Jun 2024","","","IEEE","IEEE Conferences"
"The Avalanche of Artificial Intelligence and its Ethical Implications on Multicultural Diverse Global Village","M. Hussain; T. R. Soomro","Karachi Section, Karachi, Sindh, Pakistan; CCSIS, Institute of Business Management, Karachi, Sindh, Pakistan",2024 Global Conference on Wireless and Optical Technologies (GCWOT),"27 Dec 2024","2024","","","1","10","The history of AI began in 1938 with the development of the Turing bombe by Alan Turing, followed by the Turing Test. Turing's work raised the question of whether machines can think, sparking extensive research. The progression of AI continued with the introduction of LISP in 1958 and Expert Systems in the 1960s. Technological advancements, such as, computing power, networking, and the rise of machine learning led to AI's rapid development. Today, AI is widely applied in various fields. This paper comprises an introduction, a literature review, a proposal for an Ethical Framework for AI development, and a conclusion. The introduction provides a concise history of AI, advance of AI, and delves into terms ethics and culture. The literature review examines different areas of applications of ethical AI and AI Ethics. Subsequently, a unique framework for ethical considerations in AI is suggested, which concludes the paper.","","979-8-3315-3427-1","10.1109/GCWOT63882.2024.10805696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10805696","Artificial Intelligence;Ethics;Global Village;Ethical Implication of AI","Wireless communication;Ethics;Bibliographies;Machine learning;History;Proposals;Artificial intelligence;Expert systems","","","","109","IEEE","27 Dec 2024","","","IEEE","IEEE Conferences"
"On the Importance of Building High-quality Training Datasets for Neural Code Search","Z. Sun; L. Li; Y. Liu; X. Du; L. Li","Monash University, Melbourne, Victoria, Australia; Tongji University, Shanghai, China; Tongji University, Shanghai, China; Monash University, Melbourne, Victoria, Australia; Monash University, Melbourne, Victoria, Australia",2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE),"20 Jun 2022","2022","","","1609","1620","The performance of neural code search is significantly influenced by the quality of the training data from which the neural models are derived. A large corpus of high-quality query and code pairs is demanded to establish a precise mapping from the natural language to the programming language. Due to the limited availability, most widely-used code search datasets are established with compromise, such as using code comments as a replacement of queries. Our empirical study on a famous code search dataset reveals that over one-third of its queries contain noises that make them deviate from natural user queries. Models trained through noisy data are faced with severe performance degradation when applied in real-world scenarios. To improve the dataset quality and make the queries of its samples semantically identical to real user queries is critical for the practical usability of neural code search. In this paper, we propose a data cleaning framework consisting of two subsequent filters: a rule-based syntactic filter and a model-based semantic filter. This is the first framework that applies semantic query cleaning to code search datasets. Experimentally, we evaluated the effectiveness of our framework on two widely-used code search models and three manually-annotated code retrieval benchmarks. Training the popular DeepCS model with the filtered dataset from our framework improves its performance by 19.2% MRR and 21.3% Answer@l, on average with the three validation benchmarks.","1558-1225","978-1-4503-9221-1","10.1145/3510003.3510160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793971","Code search;dataset;data cleaning;deep learning","Training;Codes;Computational modeling;Semantics;Training data;Benchmark testing;Data models","","19","","57","","20 Jun 2022","","","IEEE","IEEE Conferences"
"HexT5: Unified Pre-Training for Stripped Binary Code Information Inference","J. Xiong; G. Chen; K. Chen; H. Gao; S. Cheng; W. Zhang","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China",2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE),"8 Nov 2023","2023","","","774","786","Decompilation is a widely used process for reverse engineers to significantly enhance code readability by lifting assembly code to a higher-level C-like language, pseudo-code. Nevertheless, the process of compilation and stripping irreversibly discards high-level semantic information that is crucial to code comprehension, such as comments, identifier names, and types. Existing approaches typically recover only one type of information, making them suboptimal for semantic inference. In this paper, we treat pseudo-code as a special programming language, then present a unified pre-trained model, HexT5, that is trained on vast amounts of natural language comments, source identifiers, and pseudo-code using novel pseudo-code-based pre-training objectives. We fine-tune HexT5 on various downstream tasks, including code summarization, variable name recovery, function name recovery, and similarity detection. Comprehensive experiments show that HexT5 achieves state-of-the-art performance on four downstream tasks, and it demonstrates the robust effectiveness and generalizability of HexT5 for binary-related tasks.","2643-1572","979-8-3503-2996-4","10.1109/ASE56229.2023.00099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298504","Reverse Engineering;Deep Learning;Binary Diffing;Information Inference;Programming Language Model","Computer languages;Semantics;Natural languages;Binary codes;Object recognition;Data mining;Task analysis","","1","","62","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services","M. Xu; H. Du; D. Niyato; J. Kang; Z. Xiong; S. Mao; Z. Han; A. Jamalipour; D. I. Kim; X. Shen; V. C. M. Leung; H. V. Poor","School of Computer Science and Engineering, Nanyang Technological University, Nanyang, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Nanyang, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Nanyang, Singapore; School of Automation, the Key Laboratory of Intelligent Information Processing and System Integration of IoT, Ministry of Education, and the Guangdong-HongKong-Macao Joint Laboratory for Smart Discrete Manufacturing, Guangdong University of Technology, Guangzhou, China; Pillar of Information Systems Technology and Design, Singapore University of Technology and Design, Chinatown, Singapore; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; School of Electrical and Information Engineering, University of Sydney, Sydney, NSW, Australia; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, USA",IEEE Communications Surveys & Tutorials,"22 May 2024","2024","26","2","1127","1170","Artificial Intelligence-Generated Content (AIGC) is an automated method for generating, manipulating, and modifying valuable and diverse data using AI algorithms creatively. This survey paper focuses on the deployment of AIGC applications, e.g., ChatGPT and Dall-E, at mobile edge networks, namely mobile AIGC networks, that provide personalized and customized AIGC services in real time while maintaining user privacy. We begin by introducing the background and fundamentals of generative models and the lifecycle of AIGC services at mobile AIGC networks, which includes data collection, training, fine-tuning, inference, and product management. We then discuss the collaborative cloud-edge-mobile infrastructure and technologies required to support AIGC services and enable users to access AIGC at mobile edge networks. Furthermore, we explore AIGC-driven creative applications and use cases for mobile AIGC networks. Additionally, we discuss the implementation, security, and privacy challenges of deploying mobile AIGC networks. Finally, we highlight some future research directions and open issues for the full realization of mobile AIGC networks.","1553-877X","","10.1109/COMST.2024.3353265","NSFC(grant numbers:62102099,U22A2054); Guangzhou Basic Research Program(grant numbers:SL2022A04J01471); Guangdong Provincial Pearl River Talents Program(grant numbers:2021QN02S643); National Research Foundation, Singapore; Infocomm Media Development Authority under its Future Communications Research and Development Programme; DSO National Laboratories through the AI Singapore Programme (AISG)(grant numbers:Award AISG2-RP-2020-019,FCP-ASTAR-TG-2022-003); Energy Research Test-Bed and Industry Partnership Funding Initiative, Energy Grid (EG) 2.0 Programme; DesCartes and the Campus for Research Excellence and Technological Enterprise (CREATE) Programme; MOE Tier 1(grant numbers:RG87/22); NSF(grant numbers:CNS-2148382); Infocomm Media Development Authority under the Future Communications Research Development Programme (FCP); Singapore University of Technology and Design(grant numbers:SRG-ISTD-2021-165); Ministry of Education, Singapore, under its SMU-SUTD Joint Grant(grant numbers:22-SIS-SMU-048); Ministry of Science and ICT (MSIT), South Korea, through the ICT Creative Consilience Program supervised by the Institute for Information and Communications Technology Planning and Evaluation (IITP)(grant numbers:IITP-2020-0-01821); U.S. National Science Foundation(grant numbers:CNS-2128448,ECCS-2335876); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10398474","AIGC;generative AI;mobile edge networks;communication and networking;AI training and inference;Internet technology","Computational modeling;Servers;Biological system modeling;Artificial intelligence;Generative AI;Surveys;Mobile handsets","","97","","304","IEEE","12 Jan 2024","","","IEEE","IEEE Journals"
"Generative Adversarial Network for Integrated Circuits Physical Assurance Using Scanning Electron Microscopy","M. M. Al Hasan; N. Vashistha; S. Taheri; M. Tehranipoor; N. Asadizanjani","Department of Electrical and Computer Engineering, Florida Institute for Cybersecurity (FICS) Research, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, Florida Institute for Cybersecurity (FICS) Research, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, Florida Institute for Cybersecurity (FICS) Research, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, Florida Institute for Cybersecurity (FICS) Research, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, Florida Institute for Cybersecurity (FICS) Research, University of Florida, Gainesville, FL, USA",2021 IEEE International Symposium on the Physical and Failure Analysis of Integrated Circuits (IPFA),"29 Nov 2021","2021","","","1","12","Recent advancements in Artificial Intelligence (AI) and Computer Vision (CV) provide the researchers in Failure Analysis and Reliability (FAR) as well as Hardware Security (HS) with new opportunities to design novel systems to locate security failures or malicious modifications. Such developments in automation and verification modes are extremely helpful in particular for government agencies who must physically assure chips with billions of transistors within critical applications. AI based techniques such as deep learning can provide a high-performance detection and recognition of elements from Scanning Electron Microscopic (SEM) images acquired from Integrated Circuits (ICs) and understand unseen images if they are trained well. However, they require a large and diverse set of images for building their knowledge. Possessing a large number of manufactured designs as well as the high cost and execution time associated with the image acquisition process are the major bottlenecks for creating a sufficient dataset. Alternatively, conventional data augmentation techniques such as intensity change, noise injection, rotation, and translation are not always able to project the variations of images acquired by SEM with different acquisition parameters. Furthermore, augmentations like rotation, translation, and shear might generate unacceptable augmented cell structures. This paper proposes a unique approach to detect logic cells on SEM images and use the extracted samples to generate diversified synthetic logic cell images by a Generative Adversarial Network (GAN) to address insufficient data problems. We introduce an image quality assessment metric for the synthetic dataset in order to study the qualification of generated samples for recognition computations.","1946-1550","978-1-6654-3988-6","10.1109/IPFA53173.2021.9617416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9617416","Artificial intelligence;Computer Vision;Failure Analysis and Reliability;Generative Adversarial Network;Jensen-Shannon Divergence;Hardware Security;Image Analytics;and SEM Microscopy","Integrated circuits;Scanning electron microscopy;Image recognition;Microscopy;Generative adversarial networks;Real-time systems;Hardware","","7","","21","IEEE","29 Nov 2021","","","IEEE","IEEE Conferences"
"Design and Performance Analysis of an Anti-Malware System Based on Generative Adversarial Network Framework","F. B. Khan; M. H. Durad; A. Khan; F. A. Khan; M. Rizwan; A. Ali","Department of Computer and Information Sciences (DCIS), CIPMA Laboratory, Pakistan Institute of Engineering and Applied Sciences (PIEAS), Islamabad, Pakistan; Department of Computer and Information Sciences (DCIS), CIPMA Laboratory, Pakistan Institute of Engineering and Applied Sciences (PIEAS), Islamabad, Pakistan; Department of Computer and Information Sciences (DCIS), Pattern Recognition Laboratory, Pakistan Institute of Engineering and Applied Sciences (PIEAS), Nilore, Islamabad, Pakistan; PIEAS Artificial Intelligence Center (PAIC), Pakistan Institute of Engineering and Applied Sciences (PIEAS), Nilore, Islamabad, Pakistan; Department of Computer and Information Sciences (DCIS), CIPMA Laboratory, Pakistan Institute of Engineering and Applied Sciences (PIEAS), Islamabad, Pakistan; School of Computing, Ulster University, Belfast, U.K",IEEE Access,"27 Feb 2024","2024","12","","27683","27708","The cyber realm is overwhelmed with dynamic malware that promptly penetrates all defense mechanisms, operates unapprehended to the user, and covertly causes damage to sensitive data. The current generation of cyber users is being victimized by the interpolation of malware each day due to the pervasive progression of Internet connectivity. Malware is dispersed to infiltrate the security, privacy, and integrity of the system. Conventional malware detection systems do not have the potential to detect novel malware without the accessibility of their signatures, which gives rise to a high False Negative Rate (FNR). Previously, there were numerous attempts to address the issue of malware detection, but none of them effectively combined the capabilities of signature-based and machine learning-based detection engines. To address this issue, we have developed an integrated Anti-Malware System (AMS) architecture that incorporates both conventional signature-based detection and AI-based detection modules. Our approach employs a Generative Adversarial Network (GAN) based Malware Classifier Optimizer (MCOGAN) framework, which can optimize a malware classifier. This framework utilizes GANs to generate fabricated benign files that can be used to train external discriminators for optimization purposes. We describe our proposed framework and anti-malware system in detail to provide a better understanding of how a malware detection system works. We evaluate our approach using the Figshare dataset and state-of-the-art models as discriminators. Our results showcase enhanced malware detection performance, yielding a 10% performance boost, thus affirming the efficacy of our approach compared to existing models.","2169-3536","","10.1109/ACCESS.2024.3358454","Deputyship for Research and Innovation, Ministry of Education in Saudi Arabia(grant numbers:IFKSUOR3-507-2); Information Technology & Telecom (IT&T) Endowment Fund Pakistan Institute of Engineering and Applied Sciences (PIEAS), Higher Education Commission of Pakistan(grant numbers:PIN 315-7318-2EG3-116); National Center in Cyber Security, Pakistan, through the Critical Infrastructure Protection and Malware Analysis Laboratory, PIEAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10414101","Anti-malware system;generative adversarial networks;malware sandboxes;malware;unpacker;performance","Malware;Generative adversarial networks;Support vector machines;Machine learning;Generators;Terminology;Training;Performance evaluation","","1","","77","CCBYNCND","25 Jan 2024","","","IEEE","IEEE Journals"
"Adversarial Machine Learning for Blockchain Security","A. Dhamiwal; Shubham; S. Mahajan","The Northcap University, Gurgaon, Haryana, India; World College of Technology and Management, Gurgaon, Haryana, India; The Northcap University, Gurgaon, Haryana, India",2024 International Conference on Intelligent Systems for Cybersecurity (ISCS),"12 Jul 2024","2024","","","1","5","This paper explores the intersection of adversarial machine learning (AML) and blockchain security, presenting a comprehensive analysis of the challenges and solutions in leveraging AML techniques to protect blockchain networks. The decentralized and non-regular nature of blockchain technology exposes it to various adversarial threats, including 51% attacks, double spending, and smart contract vulnerabilities. To address these challenges, AML offers a range of techniques, such as adversarial training, generative adversarial networks (GANs), feature engineering, and robust ML models. These techniques enhance the resilience of blockchain systems against adversarial attacks and safeguard sensitive data. However, implementing ML models in blockchain environments presents challenges, including scalability and cross-chain compatibility. The paper discusses these challenges and proposes solutions to overcome them, emphasizing the importance of integrating AML into blockchain security frameworks. Overall, the paper highlights the potential of AML in enhancing the security of blockchain networks and mitigating the evolving threats posed by malicious actors.","","979-8-3503-7523-7","10.1109/ISCS61804.2024.10581129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581129","Adversarial Machine Learning;Blockchain Security;Artificial Intelligence;Distributed Ledger Technology","Training;Systematics;Scalability;Smart contracts;Generative adversarial networks;Adversarial machine learning;Blockchains","","","","27","IEEE","12 Jul 2024","","","IEEE","IEEE Conferences"
"CGR-GAN: CG Facial Image Regeneration for Antiforensics Based on Generative Adversarial Network","F. Peng; L. -P. Yin; L. -B. Zhang; M. Long","College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer and Communication Engineering, Changsha University of Science and Technology Hunan University, Changsha, China",IEEE Transactions on Multimedia,"23 Sep 2020","2020","22","10","2511","2525","In this paper, a Computer-generated graphics (CG) facial image regeneration scheme for anti-forensics based on generative adversarial network (CGR-GAN) is proposed. The generator of CGR-GAN utilizes a deep U-Net structure, and its discriminator utilizes some stacked convolution layers. Besides, content loss and style loss are both designed to guarantee that the regenerated CG facial images (CGR) retain both the facial profile of the original CG and the characteristics of natural image (NI). Experimental results and analysis demonstrate that the CG facial images regenerated by the proposed anti-forensics scheme can achieve better visual quality compared with those of the existing CG facial image anti-forensics and domain adaptation methods, and it can strike a good balance between visual quality and deception ability.","1941-0077","","10.1109/TMM.2019.2959443","National Natural Science Foundation of China(grant numbers:U1936115,61572182); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931757","Image anti-forensics;generative adversarial network (GAN);natural images (NI);computer-generated image;computer-generated graphics (CG) detector","Feature extraction;Detectors;Forensics;Image color analysis;Convolution;Histograms;Generative adversarial networks","","26","","39","IEEE","12 Dec 2019","","","IEEE","IEEE Journals"
"The Machine Learning Solutions Architect Handbook: Practical strategies and best practices on the ML lifecycle, system design, MLOps, and generative AI","D. Ping",NA,"The Machine Learning Solutions Architect Handbook: Practical strategies and best practices on the ML lifecycle, system design, MLOps, and generative AI","","2024","","","","","Design, build, and secure scalable machine learning (ML) systems to solve real-world business problems with Python and AWS Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesGo in-depth into the ML lifecycle, from ideation and data management to deployment and scalingApply risk management techniques in the ML lifecycle and design architectural patterns for various ML platforms and solutionsUnderstand the generative AI lifecycle, its core technologies, and implementation risksBook DescriptionDavid Ping, Head of GenAI and ML Solution Architecture for global industries at AWS, provides expert insights and practical examples to help you become a proficient ML solutions architect, linking technical architecture to business-related skills. You'll learn about ML algorithms, cloud infrastructure, system design, MLOps , and how to apply ML to solve real-world business problems. David explains the generative AI project lifecycle and examines Retrieval Augmented Generation (RAG), an effective architecture pattern for generative AI applications. You’ll also learn about open-source technologies, such as Kubernetes/Kubeflow, for building a data science environment and ML pipelines before building an enterprise ML architecture using AWS. As well as ML risk management and the different stages of AI/ML adoption, the biggest new addition to the handbook is the deep exploration of generative AI. By the end of this book , you’ll have gained a comprehensive understanding of AI/ML across all key aspects, including business use cases, data science, real-world solution architecture, risk management, and governance. You’ll possess the skills to design and construct ML solutions that effectively cater to common use cases and follow established ML architecture patterns, enabling you to excel as a true professional in the field.What you will learnApply ML methodologies to solve business problems across industriesDesign a practical enterprise ML platform architectureGain an understanding of AI risk management frameworks and techniquesBuild an end-to-end data management architecture using AWSTrain large-scale ML models and optimize model inference latencyCreate a business application using artificial intelligence services and custom modelsDive into generative AI with use cases, architecture patterns, and RAGWho this book is forThis book is for solutions architects working on ML projects, ML engineers transitioning to ML solution architect roles, and MLOps engineers. Additionally, data scientists and analysts who want to enhance their practical knowledge of ML systems engineering, as well as AI/ML product managers and risk officers who want to gain an understanding of ML solutions and AI risk management, will also find this book useful. A basic knowledge of Python, AWS, linear algebra, probability, and cloud infrastructure is required before you get started with this handbook.","","9781805124825","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10522581.pdf&bkn=10522580&pdfType=book","","","","","","","","8 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"Examining the legal challenges and loopholes of artificial intelligence by considering the upstream documents of the country","M. H. Bokaei; E. Rafati; D. H. Kani; F. R. Fard; A. D. Mehrabani; I. D. Mehrabani; N. Bodaghi; R. Baderestani; N. Nedaei","Artificial Intelligence Innovation Development Group, ICT Research Institute (ITRC), Tehran, Iran; Artificial Intelligence Innovation Development Group, ICT Research Institute (ITRC), Tehran, Iran; Artificial Intelligence Innovation Development Group, ICT Research Institute (ITRC), Tehran, Iran; Artificial Intelligence Innovation Development Group, ICT Research Institute (ITRC), Tehran, Iran; Artificial Intelligence Innovation Development Group, ICT Research Institute (ITRC), Tehran, Iran; Artificial Intelligence Innovation Development Group, ICT Research Institute (ITRC), Tehran, Iran; Artificial Intelligence Innovation Development Group, ICT Research Institute (ITRC), Tehran, Iran; Artificial Intelligence Innovation Development Group, ICT Research Institute (ITRC), Tehran, Iran; Artificial Intelligence Innovation Development Group, ICT Research Institute (ITRC), Tehran, Iran",2024 11th International Symposium on Telecommunications (IST),"21 Jan 2025","2024","","","249","256","In this article, the challenges and loopholes of laws in the world have been examined first. Issues that are generally challenging in the field of artificial intelligence and require risk management have been investigated. Based on this, some of the most important challenges and risks include the lack of algorithmic transparency - the lack of the possibility of protest - unfairness, bias and discrimination - liability for damages, etc. Also, due to the importance of artificial intelligence, the challenges related to this issue have also been investigated and studied. And finally, for the policies extracted from the previous phase of the project, legal criteria have been determined to evaluate these policies. Each of these legal criteria has been adapted to the obtained policies based on legal dimensions and aspects. The purpose of this process was to determine the legal loopholes in the above-mentioned documents based on the subject of artificial intelligence. For improvement and high accuracy, as well as for better analysis on the criteria that were adapted to the extracted policies, the data has been aggregated in an Excel file. For this purpose, the multiplicity of legal criteria in each of the legal dimensions related to each document was specified. Also, at the end, each of the documents is weighted based on the importance of connection with technology and the up-to-date ness of the subject in relation to the field of artificial intelligence. Based on the obtained analysis, the legal gaps in the documents related to artificial intelligence have been identified. Therefore, according to the rapid process of artificial intelligence, legal loopholes in the formulation of laws should be considered more sensitively.","","979-8-3503-5625-0","10.1109/IST64061.2024.10843534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10843534","artificial intelligence;challenges;gaps;legal issues;legal requirements","Training;Ethics;Technological innovation;Law;Government;Regulation;Telecommunications;Risk management;Artificial intelligence;Monitoring","","","","14","IEEE","21 Jan 2025","","","IEEE","IEEE Conferences"
"An Efficient Robust Color Watermarking Algorithm Based on DWT, DCT, BFO and Implementation","O. M. S. Hassan; A. M. Abdulazeez; A. I. Mohammed; S. O. Salih; S. H. Alih; F. Y. H. Ahmed; D. Q. Zeebaree","Computer Engineering Duhok Polytechnic University, Duhok, Iraq; Computer Engineering Duhok Polytechnic University, Duhok, Iraq; Computer Science Duhok Polytechnic University, Duhok, Iraq; Computer Engineering Duhok Polytechnic University, Duhok, Iraq; Computer Engineering Duhok Polytechnic University, Duhok, Iraq; Faculty of Information Science and Engineering (FISE), Management and Science University, Shah Alam, Selangor, Malaysia; Research Center, Duhok Polytechnic University, Duhok, Iraq",2021 IEEE 11th International Conference on System Engineering and Technology (ICSET),"30 Nov 2021","2021","","","90","95","Digital watermarking is getting more research and industry attention. Digital multimedia data allows for robust and simple data editing and modification. However, the spread of digital media presents concerns for digital content owners. It is important to note that digital data can be copied without quality or content loss. This has a considerable impact on copyright holders' ability to safeguard their intellectual property rights. The method of transmitting information by imperceptibly embedding it into digital media is digital watermarking. There are various methods in literature, such as DWT and DCT, which take full energy, are seen and integrated. New strategies and procedures for optimization are required. The present study proposes a novel design and computation technique based on the discrete wavelet and discrete cosine transforms. Watermarking techniques have been progressing to shield media content such as text, audio, video, etc. From copyright. The proposed hybrid DWT-DCT Bacterial Foraging Optimization (BFO) technique improves the efficiency of watermarking digital images by 97%. Bacterial foraging optimization (BFO) is an innovative technique for intelligent optimization. It is a widely used optimization algorithm in a wide variety of applications. However, when compared to other optimizers, the BFO performs poorly in terms of convergence. This technique uses a high-frequency image region. A variety of techniques are compared with the (NCC) Normalized Cross Correlations, (PSNR) Peak Noise Signal Ratio and IF (Image Fidelity). The highest performance is seen in DWT-DCT-BFO watermarking.","2470-640X","978-1-6654-3766-0","10.1109/ICSET53708.2021.9612547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612547","Discrete wavelet transform;Watermarking;Discrete cosine transform;Bacterial foraging optimization","Microorganisms;Image color analysis;Watermarking;Media;Systems engineering and theory;Robustness;Discrete wavelet transforms","","1","","32","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"A Deep Learning-Based Pipeline for the Generation of Synthetic Tabular Data","D. Panfilo; A. Boudewijn; S. Saccani; A. Coser; B. Svara; C. R. Chauvenet; C. A. Mami; E. Medvet","Department of Engineering and Architecture, University of Trieste, Trieste, Italy; Aindo s.r.l., Trieste, Italy; Aindo s.r.l., Trieste, Italy; Aindo s.r.l., Trieste, Italy; Department of Engineering and Architecture, University of Trieste, Trieste, Italy; Angelo Sraffa Department of Legal Studies, Bocconi University, Milan, Italy; Aindo s.r.l., Trieste, Italy; Department of Engineering and Architecture, University of Trieste, Trieste, Italy",IEEE Access,"29 Jun 2023","2023","11","","63306","63323","The recent and rapid progresses in Machine Learning (ML) tools and methodologies paved the way for an accessible market of ML services. In principle, small and medium-sized enterprises, as well as big companies, could act as providers and consumers of services, resulting in an intense exchange of ML services where a consumer may ask many providers for a service preview based on its particular business case, that is, its data. In practice, however, many potential service consumers are reluctant to release their data, when seeking for ML services, because of privacy or intellectual property concerns. As a consequence, the market of ML services is not as fluid as it could be. An alternative to providing real data when looking for an ML service consists in generating and releasing synthetic data. The synthetic data should 1) allow the service provider to preview an ML service whose performance is predictive of the one the same service will achieve on the real data; and 2) prevent the disclosure of the real data. In this paper, we propose a data synthesis technique tailored to a family of very relevant business cases: supervised and unsupervised learning on single-table datasets and relational datasets. Our technique is based on generative deep learning models and we instantiate it in three variants: standard Variational Autoencoders (VAEs),  $\beta $ -VAEs, and Introspective VAEs. We experimentally evaluate the two variants to measure the degree to which they meet the two requirements above, using several performance indexes that capture different aspects of the quality of the generated data. The results suggest that data synthesis is a practical answer to the need of decoupling ML service providers and consumers and, hence, can favor the arising of an active and accessible market of ML services.","2169-3536","","10.1109/ACCESS.2023.3288336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10158698","Synthetic data;variational auto encoders;data privacy;tabular data","Synthetic data;Data models;Data privacy;Information integrity;Information filtering;Performance analysis;Generative adversarial networks","","3","","29","CCBYNCND","21 Jun 2023","","","IEEE","IEEE Journals"
"The Complete Obsolete Guide to Generative AI","D. Clinton",Manning Publications,The Complete Obsolete Guide to Generative AI,"","2024","","","","","The last book on AI you’ll ever need. We swear! AI technology moves so fast that this book is probably already out of date! But don’t worry—The Complete Obsolete Guide to Generative AI is still an essential read for anyone who wants to make generative AI into a tool rather than a toy. It shows you how to get the best out of AI no matter what changes come in the future. You’ll be able to use common automation and scripting tools to take AI to a new level, and access raw (and powerful) GPT models via API. Inside The Complete Obsolete Guide to Generative AI you will find:  Just enough background info on AI! What an AI model is how it works Ways to create text, code, and images for your organization's needs Training AI models on your local data stores or on the internet Business intelligence and analytics uses for AI Building your own custom AI models Looking ahead to the future of generative AI  Where to get started? How about creating exciting images, video, and even audio with AI. Need more? Learn to harness AI to speed up any everyday work task, including writing boilerplate code, creating specialized documents, and analyzing your own data. Push beyond simple ChatGPT prompts! Discover ways to double your productivity and take on projects you never thought were possible! AI—and this book—are here to show you how.","","9781633436985","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10745317.pdf&bkn=10745316&pdfType=book","models;text;code;images;custom;API;ChatGPT;Copilot;OpenAI;LLMs;large language models;video;voice;prompts;business intelligence;analytics;productivity;interactive","","","","","","","6 Nov 2024","","","Manning","Manning eBooks"
"Reliability Issues of LLMs: ChatGPT a Case Study","A. Majeed; S. O. Hwang","Department of Computer Engineering, Gachon University, Seongnam, South Korea; Department of Computer Engineering, Gachon University, Seongnam, South Korea",IEEE Reliability Magazine,"4 Dec 2024","2024","1","4","36","46","ChatGPT is a groundbreaking artificial intelligence (AI) invention, and this technology will see tremendous growth per the IEEE Computer Society’s 2024 technology predictions report.1 According to the report, generative AI applications top the list, and this paradigm is predicted to experience most of the advancements in the coming years. ChatGPT, a generative AI product, has demonstrated its effectiveness in many ways (e.g., answering questions, summarizing text, generating computer code, fixing programming bugs, and generating synthetic data). Despite the many promising applications, ChatGPT cannot produce desirable results for many difficult and pragmatic tasks [1]. For example, the inaccuracy from ChatGPT answers related to the emotional text is significantly high, owing to limited amounts of data—or no available data—concerning these tasks [1]. Similarly, ChatGPT can be manipulated to generate fake content, which can be hard to distinguish from real content. There are two schools of thought in the AI community about ChatGPT technology.","2641-8819","","10.1109/MRL.2024.3420849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10602758","","Chatbots;Codes;Servers;Reliability engineering;Generative AI;Training;Artificial intelligence;Generative AI;Large language models","","","","28","IEEE","18 Jul 2024","","","IEEE","IEEE Magazines"
"Generative AI Models with Their Full Reveal*","Y. Chekhovich; A. Grabovoy; G. Gritsai","Antiplagiat Company, Moscow, Russia; Antiplagiat Company Moscow Institute of Physics and Technology, Moscow, Russia; Antiplagiat Company Moscow Institute of Physics and Technology, Moscow, Russia",2024 4th International Conference on Technology Enhanced Learning in Higher Education (TELE),"31 Jul 2024","2024","","","17","22","The paper deals with Large Language Models (LLM). We present a historical overview of the development of text generation algorithms. The aim of the paper is to show the main properties and limitations of services based on Large Language Models when their results are used in scientific and educational texts. We touch on concepts such as sampling from distribution, Recurrent Neural Network, Attention mechanism, Transformer architecture and provide guidelines for the ethical use of generated texts in scientific and academic papers. We have presented the material in a popular form, so that it is possible to understand the principles of generative services with general erudition and certain computer skills. At the same time, the key concepts are provided with references that will allow readers to delve into the area on their own if necessary.","","979-8-3503-5353-2","10.1109/TELE62556.2024.10605662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10605662","text generation;AI detection;Artificial intelligence;LLM","Ethics;Recurrent neural networks;Generative AI;Large language models;Education;Computer architecture;Search engines","","","","23","IEEE","31 Jul 2024","","","IEEE","IEEE Conferences"
"Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs","Z. Li; D. Shin","University of Sheffield, Sheffield, UK; University of Sheffield, Sheffield, UK",2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN),"18 Jun 2024","2024","","","150","159","Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.In this paper, we propose a novel method, called Mutation-based Consistency Testing (MCT), to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. MCT uses different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. MCT then uses these pairs to test the ability of LLMs to detect the inconsistencies correctly.We conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of 164 programming problems written in six programming languages (Python, C++, Java, Go, JavaScript, and Rust). The results show that the LLMs have significant variations in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language. We further explain conditions under which the LLMs result in correct answers using input characteristics (e.g., number of tokens) and investigate to what extent the test results can be improved using one-shot prompts (i.e., providing an additional example). Our MCT method and the case study results provide valuable implications for future research and development of LLM-based software engineering.CCS CONCEPTS• Software and its engineering → Software testing and debug-ging; Empirical software validation.","","979-8-4007-0591-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556138","Large Language Models;Software Engineering;Mutation Analysis","Software testing;Training;Analytical models;Codes;Sensitivity;Semantics;Benchmark testing","","1","","47","","18 Jun 2024","","","IEEE","IEEE Conferences"
"The AI plagiarism minefield: A series of legal disputes highlighted complex relationship between artificial intelligence copyright law","W. C. Edwards",NA,Engineering & Technology,"4 Apr 2024","2023","18","9","42","47","It used to be easy to deduce if something is a copy - all you needed was a simple comparison. For most traditional plagiarism software, as long as enough of the text matched an original, it was easy to determine if it had been copied and passed off as original. Naturally, people who sought to profit from plagiarism found ways to avoid detection. The phenomenon of the ‘splog’ a spam blog that rips off copy from more famous providers - appeared not long after blogs themselves became fashionable in the mid-2000s. These sites, set up primarily to capture pay-per-click payments from online advertisers, used software to rewrite the source - usually not very well. For the most part, the programs based on simple techniques from the early days of artificial intelligence (AI) simply swapped words for synonyms, sometimes turning the copy into nonsense.","1750-9637","","10.1049/et.2023.0904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10492913","","","","","","","","4 Apr 2024","","","IET","IET Magazines"
"Misconceptions, Pragmatism, and Value Tensions: Evaluating Students' Understanding and Perception of Generative AI for Education","A. Johri; A. Hingle; J. Schleiss","Information Sciences and Technology, George Mason University, Fairfax, USA; Information Sciences and Technology, George Mason University, Fairfax, USA; Artificial Intelligence Lab, Otto-von-Guericke-Universität Magdeburg, Magdeburg, Germany",2024 IEEE Frontiers in Education Conference (FIE),"26 Feb 2025","2024","","","1","9","In this research paper we examine undergraduate students' use of and perceptions of generative AI (GenAI). Although the initial hype around ChatGPT has subsided, GenAI applications continue to make inroads across learning activities. Like any other emerging technology, there is a lack of consensus around using GenAI within higher education. Students are early adopters of the technology, utilizing it in atypical ways and forming a range of perceptions and aspirations about it. To understand where and how students are using these tools and how they view them, we present findings from an open-ended survey response study with undergraduate students pursuing information technology degrees. Students were asked to describe 1) their understanding of GenAI; 2) their use of GenAI; 3) their opinions on the benefits, downsides, and ethical issues pertaining to its use in education; and 4) how they envision GenAI could ideally help them with their education. Thirty-seven students provided responses ranging in length from 20 to 300 words for each question. Responses were iteratively coded by researchers to uncover patterns in the data and then categorized thematically. Findings reveal that students' definitions of GenAI differed substantially and included many misconceptions - some highlight it as a technique, an application, or a tool, while others described it as a type of AI. There was a wide variation in the use of GenAI by students, with two common uses being writing and coding. They identified the ability of GenAI to summarize information and its potential to personalize learning as an advantage. Students identified two primary ethical concerns with using GenAI: plagiarism and dependency, which means that students do not learn independently. They also cautioned that responses from GenAI applications are often untrustworthy and need verification. Overall, they appreciated that they could do things quickly with GenAI but were cautious as using the technology was not necessarily in their best long-term as it interfered with the learning process. In terms of aspirations for GenAI, students expressed both practical advantages and idealistic and improbable visions. They said it could serve as a tutor or coach and allow them to understand the material better. We discuss the implications of the findings for student learning and instruction.","2377-634X","979-8-3503-5150-7","10.1109/FIE61694.2024.10893017","U.S. NSF(grant numbers:2319137,1954556); USDA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893017","generative artificial intelligence (GenAI);survey study;thematic analysis;undergraduate students","Surveys;Ethics;Generative AI;Plagiarism;Education;Writing;Encoding;Distance measurement;Stakeholders;Information technology","","","","22","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"Can Protective Perturbation Safeguard Personal Data from Being Exploited by Stable Diffusion?","Z. Zhao; J. Duan; K. Xu; C. Wang; R. Zhang; Z. Du; Q. Guo; X. Hu","SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences; Drexel University; Drexel University; Drexel University; SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences; SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences; SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences; SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","24398","24407","Stable Diffusion has established itself as a foundation model in generative AI artistic applications, receiving widespread research and application. Some recent fine-tuning methods have made it feasible for individuals to implant personalized concepts onto the basic Stable Diffusion model with minimal computational costs on small datasets. However, these innovations have also given rise to issues like facial privacy forgery and artistic copyright infringement. In recent studies, researchers have explored the addition of imperceptible adversarial perturbations to images to prevent potential unauthorized exploitation and infringements when personal data is used for fine-tuning Stable Dif-fusion. Although these studies have demonstrated the ability to protect images, it is essential to consider that these methods may not be entirely applicable in real-world scenarios. In this paper, we systematically evaluate the use of perturbations to protect images within a practical threat model. The results suggest that these approaches may not be sufficient to safeguard image privacy and copyright effectively. Furthermore, we introduce a purification method capable of removing protected perturbations while preserving the original image structure to the greatest extent possible. Experiments reveal that Stable Diffusion can effectively learn from purified images over all protective methods1.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.02303","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10656194","protective perturbation;diffusion models;copyright protection;adversarial examples","Threat modeling;Privacy;Technological innovation;Systematics;Perturbation methods;Semantics;Transform coding","","","","44","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"Mapping the Technological Landscape of Emerging Industry Value Chain Through a Patent Lens: An Integrated Framework With Deep Learning","G. Xu; F. Dong; J. Feng","School of Economics and Management, Beijing University of Posts and Telecommunications, Beijing, China; School of Public Policy and Management, Tsinghua University, Beijing, China; School of Economics and Management, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Transactions on Engineering Management,"3 Nov 2022","2022","69","6","3367","3378","Recent research applies patent autoclassification using machine learning to map the technological landscape of an industry value chain. However, when these methods are applied to emerging industries, the available patent sample data are small-scale and unevenly distributed, which cause overfitting and reduce the accuracy of patent classification. Therefore, this article proposes a framework to map the technological landscape of an emerging industry value chain through patent analysis with deep learning, which integrates a generative adversarial network as a data-augmentation method to overcome the problem of low-quality emerging-industry patent samples, and a deep neural network as a patent classifier. Based on this framework, this article conducts an application case of the 3-D printing industry. The evaluation results show that the integrated framework can effectively classify the patents with small-scale and unevenly distributed sample data, and depict the technological landscape of an emerging industry value chain. This article develops an efficient, reliable framework for patent autoclassification of emerging industries to overcome the lack of high-quality training samples, and it sheds light on the emerging industry value chain analysis with deep learning.","1558-0040","","10.1109/TEM.2020.3041933","National Natural Science Foundation of China(grant numbers:71872019,71974107,91646102,L1924062,L1824040,L1924058,L1824039,L1724034); Beijing Natural Science Foundation(grant numbers:9182013); Beijing Social Science Foundation(grant numbers:17GLC058); Fundamental Research Funds for the Central Universities(grant numbers:2018XKJC04); Ministry of Education in China(grant numbers:16JDGC011); CAE Advisory Project(grant numbers:2019-ZD-9); National Science and Technology Major Project(grant numbers:2016ZX04005002); Chinese Academy of Engineering's China(grant numbers:CKCEST-2020-2-5,CKCEST-2019-2-13,CKCEST-2018-1-13,CKCEST-2017-1-10,CKCEST-2015-4-2); UK-China Industry Academia Partnership Programme(grant numbers:UK-CIAPP\260); Volvo-supported Green Economy and Sustainable Development Tsinghua University(grant numbers:20153000181); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9306890","Deep neural network (DNN);emerging industry;generative adversarial network (GAN);patent auto-lassification;value chain","Patents;Generative adversarial networks;Deep learning","","5","","56","IEEE","24 Dec 2020","","","IEEE","IEEE Journals"
"Research on Multiparty Participation Collaborative Supervision Strategy of AIGC","N. Su","Center of Network Information, Shandong University of Political Science and Law, Jinan, China",2023 IEEE 13th International Conference on Electronics Information and Emergency Communication (ICEIEC),"7 Aug 2023","2023","","","268","272","The emergence and popularity of ChatGPT has sparked a new wave of “artificial intelligence” worldwide. AIGC is a true industrial revolution-level artificial intelligence technology revolution that will bring significant changes to the entire country, the world, and humanity. Like all technologies, the development of AIGC technology not only brings convenience and innovation to people, but also brings many risks and challenges. By studying the principle of AIGC technology and combining with typical cases, this paper analyzes the causes of security problems caused by the model defects and abuse propagation, focusing on the analysis of privacy Data breach, prejudice and discrimination, copyright disputes caused by data collection, processing, output and other links, as well as the impact of controlled, malicious application, abuse, misuse and other propagation on personal privacy, social stability, national security and international order. From the perspective of both security issues inherent to AIGC and those caused by its application, a multi-party collaborative regulatory strategy is proposed to safeguard the innovative development of AIGC technology, provide theoretical support for the healthy and standardized development of related industries, and help with economic and social transformation and development.","2377-844X","979-8-3503-3172-1","10.1109/ICEIEC58029.2023.10200392","Shandong University of Political Science and Law(grant numbers:2019Z03B,2021Z04B,2021JGA001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10200392","AIGC;Regulatory strategy;ChatGPT","Industries;Data privacy;Technological innovation;Humanities;Collaboration;Process control;Stability analysis","","3","","25","IEEE","7 Aug 2023","","","IEEE","IEEE Conferences"
"Pitfalls of Generic Large Language Models (GLLMs) from reliability and security perspectives","D. Dasgupta; A. Roy","Dept. of Computer Science, The University of Memphis, Memphis, TN, USA; Dept. of Computer Science, The University of Memphis, Memphis, TN, USA","2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)","16 Jan 2025","2024","","","412","419","Generic Large Language Models (GLLMs) have grown popularity in many professions with limited or no technical knowledge. Larger and larger GLLMs are continuously being released with enhanced capabilities, promoting the abilities of these Generative AI at the grassroots level in businesses. These tools excel in text, image, and video generation (assembling, summarizing, translating) when proper queries and prompts are given; moreover, various augmentation of up-to-date knowledge bases, making these more efficient in providing current events. Practitioners and marketers showcase the benefits of GLLMs by demonstrating various use cases. However, the reliability of GLLMs' responses is yet questionable in certain scenarios, particularly due to issues like hallucinations, factual inaccuracies, and inappropriate or unrelated responses. Also there remain many open questions on data collection, privacy and ethical issues that need to be addressed. This study emphasizes the reliability and security aspects of GLLMs while recognizing significant benefits in a wide variety of applications. We also provide some insides of social impacts and future directions of AI/ML applications.","","979-8-3503-8674-5","10.1109/TPS-ISA62245.2024.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10835578","Generative AI;Large Language Models (LLMs);Generative Pre-Trained Models (GPTs);Small Parameterized Data Models (SPDM)","Data privacy;Ethics;Translation;Generative AI;Large language models;Knowledge based systems;Data models;Security;Reliability;Intelligent systems","","","","56","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Anomaly-based Intrusion Detection using GAN for Industrial Control Systems","S. KK; S. Shrivastava; S. V","TIFAC-CORE in Cyber Security, Amrita School of Engineering Coimbatore, INDIA; TIFAC-CORE in Cyber Security, Amrita School of Engineering Coimbatore, INDIA; TIFAC-CORE in Cyber Security, Amrita School of Engineering Coimbatore, INDIA","2022 10th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)","8 Dec 2022","2022","","","1","6","In recent years, cyber-attacks on modern industrial control systems (ICS) have become more common and it acts as a victim to various kind of attackers. The percentage of attacked ICS computers in the world in 2021 is 39.6%. To identify the anomaly in a large database system is a challenging task. Deep-learning model provides better solutions for handling the huge dataset with good accuracy. On the other hand, real time datasets are highly imbalanced with their sample proportions. In this research, GAN based model, a supervised learning method which generates new fake samples that is similar to real samples has been proposed. GAN based adversarial training would address the class imbalance problem in real time datasets. Adversarial samples are combined with legitimate samples and shuffled via proper proportion and given as input to the classifiers. The generated data samples along with the original ones are classified using various machine learning classifiers and their performances have been evaluated. Gradient boosting was found to classify with 98% accuracy when compared to other","","978-1-6654-7433-7","10.1109/ICRITO56286.2022.9964997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964997","Adversarial training;Anomaly;Classification;Generative Adversarial Network;Intrusion detection system","Integrated circuits;Training;Industrial control;Intrusion detection;Generative adversarial networks;Market research;Real-time systems","","6","","19","IEEE","8 Dec 2022","","","IEEE","IEEE Conferences"
"Lite Localization Network and DUE-Based Watermarking for Color Image Copyright Protection","L. Zhu; Y. Fang; Y. Zhao; Y. Peng; J. Wang; J. Ni","Mechanical and Electronic Engineering Department, Jingdezhen Ceramic University, Jingdezhen, China; Mechanical and Electronic Engineering Department, Jingdezhen Ceramic University, Jingdezhen, China; Mechanical and Electronic Engineering Department, Jingdezhen Ceramic University, Jingdezhen, China; Mechanical and Electronic Engineering Department, Jingdezhen Ceramic University, Jingdezhen, China; Mechanical and Electronic Engineering Department, Jingdezhen Ceramic University, Jingdezhen, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China",IEEE Transactions on Circuits and Systems for Video Technology,"30 Oct 2024","2024","34","10","9311","9325","Deep learning-based watermarking frameworks have received extensive research attention in recent years. The main structure of this framework consists of an encoder, a noise layer and a decoder (Encoder-NoiseLayer-Decoder). However, such a framework has the major drawback that it requires visible markers to locate a watermarked image, which compromises the imperceptibility of watermarking. To address this restriction, a novel Lite localization network based on Lite-HRNet is proposed. In order to generate high-quality watermarked image, we designed the Double U-Net Encoder (DUE), which can better hide the watermarking information in image pixels that are invisible to the human eye. Meanwhile, to improve robustness, two bicubic interpolation operations are added to the noise layer to increase the type of distortion. In addition, to further enhance the performance of the watermarking algorithm, the novel WGAN-GP loss function based on discriminator is designed to guide the training of the model. Numerous experiments demonstrate the superior performance of our proposed scheme in terms of localization function, visual quality, and robustness. The proposed scheme shows better results compared to state-of-the-art algorithms.","1558-2205","","10.1109/TCSVT.2024.3395304","National Natural Science Foundation of China(grant numbers:62062044,61762054,U22A2030,U23B2022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10510489","Digital watermarking;double U-Net encoder (DUE);lite localization network;WGAN-GP;DNN","Watermarking;Distortion;Noise measurement;Location awareness;Decoding;Robustness;Transforms;Watermarking;Encoding;Generative adversarial networks","","1","","41","IEEE","30 Apr 2024","","","IEEE","IEEE Journals"
"How to Regulate Large Language Models for Responsible AI","J. Berengueres","College of Information Technology, UAE University, Al Ain, UAE",IEEE Transactions on Technology and Society,"9 Aug 2024","2024","5","2","191","197","Large Language Models (LLMs) are predictive probabilistic models capable of passing several professional tests at a level comparable to humans. However, these capabilities come with ethical concerns. Ethical oversights in several LLM-based products include: (i) a lack of content or source attribution, and (ii) a lack of transparency in what was used to train the model. This paper identifies four touchpoints where ethical safeguards can be applied to realize a more responsible AI in LLMs. The key finding is that applying safeguards before the training occurs aligns with established engineering practices of addressing issues at the source. However, this approach is currently shunned. Finally, historical parallels are drawn with the U.S. automobile industry, which initially resisted safety regulations but later embraced them once consumer attitudes evolved.","2637-6415","","10.1109/TTS.2024.3403681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10536000","Artificial intelligence;ethical computing;codes of ethics;algorithmic bias;AI governance;accountability in AI;responsible AI","Ethics;Codes;Artificial intelligence;Benchmark testing;Regulation;General Data Protection Regulation;Large language models;Predictive models;Probabilistic logic;Training;Data integrity;Information integrity;Data integrity","","","","88","CCBY","21 May 2024","","","IEEE","IEEE Journals"
"Empowering the Metaverse with Generative AI: Survey and Future Directions","H. X. Qin; P. Hui","Computational Media and Arts, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Computational Media and Arts, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China",2023 IEEE 43rd International Conference on Distributed Computing Systems Workshops (ICDCSW),"2 Nov 2023","2023","","","85","90","This paper aims to motivate the development of the metaverse by highlighting the potential of artificial-intelligence-generated content (AIGC) for the metaverse. We present the first literature review on AIGC in the metaverse with state-of-the-art research classified into 5 key application areas (avatars and Non-player Characters (NPCs), content creation, virtual world generation, automatic digital twin, and personalization). Having noticed a notable gap in research through our review, we propose ways in which state-of-the-art generative AI can be applied to the metaverse. Additionally, we offer a roadmap for future research with related ethical implications.","2332-5666","979-8-3503-2812-7","10.1109/ICDCSW60045.2023.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10302997","metaverse;generative AI;co-creative AI;virtual reality;augmented reality;AI-generated content (AIGC)","Surveys;Ethics;Codes;Metaverse;Conferences;Distributed databases;Generators","","14","","38","IEEE","2 Nov 2023","","","IEEE","IEEE Conferences"
"Employing Deep Neural Networks for Real-Time Anomaly Detection and Mitigation in IoT-Based Smart Grid Cybersecurity Systems","A. Quraishi; M. A. Rusho; A. Prasad; I. Keshta; R. Rivera; M. W. Bhatt","M.D. Research, Intervention Treatment Institute, Houston, Texas, US; Department of Lockheed Martin Engineering Management, University of Colorado, Boulder, Boulder, Colorado; IIT Kharagpur (Rajiv Gandhi School Of Intellectual Property Law), Gurugram, India; Computer Science and Information Systems Department, College of Applied Sciences, AlMaarefa University, Riyadh, Saudi Arabia; Department of Informatics and Computer Science, Escuela Politécnica Nacional, Quito, Ecuador; Model Institute of Engineering and Technology, Jammu, J&K, India",2024 Third International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE),"12 Jun 2024","2024","","","1","6","This research introduces a novel anomaly detection framework for IoT -based Smart Grid Cybersecurity Systems. Leveraging autoencoders, LSTM networks, GANs, SOMs, and transfer learning, our approach achieves superior precision, recall, and execution time compared to existing methods. Visualizations and an ablation study further validate the method's efficiency, emphasizing the critical roles of attention mechanisms and transfer learning. This comprehensive solution addresses the dynamic challenges of smart grid cybersecurity, offering a versatile and adaptive anomaly detection mechanism for real-world applications. This indicates the real-time efficacy of our anomaly detection method. Through our study of ablation and all aspects of computing, we discovered that attention processes and transfer learning facilitate faster problem solving in a dynamic smart grid. Our method is distinct and adaptable enough to address every problem arising from the discovery of anomalies in IoT-driven Smart Grid Cybersecurity Systems.","","979-8-3503-1860-9","10.1109/ICDCECE60827.2024.10548160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548160","Anomaly Detection;Autoencoders;Cybersecurity;Flowchart;Generative Adversarial Networks (GANs);LSTM Networks;Self-Organizing Maps (SOMs);Transfer Learning;Training Algorithm;IoT-based Smart Grid","Visualization;Transfer learning;Circuits;Real-time systems;Smart grids;Problem-solving;Computer security","","","","21","IEEE","12 Jun 2024","","","IEEE","IEEE Conferences"
"Machine Learning and Generative AI for Marketing: Take your data-driven marketing strategies to the next level using Python","Y. H. Hwang; N. C. Burtch",NA; NA,Machine Learning and Generative AI for Marketing: Take your data-driven marketing strategies to the next level using Python,"","2024","","","","","Start transforming your data-driven marketing strategies and increasing customer engagement. Learn how to create compelling marketing content using advanced gen AI techniques and stay in touch with the future AI ML landscape. Purchase of the print or Kindle book includes a free eBook in PDF formatKey FeaturesEnhance customer engagement and personalization through predictive analytics and advanced segmentation techniquesCombine Python programming with the latest advancements in generative AI to create marketing content and address real-world marketing challengesUnderstand cutting-edge AI concepts and their responsible use in marketingBook DescriptionIn the dynamic world of marketing, the integration of artificial intelligence (AI) and machine learning (ML) is no longer just an advantage—it's a necessity. Moreover, the rise of generative AI (GenAI) helps with the creation of highly personalized, engaging content that resonates with the target audience. This book provides a comprehensive toolkit for harnessing the power of GenAI to craft marketing strategies that not only predict customer behaviors but also captivate and convert, leading to improved cost per acquisition, boosted conversion rates, and increased net sales. Starting with the basics of Python for data analysis and progressing to sophisticated ML and GenAI models, this book is your comprehensive guide to understanding and applying AI to enhance marketing strategies. Through engaging content & hands-on examples, you'll learn how to harness the capabilities of AI to unlock deep insights into customer behaviors, craft personalized marketing messages, and drive significant business growth. Additionally, you'll explore the ethical implications of AI, ensuring that your marketing strategies are not only effective but also responsible and compliant with current standards By the conclusion of this book, you'll be equipped to design, launch, and manage marketing campaigns that are not only successful but also cutting-edge.What you will learnMaster key marketing KPIs with advanced computational techniquesUse explanatory data analysis to drive marketing decisionsLeverage ML models to predict customer behaviors, engagement levels, and customer lifetime valueEnhance customer segmentation with ML and develop highly personalized marketing campaignsDesign and execute effective A/B tests to optimize your marketing decisionsApply natural language processing (NLP) to analyze customer feedback and sentimentsIntegrate ethical AI practices to maintain privacy in data-driven marketing strategiesWho this book is forThis book targets a diverse group of professionals: Data scientists and analysts in the marketing domain looking to apply advanced AI ML techniques to solve real-world marketing challenges Machine learning engineers and software developers aiming to build or integrate AI-driven tools and applications for marketing purposes Marketing professionals, business leaders, and entrepreneurs who must understand the impact of AI on marketing Reader are presumed to have a foundational proficiency in Python and a basic to intermediate grasp of ML principles and data science methodologies","","9781835889411","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769306.pdf&bkn=10769305&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"MODA: Model Ownership Deprivation Attack in Asynchronous Federated Learning","X. Zhang; S. Lin; C. Chen; X. Chen","State Key Laboratory of Integrated Service Networks (ISN), Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks (ISN), Xidian University, Xi’an, China; School of Accounting, Information Systems and Supply Chain, RMIT University, Melbourne, VIC, Australia; State Key Laboratory of Integrated Service Networks (ISN), Xidian University, Xi’an, China",IEEE Transactions on Dependable and Secure Computing,"11 Jul 2024","2024","21","4","4220","4235","Training a deep learning model from scratch requires a great deal of available labeled data, computation resources, and expert knowledge. Thus, the time-consuming and complicated learning procedure catapulted the trained model to valuable intellectual property (IP), spurring interest from attackers in model copyright infringement and stealing. Recently, a new defense approach leverages watermarking techniques to inject watermarks into the training procedure and verify model ownership when necessary. To our best knowledge, there is no research work on model ownership stealing attacks in federated learning, and the existing defense or mitigation methods can not be directly used for federated learning scenarios. In this article, we introduce watermarking neural networks in asynchronous federated learning and propose a novel model privacy attack, dubbed model ownership deprivation attack (MODA). MODA is launched by an inside adversarial participant, targeting occupying and depriving the remaining participants’ (victims) copyright to achieve his maximum profit. The extensive experimental results on five benchmark datasets (MNIST, Fashion-MNIST, GTSRB, SVHN, CIFAR10) show that MODA is highly effective in a two-participant learning scenario with a minor impact on model's performance. When extending MODA into multiple participants scenario, MODA still maintains high attack success rate and classification accuracy. Compared to the state-of-the-art works, MODA has a higher attack success rate than the black-box solution and comparable efficacy with the approach in the white-box scenario.","1941-0018","","10.1109/TDSC.2023.3348204","National Natural Science Foundation of China(grant numbers:62102300,61960206014,62121001); China 111 Project(grant numbers:B16037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10376357","Asynchronous federated learning;DNN watermarking;ownership verification;privacy attack","Watermarking;Training;Computational modeling;Federated learning;Servers;Data models;Mathematical models","","5","","53","IEEE","29 Dec 2023","","","IEEE","IEEE Journals"
"A Comprehensive Review on Generative AI for Education","U. Mittal; S. Sai; V. Chamola; D. Sangwan","Department of Electrical and Electronics Engineering, Birla Institute of Technology and Science at Pilani (BITS), Pilani, Rajasthan, India; Department of Electrical and Electronics Engineering, Birla Institute of Technology and Science at Pilani (BITS), Pilani, Rajasthan, India; Department of Electrical and Electronics Engineering, Birla Institute of Technology and Science at Pilani (BITS), Pilani, Rajasthan, India; Department of Humanities and Social Science, Birla Institute of Technology and Science at Pilani (BITS), Pilani, Rajasthan, India",IEEE Access,"7 Oct 2024","2024","12","","142733","142759","Artificial Intelligence (AI) has immense potential for personalized learning experiences, content generation, and vivid educational support. This paper delves into generative AI (GAI) and its potential applications within GAI, specifically mentioning generative adversarial networks (GANs). The article delves into the transformative impact of GAI in education, underscoring its expertise in creating diverse instructional materials, from texts and images to videos. Adaptive learning, one of the chief abilities of GAI, has been highlighted, emphasizing its capability to select content customized to individual student profiles, learning habits, and preferences. The paper further explores the fusion of GAI with innovative education systems, highlighting how these models can mimic conversational interfaces, promoting an engaging, customized learning journey. The exploration doesn’t stop at the benefits; it delves into challenges like ensuring data privacy, mitigating biases, and ensuring accountability in AI-driven educational systems. The conclusion contemplates the potential limitations and assurances of embedding GAI within educational setups. An appeal has been made for more profound research and enhancement of AI’s educational function. The intersection of pedagogical insights and effective human-AI collaboration is pivotal in this journey. This paper serves as a compass, guiding educators, researchers, and policymakers toward harnessing GAI’s potential to sculpt enriched, immersive educational landscapes.","2169-3536","","10.1109/ACCESS.2024.3468368","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10695056","GAI;education;applications;case studies;challenges;metaverse","Artificial intelligence;Education;Computational modeling;Solid modeling;Videos;Three-dimensional displays;Metaverse;Learning systems;Generative AI","","2","","157","CCBYNCND","26 Sep 2024","","","IEEE","IEEE Journals"
"Information Forensics and Security: A quarter-century-long journey","M. Barni; P. Campisi; E. J. Delp; G. Doërr; J. Fridrich; N. Memon; F. Pérez-González; A. Rocha; L. Verdoliva; M. Wu","Department of Information Engineering and Mathematics, University of Siena, Siena, Italy; Department of Industrial, Electronics, and Mechanical Engineering, Roma Tre University, Rome, Italy; Purdue University, West Lafayette, IN, USA; Synamedia, Rennes, France; State University of New York, Binghamton, Binghamton, NY, USA; Tandon School of Engineering, New York University, New York City, NY, USA; University of Vigo, Vigo, Spain; Institute of Computing, University of Campinas, Campinas, Brazil; Department of Electrical Engineering and Information Technology, University Federico II of Naples, Naples, Italy; University of Maryland, College Park, College Park, MD, USA",IEEE Signal Processing Magazine,"20 Jul 2023","2023","40","5","67","79","Information forensics and security (IFS) is an active R&D area whose goal is to ensure that people use devices, data, and intellectual properties for authorized purposes and to facilitate the gathering of solid evidence to hold perpetrators accountable. For over a quarter century, since the 1990s, the IFS research area has grown tremendously to address the societal needs of the digital information era. The IEEE Signal Processing Society (SPS) has emerged as an important hub and leader in this area, and this article celebrates some landmark technical contributions. In particular, we highlight the major technological advances by the research community in some selected focus areas in the field during the past 25 years and present future trends.","1558-0792","","10.1109/MSP.2023.3275319","Spanish Ministry for Science and Innovation(grant numbers:MCIN/AEI/10.13039/501100011033); European Regional Development(grant numbers:ED431C 2021/47); National Science Foundation(grant numbers:2028119); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10188887","","Privacy;Forensics;Surveillance;Machine learning;Intellectual property;Signal processing;Information processing;Information security;History","","8","","57","IEEE","20 Jul 2023","","","IEEE","IEEE Magazines"
"Generative AI: Threatening Established Human Rights Instruments at Scale","M. Briggs; M. Cross","The Alan Turing Institute, Queen Mary University of London, London, UK; The Alan Turing Institute, London, UK",2024 4th International Conference on Applied Artificial Intelligence (ICAPAI),"31 May 2024","2024","","","1","8","We assess the impacts of generative AI technologies within the context of rights and freedoms enshrined in two codified international covenants, the International Covenant on Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social, and Cultural Rights (ICESCR). Additionally, the UN Guiding Principles on Business and Human Rights (UNGPs) are explored in tandem to further motivate the requirement for businesses and States to carry out human rights due diligence processes. By providing specific use cases and examples of how generative AI’s cross-sectoral risks threaten established human rights and freedoms, such as the freedom of opinion and expression and the right to privacy, we argue that proper governance and accountability mechanisms for generative AI should be based in codified international human rights instruments and support frameworks such as the UNGPs. This paper is intended to serve as a catalogue of concrete evidence to support the enforcement and uptake of human rights due diligence processes and foster conversations at the policy level.","","979-8-3503-4976-4","10.1109/ICAPAI61893.2024.10541170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10541170","Generative AI;AI Ethics;Human Rights;Human Rights Due Diligence;UN Guiding Principles on Business and Human Rights","Economics;Privacy;Ethics;Generative AI;Instruments;Oral communication;Cultural differences","","","","50","IEEE","31 May 2024","","","IEEE","IEEE Conferences"
"Build a Website with ChatGPT: No coding experience necessary","P. McFedries",Manning Publications,Build a Website with ChatGPT: No coding experience necessary,"","2024","","","","","Create a portfolio of cool and creative websites—all without having to write your own code. Build a Website with ChatGPT teaches you zero-coding web development utilizing powerful generative AI tools like ChatGPT. If you can open a web browser, you’re ready to start building—absolutely no coding experience required. Inside Build a Website with ChatGPT you’ll learn the important skills of AI-assisted web programming, such as:  Crafting effective prompts to generate HTML, CSS, and JavaScript Converting text into images with DALL-E integration Building navigation bars, image galleries, and contact forms Deploying fully functional sites to the web for free Customizing the generated code for unique sites  Inside Build a Website with ChatGPT you’ll learn the high-level coding concepts that let you check and perfect AI output, prompting skills that deliver the exact code you need, and how to properly deploy your site to the web—for free! Annotated code samples and advice on code customization give you the perfect balance of understanding and convenience. Plus, you’ll get access to a tried-and-tested repository of prompts and working code.","","9781633436961","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10745299.pdf&bkn=10745298&pdfType=book","generative;AI;zero-coding;HTML;CSS;JavaScript;DALL-E;free;prompts;fully functional;fonts;colors;headings;structure;image galleries;contact forms;navigation bars;image conversion;deployment","","","","","","","6 Nov 2024","","","Manning","Manning eBooks"
"LLMIF: Augmented Large Language Model for Fuzzing IoT Devices","J. Wang; L. Yu; X. Luo",The Hong Kong Polytechnic University; Nanjing University of Posts and Telecommunications; The Hong Kong Polytechnic University,2024 IEEE Symposium on Security and Privacy (SP),"5 Sep 2024","2024","","","881","896","Despite the efficacy of fuzzing in verifying the implementation correctness of network protocols, existing IoT protocol fuzzing approaches grapple with several limitations, including obfuscated message formats, unresolved message dependencies, and a lack of evaluations on the testing cases. These limitations significantly curtail the capabilities of IoT fuzzers in vulnerability identification. In this work, we show that the protocol specification contains fruitful descriptions of protocol messages, which can be used to overcome the above limitations and guide IoT protocol fuzzing. To automate the specification analysis, we augment the large language model with the specification contents, and drive it to perform two tasks (i.e., protocol information extraction, and device response reasoning). We further design and implement a fuzzing algorithm, LLMIF, which incorporates the LLM into IoT fuzzing. Finally, we select Zigbee as the target protocol and initiate comprehensive evaluations. The evaluation result shows that LLMIF successfully addressed the above limitations. Compared with the existing Zigbee fuzzers, it increases the protocol message coverage and code coverage by 55.2% and 53.9%, respectively. Besides the enhanced coverage, LLMIF unearthed 11 vulnerabilities on real-world Zigbee devices, which include eight previously unknown vulnerabilities. Seven of them are not covered by the existing Zigbee fuzzers.","2375-1207","979-8-3503-3130-1","10.1109/SP54263.2024.00211","National Natural Science Foundation of China; Nanjing University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646659","fuzzing;IoT device;large language model","Privacy;Protocols;Codes;Large language models;Zigbee;Fuzzing;Internet of Things","","4","","52","IEEE","5 Sep 2024","","","IEEE","IEEE Conferences"
"From ChatGPT to Sora: Analyzing Public Opinions and Attitudes on Generative Artificial Intelligence in Social Media","W. Feng; Y. Li; C. Ma; L. Yu","Zhijiang College, Zhejiang University of Technology, Shaoxing, China; Zhijiang College, Zhejiang University of Technology, Shaoxing, China; Department of Railroad Construction Services, China Railway Materials Tianjin Company Ltd., Tianjin, China; Center for the Study of Language and Cognition, Zhejiang University, Hangzhou, China",IEEE Access,"24 Jan 2025","2025","13","","14485","14498","This study examines public opinions, emotional tendencies, and psychological linguistic characteristics associated with the launch of OpenAI’s ChatGPT and the advanced video generation model, Sora, by analyzing discussions on the Chinese social media platform Weibo. A total of 24,727 valid user-generated texts (1,762,296 words) were collected and analyzed using Python and its associated APIs. Word co-occurrence network analysis, topic modeling based on Latent Dirichlet Allocation (LDA), and emotional characteristics based on the DLUT Emotion Ontology and psycholinguistic analyses based on the Linguistic Inquiry and Word Count (LIWC) dictionary were employed to explore public views on these generative AI technologies. The findings reveal a shift in public focus over time, from initial excitement about technological advancements to growing interest in commercialization, labor, education, ethics, and global competition. The public’s emotional responses to AI were a mix of excitement and apprehension. The study identifies seven distinct emotional types, providing a nuanced understanding of public psychological reactions, which contrasts with previous binary classifications. This research contributes valuable insights for policymakers, businesses, and researchers, highlighting the public’s evolving acceptance of generative AI technologies.","2169-3536","","10.1109/ACCESS.2025.3530683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10843672","Generative artificial intelligence;ChatGPT;sora;topic modeling;sentiment analysis;psycholinguistics","Chatbots;Artificial intelligence;Generative AI;Social networking (online);Blogs;Psychology;Sentiment analysis;Analytical models;Text mining;Ethics","","","","60","CCBY","16 Jan 2025","","","IEEE","IEEE Journals"
"Generative AI Application Integration Patterns: Integrate large language models into your applications","J. P. Bustos; L. L. Soria; D. A. Arsanjani",NA; NA; NA,Generative AI Application Integration Patterns: Integrate large language models into your applications,"","2024","","","","","Unleash the transformative potential of GenAI with this comprehensive guide that serves as an indispensable roadmap for integrating large language models into real-world applications. Gain invaluable insights into identifying compelling use cases, leveraging state-of-the-art models effectively, deploying these models into your applications at scale, and navigating ethical considerations.Key FeaturesGet familiar with the most important tools and concepts used in real scenarios to design GenAI appsInteract with GenAI models to tailor model behavior to minimize hallucinationsGet acquainted with a variety of strategies and an easy to follow 4 step frameworks for integrating GenAI into applicationsBook DescriptionExplore the transformative potential of GenAI in the application development lifecycle. Through concrete examples, you will go through the process of ideation and integration, understanding the tradeoffs and the decision points when integrating GenAI. With recent advances in models like Google Gemini, Anthropic Claude, DALL-E and GPT-4o, this timely resource will help you harness these technologies through proven design patterns. We then delve into the practical applications of GenAI, identifying common use cases and applying design patterns to address real-world challenges. From summarization and metadata extraction to intent classification and question answering, each chapter offers practical examples and blueprints for leveraging GenAI across diverse domains and tasks. You will learn how to fine-tune models for specific applications, progressing from basic prompting to sophisticated strategies such as retrieval augmented generation (RAG) and chain of thought. Additionally, we provide end-to-end guidance on operationalizing models, including data prep, training, deployment, and monitoring. We also focus on responsible and ethical development techniques for transparency, auditing, and governance as crucial design patterns.What you will learnConcepts of GenAI: pre-training, fine-tuning, prompt engineering, and RAGFramework for integrating AI: entry points, prompt pre-processing, inference, post-processing, and presentationPatterns for batch and real-time integrationCode samples for metadata extraction, summarization, intent classification, question-answering with RAG, and moreEthical use: bias mitigation, data privacy, and monitoringDeployment and hosting options for GenAI modelsWho this book is forThis book is not an introduction to AI/ML or Python. It offers practical guides for designing, building, and deploying GenAI applications in production. While all readers are welcome, those who benefit most include: Developer engineers with foundational tech knowledge Software architects seeking best practices and design patterns Professionals using ML for data science, research, etc., who want a deeper understanding of Generative AI Technical product managers with a software development background This concise focus ensures practical, actionable insights for experienced professionals","","9781835887615","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769231.pdf&bkn=10769230&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Image Style Transfer Using Deep Learning Methods","S. Ren; Y. Sheng","Department of Computer and Electronic Information, Nanjing Normal University, Nanjing, China; Department of New Energy, North China Electric Power University, Beijing, China","2022 IEEE International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)","6 Apr 2022","2022","","","1190","1195","Image style transfer is an increasingly popular technology that can learn the style of an existing picture through neural network algorithms and apply this style to another picture. It is widely used in the field of art, such as oil painting, cartoon animation production, image season conversion and text style conversion. Meanwhile, deep learning methods are attracting more and more attention both in research and applications in various areas. In this paper, we give an overview on current research progress and results of image style transfer using deep learning methods. The deep learning methods are categorized into Convolutional Neural Networks (CNN) and Generative Adversarial Networks (GAN). As for CNN methods, we mainly talk about models based on VGG; and in terms of GAN methods, conditional GAN, Cycle GAN, and cartoon-GAN methods are contained. Finally, we summarized the shortcomings of the current results and the future study direction.","","978-1-6654-1606-1","10.1109/EEBDA53927.2022.9745023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745023","image style transfer;deep learning;convolutional neural network;generative adversarial network","Deep learning;Training;Image quality;PSNR;Image coding;Neural networks;Production","","2","","31","IEEE","6 Apr 2022","","","IEEE","IEEE Conferences"
"Building AI Applications with Microsoft Semantic Kernel: Easily integrate generative AI capabilities and copilot experiences into your applications","L. A. Meyer",NA,Building AI Applications with Microsoft Semantic Kernel: Easily integrate generative AI capabilities and copilot experiences into your applications,"","2024","","","","","Unlock the power of GenAI by effortlessly linking your C# and Python apps with cutting-edge models, orchestrating diverse AI services with finesse, and crafting bespoke applications through immersive, real-world examplesKey FeaturesLink your C# and Python applications with the latest AI models from OpenAICombine and orchestrate different AI services such as text and image generatorsCreate your own AI apps with real-world use case examples that show you how to use basic generative AI, create images, process documents, use a vector databasePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionIn the fast-paced world of AI, developers are constantly seeking efficient ways to integrate AI capabilities into their apps. Microsoft Semantic Kernel simplifies this process by using the GenAI features from Microsoft and OpenAI. Written by Lucas A. Meyer, a Principal Research Scientist in Microsoft’s AI for Good Lab, this book helps you get hands on with Semantic Kernel. It begins by introducing you to different generative AI services such as GPT-3.5 and GPT-4, demonstrating their integration with Semantic Kernel. You’ll then learn to craft prompt templates for reuse across various AI services and variables. Next, you’ll learn how to add functionality to Semantic Kernel by creating your own plugins. The second part of the book shows you how to combine multiple plugins to execute complex actions, and how to let Semantic Kernel use its own AI to solve complex problems by calling plugins, including the ones made by you. The book concludes by teaching you how to use vector databases to expand the memory of your AI services and how to help AI remember the context of earlier requests. You’ll also be guided through several real-world examples of applications, such as RAG and custom GPT agents. By the end of this book, you'll have gained the knowledge you need to start using Semantic Kernel to add AI capabilities to your applications.What you will learnWrite reusable AI prompts and connect to different AI providersCreate new plugins that extend the capabilities of AI servicesUnderstand how to combine multiple plugins to execute complex actionsOrchestrate multiple AI services to accomplish a taskLeverage the powerful planner to automatically create appropriate AI callsUse vector databases as additional memory for your AI tasksDeploy your application to ChatGPT, making it available to hundreds of millions of usersWho this book is forThis book is for beginner-level to experienced .NET or Python software developers who want to quickly incorporate the latest AI technologies into their applications, without having to learn the details of every new AI service. Product managers with some development experience will find this book helpful while creating proof-of-concept applications. This book requires working knowledge of programming basics.","","9781835469590","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769216.pdf&bkn=10769215&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Python Natural Language Processing Cookbook: Over 60 recipes for building powerful NLP solutions using Python and LLM libraries","Z. Antić; S. Chakravarty",NA; NA,Python Natural Language Processing Cookbook: Over 60 recipes for building powerful NLP solutions using Python and LLM libraries,"","2024","","","","","Updated to include three new chapters on transformers, natural language understanding (NLU) with explainable AI, and dabbling with popular LLMs from Hugging Face and OpenAIKey FeaturesLeverage ready-to-use recipes with the latest LLMs, including Mistral, Llama, and OpenAI modelsUse LLM-powered agents for custom tasks and real-world interactionsGain practical, in-depth knowledge of transformers and their role in implementing various NLP tasks with open-source and advanced LLMsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionHarness the power of Natural Language Processing to overcome real-world text analysis challenges with this recipe-based roadmap written by two seasoned NLP experts with vast experience transforming various industries with their NLP prowess. You’ll be able to make the most of the latest NLP advancements, including large language models (LLMs), and leverage their capabilities through Hugging Face transformers. Through a series of hands-on recipes, you’ll master essential techniques such as extracting entities and visualizing text data. The authors will expertly guide you through building pipelines for sentiment analysis, topic modeling, and question-answering using popular libraries like spaCy, Gensim, and NLTK. You’ll also learn to implement RAG pipelines to draw out precise answers from a text corpus using LLMs. This second edition expands your skillset with new chapters on cutting-edge LLMs like GPT-4, Natural Language Understanding (NLU), and Explainable AI (XAI)—fostering trust and in your NLP models. By the end of this book, you'll be equipped with the skills to apply advanced text processing techniques, use pre-trained transformer models, build custom NLP pipelines to extract valuable insights from text data to drive informed decision-making.What you will learnUnderstand fundamental NLP concepts along with their applications using examples in PythonClassify text quickly and accurately with rule-based and supervised methodsTrain NER models and perform sentiment analysis to identify entities and emotions in textExplore topic modeling and text visualization to reveal themes and relationships within textLeverage Hugging Face and OpenAI LLMs to perform advanced NLP tasksUse question-answering techniques to handle both open and closed domainsApply XAI techniques to better understand your model predictionsWho this book is forThis updated edition of the Python Natural Language Processing Cookbook is for data scientists, machine learning engineers, and developers with a background in Python. Whether you’re looking to learn NLP techniques, extract valuable insights from textual data, or create foundational applications, this book will equip you with basic to intermediate skills. No prior NLP knowledge is necessary to get started. All you need is familiarity with basic programming principles. For seasoned developers, the updated sections offer the latest on transformers, explainable AI, and Generative AI with LLMs.","","9781803241449","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769275.pdf&bkn=10769274&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Image Steganography: A Review of the Recent Advances","N. Subramanian; O. Elharrouss; S. Al-Maadeed; A. Bouridane","Department of Computer Science and Engineering, Qatar University, Doha, Qatar; Department of Computer Science and Engineering, Qatar University, Doha, Qatar; Department of Computer Science and Engineering, Qatar University, Doha, Qatar; Department of Computer and Information Sciences, Northumbria University, Newcastle upon Tyne, U.K.",IEEE Access,"9 Feb 2021","2021","9","","23409","23423","Image Steganography is the process of hiding information which can be text, image or video inside a cover image. The secret information is hidden in a way that it not visible to the human eyes. Deep learning technology, which has emerged as a powerful tool in various applications including image steganography, has received increased attention recently. The main goal of this paper is to explore and discuss various deep learning methods available in image steganography field. Deep learning techniques used for image steganography can be broadly divided into three categories - traditional methods, Convolutional Neural Network-based and General Adversarial Network-based methods. Along with the methodology, an elaborate summary on the datasets used, experimental set-ups considered and the evaluation metrics commonly used are described in this paper. A table summarizing all the details are also provided for easy reference. This paper aims to help the fellow researchers by compiling the current trends, challenges and some future direction in this field.","2169-3536","","10.1109/ACCESS.2021.3053998","Qatar National Research Fund (a member of Qatar Foundation)(grant numbers:NPRP11S-0113-180276); Qatar National Library; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9335027","Image steganography;GAN steganography;CNN steganography;information hiding;image data hiding","Deep learning;Media;Ciphers;Tools;Market research;Image color analysis","","158","","72","CCBY","25 Jan 2021","","","IEEE","IEEE Journals"
"Discovering Research Areas from Patents: A Case Study in Autonomous Vehicles Industry","J. Ko; J. Lee","Ajou University,Department of Industrial Engineering,Suwon,South Korea; Ajou University,Department of Industrial Engineering,Suwon,South Korea",2021 IEEE International Conference on Big Data and Smart Computing (BigComp),"10 Mar 2021","2021","","","203","209","Seven research areas introduced by the `Autonomous Systems' research lab provide research areas required to enable the autonomous vehicle industry. For ensuring the validity of the research areas with the baseline, we apply Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) on the US patents containing `Autonomous Vehicles' to identify keywords and research areas of relevant technologies. Keyword clustering and TF-IDF are repeatedly applied to the retrieved keywords to further filter out irrelevant words. Coherence values for LSA and LDA are evaluated to determine an adequate number of topics that need to be generated. We found that results from LSA provide a list of technologies already included in the baseline while topics from LDA provide associated keywords to support defining each technology. We conclude the numbers and topics provided by the baseline model closely represent the industry of autonomous vehicles but the identified topics from us provide a significant extension in research areas. The resulting research areas may provide overviews and guidelines on the autonomous vehicles industry for researchers and institutes.","2375-9356","978-1-7281-8924-6","10.1109/BigComp51126.2021.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373209","Autonomous Vehicles;Big Data;Natural Language Processing;Topic Modelling;Patents Analysis","Industries;Patents;Time series analysis;Coherence;Market research;Autonomous vehicles;Guidelines","","1","","20","IEEE","10 Mar 2021","","","IEEE","IEEE Conferences"
"Generative Probabilistic Entropy Modeling With Conditional Diffusion for Learned Image Compression","M. Cao; W. Dai; S. Li; C. Li; J. Zou; W. Hu; H. Xiong","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Tsinghua Shenzhen International Graduate School, Shenzhen, Guangdong, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Entropy modeling is the core component of learned image compression (LIC) that models the distribution of latent representation learned from input images via neural networks for bit-rate estimation. However, existing entropy models employ presumed parameterized distributions such as Gaussian models and are limited for the learned latent representation characterized by complex distributions. To address this problem, in this paper, we for the first time achieve generative probabilistic entropy modeling of latent representation based on conditional diffusion models. Specifically, we propose a conditional diffusion-based probabilistic entropy model (CDPEM) to parameterize the latent representation with distributions of arbitrary forms that are generated by well designed training-test consistent denoising diffusion implicit model (TC-DDIM) without introducing any presumption. TC-DDIM is designed to leverage ancestral sampling to gradually approximate the distribution of latent representation with guaranteed consistency in generation for training and test. Furthermore, we develop a hierarchical spatial-channel context model to incorporate with TC-DDIM to sufficiently exploit spatial correlations with the approximate contextual information produced by ancestral sampling and channel-wise correlations using channel-wise information aggregation with reweighted training loss. Experimental results demonstrate that the proposed entropy model achieves state-of-the-art performance on the Kodak, CLIC, and Tecnick datasets compared to existing LIC methods. Remarkably, when incorporated with recent baselines, the proposed model outperforms latest VVC standard by an evident gain in R-D performance.","1558-2205","","10.1109/TCSVT.2025.3551780","National Natural Science Foundation of China(grant numbers:62320106003,62371288); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10929015","Learned image compression;entropy modeling;conditional diffusion model","Entropy;Context modeling;Diffusion models;Training;Image coding;Probabilistic logic;Adaptation models;Transforms;Noise reduction;Image reconstruction","","","","","IEEE","18 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Improve Information Service Capabilities from Content Aggregation to Knowledge Provision with Generative Pre-trained Transformer (GPT)","L. Yu; P. Bohao; Y. Qiang; Z. Wei","Northwestern Polytechnical University, Xi'an, China; School of Software Northwestern Polytechnical University, Xi'an, China; School of Computer Northwestern Polytechnical University, Xi'an, China; School of Software Northwestern Polytechnical University, Xi'an, China","2023 26th ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)","28 Aug 2023","2023","","","188","192","The exponential growth of data and information has led to unprecedented opportunities and challenges in processing, analyzing, and utilizing this vast amount of knowledge. Nowadays, with the emerging and evolving of some novel strong Artificial Intelligence (AI) technologies, especially GPT (short for Generative Pre-trained Transformer), it suddenly becomes possible for traditional information providers (such as Libraries, Data Intelligence Service Centers) to conduct business from resource and content aggregation to high-level knowledge provision. After analyzing the status of information businesses and mechanism of GPT, in this article, several possible changes and some corresponsive measures to achieve a higher intelligent level of knowledge service for traditional information providers are mainly discussed in depth. It's expected that such exploration will provide meaningful reference for facilitating the intelligent development of traditional information services.","","979-8-3503-4586-5","10.1109/SNPD-Winter57765.2023.10223888","National Natural Science Foundation of China(grant numbers:61972318,61572403); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10223888","Information Service;Content Provision;Knowledge Generation;Artificial Intelligence;ChatGPT","Knowledge acquisition;Focusing;Transformers;Chatbots;Libraries;Artificial intelligence;Information technology","","","","11","IEEE","28 Aug 2023","","","IEEE","IEEE Conferences"
"Generative AI: The key for everyday problems. A comparison proposal for new users","O. I. I. R; C. G. Q. M","Department of Electrical and Electronics Engineering, Universidad del Norte, Barranquilla, Colombia; Department of Electrical and Electronics Engineering, Universidad del Norte, Barranquilla, Colombia",2023 IEEE Colombian Caribbean Conference (C3),"21 Feb 2024","2023","","","1","6","Generative AI is yet one of the biggest types of artificial intelligence brought to the public view, proposing a new vision and path for many industries around the world. This artificial intelligence model has brought a huge audience due to its impact on almost every industry, transforming the way some jobs can be pursued. Through an objective position, in this paper generative AI is evaluated to propose a comparison of some of their important tools to give new users a guide to solving their daily life problems, whether in their households or in their jobs, demonstrating the importance of knowing and using this type of AI.","","979-8-3503-4179-9","10.1109/C358072.2023.10436249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10436249","Generative AI;AI Tools;AI Models","Industries;Generative AI;Proposals","","","","80","IEEE","21 Feb 2024","","","IEEE","IEEE Conferences"
"Reconceptualizing Self-Directed Learning in the Era of Generative AI: An Exploratory Analysis of Language Learning","B. Li; C. J. Bonk; C. Wang; X. Kou","Purdue University, West Lafayette, IN, USA; Indiana University, Bloomington, IN, USA; Colby College, Waterville, ME, USA; Indiana University, Bloomington, IN, USA",IEEE Transactions on Learning Technologies,"1 May 2024","2024","17","","1489","1503","This exploratory analysis investigates the integration of ChatGPT in self-directed learning (SDL). Specifically, this study examines YouTube content creators’ language-learning experiences and the role of ChatGPT in their SDL, building upon Song and Hill's conceptual model of SDL in online contexts. Thematic analysis of interviews with 19 YouTubers and relevant video contents reveals distinct constructs of ChatGPT-integrated SDL, suggesting a reconceptualization and refinement of the SDL framework in the consideration of generative artificial intelligence (AI). This framework emphasizes critical aspects of utilizing ChatGPT as an SDL tool on two distinct levels: 1) the interactive relationships and interplay between learners’ personal traits and their ongoing learning processes (local) and 2) the evolving nature of SDL in the rapidly advancing landscape of generative AI, with socio-political-cultural foundations of AI constantly shaping the learning environment where SDL occurs (global). The study highlights the potential of ChatGPT as a tool for promoting self-directed language learning (SDLL) and provides implications for the development of learning technologies and research on AI-facilitated SDL.","1939-1382","","10.1109/TLT.2024.3386098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496545","Artificial intelligence (AI);ChatGPT;language learning;self-directed learning (SDL);YouTuber","Chatbots;Education;Artificial intelligence;Web sites;Video on demand;Generative AI;Web 2.0","","5","","77","IEEE","10 Apr 2024","","","IEEE","IEEE Journals"
"Secure Implementation of Artificial Intelligence Applications for Anti-Money Laundering using Confidential Computing","R. Searle; P. Gururaj; A. Gupta; K. Kannur","Fortanix, Inc., Mountain View, USA; Fortanix, Inc., Mountain View, USA; Fortanix, Inc., Mountain View, USA; Fortanix, Inc., Mountain View, USA",2022 IEEE International Conference on Big Data (Big Data),"26 Jan 2023","2022","","","3092","3098","Money laundering not only facilitates the perpetration of dangerous and illegal activities it also damages the credibility and integrity of the global financial system and the financial institutions through whom money is laundered. Despite most financial institutions adhering to prevailing laws and regulations designed to prevent the practice of money laundering, it has been difficult to stop illicit activity using conventional methods. Hence, to combat money laundering, financial institutions are increasingly focused on the adoption of new technologies involving the use of artificial intelligence (AI) and machine learning (ML). One barrier to adoption of these new techniques for anti-money laundering (AML), however, is the need to maintain the confidentiality of the massive quantities of data required to train AI models, a financial data is the subject of regulatory controls and a target for cyber threat actors. In response to these challenges, this paper presents a secure and scalable architecture for AI implementation that uses confidential computing technology to provide complete end-to-end protection of sensitive financial data and the intellectual property of AML algorithm developers. Generative adversarial networks (GANs) are demonstrated using cloud infrastructure secured using Intel® Software Guard Extensions (Intel® SGX). The reported solution architecture can be adapted to support federated machine learning (FML), at scale, between mutually distrusting institutions, with independent control of data security at rest, in transit, and in use by individual data owners.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10021108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021108","Anti-money laundering;confidential computing;artificial intelligence;machine learning;data security","Training;Computers;Law enforcement;Computational modeling;Collaboration;Computer architecture;Machine learning","","3","","46","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"Bi-Directional Transformers vs. word2vec: Discovering Vulnerabilities in Lifted Compiled Code","G. A. McCully; J. D. Hastings; S. Xu; A. Fortier","The Beacom College of Computer and Cyber Sciences, Dakota State University, Madison, SD, USA; The Beacom College of Computer and Cyber Sciences, Dakota State University, Madison, SD, USA; Department of Cyber, Intelligence, and Information Operations, College of Applied Science & Technology, University of Arizona, Tucson, AZ, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA",2024 Cyber Awareness and Research Symposium (CARS),"13 Dec 2024","2024","","","1","8","Detecting vulnerabilities within compiled binaries is challenging due to lost high-level code structures and other factors such as architectural dependencies, compilers, and optimization options. To address these obstacles, this research explores vulnerability detection using natural language processing (NLP) embedding techniques with word2vec, BERT, and RoBERTa to learn semantics from intermediate representation (LLVM IR) code. Long short-term memory (LSTM) neural networks were trained on embeddings from encoders created using approximately 48k LLVM functions from the Juliet dataset. This study is pioneering in its comparison of word2vec models with multiple bidirectional transformers (BERT, RoBERTa) embeddings built using LLVM code to train neural networks to detect vulnerabilities in compiled binaries. Word2vec Skip-Gram models achieved 92% validation accuracy in detecting vulnerabilities, outperforming word2vec Continuous Bag of Words (CBOW), BERT, and RoBERTa. This suggests that complex contextual embeddings may not provide advantages over simpler word2vec models for this task when a limited number (e.g. 48K) of data samples are used to train the bidirectional transformer-based models. The comparative results provide novel insights into selecting optimal embeddings for learning compiler-independent semantic code representations to advance machine learning detection of vulnerabilities in compiled binaries.","","979-8-3503-8641-7","10.1109/CARS61786.2024.10778724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10778724","Machine Learning;Buffer Overflows;BERT;RoBERTa;Binary Security;LLVM;word2vec","Codes;Accuracy;Semantics;Neural networks;Bidirectional control;Transformers;Encoding;Natural language processing;Long short term memory;Context modeling","","","","40","IEEE","13 Dec 2024","","","IEEE","IEEE Conferences"
"Robust Video watermarking based on deep neural network and curriculum learning","Z. Ke; H. Huang; Y. Liang; Y. Ding; X. Cheng; Q. Wu","School of Software Engineering, South China University of Technology, Guangzhou, China; Guangzhou Easefun Co. Ltd. Information and Technology, Guangzhou, China; Guangzhou Easefun Co. Ltd. Information and Technology, Guangzhou, China; Guangzhou Easefun Co. Ltd. Information and Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China",2022 IEEE International Conference on e-Business Engineering (ICEBE),"7 Feb 2023","2022","","","80","85","With the rapid development of multimedia and short video, there is a growing concern for video copyright protection. Some work has been proposed to add some copyright or fingerprint information to the video to trace the source of the video when it is stolen and protect video copyright. This paper proposes a video watermarking method based on a deep neural network and curriculum learning for watermarking of sliced videos. The first frame of the segmented video is perturbed by an encoder network, which is invisible and can be distinguished by the decoder network. Our model is trained and tested on an online educational video dataset consisting of 2000 different video clips. Experimental results show that our method can successfully discriminate most watermarked and non-watermarked videos with low visual disturbance, which can be achieved even under a relatively high video compression rate(H.264 video compress with CRF 32).","","978-1-6654-9244-7","10.1109/ICEBE55470.2022.00023","National Natural Science Foundation of China (NSFC)(grant numbers:61876208,62272172); Tip-top Scientific and Technical Innovative Youth Talents of Guangdong Special Support Program(grant numbers:2019TQ05X200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035063","robust video watermark;deep neural network;copyright protection;curriculum learning","Deep learning;Visualization;Neural networks;Watermarking;Video compression;Streaming media;Fingerprint recognition","","8","","29","IEEE","7 Feb 2023","","","IEEE","IEEE Conferences"
"Breaking the Silence: the Threats of Using LLMs in Software Engineering","J. Sallou; T. Durieux; A. Panichella","TU Delft, The Netherlands; TU Delft, The Netherlands; TU Delft, The Netherlands",2024 IEEE/ACM 46th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),"29 Oct 2024","2024","","","102","106","Large Language Models (LLMs) have gained considerable traction within the Software Engineering (SE) community, impacting various SE tasks from code completion to test generation, from program repair to code summarization. Despite their promise, researchers must still be careful as numerous intricate factors can influence the outcomes of experiments involving LLMs. This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings. In response, this paper proposes a set of guidelines tailored for SE researchers and Language Model (LM) providers to mitigate these concerns. The implications of the guidelines are illustrated using existing good practices followed by LLM providers and a practical example for SE researchers in the context of test case generation.","2832-7632","979-8-4007-0500-7","10.1145/3639476.3639764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10727145","Large Language Models;Software Engineering;Evaluation","Codes;Large language models;Training data;Maintenance engineering;Reproducibility of results;Data models;Test pattern generators;Software engineering;Guidelines","","4","","46","CCBY","29 Oct 2024","","","IEEE","IEEE Conferences"
"Guidelines to Develop AI Ethics Policy in Organizations: Perspectives Informed from Two Different Countries’ Laws","A. Ghandour; B. J. Woodford","Faculty of Business, Middle East University, Amman, Jordan; School of Computing, University of Otago, Dunedin, New Zealand",2023 24th International Arab Conference on Information Technology (ACIT),"18 Mar 2024","2023","","","1","9","This paper draws on actual attempts being made to develop and implement ethical frameworks and discusses AI regulatory approaches of two countries (United Arab Emirates and New Zealand) and provides recommendations for organizations developing their own AI ethics policies. These recommendations aim to address key ethical considerations related to the adoption and implementation of AI tools, including data protection and ownership, accountability and responsibility, error management, physical safety, societal harms, and economic implications.","2831-4948","979-8-3503-8430-7","10.1109/ACIT58888.2023.10453750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10453750","AI;AI Ethics;AI Policy;UAE;New Zealand","Economics;Ethics;Shape;Companies;Regulation;Safety;Artificial intelligence","","1","","69","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"A Practical Black-Box Attack on Source Code Authorship Identification Classifiers","Q. Liu; S. Ji; C. Liu; C. Wu","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Department of Distributed AI, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; College of Computer Science and Technology, Zhejiang University, Hangzhou, China",IEEE Transactions on Information Forensics and Security,"1 Jul 2021","2021","16","","3620","3633","Existing researches have recently shown that adversarial stylometry of source code can confuse source code authorship identification (SCAI) models, which may threaten the security of related applications such as programmer attribution, software forensics, etc. In this work, we propose source code authorship disguise (SCAD) to automatically hide programmers' identities from authorship identification, which is more practical than the previous work that requires to known the output probabilities or internal details of the target SCAI model. Specifically, SCAD trains a substitute model and develops a set of semantically equivalent transformations, based on which the original code is modified towards a disguised style with small manipulations in lexical features and syntactic features. When evaluated under totally black-box settings, on a real-world dataset consisting of 1,600 programmers, SCAD induces state-of-the-art SCAI models to cause above 30% misclassification rates. The efficiency and utility-preserving properties of SCAD are also demonstrated with multiple metrics. Furthermore, our work can serve as a guideline for developing more robust identification methods in the future.","1556-6021","","10.1109/TIFS.2021.3080507","NSFC(grant numbers:U1936215,61772466,U1836202); National Key Research and Development Program of China(grant numbers:2020YFB2103802,2018YFB0804102,2020AAA0140004); Zhejiang Provincial Natural Science Foundation for Distinguished Young Scholars(grant numbers:LR19F020003); Fundamental Research Funds for the Central Universities (Zhejiang University NGICS Platform); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9454564","Source code;authorship identification;adversarial stylometry","Feature extraction;Tools;Training;Syntactics;Predictive models;Perturbation methods;Transforms","","9","","36","IEEE","15 Jun 2021","","","IEEE","IEEE Journals"
"Generative AI in Action","A. Bahree",Manning Publications,Generative AI in Action,"","2024","","","","","Generative AI can transform your business by streamlining the process of creating text, images, and code. This book will show you how to get in on the action! Generative AI in Action is the comprehensive and concrete guide to generative AI you’ve been searching for. It introduces both AI’s fundamental principles and its practical applications in an enterprise context—from generating text and images for product catalogs and marketing campaigns, to technical reporting, and even writing software. Inside, author Amit Bahree shares his experience leading Generative AI projects at Microsoft for nearly a decade, starting well before the current GPT revolution. Inside Generative AI in Action you will find:  A practical overview of of generative AI applications Architectural patterns, integration guidance, and best practices for generative AI The latest techniques like RAG, prompt engineering, and multi-modality The challenges and risks of generative AI like hallucinations and jailbreaks How to integrate generative AI into your business and IT strategy  Generative AI in Action is full of real-world use cases for generative AI, showing you where and how to start integrating this powerful technology into your products and workflows. You’ll benefit from tried-and-tested implementation advice, as well as application architectures to deploy GenAI in production at enterprise scale.","","9781633436947","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10745289.pdf&bkn=10745288&pdfType=book","prompt engineering;model fine tuning;enterprise;safety;ethics;integration;RAG;multi-modality;LLMs;hallucinations;jailbreaks;architectural patterns;ChatGPT;Bard;Copilot","","","","","","","6 Nov 2024","","","Manning","Manning eBooks"
"Knowledge-Reinforced Cross-Domain Recommendation","L. Huang; X. -D. Huang; H. Zou; Y. Gao; C. -D. Wang; P. S. Yu","College of Mathematics and Informatics, South China Agricultural University, Guangzhou, China; College of Mathematics and Informatics, South China Agricultural University, Guangzhou, China; College of Mathematics and Informatics, South China Agricultural University, Guangzhou, China; College of Mathematics and Informatics, South China Agricultural University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA",IEEE Transactions on Neural Networks and Learning Systems,"","2024","PP","99","1","15","Over the past few years, cross-domain recommendation has gained great attention to resolve the cold-start issue. Many existing cross-domain recommendation methods model a preference bridge between the source and target domains to transfer preferences by the overlapping users. However, when there are insufficient cross-domain users available to bridge the two domains, it will negatively impact the recommender system’s accuracy (ACC) and performance. Therefore, in this article, we propose to create a link between the source and the target domains by leveraging knowledge graph (KG) as the auxiliary information, and propose a novel knowledge-reinforced cross-domain recommendation (KR-CDR) method. First of all, we construct a new cross-domain KG (CDKG) by using the KGs that represent the source and target domains, respectively. Additionally, we employ reinforcement learning (RL) with meta learning on CDKG to discover meta-paths between the source and target domains. With these meta-paths, we obtain meta-path aggregated embedding vectors for cold-start users. Ultimately, the predicted rating can be acquired from the user meta-path aggregated embedding vector and item embedding vector. Experiments carried out on five real-world datasets show that the proposed method performs better than the state-of-the-art methods.","2162-2388","","10.1109/TNNLS.2024.3500096","National Natural Science Foundation of China(grant numbers:62106079,62276277); Natural Science Foundation of Guangdong Province(grant numbers:2020A1515110337); Guangdong Provincial Key Laboratory of Intellectual Property and Big Data(grant numbers:2018B030322016); NSF(grant numbers:III-2106758,POSE-2346158); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771593","Cross-domain recommendation;knowledge graph (KG);meta learning;reinforcement learning (RL)","Vectors;Cognition;Bridges;Metalearning;Training;Feature extraction;Reviews;Recommender systems;Matrix decomposition;Data mining","","","","","IEEE","28 Nov 2024","","","IEEE","IEEE Early Access Articles"
"Sentiment Analysis of Using ChatGPT in Education","M. Tubishat; F. Al-Obeidat; A. Shuhaiber","College of Technological Innovation, Zayed University, UAE; College of Technological Innovation, Zayed University, UAE; College of Technological Innovation, Zayed University, UAE","2023 International Conference on Smart Applications, Communications and Networking (SmartNets)","22 Aug 2023","2023","","","1","7","This paper presents a study on the use of the Chat Generative Pretrained Transformer (ChatGPT) in education. In this work, we propose a sentiment analysis model of tweets related to the use of the ChatGPT in education. The purpose of this research is to identify common sentiments, topics, and perspectives that are expressed towards ChatGPT in the education field based on the data collected from Twitter. Twitter was used to collect 11830 tweets about the use of ChatGPT in education. Topics and emotions expressed in the tweets were extracted using NLP algorithms and organized into distinct groups. Also, the most frequent words in the positive and negative opinion words are determined. The findings of the paper indicate that most tweets about ChatGPT are either positive or neutral, with a small percentage expressing negative sentiments. In addition, the study analyzes the sentiments expressed in tweets about the employment of ChatGPT in education using four different classifiers: Naive Bayes (NB), Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Random Forest (RF). According to the results, the SVM classifier has the highest accuracy of 81.4 percent.","","979-8-3503-0252-3","10.1109/SmartNets58706.2023.10215977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10215977","ChatGPT;Sentiment Analysis;Education;Twitter","Support vector machines;Sentiment analysis;Analytical models;Social networking (online);Education;Blogs;Employment","","6","","16","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Empowering Hardware Security with LLM: The Development of a Vulnerable Hardware Database","D. Saha; K. Yahyaei; S. Kumar Saha; M. Tehranipoor; F. Farahmandi","Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA",2024 IEEE International Symposium on Hardware Oriented Security and Trust (HOST),"6 Jun 2024","2024","","","233","243","The scarcity of comprehensive databases and bench-marks in hardware design specifically tailored for security tasks is a significant challenge in the community. Such databases are crucial for developing machine learning-based methods and benchmarking, providing a foundation for evaluating and improving hardware security solutions. However, manually creating these extensive datasets is impractical due to the significant time and effort required. Given the proficiency of large language models (LLM) in natural language processing, coding, and advanced reasoning tasks, using LLM as an artificial intelligence (AI) agent presents a viable option to efficiently create such extensive datasets. In this light, this paper introduces Vul-FSM, a database of 10,000 vulnerable finite state machine (FSM) designs incorporating 16 distinct security weaknesses and vulnerabilities generated using the proposed SecRT-Llmframework. The framework combines the in-context learning capability of LLM, the guidance of developed prompting strategies, and the scrutiny of fidelity-check to not only insert but also detect hardware vulnerabilities and weaknesses. To demonstrate the efficacy of SecRT-LLM, we present an exhaustive analysis, highlighting the proficiency of GPT models in vulnerability insertion, detection, and mitigation. Our proposed SecRT-LLM framework, using gpt-3.5-turbo, demonstrates strong effectiveness, achieving macroaverage pass rates of 81.98% and 80.30% on the first attempt and 97.37% and 99.07% within five attempts for vulnerability insertion and detection, respectively.","2765-8406","979-8-3503-7394-3","10.1109/HOST55342.2024.10545393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10545393","Large Language Model;ChatGPT;RTL design;Hardware Security;Vulnerability Analysis;Common Weakness Enumeration (CWE)","Learning systems;Analytical models;Databases;Hardware security;Instruments;Linguistics;Cognition","","6","","36","IEEE","6 Jun 2024","","","IEEE","IEEE Conferences"
"Dynamic and Super-Personalized Media Ecosystem Driven by Generative AI: Unpredictable Plays Never Repeating the Same","S. Ahn; H. -J. Yim; Y. Lee; S. -I. Park","Media Research Division, Electronics and Telecommunications Research Institute, Daejeon, South Korea; Media Research Division, Electronics and Telecommunications Research Institute, Daejeon, South Korea; Intelligence Information Research Division, Electronics and Telecommunications Research Institute, Daejeon, South Korea; Media Research Division, Electronics and Telecommunications Research Institute, Daejeon, South Korea",IEEE Transactions on Broadcasting,"16 Sep 2024","2024","70","3","980","994","This paper introduces a media service model that exploits artificial intelligence (AI) video generators at the receive end. This proposal deviates from the traditional multimedia ecosystem, completely relying on in-house production, by shifting part of the content creation onto the receiver. We bring a semantic process into the framework, allowing the distribution network to provide service elements that prompt the content generator rather than distributing encoded data of fully finished programs. The service elements include fine-tailored text descriptions, lightweight image data of some objects, or application programming interfaces, comprehensively referred to as semantic sources, and the user terminal translates the received semantic data into video frames. Empowered by the random nature of generative AI, users can experience super-personalized services accordingly. The proposed idea incorporates situations in which the user receives different service providers’ element packages, either in a sequence over time or multiple packages at the same time. Given promised in-context coherence and content integrity, the combinatory dynamics will amplify the service diversity, allowing the users to always chance upon new experiences. This work particularly aims at short-form videos and advertisements, which the users would easily feel fatigued by seeing the same frame sequence every time. In those use cases, the content provider’s role will be recast as scripting semantic sources, transformed from a thorough producer. Overall, this work explores a new form of media ecosystem facilitated by receiver-embedded generative models, featuring both random content dynamics and enhanced delivery efficiency simultaneously.","1557-9611","","10.1109/TBC.2024.3380474","Institute of Information and Communications Technology Planning and Evaluation (IITP) Grant; Korea Government [MSIT, Development of Receiver Chip for ATSC 3.0 Mobile Broadcast](grant numbers:RS-2023-00224660); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506327","Generative AI;semantic communications;6G multimedia casting;on-device AI","Media;Semantics;Artificial intelligence;Production;Ecosystems;Task analysis;Streaming media","","2","","88","IEEE","22 Apr 2024","","","IEEE","IEEE Journals"
"The Generative AI Landscape in Education: Mapping the Terrain of Opportunities, Challenges, and Student Perception","Z. Ahmed; S. S. Shanto; M. H. K. Rime; M. K. Morol; N. Fahad; M. J. Hossen; M. Abdullah-Al-Jubair","Department of Computer Science, American International University-Bangladesh (AIUB), Dhaka, Bangladesh; Department of Computer Science, American International University-Bangladesh (AIUB), Dhaka, Bangladesh; Department of Computer Science and Engineering, University of Rajshahi, Rajshahi, Bangladesh; Department of Computer Science, American International University-Bangladesh (AIUB), Dhaka, Bangladesh; Department of Computer Science, American International University-Bangladesh (AIUB), Dhaka, Bangladesh; Faculty of Engineering and Technology, Multimedia University, Melaka, Malaysia; Department of Computer Science, American International University-Bangladesh (AIUB), Dhaka, Bangladesh",IEEE Access,"17 Oct 2024","2024","12","","147023","147050","Generative AI (GAI) technologies like ChatGPT are permanently changing academic education. Their integration opens up vast opportunities for bespoke learning and better student interaction but also brings about academic honesty issues and the application of real-life educators. This study aims to fill the literature gap regarding the use of multiple GAI tools and their effect on academic outcomes via a comprehensive review. A systematic literature review was performed following PRISMA guidelines to synthesize results on the potential and drawbacks of GAI in educational domains. We included theoretical and empirical papers that used qualitative, quantitative, or mixed-methods study designs. We have also explored conceptual frameworks and the most creative AI applications with a special emphasis on uniqueness and practicability. Experiences, and Perceptions Concerning To compile the information needed we gathered insights into what students were going through by conducting the survey which contains 200 respondents of undergraduate university students gathering insights into the college students’ experiences and perceptions related to GAI used for educational purposes. At the basic level, GAI comprises areas like personalization, task automation, teacher assistance, and efficiency among others, and respective solutions for the immersion of a learner in learning processes to reform directions. However, it generates plenty of challenges such as the question of assessment integrity, the risk that too much automated grading could overwhelm educational value, and relevantly the veracity of AI-generated content as well as the potential disruption to skills like critical thinking, in addition to data privacy and ethical issues. Student Perception Survey the text also indicates that most students, as per the student perception survey found AI systems useful in academic support. However, they also know the other side of the coin and are very familiar with the technology constraints and challenges.","2169-3536","","10.1109/ACCESS.2024.3461874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10681094","Chatbots;education;generative AI;opportunities and challenges;student perception","Education;Generative AI;Artificial intelligence;Surveys;Chatbots;Ethics;Market research","","1","","98","CCBYNCND","16 Sep 2024","","","IEEE","IEEE Journals"
"Perception-Aware Attack Against Music Copyright Detection: Impacts and Defenses","R. Duan; Z. Qu; S. Zhao; L. Ding; Y. Liu; Z. Lu","School of Science and Engineering, University of Missouri-Kansas City, Kansas City, MO, USA; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; School of Computer Science, University of Oklahoma, Tulsa, OK, USA; Department of Computer Science, American University, Washington, DC, USA; Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA; Department of Electrical Engineering, University of South Florida, Tampa, USA",IEEE Transactions on Dependable and Secure Computing,"","2024","PP","99","1","18","Recently, adversarial machine learning attacks have posed serious security threats against practical audio signal classification systems, including speech recognition, speaker recognition, music copyright detection. Most existing studies have mainly focused on ensuring the effectiveness of attacking an audio signal classifier via creating a noise- like perturbation on the original signal, which remains a gap in preserving the human perception of adversarial audios. This paper presents a novel perspective to create adversarial audios by integrating the human perception model into the attack formulation to generate well-perceived adversarial examples. Different from conventional approaches which primarily focused on using $L_{p}$ norm to preserve the audio quality, we adopt a human study to understand how human participants react to different types of music perturbations, build a Siamese Neural Network (SNN) based model to characterize the human perception. The new findings of the human perception study guide us to formulate a new computationally efficient, multiple-feature-based perception-aware (CEMF-PA) attack, which manipulates different audio signal features to find an optimal perturbed music signal against music copyright detection. This novel attack vector opens a new door to generating highly effective, well-perceived adversarial audio signals via manipulating the auditory features. Experimental results show that the proposed attack is effective against YouTube's copyright detection. Finally, we propose the defense strategy design to make the copyright detection more robust to adversarial music signals generated by the CEMF-PA attack.","1941-0018","","10.1109/TDSC.2024.3522849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10816512","Computer audio systems;applications;machine learning;and security","Multiple signal classification;Perturbation methods;Copyright protection;Speech recognition;Computational modeling;Neural networks;Fingerprint recognition;Adversarial machine learning;Web sites;Video on demand","","","","","IEEE","26 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Data Labeling in Machine Learning with Python: Explore modern ways to prepare labeled data for training and fine-tuning ML and generative AI models","V. K. Suda",NA,Data Labeling in Machine Learning with Python: Explore modern ways to prepare labeled data for training and fine-tuning ML and generative AI models,"","2024","","","","","Take your data preparation, machine learning, and GenAI skills to the next level by learning a range of Python algorithms and tools for data labelingKey FeaturesGenerate labels for regression in scenarios with limited training dataApply generative AI and large language models (LLMs) to explore and label text dataLeverage Python libraries for image, video, and audio data analysis and data labelingPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionData labeling is the invisible hand that guides the power of artificial intelligence and machine learning. In today’s data-driven world, mastering data labeling is not just an advantage, it’s a necessity. Data Labeling in Machine Learning with Python empowers you to unearth value from raw data, create intelligent systems, and influence the course of technological evolution. With this book, you'll discover the art of employing summary statistics, weak supervision, programmatic rules, and heuristics to assign labels to unlabeled training data programmatically. As you progress, you'll be able to enhance your datasets by mastering the intricacies of semi-supervised learning and data augmentation. Venturing further into the data landscape, you'll immerse yourself in the annotation of image, video, and audio data, harnessing the power of Python libraries such as seaborn, matplotlib, cv2, librosa, openai, and langchain. With hands-on guidance and practical examples, you'll gain proficiency in annotating diverse data types effectively. By the end of this book, you’ll have the practical expertise to programmatically label diverse data types and enhance datasets, unlocking the full potential of your data.What you will learnExcel in exploratory data analysis (EDA) for tabular, text, audio, video, and image dataUnderstand how to use Python libraries to apply rules to label raw dataDiscover data augmentation techniques for adding classification labelsLeverage K-means clustering to classify unsupervised dataExplore how hybrid supervised learning is applied to add labels for classificationMaster text data classification with generative AIDetect objects and classify images with OpenCV and YOLOUncover a range of techniques and resources for data annotationWho this book is forThis book is for machine learning engineers, data scientists, and data engineers who want to learn data labeling methods and algorithms for model training. Data enthusiasts and Python developers will be able to use this book to learn data exploration and annotation using Python libraries. Basic Python knowledge is beneficial but not necessary to get started.","","9781804613788","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769318.pdf&bkn=10769317&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Google Machine Learning and Generative AI for Solutions Architects: ​Build efficient and scalable AI/ML solutions on Google Cloud","K. Kavanagh; P. Vergadia",NA; NA,Google Machine Learning and Generative AI for Solutions Architects: ​Build efficient and scalable AI/ML solutions on Google Cloud,"","2024","","","","","Architect and run real-world AI/ML solutions at scale on Google Cloud, and discover best practices to address common industry challenges effectivelyKey FeaturesUnderstand key concepts, from fundamentals through to complex topics, via a methodical approachBuild real-world end-to-end MLOps solutions and generative AI applications on Google CloudGet your hands on a code repository with over 20 hands-on projects for all stages of the ML model development lifecyclePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionMost companies today are incorporating AI/ML into their businesses. Building and running apps utilizing AI/ML effectively is tough. This book, authored by a principal architect with about two decades of industry experience, who has led cross-functional teams to design, plan, implement, and govern enterprise cloud strategies, shows you exactly how to design and run AI/ML workloads successfully using years of experience from some of the world’s leading tech companies. You’ll get a clear understanding of essential fundamental AI/ML concepts, before moving on to complex topics with the help of examples and hands-on activities. This will help you explore advanced, cutting-edge AI/ML applications that address real-world use cases in today’s market. You’ll recognize the common challenges that companies face when implementing AI/ML workloads, and discover industry-proven best practices to overcome these. The chapters also teach you about the vast AI/ML landscape on Google Cloud and how to implement all the steps needed in a typical AI/ML project. You’ll use services such as BigQuery to prepare data; Vertex AI to train, deploy, monitor, and scale models in production; as well as MLOps to automate the entire process. By the end of this book, you will be able to unlock the full potential of Google Cloud's AI/ML offerings.What you will learnBuild solutions with open-source offerings on Google Cloud, such as TensorFlow, PyTorch, and SparkSource, understand, and prepare data for ML workloadsBuild, train, and deploy ML models on Google CloudCreate an effective MLOps strategy and implement MLOps workloads on Google CloudDiscover common challenges in typical AI/ML projects and get solutions from expertsExplore vector databases and their importance in Generative AI applicationsUncover new Gen AI patterns such as Retrieval Augmented Generation (RAG), agents, and agentic workflowsWho this book is forThis book is for aspiring solutions architects looking to design and implement AI/ML solutions on Google Cloud. Although this book is suitable for both beginners and experienced practitioners, basic knowledge of Python and ML concepts is required. The book focuses on how AI/ML is used in the real world on Google Cloud. It briefly covers the basics at the beginning to establish a baseline for you, but it does not go into depth on the underlying mathematical concepts that are readily available in academic material.","","9781803247021","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769249.pdf&bkn=10769248&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"GPT (Generative Pre-Trained Transformer)— A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions","G. Yenduri; M. Ramalingam; G. C. Selvi; Y. Supriya; G. Srivastava; P. K. R. Maddikunta; G. D. Raj; R. H. Jhaveri; B. Prabadevi; W. Wang; A. V. Vasilakos; T. R. Gadekallu","School of Computer Science and Engineering, VIT-AP University, Amaravati, Andhra Pradesh, India; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India; Department of Mathematics and Computer Science, Brandon University, Brandon, MB, Canada; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India; Department of Computer Science and Engineering, School of Technology, Pandit Deendayal Energy University, Gandhinagar, India; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India; Department of Computer Science, City University of Hong Kong, Hong Kong, SAR, China; Center for AI Research (CAIR), University of Agder (UiA), Grimstad, Norway; Center of Research Impact and Outcome, Chitkara University, Rajpura, Punjab, India",IEEE Access,"26 Apr 2024","2024","12","","54608","54649","The Generative Pre-trained Transformer (GPT) represents a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. GPT is based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, GPT have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the GPT, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications. In this review, we also explored the potential challenges and limitations of a GPT. Furthermore, we discuss potential solutions and future directions. Overall, this paper aims to provide a comprehensive understanding of GPT, its enabling technologies, their impact on various applications, emerging challenges, and potential solutions.","2169-3536","","10.1109/ACCESS.2024.3389497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500411","Generative pre-trained transformer;natural language processing;artificial intelligence","Natural language processing;Solid modeling;Artificial intelligence;Surveys;Task analysis;Reviews;Transformers","","68","","209","CCBY","15 Apr 2024","","","IEEE","IEEE Journals"
"Generative AI for Software Practitioners","C. Ebert; P. Louridas","Vector Consulting Services, Stuttgart, Germany; Department of Management Science and Technology, Athens University of Economics and Business, Athens, Greece",IEEE Software,"7 Jul 2023","2023","40","4","30","38","Generative artificial intelligence (AI) tools, such as Bard, ChatGPT, and CoPilot, have rapidly gained widespread usage. They also have the potential to boost software engineering productivity. In this article, we elaborate technologies and usage of generative AI in the software industry. We address questions, such as: How does generative AI improve software productivity? How to connect generative AI to software development, and what are the risks? Which technologies have what sorts of benefits? Practitioner guidance and case studies are shared from our industry context. I look forward to hearing from you about this column and the technologies that matter most for your work.—Christof Ebert","1937-4194","","10.1109/MS.2023.3265877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10176168","","Productivity;Industries;Auditory system;Chatbots;Software engineering;Artificial intelligence;Artificial intelligence;Chatbots;Risk management","","59","","10","IEEE","7 Jul 2023","","","IEEE","IEEE Magazines"
"DeepDIST: A Black-Box Anti-Collusion Framework for Secure Distribution of Deep Models","H. Cheng; X. Li; H. Wang; X. Zhang; X. Liu; M. Wang; F. Li","School of Mathematics and Statistics, Fuzhou University, Fuzhou, China; College of Computer Science and Big Data, Fuzhou University, Fuzhou, China; School of Physical and Mathematical Sciences, Nanyang Technological University, Jurong West, Singapore; School of Computer Science, Fudan University, Shanghai, China; College of Computer Science and Big Data, Fuzhou University, Fuzhou, China; School of Mathematics and Statistics, Fuzhou University, Fuzhou, China; College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai, China",IEEE Transactions on Circuits and Systems for Video Technology,"5 Jan 2024","2024","34","1","97","109","Due to enormous computing and storage overhead for well-trained Deep Neural Network (DNN) models, protecting the intellectual property of model owners is a pressing need. As the commercialization of deep models is becoming increasingly popular, the pre-trained models delivered to users may suffer from being illegally copied, redistributed, or abused. In this paper, we propose DeepDIST, the first end-to-end secure DNNs distribution framework in a black-box scenario. Specifically, our framework adopts a dual-level fingerprint (FP) mechanism to provide reliable ownership verification, and proposes two equivalent transformations that can resist collusion attacks, plus a newly designed similarity loss term to improve the security of the transformations. Unlike the existing passive defense schemes that detect colluding participants, we introduce an active defense strategy, namely damaging the performance of the model after the malicious collusion. The extensive experimental results show that DeepDIST can maintain the accuracy of the host DNN after embedding fingerprint conducted for true traitor tracing, and is robust against several popular model modifications. Furthermore, the anti-collusion effect is evaluated on two typical classification tasks (10-class and 100-class), and the proposed DeepDIST can drop the prediction accuracy of the collusion model to 10% and 1% (random guess), respectively.","1558-2205","","10.1109/TCSVT.2023.3284914","National Natural Science Foundation of China(grant numbers:62172098,62072109,61702105); Natural Science Foundation of Fujian Province(grant numbers:2020J01497); Education Research Project for Young and Middle-Aged Teachers of the Education Department of Fujian Province(grant numbers:JAT200064); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10147844","Deep neural networks;anti-collusion;digital watermarking;digital fingerprinting","Watermarking;Neural networks;Computational modeling;Closed box;Predictive models;Integrated circuit modeling;Glass box","","2","","46","IEEE","9 Jun 2023","","","IEEE","IEEE Journals"
"Generative AI for the Optimization of Next-Generation Wireless Networks: Basics, State-of-the-Art, and Open Challenges","F. Khoramnejad; E. Hossain","Department of Electrical and Computer Engineering, University of Manitoba, Winnipeg, Manitoba, Canada; Department of Electrical and Computer Engineering, University of Manitoba, Winnipeg, Manitoba, Canada",IEEE Communications Surveys & Tutorials,"","2025","PP","99","1","1","Next-generation (xG) wireless networks, with their complex and dynamic nature, present significant challenges to using traditional optimization techniques. Generative Artificial Intelligence (GAI) emerges as a powerful tool due to its unique strengths. Unlike traditional optimization techniques and other machine learning methods, GAI excels at learning from real-world network data, capturing its intricacies. This enables safe, offline exploration of various configurations and generation of diverse, unseen scenarios, empowering proactive, data-driven exploration and optimization for xG networks. Additionally, GAI’s scalability makes it ideal for large-scale xG networks. This paper surveys how GAI-based models unlock optimization opportunities in xG wireless networks. We begin by providing a review of GAI models and some of the major communication paradigms of xG (e.g., Sixth Generation) wireless networks. We then delve into exploring how GAI can be used to improve resource allocation and enhance overall network performance. Additionally, we briefly review the networking requirements for supporting GAI applications in xG wireless networks. The paper further discusses the key challenges and future research directions in leveraging GAI for network optimization. Finally, a case study demonstrates the application of a diffusion-based GAI model for load balancing, carrier aggregation, and backhauling optimization in non-terrestrial networks, a core technology of xG networks. This case study serves as a practical example of how the combination of reinforcement learning and GAI can be implemented to address real-world network optimization problems.","1553-877X","","10.1109/COMST.2025.3535554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10855897","Generative AI;xG wireless networks;data-driven optimization;resource allocation;network-assisted generative AI","Artificial intelligence;Optimization;Wireless networks;Reliability;6G mobile communication;Training;Semantics;Resource management;Reconfigurable intelligent surfaces;Decoding","","1","","","IEEE","28 Jan 2025","","","IEEE","IEEE Early Access Articles"
"SPICED+: Syntactical Bug Pattern Identification and Correction of Trojans in A/MS Circuits Using LLM-Enhanced Detection","J. Chaudhuri; D. Thapar; A. Chaudhuri; F. Firouzi; K. Chakrabarty","School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; NVIDIA Corporation, Santa Clara, CA, USA; School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,"21 Mar 2025","2025","33","4","1118","1131","Analog and mixed-signal (A/MS) integrated circuits (ICs) are crucial in modern electronics, playing key roles in signal processing, amplification, sensing, and power management. Many IC companies outsource manufacturing to third-party foundries, creating security risks such as syntactical bugs and stealthy analog Trojans. Traditional Trojan detection methods, including embedding circuit watermarks and hardware-based monitoring, impose significant area and power overheads while failing to effectively identify and localize the Trojans. To overcome these shortcomings, we present SPICED+, a software-based framework designed for syntactical bug pattern identification and the correction of Trojans in A/MS circuits, leveraging large language model (LLM)-enhanced detection. It uses LLM-aided techniques to detect, localize, and iteratively correct analog Trojans in SPICE netlists, without requiring explicit model training, and thus incurs zero area overhead. The framework leverages chain-of-thought reasoning and few-shot learning to guide the LLMs in understanding and applying anomaly detection rules, enabling accurate identification and correction of Trojan-impacted nodes. With the proposed method, we achieve an average Trojan coverage of 93.3%, average Trojan correction rate of 91.2%, and an average false-positive rate of 1.4%.","1557-9999","","10.1109/TVLSI.2025.3527382","National Science Foundation(grant numbers:CNS-2310142); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10843334","Anomaly detection;hardware security;large language models (LLMs);SPICE;Trojan horse","Trojan horses;Circuits;Computer bugs;SPICE;Codes;Prevention and mitigation;Hardware design languages;Syntactics;Fabrication;Watermarking","","","","34","IEEE","15 Jan 2025","","","IEEE","IEEE Journals"
"Collective Program Analysis","G. Upadhyaya; H. Rajan","Dept. of Comput. Sci., Iowa State Univ. Ames, Ames, IA, USA; Dept. of Comput. Sci., Iowa State Univ. Ames, Ames, IA, USA",2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),"2 Sep 2018","2018","","","620","631","Popularity of data-driven software engineering has led to an increasing demand on the infrastructures to support efficient execution of tasks that require deeper source code analysis. While task optimization and parallelization are the adopted solutions, other research directions are less explored. We present collective program analysis (CPA), a technique for scaling large scale source code analyses, especially those that make use of control and data flow analysis, by leveraging analysis specific similarity. Analysis specific similarity is about, whether two or more programs can be considered similar for a given analysis. The key idea of collective program analysis is to cluster programs based on analysis specific similarity, such that running the analysis on one candidate in each cluster is sufficient to produce the result for others. For determining analysis specific similarity and clustering analysis-equivalent programs, we use a sparse representation and a canonical labeling scheme. Our evaluation shows that for a variety of source code analyses on a large dataset of programs, substantial reduction in the analysis time can be achieved; on average a 69% reduction when compared to a baseline and on average a 36% reduction when compared to a prior technique. We also found that a large amount of analysis-equivalent programs exists in large datasets.","1558-1225","978-1-4503-5638-1","10.1145/3180155.3180252","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453131","Source code analysis;Clustering;Boa","Transfer functions;Cloning;Software engineering;Task analysis;Syntactics;Analytical models;Labeling","","2","","","","2 Sep 2018","","","IEEE","IEEE Conferences"
"Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk","Z. Li; J. Hong; B. Li; Z. Wang","University of Texas, Austin; University of Texas, Austin; University of Illinois Urbana-Champaign; University of Texas, Austin",2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML),"10 May 2024","2024","","","18","32","While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that fine-tuning the pre-trained models with manipulated data can amplify the existing privacy risks. We demonstrate that S2L could occur in various standard fine-tuning strategies for diffusion models, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by 5.4% (absolute difference) AUC and can increase extracted private samples from almost 0 samples to 16.3 samples on average per target domain. This discovery underscores that the privacy risk with diffusion models is even more severe than previously recognized. Codes are available at https://github.com/VITA-Group/Shake-to-Leak.","","979-8-3503-4950-4","10.1109/SaTML59370.2024.00010","National Science Foundation; National Science Foundation; National Aeronautics and Space Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516655","Deep learning;generative models;diffusion models;privacy risk;fine-tuning","Training;Privacy;Differential privacy;Systematics;Codes;Image synthesis;Oral communication","","3","","36","IEEE","10 May 2024","","","IEEE","IEEE Conferences"
"Ethical Aspects of ChatGPT in Software Engineering Research","M. A. Akbar; A. A. Khan; P. Liang","Software Engineering Department, Lappeenranta-Lahti University of Technology, Lappeenranta, Finland; M3S Empirical Software Engineering Research Unit, University of Oulu, Oulu, Finland; School of Computer Science, Wuhan University, Wuhan, China",IEEE Transactions on Artificial Intelligence,"3 Mar 2025","2025","6","2","254","267","ChatGPT can improve software engineering (SE) research practices by offering efficient, accessible information analysis, and synthesis based on natural language interactions. However, ChatGPT could bring ethical challenges, encompassing plagiarism, privacy, data security, and the risk of generating biased or potentially detrimental data. This research aims to fill the given gap by elaborating on the key elements: motivators, demotivators, and ethical principles of using ChatGPT in SE research. To achieve this objective, we conducted a literature survey, identified the mentioned elements, and presented their relationships by developing a taxonomy. Furthermore, the identified literature-based elements (motivators, demotivators, and ethical principles) were empirically evaluated by conducting a comprehensive questionnaire-based survey involving SE researchers. In addition, we employed an interpretive structure modeling approach to analyze the relationships between the ethical principles of using ChatGPT in SE research and develop a level-based decision model. We further conducted a cross-impact matrix multiplication applied to classification analysis to create a cluster-based decision model. These models aim to help SE researchers devise effective strategies for ethically integrating ChatGPT into SE research by following the identified principles by adopting the motivators and addressing the demotivators. The findings of this study will establish a benchmark for incorporating ChatGPT services in SE research with an emphasis on ethical considerations.","2691-4581","","10.1109/TAI.2023.3318183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261269","ChatGPT;demotivators;ethical principles;motivators;software engineering (SE)","Chatbots;Surveys;Ethics;Software engineering;Data models;Artificial intelligence;Sociology","","10","","70","IEEE","22 Sep 2023","","","IEEE","IEEE Journals"
"Generative Adversarial Networks-based Privacy-Preserving 3D Reconstruction","Q. Li; Z. Zheng; F. Wu; G. Chen","Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, China; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, China; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, China; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, China",2020 IEEE/ACM 28th International Symposium on Quality of Service (IWQoS),"6 Oct 2020","2020","","","1","10","A large-scale image collection is crucial to the success of 3D reconstruction. Crowdsourcing, as a new pattern, can be utilized to collect high-quality images in an efficient way. However, the sensitive information in images may be exposed during the image transmission process. The general privacy policies perhaps will cause the loss or change of critical information, which may give rise to a decline in the performance of 3D reconstruction. Hence, how to achieve image privacy-preserving while guaranteeing to reconstruct a complete 3D model is important and significant. In this paper, we propose PicPrivacy to address this problem, which consists of three parts. (1) Using a pre-trained deep convolution neural network to segment sensitive information and erase it from images. (2) Using a GAN-based image feature completion algorithm to repair blank regions and minimize the absolute information gap between generated images and raw ones. (3) Taking generated images as the input of 3D reconstruction and using a structure-from-motion algorithm to reconstruct 3D models. Finally, we extensively evaluate the performance of PicPrivacy on realworld datasets. The results demonstrate that PicPrivacy not only achieves individual privacy-preserving but also can guarantee to create complete 3D models.","1548-615X","978-1-7281-6887-6","10.1109/IWQoS49365.2020.9213037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9213037","3D reconstruction;Privacy-preserving;Generative adversarial networks;Crowdsourcing","Three-dimensional displays;Privacy;Image segmentation;Solid modeling;Image reconstruction;Crowdsourcing;Convolution","","4","","39","IEEE","6 Oct 2020","","","IEEE","IEEE Conferences"
"Automated Consistency Analysis of LLMs","A. Patwardhan; V. Vaidya; A. Kundu","Department of Computer Science, Stony Brook University, Stony Brook, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Cisco Research, San Jose, USA","2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)","16 Jan 2025","2024","","","118","127","Generative AI (Gen AI) with large language models (LLMs) are being widely adopted across the industry, academia and government. Cybersecurity is one of the key sectors where LLMs can be and/or are already being used. There are a number of problems that inhibit the adoption of trustworthy Gen AI and LLMs in cybersecurity and such other critical areas. One of the key challenge to the trustworthiness and reliability of LLMs is: how consistent an LLM is in its responses?In this paper, we have analyzed and developed a formal definition of consistency of responses of LLMs. We have formally defined what is consistency of responses and then develop a framework for consistency evaluation. The paper proposes two approaches to validate consistency: self-validation, and validation across multiple LLMs. We have carried out extensive experiments for several LLMs such as GPT4oMini, GPT3.5, Gemini, Cohere, and Llama3, on a security benchmark consisting of several cybersecurity questions: informational and situational. Our experiments corroborate the fact that even though these LLMs are being considered and/or already being used for several cybersecurity tasks today, they are often inconsistent in their responses, and thus are untrustworthy and unreliable for cybersecurity.","","979-8-3503-8674-5","10.1109/TPS-ISA62245.2024.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10835426","Cybersecurity;Generative AI;Large Language Models;Agents;Consistency;Trustworthiness;Validity;Reliability;Hallucination","Industries;Privacy;Analytical models;Generative AI;Large language models;Government;Benchmark testing;Reliability;Intelligent systems;Computer crime","","","","39","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"One-Shot Information Hiding","Y. Liu; C. T. Li","Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong, China",2024 IEEE Information Theory Workshop (ITW),"30 Dec 2024","2024","","","169","174","We present a one-shot information-theoretic analysis of the information hiding problem, which has a wide range of applications including watermarking, fingerprinting, steganogra-phy and copyright protection. The problem can be viewed as a game: one party includes an information hider and a decoder, where the former embeds a message into a host data source and introduces some tolerable distortion, and the latter wishes to reconstruct the message; another party is an attacker that is modeled as a noisy channel which aims at removing the hidden information. We derive a one-shot achievability result using the Poisson matching lemma. Unlike previous asymptotic results, our result applies to any distribution of the host data, and any class of attack channels (not necessarily memoryless or ergodic).","2475-4218","979-8-3503-4893-4","10.1109/ITW61385.2024.10806963","CUHK(grant numbers:14209823 (GRF)]); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10806963","Information hiding;one-shot achievability;finite blocklength;network information theory;watermarking","Soft sensors;Conferences;Watermarking;Games;Fingerprint recognition;Copyright protection;Distortion;Data models;Decoding;Noise measurement","","","","47","IEEE","30 Dec 2024","","","IEEE","IEEE Conferences"
"Towards Discovery and Attribution of Open-world GAN Generated Images","S. Girish; S. Suri; S. Rambhatla; A. Shrivastava","University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park",2021 IEEE/CVF International Conference on Computer Vision (ICCV),"28 Feb 2022","2021","","","14074","14083","With the recent progress in Generative Adversarial Networks (GANs), it is imperative for media and visual forensics to develop detectors which can identify and attribute images to the model generating them. Existing works have shown to attribute images to their corresponding GAN sources with high accuracy. However, these works are limited to a closed set scenario, failing to generalize to GANs unseen during train time and are therefore, not scalable with a steady influx of new GANs. We present an iterative algorithm for discovering images generated from previously unseen GANs by exploiting the fact that all GANs leave distinct fingerprints on their generated images. Our algorithm consists of multiple components including network training, out-of-distribution detection, clustering, merge and refine steps. Through extensive experiments, we show that our algorithm discovers unseen GANs with high accuracy and also generalizes to GANs trained on unseen real datasets. We additionally apply our algorithm to attribution and discovery of GANs in an online fashion as well as to the more standard task of real/fake detection. Our experiments demonstrate the effectiveness of our approach to discover new GANs and can be used in an open-world setup.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710737","Image and video manipulation detection and integrity methods;Representation learning;Transfer/Low-shot/Semi/Unsupervised Learning","Training;Visualization;Pipelines;Clustering algorithms;Fingerprint recognition;Media;Generative adversarial networks","","21","","59","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Topic Modeling for Assessment of Text Complexity in Russian Textbooks","A. Sakhovskiy; V. Solovyev; M. Solnyshkina","Research laboratory ‘Intellectual technologies for text management’, Kazan Federal University, Kazan, Russia; Research laboratory ‘Intellectual technologies for text management’, Kazan Federal University, Kazan, Russia; Dept. of theory and practice of language teaching, Research laboratory ‘Intellectual technologies for text management’, Kazan Federal University, Kazan, Russia",2020 Ivannikov Ispras Open Conference (ISPRAS),"12 Apr 2021","2020","","","102","108","This article explores the problem of assessing the complexity of Russian educational texts. We present a quantitative and qualitative investigation of topics in the textbooks of different grades by applying several topic models. The corpus compiled for the current study comprises a collection of textbooks used in middle and high schools of the Russian Federation. The corpus consists of two sets of texts derived from textbooks on Social science by L.N. Bogolyubov and A.F. Nikitin. For our analysis, we employ Latent Dirichlet Allocation (LDA) and Additive regularization of topic models (ARTM). In this paper, we use the grades of educational texts as indicators of text complexity. To identify quantitative measures of text complexity, we analyze the correlation of book grades with topic properties. We also analyze a correlation of a topic's properties with the growth of its proportion in a book of a specific complexity level. The key finding of our research is the identification of statistically significant correlations of educational texts complexity with the revealed topic features, i.e., topic coherence and distance between topic words in a semantic space. This study can be beneficial for two communities: textbook writers and their consumers, i.e., teachers and schoolchildren.","","978-1-6654-1291-9","10.1109/ISPRAS51486.2020.00022","Russian Science Foundation(grant numbers:18-18-00436); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9394151","Textbooks;Social science;Text complexity;Topic modeling;Latent Dirichlet Allocation;Spearman's correlation","Analytical models;Correlation;Social sciences;Semantics;Coherence;Complexity theory;Resource management","","2","","31","IEEE","12 Apr 2021","","","IEEE","IEEE Conferences"
"Enhancing Database Encryption: Adaptive Measures for Digital Assets Against LLMs-Based Reverse Engineering","K. Zhou; J. Qiu; Y. Wang; X. Ye","Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Academy of Military Science, Beijing, China; Tsinghua University, Beijing, China",2024 Annual Computer Security Applications Conference (ACSAC),"18 Mar 2025","2024","","","1","14","With the development of large language models (LLMs) technology, generative AI (GAI) has significantly lowered the barriers to software reverse engineering attacks. Traditional database system security controls, face new challenges when confronted with the powerful analytical capabilities of GAI. This paper provides a detailed demonstration of the reverse engineering of database cryptographic functions using GAI tools. It finds that existing encryption mechanisms in embedded DBSs need new approaches to prevent internal data attacks. The main contributions are in two aspects. First, by emulating human reverse engineering and encrypted data theft behaviors, it demonstrates how to perform software reverse engineering using GAI tools. It reveals that LLMs can accelerate analysts in obtaining the keys needed for decryption or in identifying cache extraction points for decrypted data, enabling attackers to access sensitive information. Second, to counteract the strong code generation and analytical capabilities of LLMs, the paper proposes a solution based on random data blocks and timestamp-based decryption mechanisms. This approach aims to increase the cost for attackers using GAI tools to perform software reverse engineering and exploit potential vulnerabilities in database systems.","2576-9103","979-8-3315-2088-5","10.1109/ACSAC63791.2024.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10917597","Software Vulnerability Analysis;Large Language Models;Software Reverse Engineering;Database Encryption","Costs;Generative AI;Large language models;Reverse engineering;Software;Database systems;Encryption;Software measurement;Data mining;Faces","","","","27","IEEE","18 Mar 2025","","","IEEE","IEEE Conferences"
"AI Asyet Another Tool in Undergraduate Student Projects: Preliminary Results","I. J. Pérez-Colado; M. Freire-Morán; A. Calvo-Morata; V. M. Pérez-Colado; B. Fernández-Manjón","Departamento de Ingeniería del Software e Inteligencia Artificial U. Complutense de Madrid, Madrid, Spain; Departamento de Ingeniería del Software e Inteligencia Artificial U. Complutense de Madrid, Madrid, Spain; Departamento de Ingeniería del Software e Inteligencia Artificial U. Complutense de Madrid, Madrid, Spain; Nord University BodØ, Norway; Departamento de Ingeniería del Software e Inteligencia Artificial U. Complutense de Madrid, Madrid, Spain",2024 IEEE Global Engineering Education Conference (EDUCON),"8 Jul 2024","2024","","","1","7","How do students use artificial intelligence tools in coursework projects when given the liberty to do so, with the only requirement of documenting how, where and why? We describe experiences with two groups of undergraduates in courses related to serious game authoring and human-computer interaction, both carried out in the second semester of 2023. In the serious games course, students were given the option of following a teacher-developed methodology for generating graphical assets for their serious games using a set of generative AI tools. This methodology was explained in the class but not hands on lab was carried out. In the interaction course, students were free to choose which AI tools to use when designing their system or in the development of their project documentation. Despite the limited number of participants (41 in total) we can see very different views and degrees of involvement: while some tried to use AI for as many tasks as possible, others considered that the learning curve for those tools was too steep to be worthwhile. Both experiences included a free-text survey at the end, and taken together, provide insights into how both supervised and unsupervised generative AI use could impact undergraduate projects in similar subjects. In addition to describing how students chose to use the tools, and the main takeaways from their survey response, we also discuss some of the ethical aspects about the access to the tools and what should be the minimal conditions to be met to allow the equitable use of AI in the classroom.","2165-9567","979-8-3503-9402-3","10.1109/EDUCON60312.2024.10578883","Ministry of Education(grant numbers:PID2020-119620RB-100); European Commission(grant numbers:2021-1-SE0l-KA220-SCH-000023932); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578883","AI in education;generative artificial intelligence;game development;serious games authoring;goal-driven design","Surveys;Human computer interaction;Generative AI;Games;Documentation;Task analysis;Engineering education","","1","","13","IEEE","8 Jul 2024","","","IEEE","IEEE Conferences"
"Watermarking Neural Networks With Watermarked Images","H. Wu; G. Liu; Y. Yao; X. Zhang","School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China",IEEE Transactions on Circuits and Systems for Video Technology,"1 Jul 2021","2021","31","7","2591","2601","Watermarking neural networks is a quite important means to protect the intellectual property (IP) of neural networks. In this paper, we introduce a novel digital watermarking framework suitable for deep neural networks that output images as the results, in which any image outputted from a watermarked neural network must contain a certain watermark. Here, the host neural network to be protected and a watermark-extraction network are trained together, so that, by optimizing a combined loss function, the trained neural network can accomplish the original task while embedding a watermark into the outputted images. This work is totally different from previous schemes carrying a watermark by network weights or classification labels of the trigger set. By detecting watermarks in the outputted images, this technique can be adopted to identify the ownership of the host network and find whether an image is generated from a certain neural network or not. We demonstrate that this technique is effective and robust on a variety of image processing tasks, including image colorization, super-resolution, image editing, semantic segmentation and so on.","1558-2205","","10.1109/TCSVT.2020.3030671","National Natural Science Foundation of China(grant numbers:61902235,U1636206,U1936214,61525203); Shanghai “Chen Guang” Project(grant numbers:19CG46); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9222304","Watermarking;neural networks;deep learning;image transformation;information hiding","Watermarking;Neural networks;Training;Task analysis;Media;IP networks;Intellectual property","","103","","56","IEEE","13 Oct 2020","","","IEEE","IEEE Journals"
"On Hardware Security Bug Code Fixes by Prompting Large Language Models","B. Ahmad; S. Thakur; B. Tan; R. Karri; H. Pearce","Department of Electrical and Computer Engineering, New York University Tandon School of Engineering, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, New York University Tandon School of Engineering, Brooklyn, NY, USA; Department of Electrical and Software Engineering, University of Calgary, Calgary, AB, Canada; Department of Electrical and Computer Engineering, New York University Tandon School of Engineering, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, University of New South Wales, Sydney, NSW, Australia",IEEE Transactions on Information Forensics and Security,"2 May 2024","2024","19","","4043","4057","Novel AI-based code-writing Large Language Models (LLMs) such as OpenAI’s Codex have demonstrated capabilities in many coding-adjacent domains. In this work, we consider how LLMs may be leveraged to automatically repair identified security-relevant bugs present in hardware designs by generating replacement code. We focus on bug repair in code written in Verilog. For this study, we curate a corpus of domain-representative hardware security bugs. We then design and implement a framework to quantitatively evaluate the performance of any LLM tasked with fixing the specified bugs. The framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the LLM. We show that an ensemble of LLMs can repair all fifteen of our benchmarks. This ensemble outperforms a state-of-the-art automated hardware bug repair tool on its own suite of bugs. These results show that LLMs have the ability to repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair tool.","1556-6021","","10.1109/TIFS.2024.3374558","Intel Corporation; Natural Sciences and Engineering Research Council of Canada (NSERC)(grant numbers:RGPIN-2022-03027); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10462177","Hardware security;large language models;bug repair","Maintenance engineering;Computer bugs;Codes;Hardware;Security;Software;Registers","","12","","38","IEEE","7 Mar 2024","","","IEEE","IEEE Journals"
"Towards Safe, Secure, and Usable LLMs4Code","A. Al-Kaswan","Delft University of Technology, Delft, The Netherlands",2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"20 Jun 2024","2024","","","258","260","Large Language Models (LLMs) are gaining popularity in the field of Natural Language Processing (NLP) due to their remarkable accuracy in various NLP tasks. LLMs designed for coding are trained on massive datasets, which enables them to learn the structure and syntax of programming languages. These datasets are scraped from the web and LLMs memorise information in these datasets. LLMs for code are also growing, making them more challenging to execute and making users increasingly reliant on external infrastructure. We aim to explore the challenges faced by LLMs for code and propose techniques to measure and prevent memorisation. Additionally, we suggest methods to compress models and run them locally on consumer hardware.","2574-1934","979-8-4007-0502-1","10.1145/3639478.3639803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554981","large language models;privacy;memorisation;data leakage;compression","Computer languages;Codes;Accuracy;Syntactics;Natural language processing;Hardware;Encoding","","","","42","CCBY","20 Jun 2024","","","IEEE","IEEE Conferences"
"Advanced Media-Based Smart Big Data on Intelligent Cloud Systems","K. E. Psannis; C. Stergiou; B. B. Gupta","Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece; Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece; National Institute of Technology Kurukshetra, Kurukshetra, Haryana, India",IEEE Transactions on Sustainable Computing,"7 Mar 2019","2019","4","1","77","87","Today's advanced media technology preaches an enthralling time that will enormously bear on daily life. Moreover, the rapid raise of wireless communications and networking will ultimately bring advanced media to our lives anytime, anywhere, and on any device. According to the National Institute of Standards and Technology (NIST), Cloud Computing (CC) is a scheme for enabling convenient, on-demand network access to a shared pool of configurable computing pores (for example networks, applications, storage, servers, and services) which could be promptly foresighted and delivered with minimal management effort or service provider interaction. This paper proposed an efficient algorithm for advanced scalable Media-basedSmart Big Data (3D, Ultra HD) on Intelligent Cloud Computing systems. The proposed encoding algorithmoutperforms the conventional HEVC standard which demonstrated by the performance evaluations. In order to ratify the proposed approach, in addition, a relative study has been carried out. The proposed method could be used and integrated into HEVC, as a Smart Big Data, without violating the standard.","2377-3782","","10.1109/TSUSC.2018.2817043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8320864","Algorithm;HEVC media;3D;ultra HD;big data delivery;cloud computing","Video coding;Standards;Cloud computing;Encoding;Big Data;Transform coding;Complexity theory","","82","","72","IEEE","21 Mar 2018","","","IEEE","IEEE Journals"
"Digital Watermarking—A Meta-Survey and Techniques for Fake News Detection","A. Malanowska; W. Mazurczyk; T. K. Araghi; D. Megías; M. Kuribayashi","Institute of Computer Science, Warsaw University of Technology, Warsaw, Poland; Institute of Computer Science, Warsaw University of Technology, Warsaw, Poland; Internet Interdisciplinary Institute (IN3), Center for Cybersecurity Research of Catalonia (CYBERCAT), Universitat Oberta de Catalunya, Barcelona, Spain; Internet Interdisciplinary Institute (IN3), Center for Cybersecurity Research of Catalonia (CYBERCAT), Universitat Oberta de Catalunya, Barcelona, Spain; Center for Data-Driven Science and Artificial Intelligence, Tohoku University, Sendai, Japan",IEEE Access,"13 Mar 2024","2024","12","","36311","36345","During the past few decades, research on digital media watermarking–initially designed for digital images with the envisioned applications of copyright protection or copy control– has significantly evolved with respect to other covers (i.e., video, audio, speech) and many more potential applications, including tamper detection, broadcast monitoring, and, more recently, fake news detection. As a result, various surveys have tried to summarize certain aspects of this research field as it has grown. This has led to more than 130 survey papers being written at different points in time, describing various parts of the scientific efforts focused on digital media watermarking. Considering the above, the aim of this paper is twofold. First, we conduct a meta-survey based on 64 selected research works, in order to summarize the most notable survey papers in this field, which allows us to “draw a map” of this research area. Second, we focus on providing the requirements for digital watermarking techniques when applied to their most recent application: detecting fake news in multimedia content. Finally, an outline of the approach taken within the DISSIMILAR (Detection of fake newS on SocIal MedIa pLAtfoRms) project for the detection of disinformation is presented.","2169-3536","","10.1109/ACCESS.2024.3374201","European Interest Group (EIG) Concert Japan Call to the Project Detection of fake newS on SocIal MedIa pLAtfoRms (DISSIMILAR) through Spanish Government(grant numbers:PCI2020-120689-2); National Centre for Research and Development, Poland(grant numbers:EIG CONCERT-JAPAN/05/2021); Japan Science and Technology Agency, Japan(grant numbers:JPMJSC20C3); “SECURING” Project from Ministerio de Ciencia e Innovación, la Agencia Estatal de Investigación, and the European Regional Development Fund (ERDF)(grant numbers:PID2021-125962OB-C31); ARTEMISA International Chair of Cybersecurity and the DANGER Strategic Project of Cybersecurity; Spanish National Institute of Cybersecurity through the European Union (EU)—NextGenerationEU and the Recovery, Transformation and Resilience Plan; JSPS KAKENHI(grant numbers:22K19777); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10460548","Digital watermarking;fake news detection;information hiding;meta-survey;signal processing","Watermarking;Robustness;Transforms;Surveys;Fake news;Feature extraction;Copyright protection;Digital systems","","1","","118","CCBYNCND","5 Mar 2024","","","IEEE","IEEE Journals"
"Accelerating IoT Development with ChatGPT: A practical guide to building your first IoT project using AI-assisted coding and cloud integration","J. Wen",NA,Accelerating IoT Development with ChatGPT: A practical guide to building your first IoT project using AI-assisted coding and cloud integration,"","2024","","","","","Build cutting-edge projects with ChatGPT, PlatformIO, ESP32, and Arduino-compatible sensors by integrating AWS Cloud and the ThingsBoard dashboardKey FeaturesLeverage ChatGPT to generate code on ESP32 for sending sensor data to AWS CloudCreate your own visualization dashboard on ThingsBoard CloudFollow step-by-step configuration guidance to ingest, process, store, and query data on AWS CloudPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionUnlike other IoT books that focus on theory and generic applications, this guide takes a practical approach, empowering you to leverage ChatGPT to build your very first IoT prototype. With over 20 years of experience in wireless and IoT technologies and a background as an instructor, Jun Wen expertly guides you from project kick-off to a fully functional prototype. The book emphasizes the transformative impact of ChatGPT for IoT, teaching you how to use ChatGPT to generate code for your applications, even with limited coding experience. You’ll be introduced to using PlatformIO IDE within Visual Studio Code and discover the cutting-edge RISC-V architecture, the ESP32 MCU, Arduino-compatible sensors, and integration methods for AWS and the ThingsBoard dashboard. Working through 10 different project examples, including flame detection, smoke detection, and air quality measurement, you’ll become proficient in the functions and specifications of each sensor and the use cases they solve. By the end of this book, you’ll be ready to undertake IoT development projects, bridging the gap between your ideas and functional creations.What you will learnMaster IoT essentials, such as networks, end devices, wireless connectivity, and the cloudExplore the ChatGPT prompting framework and build crucial skills for IoT projectsDiscover best practices for building robust IoT hardware prototypesFind out how to set up Visual Studio Code and PlatformIO IDEConnect ESP32 to AWS through TLS and MQTTExplore popular connectivity technologies widely adopted in IoTIntegrate IoT sensors with ESP32 to capture accurate data using ChatGPT's assistanceWho this book is forIf you’re a beginner interested in applying IoT technology to your projects but face challenges due to limited experience in embedded software coding, specifically in C and C++, this book is for you. Whether you’re a student, hardware hobbyist, DIY enthusiast, IoT developer, or professional from a non-technical background, if you feel that your ability to innovate is often stalled by the complexity of software coding, this easy-to-follow guide to using ChatGPT for generating example code will boost your IoT prototype development.","","9781835467879","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769324.pdf&bkn=10769323&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Automating Security Detection Engineering: A hands-on guide to implementing Detection as Code","D. Chow; D. Bruskin",NA; NA,Automating Security Detection Engineering: A hands-on guide to implementing Detection as Code,"","2024","","","","","Accelerate security detection development with AI-enabled technical solutions using threat-informed defenseKey FeaturesCreate automated CI/CD pipelines for testing and implementing threat detection use casesApply implementation strategies to optimize the adoption of automated work streamsUse a variety of enterprise-grade tools and APIs to bolster your detection programPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionToday's global enterprise security programs grapple with constantly evolving threats. Even though the industry has released abundant security tools, most of which are equipped with APIs for integrations, they lack a rapid detection development work stream. This book arms you with the skills you need to automate the development, testing, and monitoring of detection-based use cases. You’ll start with the technical architecture, exploring where automation is conducive throughout the detection use case lifecycle. With the help of hands-on labs, you’ll learn how to utilize threat-informed defense artifacts and then progress to creating advanced AI-powered CI/CD pipelines to bolster your Detection as Code practices. Along the way, you'll develop custom code for EDRs, WAFs, SIEMs, CSPMs, RASPs, and NIDS. The book will also guide you in developing KPIs for program monitoring and cover collaboration mechanisms to operate the team with DevSecOps principles. Finally, you'll be able to customize a Detection as Code program that fits your organization's needs. By the end of the book, you'll have gained the expertise to automate nearly the entire use case development lifecycle for any enterprise.What you will learnUnderstand the architecture of Detection as Code implementationsDevelop custom test functions using Python and TerraformLeverage common tools like GitHub and Python 3.x to create detection-focused CI/CD pipelinesIntegrate cutting-edge technology and operational patterns to further refine program efficacyApply monitoring techniques to continuously assess use case healthCreate, structure, and commit detections to a code repositoryWho this book is forThis book is for security engineers and analysts responsible for the day-to-day tasks of developing and implementing new detections at scale. If you’re working with existing programs focused on threat detection, you’ll also find this book helpful. Prior knowledge of DevSecOps, hands-on experience with any programming or scripting languages, and familiarity with common security practices and tools are recommended for an optimal learning experience.","","9781837631421","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769220.pdf&bkn=10769219&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Semisupervised Domain Adaptation for Wafer Map Defect Recognition via Cross-Alignment Network","Y. Wang; P. Wu; S. Fang; B. Cao; X. Wu; X. Lu; Z. Yi","Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; School of Computer Science, Guangdong Polytechnic Normal University, Guangzhou, China; Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China",IEEE Transactions on Instrumentation and Measurement,"17 Mar 2025","2025","74","","1","13","Wafer map defect recognition is a crucial step in semiconductor manufacturing and measurement. Semisupervised domain adaptation (SSDA) aims to leverage abundant labeled data in the source domain to achieve accurate classification in the target domain with limited labeled data, effectively addressing the challenge of insufficient labels in wafer map datasets. However, a significant obstacle lies in the label mismatch among wafer map datasets, a problem that most current SSDA methods struggle to mitigate. To solve this problem, this article proposes a new SSDA algorithm named cross-alignment network (CAN). CAN decouples the feature subspace and label space using different encoders and classifiers. To facilitate knowledge transfer between the source and target domains, we propose cross-alignment learning (CAL). CAL consists of sample-wise cross-alignment and class-wise cross-alignment, designed to ensure the consistency of encoders’ outputs and facilitate the formation of robust clusters. We also integrate consistency regularization to enforce uniformity in outputs across different views of a sample. Additionally, we introduce a dynamic threshold strategy that tailors thresholds for each class based on its learning effect, thereby enhancing the model’s efficiency and data utilization. Empirically, our method outperforms the current state-of-the-art SSDA methods on public wafer map datasets. Furthermore, detailed experiments are conducted to verify the effectiveness of each component in CAN.","1557-9662","","10.1109/TIM.2025.3545980","National Natural Science Foundation of China(grant numbers:62176067); Key-Area Research and Development Program of Guangdong Province(grant numbers:2023B0303020001); Scientific and Technological Planning Project of Guangzhou(grant numbers:2023B03J1378); Research Project of Guangdong Polytechnic Normal University(grant numbers:22GPNUZDJS14); Science and Technology Innovation Commission of Shenzhen(grant numbers:JCYJ20220818101205011,JSGGZD20220822095401004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10904900","Defect recognition;domain alignment;label mismatch;semiconductor manufacturing;semisupervised domain adaptation (SSDA);wafer map","Deep learning;Production;Semiconductor device modeling;Training;Knowledge transfer;Data models;Classification algorithms;Accuracy;Training data;Service robots","","","","42","IEEE","26 Feb 2025","","","IEEE","IEEE Journals"
"Deep Model Intellectual Property Protection via Deep Watermarking","J. Zhang; D. Chen; J. Liao; W. Zhang; H. Feng; G. Hua; N. Yu","School of Cyber Science and Security, University of Science and Technology of China, Hefei, Anhui, China; Microsoft Research, Redmond, WA, USA; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong; School of Cyber Science and Security, University of Science and Technology of China, Hefei, Anhui, China; Beijing Electronic Science and Technology Institute, Beijing, China; Wormpex AI Research LLC, WA, USA; School of Cyber Science and Security, University of Science and Technology of China, Hefei, Anhui, China",IEEE Transactions on Pattern Analysis and Machine Intelligence,"1 Jul 2022","2022","44","8","4005","4020","Despite the tremendous success, deep neural networks are exposed to serious IP infringement risks. Given a target deep model, if the attacker knows its full information, it can be easily stolen by fine-tuning. Even if only its output is accessible, a surrogate model can be trained through student-teacher learning by generating many input-output training pairs. Therefore, deep model IP protection is important and necessary. However, it is still seriously under-researched. In this work, we propose a new model watermarking framework for protecting deep networks trained for low-level computer vision or image processing tasks. Specifically, a special task-agnostic barrier is added after the target model, which embeds a unified and invisible watermark into its outputs. When the attacker trains one surrogate model by using the input-output pairs of the barrier target model, the hidden watermark will be learned and extracted afterwards. To enable watermarks from binary bits to high-resolution images, a deep invisible watermarking mechanism is designed. By jointly training the target model and watermark embedding, the extra barrier can even be absorbed into the target model. Through extensive experiments, we demonstrate the robustness of the proposed framework, which can resist attacks with different network structures and objective functions.","1939-3539","","10.1109/TPAMI.2021.3064850","NSFC(grant numbers:62072421,62002334); Exploration Fund Project of University of Science and Technology of China(grant numbers:YD3480002001); Fundamental Research Funds for the Central Universities(grant numbers:WK2100000011,WK5290000001); Research Grants Council of the Hong Kong(grant numbers:CityU 21209119); APRC(grant numbers:CityU,9610488); National Key R&D Program of China(grant numbers:2018AAA0101400); NSFC(grant numbers:61629301); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373945","Deep model IP protection;model watermarking;image processing","Watermarking;Computational modeling;Training;Task analysis;IP networks;Image processing;Media","","59","","68","IEEE","9 Mar 2021","","","IEEE","IEEE Journals"
"Visual Deepfake Detection: Review of Techniques, Tools, Limitations, and Future Prospects","N. Ur Rehman Ahmed; A. Badshah; H. Adeel; A. Tajammul; A. Daud; T. Alsahfi","Department of Computing, Hamdard University, Islamabad Campus, Islamabad, Pakistan; Department of Software Engineering, University of Sargodha, Sargodha, Pakistan; Department of Computing, Hamdard University, Islamabad Campus, Islamabad, Pakistan; U.S.-Pakistan Center for Advanced Studies in Water, Mehran University of Engineering and Technology, Jamshoro, Sindh, Pakistan; Faculty of Resilience, Rabdan Academy, Abu Dhabi, United Arab Emirates; Department of Information Systems and Technology, College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia",IEEE Access,"3 Jan 2025","2025","13","","1923","1961","In recent years, rapid advancements in deepfakes (incorporating Artificial Intelligence (AI), machine, and deep learning) have updated tools and techniques for manipulating multimedia. Though technology has primarily been utilized for beneficial purposes, such as education and entertainment, it is also used for malicious or unethical tasks to spread disinformation or ruin someone’s dignity, even if it encompasses harassing and blackmailing victims. Deepfakes refer to high-quality and realistic multimedia-manipulated content that has been digitally modified or synthetically generated. We conducted a systematic literature review of deepfake detection to offer an updated overview of existing research work that initially describes the widely accessible deepfake generation tools, classifications, and detection process. We highlighted recent techniques in visual deepfake detection based on the feature representations, grouped into four domains: spatial, temporal, frequency, and spatio-temporal, including their key features and limitations by providing details of existing datasets, together with the potentials of deepfake and its future directions. This study tried to add an updated repository of technological change in deepfake, which could help researchers to develop robust deepfake models.","2169-3536","","10.1109/ACCESS.2024.3523288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10816641","Deepfake detection;machine learning;deep learning;deepfake applications;deepfake datasets;deepfake generation tools","Deepfakes;Face recognition;Deep learning;Faces;Training;Generators;Decoding;Social networking (online);Generative adversarial networks;Feature extraction","","","","282","CCBY","27 Dec 2024","","","IEEE","IEEE Journals"
"CAED: A Comprehensive Android Emulator Detection Framework with Data Augmentation","R. Yan; W. Niu; Q. Hou; Y. Su; J. Gong; X. Zhang","Institute for Cyber Security, School of Computer Science and Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Institute for Cyber Security, School of Computer Science and Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; School of Cyber Science and Engineering, Shanghai Jiao Tong University, China; Institute for Cyber Security, School of Computer Science and Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Institute for Cyber Security, School of Computer Science and Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Institute for Cyber Security, School of Computer Science and Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China",IEEE Internet of Things Journal,"","2025","PP","99","1","1","Anti-emulation is crucial for Android and IoT security as it helps apps determine whether they are running on a real mobile device or in an emulation environment. This prevents apps from being analyzed, debugged, or reverse-engineered in emulators, ultimately stopping criminals from making illegal profits. Current emulator detection methods cannot balance accuracy, universality, robustness, and compatibility. Their universality is often hindered by limited data diversity and accessibility. To address these issues, we propose the Comprehensive Android Emulator Detection (CAED) framework. The Preprocessing Module of CAED collects and normalizes data from both phones and emulators. We propose the first data augmentation method for emulator detection, EDA-GAN, which is tailored to the characteristics of our data and effectively enhances data diversity. The classifier module MFBoost employs an adaptive imputation algorithm and multiple Classification and Regression Trees (CART) for precise classification. Experiments on 324 devices show that CAED improves detection rate by at least 12.5% and up to 44.71% over state-of-the-art (SOTA) methods. The EDA-GAN data augmentation method boosts classifier accuracy, achieving a performance of up to 99.62%. Additionally, CAED’s unique loss function and imputation algorithm enhance the robustness and compatibility of CAED, with a 24% smaller accuracy drop than other methods when features are modified or unavailable. This study presents the CAED framework as an effective solution for protecting apps against real-world security threats in Android and IoT environments.","2327-4662","","10.1109/JIOT.2025.3552426","National Natural Science Foundation of China(grant numbers:No. 62372086); Natural Science Foundation of Sichuan Province(grant numbers:No. 2025ZNSFSC0500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10935812","Emulator detection;roid;Anti-emulation;Generative Model;Security","Accuracy;Smart phones;Robustness;Data augmentation;Training;Internet of Things;Imputation;Generators;Generative adversarial networks;Classification algorithms","","","","","IEEE","24 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Exploring Graph Neural Backdoors in Vehicular Networks: Fundamentals, Methodologies, Applications, and Future Perspectives","X. Yang; G. Li; K. Zhou; J. Li; X. Lin; Y. Liu","Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong, China; Shanghai Jiao Tong University, Shanghai, China; NVIDIA, Santa Clara, CA, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA",IEEE Open Journal of Vehicular Technology,"","2025","PP","99","1","22","Advances in Graph Neural Networks (GNNs) have substantially enhanced Vehicular Networks (VNs) across primary domains, encompassing traffic forecasting and management, route optimization and algorithmic planning, and cooperative driving. Despite the boosts of the GNN for VNs, recent research has empirically demonstrated its potential vulnerability to backdoor attacks, wherein adversaries integrate triggers into inputs to manipulate GNNs to generate adversary-premeditated malicious outputs (e.g., misclassification of vehicle actions or traffic signals). This susceptibility is attributable to adversarial manipulation attacks targeting the training process of GNN-based VN systems. Although there is a rapid increase in research on GNN backdoors, systematic surveys within this domain remain lacking. To bridge this gap, we present the first survey dedicated to GNN backdoors. We start with outlining the fundamental definition of GNNs, followed by the detailed summarization and categorization of current GNN backdoors and countermeasures based on their technical features and application scenarios. Subsequently, an analysis of the applicability paradigms of GNN backdoors is conducted, and prospective research trends are presented. Unlike prior surveys on vision-centric backdoors, we uniquely investigate GNN-oriented backdoor attacks in VNs, which aims to explore attack surfaces across spatiotemporal vehicular graphs and provide insights to security research.","2644-1330","","10.1109/OJVT.2025.3550411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10921674","Vehicular networks;graph neural networks;backdoor attacks;backdoor defenses;backdoor applications;deep network security;adversarial learning","Graph neural networks;Training;Vehicle dynamics;Surveys;Roads;Data models;Predictive models;Optimization;Vehicle-to-everything;Vehicular and wireless technologies","","","","","CCBY","11 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Data Hiding With Deep Learning: A Survey Unifying Digital Watermarking and Steganography","Z. Wang; O. Byrnes; H. Wang; R. Sun; C. Ma; H. Chen; Q. Wu; M. Xue","CSIRO’s Data61, Marsfield, NSW, Australia; School of Computer and Mathematical Science, The University of Adelaide, Adelaide, SA, Australia; School of Computer and Mathematical Science, The University of Adelaide, Adelaide, SA, Australia; CSIRO’s Data61, Marsfield, NSW, Australia; School of Computer and Mathematical Science, The University of Adelaide, Adelaide, SA, Australia; School of Electrical & Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Computer and Mathematical Science, The University of Adelaide, Adelaide, SA, Australia; CSIRO’s Data61, Marsfield, NSW, Australia",IEEE Transactions on Computational Social Systems,"8 Dec 2023","2023","10","6","2985","2999","The advancement of secure communication and identity verification fields has significantly increased through the use of deep learning techniques for data hiding. By embedding information into a noise-tolerant signal, such as audio, video, or images, digital watermarking and steganography techniques can be used to protect sensitive intellectual property (IP) and enable confidential communication, ensuring that the information embedded is only accessible to authorized parties. This survey provides an overview of recent developments in deep learning techniques deployed for data hiding, categorized systematically according to model architectures and noise injection methods. In addition, potential future research directions that unite digital watermarking and steganography on software engineering to enhance security and mitigate risks are suggested and deliberated. This contribution furthers the creation of a more trustworthy digital world and advances responsible artificial intelligence (AI).","2329-924X","","10.1109/TCSS.2023.3268950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10123415","Artificial intelligence (AI);cybersecurity;software engineering","Media;Watermarking;Steganography;Deep learning;Data models;Surveys;Data mining;Artificial intelligence;Software engineering","","20","","63","IEEE","12 May 2023","","","IEEE","IEEE Journals"
"IEEE Standard for Health informatics--Point-of-care medical device communication - Part 10101: Nomenclature - Redline","",,IEEE Std 11073-10101-2019 (Revision of ISO/IEEE 11073-10101:2004) - Redline,"5 Dec 2019","2019","","","1","1529","Within the context of the ISO/IEEE 11073 family of standards for point-of-care (POC) and personal health devices (PHD) medical device communication (MDC), this standard provides the nomenclature that supports both the domain information model and service model components of the standards family, as well as the semantic content exchanged with medical devices. The nomenclature is specialized for patient vital signs information representation and medical device informatics, with major areas including concepts for electrocardiograph (ECG), haemodynamics, respiration, blood gas, urine, fluid-related metrics, and neurology, as well as specialized units of measurement, general device events, alarms, and body sites. The standard defines both the architecture and major components of the nomenclature, along with extensive definitions for each conceptual area.","","978-1-5044-6303-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924972","codes;IEEE 11073-10101;IHE PCD-01;independent living;information model;medical device communication;nomenclature;ontology;patient;personal health devices;PHD;POC;point-of-care;semantics;service model;terminology","IEEE Standards;Biomedical monitoring;Medical devices;Point of care;Bioinformatics","","","","","","5 Dec 2019","","","IEEE","IEEE Standards"
"WateRF: Robust Watermarks in Radiance Fields for Protection of Copyrights","Y. Jang; D. I. Lee; M. Jang; J. W. Kim; F. Yang; S. Kim",Korea University; Korea University; Korea University; Korea University; Google Research; Korea University,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","12087","12097","The advances in the Neural Radiance Fields (NeRF) research offer extensive applications in diverse domains, but protecting their copyrights has not yet been researched in depth. Recently, NeRF watermarking has been considered one of the pivotal solutions for safely deploying NeRF-based 3D representations. However, existing methods are designed to apply only to implicit or explicit NeRF representations. In this work, we introduce an innovative watermarking method that can be employed in both representations of NeRF. This is achieved by fine-tuning NeRF to embed binary messages in the rendering process. In detail, we propose utilizing the discrete wavelet transform in the NeRF space for watermarking. Furthermore, we adopt a deferred back-propagation technique and introduce a combination with the patch-wise loss to improve rendering quality and bit accuracy with minimum trade-offs. We evaluate our method in three different aspects: capacity, invisibility, and robustness of the embedded watermarks in the 2D-rendered images. Our method achieves state-of-the-art performance with faster training speed over the compared state-of-the-art methods. Project page: https://kuai-lab.github.io/cvpr2024waterf/","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.01149","National Research Foundation of Korea(grant numbers:NRF-2022R1F1A1074334); IITP; MSIT(grant numbers:2019-0-00079); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10656760","NeRF;Watermark;CopyRights","Training;Solid modeling;Three-dimensional displays;Watermarking;Transforms;Neural radiance field;Rendering (computer graphics)","","2","","59","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"Improved Generative Steganography Based on Diffusion Model","Q. Zhou; P. Wei; Z. Qian; X. Zhang; S. Li","School of Computer Science Fudan University, Shanghai, China; National Pilot School of Software Yunnan University, Yunnan, China; School of Computer Science Fudan University, Shanghai, China; School of Computer Science Fudan University, Shanghai, China; School of Computer Science Fudan University, Shanghai, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","The rapid growth of generative models has led to a new direction in steganography called generative steganography (GS). It allows message-to-image generation without the need for a carrier image. Recently, generative steganography methods have been proposed using generative adversarial networks (GANs) and Flow models. On the one hand, methods that use GANs to generate stego images struggle to fully recover the hidden message because the networks are not reversible. On the other hand, methods based on Flow encounter a problem where the images they create might not look real, mainly because the network has limitations in being reversible. Diffusion models fulfill network reversibility while generating high-quality images. However, the framework of existing diffusion models is reversible, but hidden message recovery is not perfectly reversible, resulting in the recovered message being similar but not exactly the same as the hidden one. Existing diffusion models are typically trained for one-directional image generation tasks, so they face some problems when dealing with bi-directional steganography tasks. If pre-trained diffusion models are directly used to generate stego images, exact secret data extraction through the diffusion process cannot be achieved. In this paper, we present an improved generative steganography based on the diffusion model (GSD), which conceals secret data in the frequency domain of random noise to enhance the security and accuracy of steganography, and re-trains the denoising diffusion implicit model (DDIM) for steganography, called the StegoDiffusion. During training StegoDiffusion, random noise is injected into the clean natural images and then trained through the forward diffusion process to obtain the re-trained StegoDiffusion. Our proposed GSD scheme achieves a 100% extraction accuracy for hidden secret data with a payload of 1 bit-per-pixel (bpp) in a single channel, and generates high-quality stego images in PNG format.","1558-2205","","10.1109/TCSVT.2025.3539832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10877890","Generative steganography;diffusion model;DDIM","Steganography;Diffusion models;Data mining;Accuracy;Distortion;Data models;Security;Generators;Training;Noise reduction","","","","","IEEE","7 Feb 2025","","","IEEE","IEEE Early Access Articles"
"IEEE Standard for Wireless Smart Utility Network Field Area Network (FAN)","",,IEEE Std 2857-2021,"24 Jun 2021","2021","","","1","182","This document describes a complete communications specification, encompassing layers 1 to 4 of the Open Systems Integration (OSI) network model, for a secure, wireless mesh communications network, using open standards communications and cybersecurity standards from standards organizations including Institute of Electrical and Electronics Engineers (IEEE) and Internet Engineering Task Force (IETF). The specification describes the functionality of the physical (PHY layer), medium access control (MAC layer), the network layer, transport layer and security parameters including certificate format for a highly scaleable and secure wireless mesh network for critical infrastructure ipv6 wireless communications networks.;This document describes a complete communications specification, encompassing layers 1 to 4 of the Open Systems Integration (OSI) network model, for a secure, wireless mesh communications network, using open standards communications and cybersecurity standards from standards organizations including Institute of Electrical and Electronics Engineers (IEEE) and Internet Engineering Task Force (IETF). The specification describes the functionality of the physical (PHY layer), medium access control (MAC layer), the network layer, transport layer and security parameters including certificate format for a highly scaleable and secure wireless mesh network for critical infrastructure ipv6 wireless communications networks.","","978-1-5044-7696-6","10.1109/IEEESTD.2021.9464888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9464888","adoption;FAN;field area network;IEEE 2857;Internet of Things;IoT;OSI;smart city;smart utility network;Wi-SUN;wireless mesh","IEEE Standards;Wireless communication;Wireless mesh networks;Physical layer;Computer security;Open systems;Internet;Critical infrastructure;Media Access Control","","4","","","","24 Jun 2021","","","IEEE","IEEE Standards"
"MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation","G. Wang; Y. Li; Y. Liu; G. Deng; T. Li; G. Xu; Y. Liu; H. Wang; K. Wang",Beijing University of Posts and Telecommunications; University of New South Wales; Nanyang Technological University; Nanyang Technological University; Nanyang Technological University; Beijing University of Posts and Telecommunications; Nanyang Technological University; Huazhong University of Science and Technology; Huazhong University of Science and Technology,2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge) Conference Acronym:,"30 Jul 2024","2024","","","12","23","Augmented generation techniques such as Retrieval-Augmented Generation (RAG) and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing large language model (LLM) outputs with external knowledge and cached information. However, the integration of vector databases, which serve as a backbone for these augmentations, introduces critical challenges, particularly in ensuring accurate vector matching. False vector matching in these databases can significantly compromise the integrity and reliability of LLM outputs, leading to misinformation or erroneous responses. Despite the crucial impact of these issues, there is a notable research gap in methods to effectively detect and address false vector matches in LLM-augmented generation. This paper presents MeTMaP, a metamorphic testing framework developed to identify false vector matching in LLM -augmented generation systems. We derive eight metamorphic relations (MRs) from six NLP datasets, which form our method's core, based on the idea that semantically similar texts should match and dissim-ilar ones should not. MeTMaP uses these MRs to create sentence triplets for testing, simulating real-world matching scenarios. Our evaluation of MeTMaP over 203 vector matching configurations, involving 29 embedding models and 7 distance metrics, uncovers significant inaccuracies. The results, showing a maximum accuracy of only 41.51% on our tests compared to the original datasets, em-phasize the widespread issue of false matches in vector matching methods and the critical need for effective detection and mitigation in LLM -augmented applications.","","979-8-4007-0536-6","10.1145/3650105.3652297","National Natural Science Foundation of China(grant numbers:62302176); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10599579","Metamorphic Testing;Vector Matching;Augmented Generation","Accuracy;Databases;Terminology;Prevention and mitigation;Semantics;Reliability engineering;Vectors","","2","","87","","30 Jul 2024","","","IEEE","IEEE Conferences"
"Imprints: Mitigating Watermark Removal Attacks With Defensive Watermarks","X. Chen; J. Deng; Y. Chen; C. Li; X. Fang; C. Liu; W. Xu","College of Electrical Engineering, Zhejiang University, Hangzhou, Zhejiang, China; College of Electrical Engineering, Zhejiang University, Hangzhou, Zhejiang, China; College of Electrical Engineering, Zhejiang University, Hangzhou, Zhejiang, China; College of Electrical Engineering, Zhejiang University, Hangzhou, Zhejiang, China; iFLYTEK Research Institute, Hefei, China; iFLYTEK Research Institute, Hefei, China; College of Electrical Engineering, Zhejiang University, Hangzhou, Zhejiang, China",IEEE Transactions on Information Forensics and Security,"14 Feb 2025","2025","20","","1866","1881","Watermark is essential for protecting the intellectual property of private images. However, a wide range of watermark removal attacks, especially many AI-powered ones, can automatically predict and remove watermarks, posing serious concerns. In this paper, we present the design of Imprints, a defensive watermarking framework that fortifies watermarks against watermark removal attacks. By formulating an optimization problem that deters watermark removal attacks, we design image-independent/dependent defensive watermark models for effective batch/customized protection. We further enhance the watermark to be transferable to unseen watermark removal attacks and robust to editing distortions. Extensive experiments verify that Imprints outperforms existing baselines in terms of its immunity to 8 state-of-the-art watermark removal attacks and 3 commercial black-box watermark removal software. The source code is available at https://github.com/Imprints-wm/Imprints.","1556-6021","","10.1109/TIFS.2025.3536299","Ant Group through the CCF-Ant Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10857354","Adversarial watermark;automatic watermark removal;removal resistance","Watermarking;Image restoration;Optimization;Robustness;Generative adversarial networks;Prediction algorithms;Distortion;Resistance;Protection;Vaccines","","","","46","IEEE","29 Jan 2025","","","IEEE","IEEE Journals"
"A Coverless Information Hiding Approach using the Most Significant Bit","S. Gautam; M. Kumar","Department of Computer Science, Babasaheb Bhimrao Ambedkar University, Lucknow, U.P., India; Department of Computer Science, Babasaheb Bhimrao Ambedkar University, Lucknow, U.P., India",2024 IEEE Students Conference on Engineering and Systems (SCES),"3 Sep 2024","2024","","","1","6","Demands for information hiding technology are high in the area of information security, as it enables secretive communication of confidential data. Traditionally, information hiding methods have relied on designating an original image and embedding the secret information into it to create a stego-image. However, this approach leaves behind modification traces in the cover image, making it susceptible to detection by steganalysis tools. A new method for coverless information hiding has been proposed to tackle this issue. Coverless information hiding eliminates the need for a specific cover image by choosing suitable original images from an established database that already contains the secret data. The proposed method extracts the hash sequence from the most significant bit (MSB) of the average of image blocks. The original image without modification is treated as stego image where hash sequences express the secret data. The proposed approach has proven effective in resisting steganalysis tools and demonstrating robustness against various image processing attacks, indicating potential for secure data transmission. The ultimate objectives of the proposed method are undetectability, robustness, and the hiding capacity of hidden data. The experimental results show that the proposed approach outperforms over the other existing state-of-the-art coverless information hiding methods in terms of robustness and accuracy.","","979-8-3503-7471-1","10.1109/SCES61914.2024.10652263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10652263","Security;Most significant bit;Image steganography;Coverless information hiding;Robust","Accuracy;Databases;Image processing;Information security;Generative adversarial networks;Robustness;Data mining","","","","18","IEEE","3 Sep 2024","","","IEEE","IEEE Conferences"
"Let the Chart Spark: Embedding Semantic Context into Chart with Text-to-Image Generative Model","S. Xiao; S. Huang; Y. Lin; Y. Ye; W. Zeng","Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China",IEEE Transactions on Visualization and Computer Graphics,"25 Dec 2023","2024","30","1","284","294","Pictorial visualization seamlessly integrates data and semantic context into visual representation, conveying complex information in an engaging and informative manner. Extensive studies have been devoted to developing authoring tools to simplify the creation of pictorial visualizations. However, mainstream works follow a retrieving-and-editing pipeline that heavily relies on retrieved visual elements from a dedicated corpus, which often compromise data integrity. Text-guided generation methods are emerging, but may have limited applicability due to their predefined entities. In this work, we propose ChartSpark, a novel system that embeds semantic context into chart based on text-to-image generative models. ChartSpark generates pictorial visualizations conditioned on both semantic context conveyed in textual inputs and data information embedded in plain charts. The method is generic for both foreground and background pictorial generation, satisfying the design practices identified from empirical research into existing pictorial visualizations. We further develop an interactive visual interface that integrates a text analyzer, editing module, and evaluation module to enable users to generate, modify, and assess pictorial visualizations. We experimentally demonstrate the usability of our tool, and conclude with a discussion of the potential of using text-to-image generative models combined with an interactive interface for visualization design.","1941-0506","","10.1109/TVCG.2023.3326913","National Natural Science Foundation of China(grant numbers:62172398); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296520","pictorial visualization;generative model;authoring tool","Visualization;Data visualization;Semantics;Authoring systems;Pipelines;Data models;Interviews","","7","","61","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"RPA-SCD: Rhythm and Pitch Aware Dual-Branch Network for Songs Conversion Detection","M. Du; H. Wang; R. Zhang; Z. Yan","School of Cyber Science and Engineering, Sichuan University, Chengdu, China; School of Cyber Science and Engineering, Sichuan University, Chengdu, China; School of Cyber Science and Engineering, Sichuan University, Chengdu, China; School of Cyber Science and Engineering, Sichuan University, Chengdu, China",2024 International Joint Conference on Neural Networks (IJCNN),"9 Sep 2024","2024","","","1","8","Song voice conversion tools have gained more and more popularity in the recent past. People have been uploading their self-made forgery songs on video websites, and these songs have been converted in timbre. However, singing voice conversion technology may cause copyright infringement of the songs. In order to protect the copyright of songs, the method of singing voice conversion detection needs to be investigated. We propose Rhythm and Pitch Aware Songs Conversion Detection (RPA-SCD), a dual-branch network for song voice conversion detection. RPA-SCD can predict forged song fragments through rhythm and pitch which are the global and local information of music. To evaluate the proposed method, we contribute a multilingual song conversion detection(MSCD) dataset. Our proposed model achieves the EER of 2.30% in the original domain of MSCD, which is lower than other benchmarks for speech forgery detection. The experiments show that our approach achieves state-of-the-art performance on the song conversion detection task. The MSCD dataset can be found at https://drive.google.com/file/d/1rFsvMYihVtk81uFbL7UpyUEs-qBgsX6H/view?usp=drive_link. The code can be found at https://github.com/Samantha-Du/RPA-SDD.","2161-4407","979-8-3503-5931-2","10.1109/IJCNN60899.2024.10651233","National Natural Science Foundation of China; Ministry of Education; Sichuan University; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10651233","audio forensics;songs conversion detection;transformer","Codes;Neural networks;Speech enhancement;Copyright protection;Benchmark testing;Rhythm;Forgery","","","","34","IEEE","9 Sep 2024","","","IEEE","IEEE Conferences"
"Deep Convolutional Q-Learning system and VGG-16 steganalysis detector for image Steganography","C. Anthony; S. Patel","Department of Computer Applications, St. Xavier’s College, Ahmedabad, India; Department of Computer Applications, Ganpat University, Kherva, India",2024 7th International Conference on Circuit Power and Computing Technologies (ICCPCT),"16 Sep 2024","2024","1","","924","930","Image steganography is the concealment of data (text, image, or video) within a cover image. The confidential information is concealed so that human eyes cannot see it. In recent years, deep learning has garnered popularity as a potent tool for various applications, including image steganography. Image steganography techniques are increasingly used for covert communication and information concealment because of the accelerated development of digital technology. This paper uses Deep Convolutional Generative Adversarial Networks (DCGANs) to incorporate classified data within cover images. Our proposed steganography method employs DCGANs to generate realistic-looking stego images invisible to the human eye while effectively concealing sensitive information. Moreover, we present a novel Deep Convolutional Q-Learning (DCQL) method for optimizing the embedding procedure, ensuring the utmost imperceptibility while preserving data integrity. We use a cutting-edge VGG-16-based steganalysis detector that employs deep learning algorithms to scrutinize suspicious images for hidden content to uncover these hidden signals. The VGG-16 architecture, with its established image classification abilities, is ideally adapted for the challenging task of steganalysis, which involves locating even the most subtly concealed information. Our experimental results demonstrate that the proposed method effectively conceals and detects steganography content. We evaluate the system's efficacy using various steganography techniques and cover image datasets, highlighting its robustness and adaptability. In addition, the DCQL procedure increases the invisibility of stego-images while preserving the integrity of the hidden data. The combination of DCGANs, DCQL, and VGG-16-based steganalysis detectors results in a comprehensive framework that improves the security and detection of image steganography, with promising applications in cyber security and digital forensics.","","979-8-3503-7281-6","10.1109/ICCPCT61902.2024.10673401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10673401","Image steganography;Deep learning;confidential information;Deep Convolutional Generative Adversarial Networks;Deep Convolutional Q-Learning;Steganalysis","Deep learning;Steganography;Q-learning;Program processors;Convolution;Detectors;Robustness","","","","21","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"Adapting Installation Instructions in Rapidly Evolving Software Ecosystems","H. Gao; C. Treude; M. Zahedi",NA; NA; NA,IEEE Transactions on Software Engineering,"","2025","PP","99","1","24","README files play an important role in providing installation-related instructions to software users and are widely used in open source software systems on platforms such as GitHub. Software projects evolve rapidly alongside their dependencies in dynamic software ecosystems, requiring frequent updates to installation instructions. These instructions are crucial for users to start with a software project. Despite their significance, there is a lack of systematic understanding regarding the documentation efforts invested in README files and the triggers behind them. To fill the research gap, we conducted a qualitative study, investigating 400 GitHub repositories with 1,163 README commits that focused on updates in installation-related sections. Our research revealed six major categories of changes in the README commits, namely pre-installation instructions, installation instructions, post-installation instructions, help information updates, document presentation, and external resource management. We further provide detailed insights into modification behaviours and offer examples of these updates. We also studied the triggers for the documentation updates, which led to three categories including errors in the previous documentation, changes in the codebase, and need for documentation improvement. Based on our findings, we proposed a README template tailored to cover the installation-related sections for documentation maintainers to reference when updating documents. We further validated this template by conducting an online survey and a pull request study, identifying that documentation readers find the augmented documents based on our template to be generally of better quality, and documentation maintainers find it useful. We further provide recommendations to practitioners for maintaining their README files, as well as motivations for future research directions. These recommendations encompass completeness, correctness and up-to-dateness, and information presentation considerations. The proposed research directions include the development of automated tools, in particular for documentation updates, and conducting empirical studies to enhance comprehension of the needs of documentation users.","1939-3520","","10.1109/TSE.2025.3552614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10931854","Software Documentation;README Files;Qualitative Analysis;Open Source Software;Installation Instructions;Software Ecosystem","Documentation;Software;Software development management;Ecosystems;Codes;Taxonomy;Surveys;Open source software;Data mining;Training","","","","","IEEE","18 Mar 2025","","","IEEE","IEEE Early Access Articles"
"FedITD: A Federated Parameter-Efficient Tuning With Pre-Trained Large Language Models and Transfer Learning Framework for Insider Threat Detection","Z. Qiang Wang; H. Wang; A. El Saddik","Multimedia Communications Research Laboratory (MCRLab), School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada; Multimedia Communications Research Laboratory (MCRLab), School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada; Multimedia Communications Research Laboratory (MCRLab), School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada",IEEE Access,"6 Nov 2024","2024","12","","160396","160417","Insider threats cause greater losses than external attacks, prompting organizations to invest in detection systems. However, there exist challenges: 1) Security and privacy concerns prevent data sharing, making it difficult to train robust models and identify new attacks. 2) The diversity and uniqueness of organizations require localized models, as a universal solution could be more effective. 3) High resource costs, delays, and data security concerns complicate building effective detection systems. This paper introduces FedITD, a flexible, hierarchy, and federated framework with local real-time detection systems, combining Large Language Models (LLM), Federated Learning (FL), Parameter Efficient Tuning (PETuning), and Transfer Learning (TF) for insider threat detection. FedITD uses FL to protect privacy while indirect integrating client information and employs PETuning methods (Adapter, BitFit, LoRA) with LLMs (BERT, RoBERTa, XLNet, DistilBERT) to reduce resource use and time delay. FedITD customizes client models and optimizes performance via transfer learning without central data transfer, further enhancing the detection of new attacks. FedITD outperforms other federated learning methods and its performance is very close to the best centrally trained method. Extensive experiment results show FedITD’s superior performance, adaptability to varied data, and reduction of resource costs, achieving an optimal balance in detection capabilities across source data, unlabeled local data, and global data. Alternative PETuning implementations are also explored in this paper.","2169-3536","","10.1109/ACCESS.2024.3482988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10721229","Cybersecurity;insider threat;deep learning;transformer;BERT;RoBERTa;XLNet;DistilBERT;GPT;data augmentation;artificial intelligence;machine learning;pre-trained LLM;PETuning;adapter;LoRA;BitFit;LLM;NLP","Data models;Adaptation models;Threat assessment;Tuning;Security;Organizations;Costs;Computational modeling;Transfer learning;Deep learning;Computer security;Data augmentation;Artificial intelligence;Machine learning","","","","63","CCBYNCND","17 Oct 2024","","","IEEE","IEEE Journals"
"A Survey on Trends in Visual Inspection Systems Toward Industry 5.0 – A Systematic Mapping Study","F. Lupi; A. D. Rocha; A. Maffei; P. Ferreira; J. Barata; M. Lanzetta","dept. of Information Engineering (DII), University of Pisa, Pisa, Italy; dept. of Electrical and Computer Engineering, NOVA University of Lisbon, 1099–085, Portugal; dept. of Production Engineering, KTH Royal Institute of Technology, Stockholm, Sweden; Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough, UK; dept. of Electrical and Computer Engineering, NOVA University of Lisbon, 1099–085, Portugal; dept. of Civil and Industrial Engineering (DICI), University of Pisa, Pisa, Italy","2024 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)","18 Dec 2024","2024","","","1","9","Over the past decades, significant developments have occurred in the manufacturing domain, culminating in the contemporary landscape of Industry 5.0 (I5.0). This era is defined by three primary dimensions: technical aspects, sustainability concerns, and human-centricity. Although the broader manufacturing field features a dispersed array of scientific literature reviews amidst this multidisciplinary transition, a focused examination of Visual Inspection Systems (VIS) within this context appears absent. This study aims to provide a comprehensive mapping study that navigates this complex and emerging area of research for VIS, incorporating both scientific publications and Intellectual Property (IP). Employing a systematic methodology for mapping review, an initial exploratory search was conducted, retrieving 264 documents. Following a systematic screening process, 46 documents were identified as relevant, underscoring the preliminary findings and prevailing trends in the field. Results shows that three main clusters emerged with rising interest in technological solutions. Additionally, this study offers a procedural approach to further investigate these promising exploratory findings and expand upon the current review.","2693-8855","979-8-3503-6243-5","10.1109/ICE/ITMC61926.2024.10794405","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10794405","Visual inspection;Industry 5.0;Sustainability;Human-centricity;Technological evolution","Surveys;Visualization;Technological innovation;Systematics;Reviews;Inspection;Market research;Manufacturing;Fifth Industrial Revolution;Sustainable development","","","","92","IEEE","18 Dec 2024","","","IEEE","IEEE Conferences"
"Using Generative AI to Implement UDL Principles in Traditional STEM Classrooms","M. Kalaigian; M. S. Thompson; J. VanLone; R. Nickel","Department of Electrical and Computer Engineering, Bucknell University, Lewisburg, PA, USA; Department of Electrical and Computer Engineering, Bucknell University, Lewisburg, PA, USA; Department of Education, Bucknell University, Lewisburg, PA, USA; Department of Electrical and Computer Engineering, Bucknell University, Lewisburg, PA, USA",2024 IEEE Frontiers in Education Conference (FIE),"26 Feb 2025","2024","","","1","9","This innovative practice full paper presents a guided approach to integrating universal design for learning (UDL) principles into an Explicit Instruction classroom. To address the common challenges of scope and additional time requirements of integrating UDL, we are using generative artificial intelligence (GAI) tools like ChatGPT because they have reached a level of functionality that allows a GAI tool to replace multiple specialized tools. This paper provides specific guidelines of where UDL interventions can be included in an explicit instruction lesson and how to use GAI to support them. This paper has multiple goals. First, we are focused on an approach that can help embed modern, best practices in traditional, higher ed STEM classes that use an approach that is typically close to the explicit instruction model. Second, we want to show how GAI can be used to implement UDL principles in classes in a way that does not require much additional instructor time and can help UDL adoption scale to larger classes. Example GAI prompts and “real” responses are provided for those who are unfamiliar with GAI tools, in order to help demonstrate the capabilities of the tools.","2377-634X","979-8-3503-5150-7","10.1109/FIE61694.2024.10893115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893115","","Generative AI;Chatbots;Best practices;Guidelines","","","","53","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"Squarespace from Signup to Launch: Build, customize, and launch robust and user-friendly Squarespace websites with a no-code approach","K. G. Kreiling; M. Ulaszek",NA; NA,"Squarespace from Signup to Launch: Build, customize, and launch robust and user-friendly Squarespace websites with a no-code approach","","2023","","","","","Packed with expert insights, practical tools, and a library of resources, this book teaches everything you need to know to build design focused, professional websites that deliver inspiring user experiences from signup to launch. Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesUse a hands-on approach and professional insights to design a custom website on Squarespace 7.1 and Fluid EngineLaunch a website on your domain with features like commerce, member areas, blogging and schedulingGrow your audience with a mobile-optimized website you can own, edit, and updateBook DescriptionYou've heard about Squarespace; maybe you've even started a trial site, but you haven't gotten around to actually launching it yet. It looks simple enough and feels like it should be easy, so why is it such a challenge? Author and Squarespace expert Kelsey Gilbert-Kreiling is here to help. Squarespace from Signup to Launch a comprehensive guide to customizing the most design-focused and user-friendly website builder in the no-code world. More than a technical manual, the book will help you prepare to build a website, explain the foundational knowledge behind Squarespace 7.1 and Fluid Engine, and introduce you to a professional designer's mindset. Readers will learn how to build forms, use content blocks, optimize websites for mobile, build an online store, and become comfortable with Squarespace's built-in SEO, marketing, and analytics tools. Learn from Squarespace experts Christy Price, Will Myers, David Iskander, Kristine Neil, Kathryn Joachim, Beatriz Caraballo, Justin Mabee, Shelly Price and more, with professional insights and resources in each chapter. By the end of the book, you will have gained the confidence needed to build professional Squarespace websites with the developer's technical knowledge, project organization, and design intuition. You won’t just launch your site; you’ll be proud to share it with the world.What you will learnBuild a website on Squarespace, step by step, with expert insights and practical tipsPlan your site content with an easy-to-understand outline Source and create the visual elements needed for a professional siteGo beyond pre-set templates to create a polished design from navigation to footerUse advanced tools such as commerce, scheduling, member areas, blogging and email campaignsIntegrate custom code to enhance the design and functionality of your projectOptimize your website for mobile view and search engine visibilityMarket your site and grow your audience after launchWho this book is forIf you are interested in building a website on the Squarespace platform, including its newest versions, 7.1 and Fluid Engine, then this book is for you. Maybe you’re a brand designer with little or no code experience or perhaps you’re an entrepreneur who needs a website you can own and use. You could even be a website designer versed in other platforms looking to expand your skills to a new tool. No matter your entry point, this book will offer an understanding of the why and how of Squarespace, preparing you to use it as a go-to practical guide.","","9781801818797","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251278.pdf&bkn=10251277&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Erasing Concepts from Diffusion Models","R. Gandikota; J. Materzyńska; J. Fiotto-Kaufman; D. Bau",Northeastern University; Massachusetts Institute of Technology; Northeastern University; Northeastern University,2023 IEEE/CVF International Conference on Computer Vision (ICCV),"15 Jan 2024","2023","","","2426","2436","Motivated by concerns that large-scale diffusion models can produce undesirable output such as sexually explicit content or copyrighted artistic styles, we study erasure of specific concepts from diffusion model weights. We propose a fine-tuning method that can erase a visual concept from a pre-trained diffusion model, given only the name of the style and using negative guidance as a teacher. We benchmark our method against previous approaches that remove sexually explicit content and demonstrate its effectiveness, performing on par with Safe Latent Diffusion and censored training. To evaluate artistic style removal, we conduct experiments erasing five modern artists from the network and conduct a user study to assess the human perception of the removed styles. Unlike previous methods, our approach can remove concepts from a diffusion model permanently rather than modifying the output at the inference time, so it cannot be circumvented even if a user has access to model weights. Our code, data, and results are available at erasing.baulab.info.","2380-7504","979-8-3503-0718-4","10.1109/ICCV51070.2023.00230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10378568","","Training;Visualization;Computer vision;Codes;Filtering;Interference;Benchmark testing","","40","","48","IEEE","15 Jan 2024","","","IEEE","IEEE Conferences"
"An Experimental Analysis of Forensics and Deepfakes","M. Pariki; S. Katakam; M. Srimathi","Computer Science and Engineering, Geethanjali College of Engineering and Technology, Cheeryal Village, Keesara Mandal Medchal District, Telangana, India; Computer Science and Engineering, Geethanjali College of Engineering and Technology, Cheeryal Village, Keesara Mandal Medchal District, Telangana, India; Computer Science and Engineering, Teegala Krishna Reddy Engineering College, Medbowli, Saroornagar, Hyderabad, India","2024 International Conference on Modeling, Simulation & Intelligent Computing (MoSICom)","18 Feb 2025","2024","","","473","478","The growing popularity of smartphones with high-resolution digital cameras, the development of deep learning AI platforms, and the accessibility of a large selection of software programs for shooting, editing, and sharing photos and videos have all contributed to the emergence of a new phenomenon known as “Deepfake.” In response, we have developed and put into use a deep-fake detection model (DFT-MF) that is centred on mouth features. Our algorithm examines and verifies lip/mouth motions in order to detect Deepfake films using deep learning techniques. Our DFT-MF model performed well in classification, according to experiments conducted on datasets containing both actual and fraudulent videos. This was especially true when compared to other methods that have been used in this domain previously.","","979-8-3315-3331-1","10.1109/MoSICom63082.2024.10881463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10881463","Digital Forensics;Manipulation;Detection;Classification;Segmentation;Python;Deep Learning;Videos;Deepfake","Deep learning;Performance evaluation;Deepfakes;Films;Computational modeling;Biological system modeling;Software algorithms;Teeth;Software;Smart phones","","","","9","IEEE","18 Feb 2025","","","IEEE","IEEE Conferences"
"A Security Risk Taxonomy for Prompt-Based Interaction With Large Language Models","E. Derner; K. Batistič; J. Zahálka; R. Babuška","ELLIS Alicante, Alicante, Spain; Independent Researcher, Ljubljana, Slovenia; Czech Institute of Informatics, Robotics, and Cybernetics, Czech Technical University in Prague, Prague, Czech Republic; Czech Institute of Informatics, Robotics, and Cybernetics, Czech Technical University in Prague, Prague, Czech Republic",IEEE Access,"17 Sep 2024","2024","12","","126176","126187","As large language models (LLMs) permeate more and more applications, an assessment of their associated security risks becomes increasingly necessary. The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial. This paper addresses a gap in current research by specifically focusing on security risks posed by LLMs within the prompt-based interaction scheme, which extends beyond the widely covered ethical and societal implications. Our work proposes a taxonomy of security risks along the user-model communication pipeline and categorizes the attacks by target and attack type alongside the commonly used confidentiality, integrity, and availability (CIA) triad. The taxonomy is reinforced with specific attack examples to showcase the real-world impact of these risks. Through this taxonomy, we aim to inform the development of robust and secure LLM applications, enhancing their safety and trustworthiness.","2169-3536","","10.1109/ACCESS.2024.3450388","nominal grant received at the ELLIS Unit Alicante Foundation from the Regional Government of Valencia in Spain (Convenio Singular signed with Generalitat Valenciana, Conselleria de Innovación, Industria, Comercio y Turismo, Dirección General de Innovación); Intel Corporation; European Union’s Horizon 2020 Research and Innovation Programme(grant numbers:951847); European Union’s Horizon Europe Research and Innovation Programme(grant numbers:101070254 CORESENSE); European Union through the project ROBOPROX(grant numbers:CZ.02.01.01/00/22_008/0004590); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10648691","Large language models;security;jailbreak;natural language processing","Security;Taxonomy;Chatbots;Large language models;Data models;Codes;Privacy;Natural language processing;Risk analysis","","","","74","CCBY","26 Aug 2024","","","IEEE","IEEE Journals"
"Stealing Watermarks of Large Language Models via Mixed Integer Programming","Z. Zhang; X. Zhang; Y. Zhang; L. Y. Zhang; C. Chen; S. Hu; A. Gill; S. Pan","University of Technology Sydney, Sydney, Australia; Griffith University, Gold Coast, Australia; University of Technology Sydney, Sydney, Australia; Griffith University, Gold Coast, Australia; Royal Melbourne Institute of Technology, Melbourne, Australia; Huazhong University of Science and Technology, Wuhan, China; University of Technology Sydney, Sydney, Australia; Griffith University, Gold Coast, Australia",2024 Annual Computer Security Applications Conference (ACSAC),"18 Mar 2025","2024","","","46","60","The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM’s parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.","2576-9103","979-8-3315-2088-5","10.1109/ACSAC63791.2024.00021","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10917955","Large Language Model;LLM Security;Watermarking;Security and Privacy","Integer programming;Threat modeling;Vocabulary;Privacy;Large language models;Perturbation methods;Watermarking;Detectors;Programming;Robustness","","","","34","IEEE","18 Mar 2025","","","IEEE","IEEE Conferences"
"Screen-Shooting Robust Watermark Based on Style Transfer and Structural Re-Parameterization","G. Gao; X. Chen; L. Li; Z. Xia; J. Fei; Y. -Q. Shi","Engineering Research Center of Digital Forensics, Ministry of Education, and the School of Computer Science, Nanjing University of Information Science and Technology, Nanjing, China; Engineering Research Center of Digital Forensics, Ministry of Education, and the School of Computer Science, Nanjing University of Information Science and Technology, Nanjing, China; Engineering Research Center of Digital Forensics, Ministry of Education, and the School of Computer Science, Nanjing University of Information Science and Technology, Nanjing, China; College of Cyber Security, Engineering Research Center of Trustworthy AI, Ministry of Education, Jinan University, Guangzhou, China; College of Cyber Security, Engineering Research Center of Trustworthy AI, Ministry of Education, Jinan University, Guangzhou, China; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA",IEEE Transactions on Information Forensics and Security,"7 Mar 2025","2025","20","","2648","2663","In real-world applications, screen capturing represents a significant scenario where this process can induce substantial distortion to the original image. Previous methods for simulating screen-shooting distortion often involved combining different formulas. We found that these simulation methods still have a significant gap compared to real distortions, making it urgently necessary to develop a realistic and credible comprehensive noise layer to achieve robustness against screen-shooting distortion. This paper presents a watermarking scheme capable of withstanding severe screen-shooting distortion. First, a dataset is constructed to train a screen-shooting distortion simulation network based on style transfer. Subsequently, a comprehensive noise layer is built upon this network to achieve robustness against severe screen-shooting distortion. Additionally, this paper incorporates structural re-parameterization techniques into the traditional U-shaped encoder to improve the quality of encoded images. Extensive experiments demonstrate the proposed scheme’s superior performance in terms of robustness and generalization, especially under severe screen-shooting distortion conditions.","1556-6021","","10.1109/TIFS.2025.3542992","National Key Research and Development Program of China(grant numbers:2022YFB3103100); Humanities and Social Science Foundation of the Ministry of Education(grant numbers:24YJA870002); National Natural Science Foundation of China(grant numbers:U22B2062,62122032,62172233); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10898040","Data hiding;robust watermark;screen-shooting simulation network;adversarial training;visual image codes","Watermarking;Distortion;Noise;Robustness;Image edge detection;Decoding;Visualization;Training;Image coding;Electronic mail","","","","34","IEEE","21 Feb 2025","","","IEEE","IEEE Journals"
"Privacy and Security Challenges in Large Language Models","V. Rathod; S. Nabavirazavi; S. Zad; S. S. Iyengar",Florida International University; Florida International University; Florida International University; Florida International University,2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC),"5 Mar 2025","2025","","","00746","00752","Large Language Models (LLMs) are at the forefront of artificial intelligence advancements, demonstrating exceptional capabilities in natural language understanding and generation across diverse domains such as healthcare, finance, and customer service. However, their deployment introduces substantial secu-rity and privacy risks, including prompt injection, data leakage, and unauthorized data disclosures. These vulnerabilities highlight the need for robust frameworks to safeguard sensitive data and prevent misuse. This paper provides a comprehensive analysis of the security and privacy challenges in LLMs, examines existing mitigation strategies such as intelligent LLM firewalls, differen-tial privacy, and OW ASP-based security principles, and discusses future directions for ethical and secure LLM deployment. By addressing these challenges in detail, we identify gaps in current practices and propose a roadmap for the secure and responsible deployment of LLMs in high-stakes applications. Our findings underscore the importance of tailored security frameworks and privacy-preserving techniques to ensure the ethical and reliable use of LLMs in sensitive environments. Additionally, this pa-per emphasizes the significance of a human-in-the-loop (HITL) approach to ensure accountability and accuracy, particularly in critical domains. The discussion extends to emerging technologies such as retrieval-augmented generation (RAG) and adaptive threat detection systems, which hold promise for enhancing the security and ethical deployment of LLMs.","","979-8-3315-0769-5","10.1109/CCWC62904.2025.10903912","Army Research Office(grant numbers:W911NF-21-1-0264); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903912","Artificial intelligence;Natural language processing;Large language models (LLM);Privacy;OWASP;Data Protection;AI Ethics;Firewall;Threat Modeling;Data Leakage;Ethical Bias Mitigation;Federated Learning;Healthcare AI;Human-in-the-Loop (HITL);Adaptive Security Frameworks;Privacy-Preserving Computation","Industries;Ethics;Privacy;Technological innovation;Firewalls (computing);Large language models;Computational modeling;Medical services;Threat assessment;Security","","","","33","IEEE","5 Mar 2025","","","IEEE","IEEE Conferences"
"Leveraging Open Source LLMs for Software Engineering Education and Training","J. Pereira; J. -M. López; X. Garmendia; M. Azanza","Dept. of Computer Languages and Systems, University of the Basque Country, UPV/EHU, Donostia, Spain; Dept. of Computer Languages and Systems, University of the Basque Country, UPV/EHU, Donostia, Spain; Dept. of Computer Languages and Systems, University of the Basque Country, UPV/EHU, Donostia, Spain; Dept. of Computer Languages and Systems, University of the Basque Country, UPV/EHU, Donostia, Spain",2024 36th International Conference on Software Engineering Education and Training (CSEE&T),"10 Sep 2024","2024","","","1","10","Generative AI, particularly Large Language Models (LLMs), presents innovative opportunities to enhance software engineering education. Open source LLMs such as LLaMA and Mistral leverage the potential of generative AI offering distinct advantages over proprietary options including transparency, customizability, collaboration, and cost savings. This paper de-velops a catalog of LLM prompt examples tailored for software engineering training, mapped to knowledge areas from the Soft-ware Engineering Body of Knowledge (SWEBoK) framework. Example prompts demonstrate LLMs' capabilities in eliciting requirements, diagram generation, API simulation, effort esti-mation through role-playing, and other areas. The methodology involves evaluating prompt responses from ChatGPT, Mistral, and LLaMA on representative tasks. Quantitative and qualitative analysis assesses quality, usefulness, and correctness. Findings show ChatGPT and Mistral outperforming LLaMA overall, but no model perfectly executes complex interactions. We examine implications and challenges of integrating open source LLMs into classrooms, emphasizing the need for oversight, verification, and prompt design aligned with pedagogical objectives.","2377-570X","979-8-3503-7897-9","10.1109/CSEET62301.2024.10663055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10663055","Software Engineering Education;Open Source AI Models;Large Language Models;Prompt Engineering","Training;Knowledge engineering;Costs;Generative AI;Large language models;Collaboration;Chatbots","","","","28","IEEE","10 Sep 2024","","","IEEE","IEEE Conferences"
"Exploring Advanced Encryption and Steganography Techniques for Image Security","M. Kataria; K. Jain; N. Subramanian","Center for CyberSecurity Systems and Networks, Amrita Vishwa Vidyapeetham, Amritapuri, India; Center for CyberSecurity Systems and Networks, Amrita Vishwa Vidyapeetham, Amritapuri, India; Center for CyberSecurity Systems and Networks, Amrita Vishwa Vidyapeetham, Amritapuri, India",2023 11th International Symposium on Digital Forensics and Security (ISDFS),"26 May 2023","2023","","","1","6","This paper investigates the use of modern steganography techniques for secure image transmission, utilizing four encryption algorithms: AES, DES, RSA, and ChaCha20. Two steganography techniques, LSB and Spread Spectrum, are used to encrypt and embed ciphertext into five different images. The security of the created images is evaluated using metrics such as PSNR, runtime, MSE, histogram, and information entropy. Our analysis reveals that DES is less effective than AES and ChaCha20 for encryption. Furthermore, Spread Spectrum outperforms LSB in terms of security, although LSB produces better stego-image quality. Combining different steganography and encryption techniques enhances the capacity to send photos with a high level of security and robustness. This study provides insightful knowledge on the effective use of steganography and encryption for secure image transmission in various industries, including military and healthcare imaging applications","","979-8-3503-3698-6","10.1109/ISDFS58141.2023.10131890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10131890","Steganography;AES;DES;RSA;ChaCha20;LSB;Spread Spectrum;PSNR;MSE;histogram;information entropy","Measurement;Industries;Steganography;Runtime;Image communication;Imaging;Medical services","","9","","26","IEEE","26 May 2023","","","IEEE","IEEE Conferences"
"A Robust Auto-Encoder HBC Transceiver with CGAN-Based Channel Modeling","A. Ali; A. N. Abdelrahman; A. Celik; M. E. Fouda; A. M. Eltawil","Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology, Thuwal, KSA; Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology, Thuwal, KSA; Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology, Thuwal, KSA; Compumacy for Artificial Intelligence Solutions, Cairo, Egypt; Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology, Thuwal, KSA",IEEE Sensors Journal,"","2025","PP","99","1","1","Human body communication (HBC) offers a promising alternative for efficient and secure data transmission in wearable healthcare systems by leveraging the body’s conductive properties. By utilizing the conductive properties of the human body, HBC offers significant advantages over conventional radio frequency wireless communication methods, including ultra-low power consumption and minimal interference. However, HBC systems face key challenges in energy efficiency, data rate optimization, channel adaptability, and accurate body channel modeling. In this paper, we present a novel dual-mode HBC transceiver architecture designed to overcome these challenges by integrating autoencoder-based signal processing with Generative adversarial networks (GANs)-driven channel modeling framework to enhance communication reliability. Operating in both broadband and narrowband modes, the transceiver dynamically adjusts its data rate and power efficiency based on application-specific demands. The design process involves first developing a CGAN-based channel model from real HBC measurements, then using this model to train an autoencoder-based transceiver architecture. Our CGAN framework generates realistic synthetic channel responses for training, enabling the autoencoder to learn optimal encoding and decoding strategies that are robust to channel variations. Subsequently, we developed a low-power hardware architecture that supports flexible data rates of the proposed design while ensuring robust performance in diverse scenarios. This systematic approach provides key advantages: improved channel modeling accuracy achieving a 0.9 correlation coefficient between generated and real channels and mean squared error of 0.0071, reduced hardware complexity through elimination of DAC/ADC, and flexible operation with dual-mode support. Operating at a clock speed of 42 MHz in the narrowband mode, the transceiver achieves an energy efficiency of 349 pJ/bit at a data rate of 262.5 kbps with sensitivity of -64 dBm, appealing for long-range and low-power applications. In broadband mode, the transceiver achieves an energy efficiency of 16 pJ/bit at a data rate of 5.25 Mbps, suitable for applications demanding high data rates over shorter distances.","1558-1748","","10.1109/JSEN.2025.3551539","KAUST Center of Excellence for Smart Health (KCSH)(grant numbers:5932); NEOM(grant numbers:4819); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10935704","Internet of Bodies (IoB);Human Body Communication;End-to-End communication;CGAN channel modeling;Deep Learning","Transceivers;Autoencoders;Electrodes;Biosensors;Energy efficiency;Training;Narrowband;Hardware;Broadband communication;Wireless communication","","","","","IEEE","24 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Watermarking Techniques Performance Analysis in Medical Images for Secure Communication","A. R; M. C","Department of Networking and Communications, School of Computing, SRM Institute of Science and Technology, Kattankulathur, India; Department of Networking and Communications, School of Computing, SRM Institute of Science and Technology, Kattankulathur, India","2022 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)","14 Oct 2022","2022","","","1","9","Over the past few years, most medical diagnostics and treatments have shifted to digital content. COVID-19 is a viral disease first identified in Wuhan, China 2019. Its pandemic caused a dramatic loss in human health, work, and food systems worldwide. WHO recommended social distancing as a preventive measure to protect ourselves from corona viral infections. Hence, now many avail hospitals facilities are online. It enables telemedicine where patients, doctors, and medical research units can easily share their digital medical information through various communication channels. At the receiver’s end, the patient’s record must not be lost or altered during transmission. As medical imaging contains many fine features, even small changes cause confusion among medical staff for diagnosis. One of the best techniques for image authentication is digital image watermarking. When developing an effective watermark method, it’s necessary to have a balanced trade-off among imperceptibility, capacity, and robustness. The work gives a comprehensive survey of cryptography, biometrics, and blockchain-based on various watermarking schemes in medical images that gives new ideas to improve the already existing techniques.","","978-1-6654-7413-9","10.1109/ICSES55317.2022.9914343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9914343","image authentication;watermarking;capacity;robustness;imperceptibility","Pandemics;Telemedicine;Watermarking;Human factors;Social factors;Robustness;Performance analysis","","","","30","IEEE","14 Oct 2022","","","IEEE","IEEE Conferences"
"Agro-Technological Systems in Traditional Agriculture Assistance: A Systematic Review","N. Montalvo-Romero; A. Montiel-Rosales; R. Purroy-Vásquez; P. Quechulpa-Pérez","Division of Postgraduate Studies and Research, National Technological of Mexico/HTI of Misantla, Misantla, Mexico; Division of Postgraduate Studies and Research, National Technological of Mexico/HTI of Misantla, Misantla, Mexico; Academic Direction, National Technological of Mexico/HTI of Zongolica, Zongolica, Mexico; General Direction, National Technological of Mexico/HTI of Zongolica, Zongolica, Mexico",IEEE Access,"9 Nov 2023","2023","11","","123047","123069","Guaranteeing food security from agriculture in an uncertain context, derived from the effects of multiple factors, is a challenge. Traditional agricultural production is the one that faces the greatest challenges, derived from the scarce evolution in agricultural practices, despite being the one that contributes the most to the availability of food, at 80%. This systematic review aims to identify and analyze agrotechnological systems belonging to precision agriculture, which may be potentially adaptable to traditional rural agriculture. Contributions that improved crop yields from scientific and technological studies were analyzed. The PRISMA statement was used as a formal outline to collect and analyze 114 studies from the period 2018-2023. From the review, it was identified that there is a growing trend in the adoption of intelligent systems that help producers in the management of crops, accentuated in the increase of crop yield, in the determination of product quality, and in the management of water resources, mainly. Likewise, it was identified that the preponderant approach is the monitoring and control of crop development. This is achieved through emerging technologies, such as the Internet of Things, artificial intelligence, and machine learning, with information mainly collected by sensors embedded in drones, algorithms, decision support systems, sensors, and Arduino technology systems. Finally, this review shows that there are five viable systems that can be adapted to traditional agriculture to strengthen agricultural production. Therefore, the adoption of scientific-technological contributions from precision agriculture contributes to ensuring food security.","2169-3536","","10.1109/ACCESS.2023.3329087","Consejo Veracruzano de Investigación Científica y Desarrollo Tecnológico (COVEICYDET); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304125","Agricultural applications;food security;precision agriculture","Production;Statistics;Sociology;Systematics;Food security;Smart agriculture;Precision engineering","","5","","200","CCBYNCND","1 Nov 2023","","","IEEE","IEEE Journals"
"Reducing High-Frequency Artifacts for Generative Model Watermarking via Wavelet Transform","Y. Liu; L. Zhang; H. Wu; Z. Wang; X. Zhang","School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China",IEEE Internet of Things Journal,"9 May 2024","2024","11","10","18503","18515","As generative models find broader applications in Internet of Things (IoT) image processing tasks, safeguarding the copyright of these models assumes increasing significance. Embedding watermarks on the output images generated by such models has been proposed by some researchers as a means of protecting intellectual property. However, prevailing methods for generating model watermarks inadvertently introduce significant high-frequency artifacts in high-frequency regions, compromising the imperceptibility and security of the watermarking system. In pursuit of enhancing the imperceptibility of generative model watermarking, we propose a framework based on discrete wavelet transform. This framework effectively mitigates the high-frequency artifact issue and enhances the frequency-domain concealment of watermarking. Specifically, we introduce an embedded watermarking network, a frequency separation layer, and a watermark extraction network after the output of the target model. We construct a wavelet frequency domain separation layer by wavelet decomposition to decompose the image generated by the embedding network into different frequency components, and embed the watermark into the low-frequency region of the target model output image through joint training and joint loss optimization of the embedding and extraction networks. Extensive experiments conducted on two image processing tasks, i.e., painting transfer and de-raining, demonstrate that our method exhibits no discernible traces of high-frequency artifacts in the frequency domain of the image in both cases, thus boasting superior invisibility. Furthermore, our method demonstrates robustness against preprocessing attacks, such as noise addition, resizing, and image cropping.","2327-4662","","10.1109/JIOT.2024.3363613","Natural Science Foundation of China(grant numbers:62376148,U23B2023,U22B2047); Chenguang Program of Shanghai Education Development Foundation and Shanghai Municipal Education Commission(grant numbers:22CGA46); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10423747","High-frequency artifacts;imperceptibility;model watermarking;security;wavelet transform","Watermarking;Internet of Things;Frequency-domain analysis;Data models;Task analysis;Image processing;Glass box","","2","","44","IEEE","7 Feb 2024","","","IEEE","IEEE Journals"
"Navigating the Ethical Terrain of AI-Generated Text Tools: A Review","Y. Abdelgadir Mohamed; A. H. H. M. Mohamed; A. Khanan; M. Bashir; M. A. E. Adiel; M. A. Elsadig","College of Business administration, A’Sharqiyah University, Ibra, Oman; College of Business administration, A’Sharqiyah University, Ibra, Oman; College of Business administration, A’Sharqiyah University, Ibra, Oman; College of Business administration, A’Sharqiyah University, Ibra, Oman; College of Computer Science and Information Technology, Imam Abudlrahman Bin Faisal University, Dammam, Saudi Arabia; College of Computer Science and Information Technology, Imam Abudlrahman Bin Faisal University, Dammam, Saudi Arabia",IEEE Access,"30 Dec 2024","2024","12","","197061","197120","This review examines the ethical, social, and technical challenges posed by AI-generated text tools, focusing on their rapid advancement and widespread adoption. An exhaustive literature search across many databases, strict inclusion/exclusion criteria, and a rigorous analysis procedure are all parts of our systematic review technique. This guarantees an impartial and complete study of the current status of AI-generated text tools. The study analyzes prominent language models, including GPT-3, GPT-4, LaMDA, PaLM, Claude, Jasper, and Llama 2, evaluating their capabilities in natural language processing and generation. The analysis reveals significant advancements, with GPT-3 demonstrating a 92% accuracy rate on standard natural language understanding benchmarks, outperforming LaMDA (88%) and PaLM (85%). To illustrate real-world implications, the review presents a case study of ChatGPT’s application in healthcare, where it achieved 80% consistency with expert opinions in assessing acute ulcerative colitis. This case highlights both the potential benefits and ethical concerns of AI in critical domains. Quantitative bias analysis shows that GPT-3 generated biased content in 15% of test cases involving sensitive topics, a higher rate than LaMDA (12%) and PaLM (10%). We provide an in-depth analysis of fairness and bias issues, particularly in image generation tasks depicting professional roles. Our research synthesizes insights from technical advancements, ethical considerations, and real-world applications across healthcare, education, and creative sectors. We address critical privacy concerns and data protection challenges, noting struggles in AI-generated text detection and investigating AI’s potential in enabling cyberattacks. We underscore the need for comprehensive governance systems and multidisciplinary cooperation. To provide a cohesive analysis of the ethical considerations surrounding AI-generated text tools, we employ a multifaceted ethical framework drawing on established theories. Utilitarianism, which seeks to maximize happiness for everyone; deontology, which places an emphasis on right and wrong; and Virtue Ethics, which analyzes the moral nature of deeds and actors, are all included in this framework. In this article, we use this approach to investigate AI ethics from a variety of angles, including privacy, prejudice, and social implications, as well as concerns of justice and fairness. Moreover, the study critically examines existing and proposed legal frameworks addressing AI ethics, identifying regulatory gaps and proposing adaptive policy recommendations to address the unique challenges posed by AI-generated text tools. Our review contributes a critical analysis of AI-generated text tools, their impacts, and the need for responsible innovation. The study provides precise guidelines for the ethical development and implementation of AI, highlighting the need to strike a balance between technical progress and ethical concerns to guarantee that AI technologies have a beneficial effect on society while protecting human values. The emergence of generative artificial intelligence (AI) signifies a substantial revolution in our methods of interacting with language and information.","2169-3536","","10.1109/ACCESS.2024.3521945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10813359","Generative AI;text generation;ethics","Ethics;Artificial intelligence;Reviews;Generative AI;Analytical models;Privacy;Object recognition;Technological innovation;Search problems;Industries","","","","139","CCBY","25 Dec 2024","","","IEEE","IEEE Journals"
"The Case for the Anonymization of Offloaded Computation","M. W. Al Azad; S. Sarwar; S. U. Taki; S. Mastorakis",University of Notre Dame; University of Nebraska at Omaha; University of Notre Dame; University of Notre Dame,2023 IEEE 16th International Conference on Cloud Computing (CLOUD),"25 Sep 2023","2023","","","84","95","Computation offloading (often to external computing resources over a network) has become a necessity for modern applications. At the same time, the proliferation of machine learning techniques has empowered malicious actors to use such techniques in order to breach the privacy of the execution process for offloaded computations. This can enable malicious actors to identify offloaded computations and infer their nature based on computation characteristics that they may have access to even if they do not have direct access to the computation code. In this paper, we first demonstrate that even non-sophisticated machine learning algorithms can accurately identify offloaded computations. We then explore the design space of anonymizing offloaded computations through the realization of a framework, called Camouflage. Camouflage features practical mechanisms to conceal characteristics related to the execution of computations, which can be used by malicious actors to identify computations and orchestrate further attacks based on identified computations. Our evaluation demonstrated that Camouflage can impede the ability of malicious actors to identify executed computations by up to 60%, while incurring modest overheads for the anonymization of computations.","2159-6190","979-8-3503-0481-7","10.1109/CLOUD60044.2023.00019","National Science Foundation(grant numbers:CNS-2104700,CNS-2306685,CNS-2016714,CBET-2124918); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254960","Computation Anonymization;Machine Learning;Computation Offloading;Cloud Computing;Computation Graphs","Data privacy;Cloud computing;Privacy;Machine learning algorithms;Codes;Machine learning;Space exploration","","","","52","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"Multi-Objective Large Language Model Unlearning","Z. Pan; S. Zhang; Y. Zheng; C. Li; Y. Cheng; J. Zhao","School of Science and Engineering, CUHKSZ, Shenzhen, China; School of Science and Engineering, CUHKSZ, Shenzhen, China; School of Science and Engineering, CUHKSZ, Shenzhen, China; School of Data Science, CUHKSZ, Shenzhen, China; School of Science and Engineering, CUHKSZ, Shenzhen, China; School of Science and Engineering, CUHKSZ, Shenzhen, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Machine unlearning in the domain of large language models (LLMs) has attracted great attention recently, which aims to effectively eliminate undesirable behaviors from LLMs without full retraining from scratch. In this paper, we explore the Gradient Ascent (GA) approach in LLM unlearning, which is a proactive way to decrease the prediction probability of the model on the target data in order to remove their influence. We analyze two challenges that render the process impractical: gradient explosion and catastrophic forgetting. To address these issues, we propose Multi-Objective Large Language Model Unlearning (MOLLM) algorithm. We first formulate LLM unlearning as a multi-objective optimization problem, in which the cross-entropy loss is modified to the unlearning version to overcome the gradient explosion issue. A common descent update direction is then calculated, which enables the model to forget the target data while preserving the utility of the LLM. Our empirical results verify that MoLLM outperforms the SOTA GA-based LLM unlearning methods in terms of unlearning effect and model utility preservation. The source code is available at https://github.com/zibinpan/MOLLM.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10889776","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10889776","large language model;machine unlearning;multi-objective optimization","Large language models;Source coding;Signal processing algorithms;Signal processing;Predictive models;Prediction algorithms;Explosions;Data models;Speech processing;Optimization","","","","31","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"An Efficient Frequency Domain Based Attribution and Detection Network","J. Zhang; Y. Wang; H. Reza Tohidypour; P. Nasiopoulos","Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada",IEEE Access,"31 Jan 2025","2025","13","","19909","19921","People nowadays can easily synthesize high fidelity fake images with different types of image content due to the rapid advances of deep learning technologies. Detecting such images and attributing them to their generative models (GMs) is crucial. Existing deep learning methods attempt to identify and classify GM-specific artifacts but often struggle with content-independence and generalizability. In this paper, we observe that while GMs leave unique artifacts in the frequency domain, they are coupled with the image content. Based on this observation, we propose a novel deep learning-based solution that learns input-adaptive masks to highlight GMs’ artifacts and achieve high accuracy on the synthesized image attribution task. In addition, we observed that GMs’ artifacts in the frequency domain remain intact in sub-images of the original image, and they are even retained when the images are distorted. To further improve the accuracy of the proposed solution, we leverage the characteristics of GMs artifacts in sub-images and distorted images to make our network perform more effectively. Our evaluation results show that our proposed solution outperforms other state-of-the-art methods on unseen image types, showing great generalizability.","2169-3536","","10.1109/ACCESS.2025.3534829","Natural Sciences and Engineering Research Council of Canada(grant numbers:NSERC–PG 11R12450); TELUS Corporation(grant numbers:PG 11R10321); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10855423","Synthesized image;attribution;detection;frequency domain","Discrete cosine transforms;Fingerprint recognition;Frequency-domain analysis;Training;Accuracy;Frequency synthesizers;Visualization;Generative adversarial networks;Faces;Distortion","","","","49","CCBYNCND","27 Jan 2025","","","IEEE","IEEE Journals"
"CoachLM: Automatic Instruction Revisions Improve the Data Quality in LLM Instruction Tuning","Y. Liu; S. Tao; X. Zhao; M. Zhu; W. Ma; J. Zhu; C. Su; Y. Hou; M. Zhang; M. Zhang; H. Ma; L. Zhang; H. Yang; Y. Jiang","Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China",2024 IEEE 40th International Conference on Data Engineering (ICDE),"23 Jul 2024","2024","","","5184","5197","Instruction tuning is crucial for enabling Language Learning Models (LLMs) in responding to human instructions. The quality of instruction pairs used for tuning greatly affects the performance of LLMs. However, the manual creation of high-quality instruction datasets is costly, leading to the adoption of automatic generation of instruction pairs by LLMs as a popular alternative. To ensure the high quality of LLM-generated instruction datasets, several approaches have been proposed. Nevertheless, existing methods either compromise dataset integrity by filtering a large proportion of samples, or are unsuitable for industrial applications. In this paper, instead of discarding low-quality samples, we propose CoachLM, a novel approach to enhance the quality of instruction datasets through automatic revisions on samples in the dataset. CoachLM is trained from the samples revised by human experts and significantly increases the proportion of high-quality samples in the dataset from 17.7% to 78.9%. The effectiveness of CoachLM is further assessed on various real-world instruction test sets. The results show that CoachLM improves the instruction-following capabilities of the instruction-tuned LLM by an average of 29.9%, which even surpasses larger LLMs with nearly twice the number of parameters. Furthermore, CoachLM is successfully deployed in a data management system for LLMs at Huawei, resulting in an efficiency improvement of up to 20% in the cleaning of 40k real-world instruction pairs. We release various assets of CoachLM, including the training data, code and test set11https://github.com/lunyiliu/CoachLM.","2375-026X","979-8-3503-1715-2","10.1109/ICDE60146.2024.00390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597991","large language model;instruction tuning;data quality;instruction revision","Training;Filtering;Data integrity;Pipelines;Training data;Manuals;Cleaning","","","","45","IEEE","23 Jul 2024","","","IEEE","IEEE Conferences"
"Understanding GenAI","M. R. Islam","Geroge Mason Univeristy, Fairfax, Virginia, United States","Generative AI, Cybersecurity, and Ethics","","2025","","","47","81","Summary <p>Generative artificial intelligence (GenAI) is an innovative subset of artificial intelligence (AI) focused on creating original content, simulating human creativity. Unlike traditional AI, which excels in tasks such as classification and prediction, GenAI generates various data types using advanced machine learning techniques. By analyzing existing datasets to identify patterns, GenAI produces novel outputs such as realistic images, videos, music, text, and designs. This chapter explores the core elements, tools, frameworks, and models of GenAI, along with their diverse applications across industries like art, entertainment, marketing, and virtual environments. Additionally, it addresses the technological landscape, validation methodologies, and ethical considerations surrounding GenAI, providing a comprehensive overview of this groundbreaking technology.</p>","","9781394279319","10.1002/9781394279326.ch3","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10897091.pdf&bkn=10896969&pdfType=chapter","","Solid modeling;Data models;Predictive models;Machine learning;Image synthesis;Context modeling;Virtual environments;Video games;Transformers;Synthetic data","","","","","","20 Feb 2025","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"CGHit: A Content-Oriented Generative-Hit Framework for Content Delivery Networks","P. Wang; Y. Liu; K. Han; Z. Liu; K. Liu; M. Wang; K. Zhou; Z. Huang","WNLO, HUST, Wuhan, China; SCST, HUST, Wuhan, China; SCST, HUST, Wuhan, China; SCST, HUST, Wuhan, China; WNLO, HUST, Wuhan, China; SCST, HUST, Wuhan, China; WNLO, HUST, Wuhan, China; Tencent Technology Co., Ltd., Shenzhen, China","2024 International Conference on Networking, Architecture and Storage (NAS)","12 Dec 2024","2024","","","1","8","The service provided by content delivery networks (CDNs) may overlook content locality, leaving the potential to improve performance. In this study, we explore the feasibility of leveraging generated data as a replacement for fetching data in missing scenarios based on content locality. Due to sufficient local computing resources and reliable generation efficiency, we propose a content-oriented generative-hit framework (CGHit) for CDNs. CGHit utilizes idle computing resources on edge nodes to generate requested data based on similar or related cached data, achieving hits. Extensive experiments in a real-world system demonstrate that CGHit reduces the average access latency by half. In addition, experiments conducted on a simulator confirm that CGHit can enhance current caching algorithms, leading to lower latency and reduced bandwidth usage.","2835-3323","979-8-3315-2050-2","10.1109/NAS63802.2024.10781374","National key research and development program(grant numbers:2023YFB4502701); National Natural Science Foundation of China(grant numbers:62232007,61821003); Natural Science Foundation of Hubei Province(grant numbers:2022CFB060); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10781374","generative-hit;content locality;content delivery network","Content distribution networks;Large language models;Computational modeling;Computer architecture;Bandwidth;Data collection;Data models;Reliability;Quality of experience;Decision trees","","","","31","IEEE","12 Dec 2024","","","IEEE","IEEE Conferences"
"Developments in Brain Tumor Segmentation Using MRI: Deep Learning Insights and Future Perspectives","S. Karim; G. Tong; Y. Yu; A. A. Laghari; A. A. Khan; M. Ibrar; F. Mehmood","Research and Development Institute, Northwestern Polytechnical University in Shenzhen, Shenzhen, China; Research and Development Institute, Northwestern Polytechnical University in Shenzhen, Shenzhen, China; Research and Development Institute, Northwestern Polytechnical University in Shenzhen, Shenzhen, China; Department of Computer Science, Sindh Madressatul Islam University, Karachi, Pakistan; Department of Computer Science, Sindh Madressatul Islam University, Karachi, Pakistan; Software College, Shenyang Normal University, Shenyang, China; School of Electronic and Computer Engineering, Peking University Shenzhen Graduate School, Shenzhen, China",IEEE Access,"23 Feb 2024","2024","12","","26875","26896","The human brain is an incredible and wonderful organ that governs all body actions. Due to its great importance, any defect in the shape of its regions should be reported quickly to reduce the death rate. The abnormal region segmentation helps to plan and monitor the treatment. The most critical procedure is isolating normal and abnormal tissues from each other. So far, remarkable imaging modalities are being used to diagnose abnormalities at their early stages, and magnetic resonance imaging (MRI) is renowned and noninvasive among those modalities. This paper investigates the current landscape of brain tumor segmentation (BTS) by exploring emerging deep learning (DL) methods for brain MRI analysis. The findings offer a comprehensive comparison of recent DL approaches, emphasizing their effectiveness in handling diverse tumor types while addressing limitations associated with data scarcity and robust validation. DL has shown a vital improvement for BTS, so our primary focus is to include significant DL robust models to analyze the brain MRI. However, DL outperforms traditional methods; still, there are several limitations, especially related to the diverse tumor types, lack of datasets, and weak validations. The future perspectives of DL-based BTS present significant potential for revolutionizing the diagnosis and treatment of brain tumors.","2169-3536","","10.1109/ACCESS.2024.3365048","Shenzhen Science and Technology Innovation Committee(grant numbers:YFJGJS1.0); Science and Development Program of Local Lead by Central Government, Shenzhen Science and Technology Innovation Committee(grant numbers:2021Szvup112); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10431786","Brain tumor segmentation;deep learning;medical imaging;MRI","Tumors;Image segmentation;Medical diagnostic imaging;Magnetic resonance imaging;Surgery;Medical services;Feature extraction;Brain cancer;Deep learning;Biomedical imaging","","7","","200","CCBYNCND","9 Feb 2024","","","IEEE","IEEE Journals"
"Appearance Matters, So Does Audio: Revealing the Hidden Face via Cross-Modality Transfer","C. Kong; B. Chen; W. Yang; H. Li; P. Chen; S. Wang","Department of Computer and Science, City University of Hong Kong, Hong Kong, China; Department of Computer and Science, City University of Hong Kong, Hong Kong, China; Department of Computer and Science, City University of Hong Kong, Hong Kong, China; Rapid-Rich Object Search Lab, Nanyang Technological University, Singapore; Department of Computer and Science, City University of Hong Kong, Hong Kong, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China",IEEE Transactions on Circuits and Systems for Video Technology,"11 Jan 2022","2022","32","1","423","436","Recently, there has been an exponential increase in the security concerns raised by faking face (e.g., deepfake), which automatically changes the identity with a specifically learned deep generative model. With numerous approaches proposed to identify the fake content, much less work has been dedicated to automatically revealing the authentic one that is originally acquired. Here, we propose a new paradigm that seeks to reveal the authentic face hidden behind the fake one by leveraging the joint information of face and audio. More specifically, given the fake face as well as the audio segment, the cross-modality transferable capability is exploited by learning to generate the feature of the authentic face, based on the underlying clues from the audio as well as the fake face appearance. The effectiveness of the proposed scheme is validated through a series of evaluations, and experimental results show that the proposed model achieves promising face reconstruction performance in revealing the hidden faces, in terms of reconstruction quality, as well as identity and face attribute inference accuracy.","1558-2205","","10.1109/TCSVT.2021.3057457","Science, Technology, and Innovation Commission of Shenzhen Municipality(grant numbers:JCYJ20180307123934031); National Natural Science Foundation of China(grant numbers:62022002); Hong Kong Research Grants Council, Early Career Scheme(grant numbers:21211018); General Research Fund(grant numbers:11203220); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9349088","Deepfake;cross modality;face reconstruction;face revealing;fake face","Faces;Videos;Information integrity;Face recognition;Training;Generative adversarial networks;Testing","","22","","84","IEEE","5 Feb 2021","","","IEEE","IEEE Journals"
"Drop2Sparse: Improving Dataset Distillation via Sparse Model","T. -F. Huang; Y. -H. Lin","National Tsing Hua University, Hsinchu, Taiwan; National Tsing Hua University, Hsinchu, Taiwan",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","The success of modern deep learning algorithms requires large amounts of training data, which leads to high computational and storage costs. Dataset Distillation (DD) is a rising research field that resolves this issue by synthesizing a compact training dataset from a large one. Recent gradient matching DD methods have achieved remarkable results. However, these methods typically utilize weak models for DD performance improvement, while well-trained models are often considered inferior choices due to their lower performance. Conversely, our study provides new insights into the role of well-trained models in DD, particularly under high-storage budget scenarios. We identify a previously overlooked design principle—a positive correlation between model capability and storage budget. Based on this principle, we propose Drop2Sparse, an approach that randomly sparsifies well-trained models to create efficient models for various storage budget scenarios. Drop2Sparse concurrently infuses significant model diversity and regularization effects into DD, outperforming previous state-of-the-art methods by up to 3.8% on CIFAR and 3.6% on ImageNet-subset. Moreover, our method exhibits remarkable cross-architecture generalization and achieves promising results even under challenging scenarios, such as using an extremely reduced model pool or highly accelerated training.","1558-2205","","10.1109/TCSVT.2025.3552047","National Science and Technology Council, Taiwan(grant numbers:112-2221-E-007-058,113-2221-E-007-113,113-2634-F-006-002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10929000","Dataset Compression;Dataset Distillation;Gradient Matching;Model Sparsification","Training;Synthetic data;Accuracy;Image coding;Computational modeling;Integrated circuit modeling;Runtime;Circuits and systems;Buildings;Adaptation models","","","","","IEEE","17 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Low-Bitrate High-Quality Digital Semantic Communication Based on RVQGAN","X. Chen; J. Wang; J. Huang; M. Zeng; Z. Zheng; Z. Fei","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China",IEEE Internet of Things Journal,"","2025","PP","99","1","1","Digital semantic communication has attracted considerable attention attributed to its potential for integration with modern digital communication systems, which has demonstrated significant performance gains. However, despite its ability to save transmission bandwidth, digital semantic communication can degrade the performance of tasks at the receiver, particularly in low-bitrate scenarios. In this paper, we propose a novel low-bitrate digital semantic communication method based on a generative model for speech transmission to achieve high-quality reconstructed speech at low-bitrate transmission. In particular, we first investigate a multi-scale semantic codec based on residual vector quantization with a generative adversary network (RVQGAN) model for extracting semantic information and obtaining high speech reconstruction quality while transmitting at a low bitrate. We then design a channel noise suppression module based on U-Net to alleviate the channel effect at low signal-to-noise ratio (SNR) by restoring high-quality semantic features, which is capable of improving the performance of the proposed method under challenging channel conditions. Moreover, a Transformer-based code predictor is utilized to further improve the robustness of the proposed method by accounting for both the channel impact and reconstruction quality. Finally, a three-stage training strategy is also presented in this paper to ensure the effective operation of the proposed multi-scale semantic codec, channel noise suppression module, and code predictor module. Experimental results demonstrate that the proposed method operating at 3 kbps can save at least 50% of bandwidth while achieving higher speech restoration quality than the baseline method.","2327-4662","","10.1109/JIOT.2025.3534462","National Natural Science Foundation of China(grant numbers:62201044); the National Key R&D Program of China(grant numbers:2022YFB2902003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10886956","Digital semantic communication;generative model;low-bitrate;speech transmission","Semantic communication;Receivers;Internet of Things;Codecs;Bandwidth;Decoding;Data mining;Bit rate;Speech coding;Image reconstruction","","","","","IEEE","13 Feb 2025","","","IEEE","IEEE Early Access Articles"
"GenAI in Cybersecurity","M. R. Islam","Geroge Mason Univeristy, Fairfax, Virginia, United States","Generative AI, Cybersecurity, and Ethics","","2025","","","83","109","Summary <p>This chapter explores the dual‐use nature of generative artificial intelligence (GenAI) in cybersecurity, highlighting both its beneficial and potentially malicious applications. GenAI can enhance cybersecurity by developing advanced models for anomaly detection, predictive modeling, and incident response. However, it also poses significant risks, including the creation of sophisticated phishing attacks, malware, and deepfakes. Effective mitigation strategies include the implementation of advanced defensive artificial intelligence technologies, adversarial machine learning, and continuous learning. The chapter emphasizes the importance of ethical and legal frameworks, public awareness campaigns, and the development of comprehensive training programs for cybersecurity professionals. Additionally, it outlines the technical and organizational infrastructure required to incorporate GenAI into cybersecurity frameworks, ensuring a scalable and secure environment for its application.</p>","","9781394279319","10.1002/9781394279326.ch4","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10897017.pdf&bkn=10896969&pdfType=chapter","","Computer security;Phishing;Artificial intelligence;Electronic mail;Predictive models;Ethics;Anomaly detection;Analytical models;Threat modeling;Testing","","","","","","20 Feb 2025","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"Cybersecurity: Understanding the Digital Fortress","M. R. Islam","Geroge Mason Univeristy, Fairfax, Virginia, United States","Generative AI, Cybersecurity, and Ethics","","2025","","","17","46","Summary <p>Cybersecurity is vital in safeguarding computers, networks, programs, and data against unauthorized access and damage. As digital systems become increasingly integral to modern life, cybersecurity ensures the protection of sensitive information and maintains operational continuity. This chapter explores the fundamental types of cybersecurity, including network security, application security, information security, and operational security. It dives into the significant financial impact of cybercrime globally and regionally, highlighting the unique challenges faced by different industries such as financial services, health care, government, e‐commerce, and critical infrastructure. The chapter also examines the role of artificial intelligence (AI) and generative AI (GenAI) in enhancing cybersecurity measures, addressing ethical considerations, and navigating the global regulatory landscape to fortify digital defenses.</p>","","9781394279319","10.1002/9781394279326.ch2","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10897112.pdf&bkn=10896969&pdfType=chapter","","Security;Firewalls (computing);Computer crime;Training;Structured Query Language;Encryption;Regulation;Personnel;Network security;NIST","","","","","","20 Feb 2025","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"Compiler Provenance Recovery for Multi-CPU Architectures Using a Centrifuge Mechanism","Y. Otsubo; A. Otsuka; M. Mimura","National Police Academy, Fuchu, Tokyo, Japan; Institute of Information Security, Yokohama, Kanagawa, Japan; Institute of Information Security, Yokohama, Kanagawa, Japan",IEEE Access,"8 Mar 2024","2024","12","","34477","34488","Bit-stream recognition (BSR) has a wide range of applications, including forensic investigations, detecting copyright infringement, and analyzing malware. In order to analyze file fragments recovered by digital forensics, it is necessary to use a BSR method that can accurately classify classes while addressing various domains without preprocessing the raw input bitstream. For example, it is important to note that in the case of compiler provenance recovery, a type of BSR, the same bit sequence can have different meanings for different CPU architectures. As a result, traditional methods that rely heavily on disassembly tools, such as IDA Pro, may have limited in applicaballity scope to programs designed for specific CPU architecture. To address the aforementioned limitation, we proposed a novel learning method. Our method involves the upstream layers (sub-net) capturing global features and instructing the downstream layers (main-net) to shift focus, even when a portion of the input bit-stream has identical values. Through our experiments, we utilized a model that was less than 1/300 the size of the state-of-the-art model. Despite its smaller size, our method achieved the highest classification performance of 99.54 on a multi-CPU architecture, outperforming existing methods.","2169-3536","","10.1109/ACCESS.2024.3371499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10453556","Binary analysis;compiler provenance recovery;machine learning;transfer learning;fine-tuning","Transfer learning;Vectors;Computer architecture;Task analysis;Optimization;Mathematical models;Learning systems;Binary sequences;Program processors;Machine learning","","2","","41","CCBYNCND","29 Feb 2024","","","IEEE","IEEE Journals"
"Toward Trustworthy Governance of AI-Generated Content (AIGC): A Blockchain-Driven Regulatory Framework for Secure Digital Ecosystems","F. Yang; M. Z. Abedin; Y. Qiao; L. Ye","School of Computer Science and Technology, Xi'an Jiaotong University, Xi'an, China; Department of Accounting and Finance, School of Management, Swansea University, Swansea, U.K.; School of Computer Science and Technology, Xi'an Jiaotong University, Xi'an, China; Xi'an Institute of Electromechanical Information Technology, Xi'an, China",IEEE Transactions on Engineering Management,"17 Oct 2024","2024","71","","14945","14962","Digital platforms are experiencing a growing presence of generative artificial intelligence (AI) content, raising concerns due to the prevalence of misinformation that disrupts market integrity. Consequently, the development of effective regulatory measures for overseeing generative AI content becomes imperative. This necessitates the establishment of mechanisms to detect and filter out inaccuracies, ensuring compliance with regulatory requirements. In addition, collaboration among experts, regulators, and AI developers is essential to encourage responsible AI deployment on digital platforms. Successful governance hinges on principles of transparency, accountability, and proactive risk management to navigate the evolving generative AI on digital platforms. Therefore, in order to address the security issues currently faced by artificial intelligence generated content (AIGC), this article first proposes a method of efficient cache mechanism for AIGC content. The secure method of determining the identity of AIGC content owners is proposed based on blockchain technology. Subsequently, it suggests mechanisms for access control and data encryption for generated content within a blockchain environment. Finally, it presents an efficient data supervision mechanism tailored to the AIGC environment. The methods outlined in this article aim to enhance security from three perspectives: protection of content creators' identities, safeguarding data security, and ensuring effective data supervision within the AIGC framework. The experimental results further confirm that our proposed method not only ensures the security of the AIGC framework but also provides an efficient data analysis and supervision solution for digital platforms.","1558-0040","","10.1109/TEM.2024.3472292","Natural Science Basic Research Program of Shaanxi(grant numbers:2023-JC-YB-490); Research Fund of Guangxi Key Lab of Multisource Information Mining & Security(grant numbers:MIMS24-06); Fundamental Research Funds for the Central Universities, JLU(grant numbers:93K172024K12); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10703091","Artificial intelligence generated content (AIGC) regulation;blockchain governance;consensus mechanism;data security;data traceability","Security;Generative AI;Artificial intelligence;Regulation;Data privacy;Blockchains;Engineering management;Data security;Data models;Reliability","","","","38","IEEE","3 Oct 2024","","","IEEE","IEEE Journals"
"A First Look at LLM-powered Smartphones","L. Wu; Y. Zhao; C. Wang; T. Liu; H. Wang","Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),"28 Nov 2024","2024","","","208","217","The integration of Large Language Models (LLMs) into edge devices such as smartphones represents a significant leap in mobile technology, promising enhanced user experiences and novel functionalities. This paper presents a first look at LLM-powered smartphones, addressing four key aspects: the current market landscape, core functions enabled by integrated LLMs, potential security risks, and user perceptions. The findings reveal a rapidly evolving market with major manufacturers competing to integrate LLMs, innovative features that improve user interaction, significant security challenges, and mixed user perceptions that balance enthusiasm for new capabilities with privacy concerns. This study contributes to understanding LLM integration in mobile devices and its implications for users, manufacturers, and the broader technological landscape.","2151-0849","979-8-4007-1249-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765104","","Industries;Privacy;Technological innovation;Ethics;Large language models;Conferences;User experience;Security;Smart phones;Software engineering","","","","50","","28 Nov 2024","","","IEEE","IEEE Conferences"
"Principles of Data Science: A beginner's guide to essential math and coding skills for data fluency and machine learning","S. Ozdemir",NA,Principles of Data Science: A beginner's guide to essential math and coding skills for data fluency and machine learning,"","2024","","","","","Transform your data into insights with must-know techniques and mathematical concepts to unravel the secrets hidden within your dataKey FeaturesLearn practical data science combined with data theory to gain maximum insights from dataDiscover methods for deploying actionable machine learning pipelines while mitigating biases in data and modelsExplore actionable case studies to put your new skills to use immediatelyPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionPrinciples of Data Science bridges mathematics, programming, and business analysis, empowering you to confidently pose and address complex data questions and construct effective machine learning pipelines. This book will equip you with the tools to transform abstract concepts and raw statistics into actionable insights. Starting with cleaning and preparation, you’ll explore effective data mining strategies and techniques before moving on to building a holistic picture of how every piece of the data science puzzle fits together. Throughout the book, you’ll discover statistical models with which you can control and navigate even the densest or the sparsest of datasets and learn how to create powerful visualizations that communicate the stories hidden in your data. With a focus on application, this edition covers advanced transfer learning and pre-trained models for NLP and vision tasks. You’ll get to grips with advanced techniques for mitigating algorithmic bias in data as well as models and addressing model and data drift. Finally, you’ll explore medium-level data governance, including data provenance, privacy, and deletion request handling. By the end of this data science book, you'll have learned the fundamentals of computational mathematics and statistics, all while navigating the intricacies of modern ML and large pre-trained models like GPT and BERT.What you will learnMaster the fundamentals steps of data science through practical examplesBridge the gap between math and programming using advanced statistics and MLHarness probability, calculus, and models for effective data controlExplore transformative modern ML with large language modelsEvaluate ML success with impactful metrics and MLOpsCreate compelling visuals that convey actionable insightsQuantify and mitigate biases in data and ML modelsWho this book is forIf you are an aspiring novice data scientist eager to expand your knowledge, this book is for you. Whether you have basic math skills and want to apply them in the field of data science, or you excel in programming but lack the necessary mathematical foundations, you’ll find this book useful. Familiarity with Python programming will further enhance your learning experience.","","9781837636006","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10460885.pdf&bkn=10460884&pdfType=book","","","","","","","","6 Mar 2024","","","Packt Publishing","Packt Publishing eBooks"
"Medical Image Watermarking in Machine Learning Environments: A review","C. Chekira; H. E. Fadili; Z. Lakhliai","Computer Science and Interdisciplinary Physics Laboratory (LIPI), National School of Applied Sciences (ENSA), Sidi Mohamed Ben Abdellah University (USMBA), Fez, Morocco; Computer Science and Interdisciplinary Physics Laboratory (LIPI), National School of Applied Sciences (ENSA), Sidi Mohamed Ben Abdellah University (USMBA), Fez, Morocco; Computer Science and Interdisciplinary Physics Laboratory (LIPI), National School of Applied Sciences (ENSA), Sidi Mohamed Ben Abdellah University (USMBA), Fez, Morocco","2024 IEEE 12th International Symposium on Signal, Image, Video and Communications (ISIVC)","4 Jul 2024","2024","","","1","6","Ensuring healthcare data protection is becoming necessary, given the vast current attempts of data unauthorized manipulation via the Internet. Hence, this paper aims to present a comprehensive overview of Medical Image Watermarking using advanced Machine Learning approaches. Our work was carried out by analyzing 16 deeply selected articles from well-known digital libraries according to specified watermarking characteristics. Research results are presented and discussed, and recommendations are made for further exploration.","2832-8337","979-8-3503-8526-7","10.1109/ISIVC61350.2024.10577842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10577842","Medical Image Watermarking;Optimization;Machine Learning;Transform domain","Reviews;Data protection;Watermarking;Machine learning;Medical services;Libraries;Internet","","","","22","IEEE","4 Jul 2024","","","IEEE","IEEE Conferences"
"Modern CMake for C++: Effortlessly build cutting-edge C++ code and deliver high-quality solutions","R. Świdziński; A. Kushnir",NA; NA,Modern CMake for C++: Effortlessly build cutting-edge C++ code and deliver high-quality solutions,"","2024","","","","","Gain proficiency in CMake and unlock the complete potential of C++ to develop exceptional projects Purchase of the print or Kindle book includes a free eBook in the PDF formatKey FeaturesGet to grips with CMake and take your C++ development skills to enterprise standardsUse hands-on exercises and self-assessment questions to lock-in your learningUnderstand how to build in an array of quality checks and tests for robust codeBook DescriptionModern CMake for C++ isn't just another reference book, or a repackaging of the documentation, but a blueprint to bridging the gap between learning C++ and being able to use it in a professional setting. It's an end-to-end guide to the automation of complex tasks, including building, testing, and packaging software. This second edition is significantly rewritten, restructured and refreshed with latest additions to CMake, such as support of C++20 Modules. In this book, you'll not only learn how to use the CMake language in CMake projects but also discover how to make those projects maintainable, elegant, and clean. As you progress, you'll dive into the structure of source directories, building targets, and packages, all while learning how to compile and link executables and libraries. You'll also gain a deeper understanding of how those processes work and how to optimize builds in CMake for the best results. You'll discover how to use external dependencies in your project – third-party libraries, testing frameworks, program analysis tools, and documentation generators. Finally, you'll gain profi ciency in exporting, installing, and packaging for internal and external purposes. By the end of this book, you'll be able to use CMake confi dently at a professional level.What you will learnUnderstand best practices to build ++ codeGain practical knowledge of the CMake languageGuarantee code quality with tests and static and dynamic analysisDiscover how to manage, discover, download, and link dependencies with CMakeBuild solutions that can be reused and maintained in the long termUnderstand how to optimize build artifacts and the build processProgram modern CMake and manage your build processesAcquire expertise in complex subjects such as CMake presetsWho this book is forThe book is for build engineers and software developers with knowledge of C/C++ programming who are looking to learn CMake to automate the process of building small and large software solutions. If you’re just getting started with CMake, a long-time GNU Make user, or simply looking to brush up on the latest best practices, this book is for you.","","9781805123361","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769316.pdf&bkn=10769315&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Perceptions of the Future of Artificial Intelligence on Social Media: A Topic Modeling and Sentiment Analysis Approach","A. Ocal","Department of Computer Engineering, Yildiz Technical University, İstanbul, Türkiye",IEEE Access,"11 Dec 2024","2024","12","","182386","182409","Today’s AI technology has various applications in many fields, thus creating opportunities to improve different aspects of daily life and optimize business operations. However, there are also societal expectations and concerns regarding AI and its future impacts. Investigating such societal opinions and feelings is essential for social acceptance, further development and distribution of such technology, regulation, and adaptation to changes and policies. Despite this situation, such an exploration has not been sufficiently conducted in the existing literature and the most appropriate methods for such an exploration have not been sufficiently investigated. To contribute to addressing this limitation in literature, this study applies topic modeling and sentiment analysis approaches to investigate societal opinions and feelings about the future of AI on social media, which includes conversations from various segments of society. A corpus consisting of 16,611 comments and 998 unique Reddit post titles was analyzed with a customized BERTopic model for topic modeling and a BERT sentiment classification model. This study highlights the significant advantages of using BERTopic and BERT models in analyzing a large sample of social media discussions. The results of this study can help realize the potential of text analytics methods through transformer-based language models to derive empirical findings from large-scale data samples.","2169-3536","","10.1109/ACCESS.2024.3510526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10772463","Artificial intelligence;BERT;BERTopic;sentiment analysis;topic modeling","Analytical models;Artificial intelligence;Social networking (online);Sentiment analysis;Data models;Computational modeling;Surveys;Interviews;Encoding;Content management","","","","67","CCBYNCND","2 Dec 2024","","","IEEE","IEEE Journals"
"Advances and Open Challenges in Federated Foundation Models","C. Ren; H. Yu; H. Peng; X. Tang; B. Zhao; L. Yi; A. Z. Tan; Y. Gao; A. Li; X. Li; Z. Li; Q. Yang","Wallenberg-NTU Presidential Postdoctoral Fellow with the School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Sweden; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; College of Computer, Nankai University, China; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Sweden; School of Medicine, Yale University, USA; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Executive Vice President of the Digital Research Institute of ENN Group, Langfang, China; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, and the Chief AI Officer of WeBank, Shenzhen, China",IEEE Communications Surveys & Tutorials,"","2025","PP","99","1","1","The integration of Foundation Models (FMs) with Federated Learning (FL) presents a transformative paradigm in Artificial Intelligence (AI). This integration offers enhanced capabilities, while addressing concerns of privacy, data decentralization and computational efficiency. This paper provides a comprehensive survey of the emerging field of Federated Foundation Models (FedFM), elucidating their synergistic relationship and exploring novel methodologies, challenges, and future directions that the FL research field needs to focus on in order to thrive in the age of FMs. A systematic multi-tiered taxonomy is proposed, categorizing existing FedFM approaches for model training, aggregation, trustworthiness, and incentivization. Key challenges, including how to enable FL to deal with high complexity of computational demands, privacy considerations, contribution evaluation, and communication efficiency, are thoroughly discussed. Moreover, this paper explores the intricate challenges of communication, scalability and security inherent in training/fine-tuning FMs via FL. It highlights the potential of quantum computing to revolutionize the processes of training, inference, optimization and security. This survey also introduces the implementation requirement of FedFM and some practical FedFM applications. It highlights lessons learned with a clear understanding of our findings for FedFM. Finally, this survey not only provides insights into the current state and challenges of FedFM, but also offers a blueprint for future research directions, emphasizing the need for developing trustworthy solutions. It serves as a foundational guide for researchers and practitioners interested in contributing to this interdisciplinary and rapidly advancing field.","1553-877X","","10.1109/COMST.2025.3552524","Internal talent award (TRACS) with Wallenberg-NTU Presidential Postdoctoral Fellowship; National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme(grant numbers:AISG2-RP-2020-019); RIE 2020 Advanced Manufacturing and Engineering (AME) Programmatic Fund(grant numbers:A20G8b0102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10930890","Federated learning;foundation models;federated foundation models;large language models;efficient training/aggregation;trustworthiness;incentivization;evaluation;quantum computing","Frequency modulation;Surveys;Training;Artificial intelligence;Adaptation models;Computational modeling;Foundation models;Data privacy;Reviews;Security","","","","","IEEE","18 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Genetic Improvement of Software: A Comprehensive Survey","J. Petke; S. O. Haraldsson; M. Harman; W. B. Langdon; D. R. White; J. R. Woodward","University College London, London, U.K.; University of Stirling, Stirling, U.K.; University College London, London, U.K.; University College London, London, U.K.; University College London, London, U.K.; University of Stirling, Stirling, U.K.",IEEE Transactions on Evolutionary Computation,"28 May 2018","2018","22","3","415","432","Genetic improvement (GI) uses automated search to find improved versions of existing software. We present a comprehensive survey of this nascent field of research with a focus on the core papers in the area published between 1995 and 2015. We identified core publications including empirical studies, 96% of which use evolutionary algorithms (genetic programming in particular). Although we can trace the foundations of GI back to the origins of computer science itself, our analysis reveals a significant upsurge in activity since 2012. GI has resulted in dramatic performance improvements for a diverse set of properties such as execution time, energy and memory consumption, as well as results for fixing and extending existing system functionality. Moreover, we present examples of research work that lies on the boundary between GI and other areas, such as program transformation, approximate computing, and software repair, with the intention of encouraging further exchange of ideas between researchers in these fields.","1941-0026","","10.1109/TEVC.2017.2693219","EPSRC Project(grant numbers:DAASE EP/J017515/1); EPSRC Project(grant numbers:GISMOE EP/I033688/1); DAASE Project(grant numbers:EP/J017515/1); FAIME Project(grant numbers:EP/N002849/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7911210","Genetic improvement (GI);survey","Genetic programming;Software;Software testing;History;Software engineering","","139","","219","CCBY","25 Apr 2017","","","IEEE","IEEE Journals"
"Exercise Generation and Student Cognitive Ability Research Based on ChatGPT and Rasch Model","Y. Niu; H. Xue","College of Computer and Information Science, Chongqing Normal University, Chongqing, China; College of Computer and Information Science, Chongqing Normal University, Chongqing, China",IEEE Access,"26 Oct 2023","2023","11","","116695","116705","In the context of generative artificial intelligence (AI), AIGCP (content generation-based AI products), represented by ChatGPT, have attracted extensive attention in the field of education. This study focuses on the discipline of university operating systems and adopts the Rasch model as the theoretical foundation. By combining ChatGPT with existing question banks and using the bidirectional fine-grained table method, it compiles questions that match the corresponding abilities for three different levels of student groups. This aims to explore personalized question matching and student cognitive ability analysis methods to support personalized teaching. The research findings indicate that ChatGPT is capable of matching exercises of similar difficulty under the Rasch model, but its accuracy in generating exercise content is relatively low, and the variety of exercise content is limited. Students’ performance in overall competency requires improvement. This study aims to leverage the combined strengths of ChatGPT and traditional educational assessment methods to introduce an innovative approach to support personalized instruction. It aims to establish the routine utilization of exercise creation by ChatGPT and personalized analysis of student cognitive abilities, thereby better fulfilling the demands of education within the classroom setting.","2169-3536","","10.1109/ACCESS.2023.3325741","Chongqing Normal University(grant numbers:20XLB035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10287345","Generative artificial intelligence;Rasch model;personalized question matching;cognitive ability;operating system exercises","Computational modeling;Chatbots;Mathematical models;Education;Analytical models;Testing;Operating systems;Generative adversarial networks;Artificial intelligence;Question answering (information retrieval)","","3","","24","CCBYNCND","18 Oct 2023","","","IEEE","IEEE Journals"
"Enhancing UAV Network Security: A Human-in-the-Loop and GAN-Based Approach to Intrusion Detection","Q. Zeng; F. Nait-Abdesselam","School of Science and Engineering, University of Missouri -Kansas City, Kansas City, MO, United States; School of Science and Engineering, University of Missouri -Kansas City, Kansas City, MO, United States",IEEE Internet of Things Journal,"","2025","PP","99","1","1","Unmanned Aerial Vehicles (UAVs) are becoming essential in various sectors such as commercial delivery, agricultural monitoring, and disaster response. Despite their benefits, the rapid adoption of UAVs poses substantial security challenges, especially in drone network intrusion detection. Traditional intrusion detection datasets often suffer from limitations like small sample sizes and uneven distribution, undermining the effectiveness of Intrusion Detection Systems (IDS). Moreover, conventional machine learning (ML) approaches generally require extensive, well-labeled datasets that are expensive and labor intensive to produce. To overcome these challenges, we introduce a Generative Adversarial Network (GAN) model designed to enhance and balance the limited datasets available for drone networks. This model significantly improves data quality and quantity, thus optimizing the training process for intrusion detection models. Furthermore, we propose a Human-in-the-Loop (HITL) ML framework that integrates human expertise to guide the learning process and mitigate the costs of labeling. Our comprehensive evaluation demonstrates that the combined application of the GAN model and the HITL framework significantly outperforms traditional baseline models. This approach not only achieves an intrusion detection accuracy of up to 99% across various experimental datasets but also dramatically reduces the requirement for large amounts of labeled data by up to 98%, providing a cost-effective solution for enhancing UAV network security.","2327-4662","","10.1109/JIOT.2025.3545389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10902040","UAV Networks;Intrusion Detection;HITL;Generative Adversarial Networks","Drones;Intrusion detection;Generative adversarial networks;Machine learning;Autonomous aerial vehicles;Training;Data models;Labeling;Accuracy;Machine learning algorithms","","","","","IEEE","24 Feb 2025","","","IEEE","IEEE Early Access Articles"
"LLM4TAP: LLM-Enhanced TAP Rule Recommendation","G. Wu; L. Hu; Y. Hu; X. Xiong; F. Wang","College of Computer Science and Technology, Jilin University, Changchun, China; College of Computer Science and Technology, Jilin University, Changchun, China; College of Computer Science and Technology, Jilin University, Changchun, China; College of Computer Science and Technology, Changchun University, Changchun, China; College of Computer Science and Technology, Jilin University, Changchun, China",IEEE Internet of Things Journal,"","2025","PP","99","1","1","Trigger-action programming (TAP) is an Internet of Things (IoT) paradigm that enables non-professional end-users to automate smart devices by formulating rules such as “IF you leave home, THEN turn off lights"". As the number of possible rules increases, manually browsing these rules becomes increasingly time-consuming for users. Recently, graph-based recommendation systems have shown promise in automatically suggesting rules, yet they face two issues. First, these studies struggle to identify and differentiate users’ demands (e.g., turning off lights) and intentions (e.g., energy saving). Second, they overlook the issue of sparse user-rule interactions. In this article, we propose LLM4TAP, a large language model (LLM) enhanced TAP rule recommendation framework, to address these issues. Prior to LLM4TAP, a user-rule graph is constructed to represent the interactions between users and rules. Within LLM4TAP, singular value decomposition is first employed to generate an augmented graph, strengthening global structural relationships between users and rules. Next, the reasoning capabilities of LLMs are utilized to infer users’ demands and intentions from the textual descriptions of rules and user-rule interactions, producing representations of these inferred demands and intentions. Finally, a dual representation alignment method is introduced, integrating user demands and intentions derived from LLMs with the global structural information from the augmentation graph within a contrastive learning framework to enhance representation performance. Extensive experiments demonstrate the effectiveness of LLM4TAP, achieving the maximum improvements of 8.96% and 4.72% over the strongest compared methods on the IFTTT and Wyze datasets, respectively.","2327-4662","","10.1109/JIOT.2025.3532977","Science and Technology Development Plan of Jilin Province of China(grant numbers:20220101115JC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10876163","Internet of Things;trigger-action programming;rule recommendation;large language models;contrastive Learning","Internet of Things;Programming;Recommender systems;Contrastive learning;Smart devices;Performance evaluation;Accuracy;Cognition;Automation;Turning","","","","","IEEE","5 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Image Generative Semantic Communication with Multi-Modal Similarity Estimation for Resource-Limited Networks","E. Hosonuma; T. Yamazaki; T. Miyoshi; A. Taya; Y. Nishiyama; K. Sezaki","Institute of Industrial Science, The University of Tokyo, Tokyo, Japan; College of Systems Engineering and Science, Shibaura Institute of Technology, Saitama-shi, Japan; College of Systems Engineering and Science, Shibaura Institute of Technology, Saitama-shi, Japan; Institute of Industrial Science, The University of Tokyo, Tokyo, Japan; Center for Spatial Information Science at The University of Tokyo, Kashiwa-shi, Japan; Center for Spatial Information Science at The University of Tokyo, Kashiwa-shi, Japan",IEICE Transactions on Communications,"10 Mar 2025","2025","E108-B","3","260","273","To reduce network traffic and support environments with limited resources, a method for transmitting images with minimal transmission data is required. Several machine learning-based image compression methods, which compress the data size of images while maintaining their features, have been proposed. However, in certain situations, reconstructing only the semantic information of images at the receiver end may be sufficient. To realize this concept, semantic-information-based communication, called semantic communication, has been proposed, along with an image transmission method using semantic communication. This method transmits only the semantic information of an image, and the receiver reconstructs it using an image-generation model. This method utilizes a single type of semantic information for image reconstruction, but reconstructing images similar to the original image using only this information is challenging. This study proposes a multimodal image transmission method that leverages various types of semantic information for efficient semantic communication. The proposed method extracts multimodal semantic information from an original image and transmits only that to a receiver. Subsequently, the receiver generates multiple images using an image-generation model and selects an output image based on semantic similarity. The receiver must select the result based only on the received features; however, evaluating semantic similarity using conventional metrics is challenging. Therefore, this study explores new metrics to evaluate the similarity between semantic features of images and proposes two scoring procedures for evaluating semantic similarity between images based on multiple semantic features. The results indicate that the proposed procedures can compare semantic similarities, such as position and composition, between the semantic features of the original and generated images.","1745-1345","","10.23919/transcom.2024EBP3056","NICT(grant numbers:JPJ012368C01101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10705025","semantic communication;image generation;image transmission;image captioning;semantic segmentation","Semantics;Receivers;Image reconstruction;Feature extraction;Image coding;Image communication;Measurement;Monitoring;Data mining;Transmitters","","","","41","","3 Oct 2024","","","IEICE","IEICE Journals"
"ConPCAC: Conditional Lossless Point Cloud Attribute Compression via Spatial Decomposition","J. Zhang; T. Chen; K. You; D. Ding; Z. Ma","School of Information Science and Technology, Hangzhou Normal University, Hangzhou, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Information Science and Technology, Hangzhou Normal University, Hangzhou, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","A conditional lossless point cloud attribute compression method, dubbed ConPCAC, is proposed. The previous work typically codes point attributes in a point cloud in an autoregressive way, incurring unbearable coding time. By contrast, ConPCAC proposes a group-wise conditional entropy model for fast coding while preserving coding performance. Specifically, ConPCAC adopts a “Group Decomposition - Attribute Initialization - Latent Distribution Prediction” framework. First, it flexibly decomposes the original point cloud into multiple groups according to the geometry coordinate distribution. Then, the first group is coded using a base coder, e.g., the standardized G-PCC, and the following groups are progressively coded using a neural coder conditioned on their preceding groups. Two key units, Attribute Initialization (Init) and Latent Distribution Prediction (LDP), are devised in the neural coder. The Init unit employs the nearest neighbor to initialize the attributes of a group, and the LDP unit further predicts the attribute probability distribution for the group. In this way, ConPCAC enables full correlation exploration across groups and parallel processing among points in a group. Finally, the predicted probabilities are fed into the arithmetic engine to code the true attribute values of each group. Extensive experiments demonstrate the performance of Con-PCAC. It achieves 14.59%, 10.32%, and 12.26% improvements over the latest G-PCC on the widely used 8iVFB, Owlii, and MVUB datasets, respectively, significantly outperforming state-of- the-art lossless PCAC methods. Moreover, its computational complexity is comparable to G-PCC and much lower than existing learning-based methods. Associated code and models will be released on the website https://github.com/3dpcc/ConPCAC.","1558-2205","","10.1109/TCSVT.2025.3540931","National Key Research and Development Program of China(grant numbers:2023YFC3706605); National Natural Science Foundation of China(grant numbers:62171174); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10879568","Point cloud;attribute compression;lossless coding;probability prediction","Point cloud compression;Encoding;Geometry;Transforms;Entropy;Decoding;Three-dimensional displays;Image color analysis;Correlation;Codes","","","","","IEEE","11 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Two-Dimensional Multiply-Accumulator for Classification of Neural Signals","Y. -C. Chen; H. -C. Chang; H. Chen","Instrument Technology Research Center, NARL, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan",IEEE Access,"23 Apr 2018","2018","6","","19714","19725","Automatic spike detection and classification have been used for a neuroelectronic interface to reduce data amount or even to interact with neurons in a closed loop. While conventional neuroelectronic interfaces employ voltage-mode circuits to amplify neural signals and convert the signals into binary data, the dynamic range and signal-to-noise ratio of these circuits are directly limited by the supply voltage. To release this constraint, this paper proposes an analog-to-time converter (ATC), which uses positive feedback to convert analog neural signals into a sequence of pulse trains. Custom-designed digital circuits, including two types of time-to-digital converters (TDCs) and a 2-D multiply-accumulator (2-D-MAC), are further proposed for processing such time-mode signals. The ATC is implemented with the standard 0.35-μm CMOS technology and proved able to convert analog voltages into pulse-width-modulated signals with a resolution of 6 bits. The TDCs and 2-D-MAC are realized in FPGA and compared to the standard digital IPs. The comparison indicates that the TDC based on dual counters minimizes area consumption and the other based on delayed clocks minimizes power consumption. The 2-D-MAC further facilitates parallel computation of partial products and allows data to be classified without summing up all partial products. Finally, the application of the proposed time-mode system is demonstrated as classifying neuronal spikes.","2169-3536","","10.1109/ACCESS.2018.2814625","Chip Implementation Center, The National Science Council, Taiwan; Ministry of Science and Technology, Taiwan(grant numbers:106-2622-8-007-014-TA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8314138","Analog-to-time converter (ATC);multiply-accumulate (MAC) operation;time-mode signal processing","Clocks;Field programmable gate arrays;Delays;Signal resolution;Dynamic range;Delay lines;Sorting","","2","","17","OAPA","12 Mar 2018","","","IEEE","IEEE Journals"
"Pyramid-VAE-GAN: Transferring hierarchical latent variables for image inpainting","H. Tian; L. Zhang; S. Li; M. Yao; G. Pan","College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China; College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China; Advanced Technology Research Institute, Zhejiang University, Hangzhou 310027, China; College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China; College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China; College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China",Computational Visual Media,"20 Feb 2025","2023","9","4","827","841","Significant progress has been made in image inpainting methods in recent years. However, they are incapable of producing inpainting results with reasonable structures, rich detail, and sharpness at the same time. In this paper, we propose the Pyramid-VAE-GAN network for image inpainting to address this limitation. Our network is built on a variational autoencoder (VAE) backbone that encodes high-level latent variables to represent complicated high-dimensional prior distributions of images. The prior assists in reconstructing reasonable structures when inpainting. We also adopt a pyramid structure in our model to maintain rich detail in low-level latent variables. To avoid the usual incompatibility of requiring both reasonable structures and rich detail, we propose a novel cross-layer latent variable transfer module. This transfers information about long-range structures contained in high-level latent variables to low-level latent variables representing more detailed information. We further use adversarial training to select the most reasonable results and to improve the sharpness of the images. Extensive experimental results on multiple datasets demonstrate the superiority of our method. Our code is available at https://github.com/thy960112/Pyramid-VAE-GAN.","2096-0662","","10.1007/s41095-022-0331-3","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897735","image inpainting;variational autoencoder (VAE);latent variable transfer (LTN);pyramid structure;generative model","Semantics;Training;Probabilistic logic;Decoding;Image reconstruction;Image restoration;Generative adversarial networks;Computational modeling;Autoencoders;Hybrid power systems","","","","","","20 Feb 2025","","","TUP","TUP Journals"
"Contrastive Learning for Adapting Language Model to Sequential Recommendation","F. -Y. Liang; W. -D. Xi; X. -X. Xing; W. Wan; C. -D. Wang; M. Chen; M. Guizani","School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; UX Center, NetEase Games, Guangzhou, China; UX Center, NetEase Games, Guangzhou, China; UX Center, NetEase Games, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Machine Learning Department, Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE",2024 IEEE International Conference on Data Mining (ICDM),"21 Feb 2025","2024","","","251","260","With the explosive growth of information, recommendation systems have emerged to alleviate the problem of information overload. In order to improve the performance of recommendation systems, many existing methods introduce Large Language Models to extract textual information from description text. However, Large Language Models are trained on large-scale generic textual data and may face a semantic gap for downstream recommendation tasks. To address the above issues, we propose Contrastive Learning for Adapting Language Model to Sequential Recommendation (CLA-Rec). In CLA-Rec, we first extract text embeddings from description text using Large Language Models and align the text embeddings learned by Large Language Models with the collaborative information through contrastive learning to obtain high-quality item representations. Through semantic alignment, we bridge the semantic gap between Large Language Models and the recommendation task. To map textual information and collaborative information into user representations, we utilize a Transformer model to learn user representations and capture user preferences by combining the semantically aligned item representations. Extensive experiments on three public datasets demonstrate that our method outperforms state-of-the-art approaches on multiple evaluation metrics, illustrating the effectiveness of the CLA-Rec model in adapting Large Language Models to recommendation tasks.","2374-8486","979-8-3315-0668-1","10.1109/ICDM59182.2024.00032","NSFC(grant numbers:62276277); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10884256","sequential recommendation;large language model;contrastive learning","Measurement;Adaptation models;Large language models;Semantics;Collaboration;Contrastive learning;Transformers;Data models;Data mining;Recommender systems","","","","60","IEEE","21 Feb 2025","","","IEEE","IEEE Conferences"
"Privacy in Neural Network Learning: Threats and Countermeasures","S. Chang; C. Li",Donghua University; Donghua University,IEEE Network,"3 Aug 2018","2018","32","4","61","67","Algorithmic breakthroughs, the feasibility of collecting huge amount of data, and increasing computational power, contribute to the remarkable achievements of NNs. In particular, since Deep Neural Network (DNN) learning presents astonishing results in speech and image recognition, the amount of sophisticated applications based on it has exploded. However, the increasing number of instances of privacy leakage has been reported, and the corresponding severe consequences have caused great worry in this area. In this article, we focus on privacy issues in NN learning. First, we identify the privacy threats during NN training, and present privacy-preserving training schemes in terms of using centralized and distributed approaches. Second, we consider the privacy of prediction requests, and discuss the privacy-preserving protocols for NN prediction. We also analyze the privacy vulnerabilities of trained models. Three types of attacks on private information embedded in trained NN models are discussed, and a differential privacy-based solution is introduced.","1558-156X","","10.1109/MNET.2018.1700447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8425302","","Training;Artificial neural networks;Privacy;Predictive models;Servers;Computational modeling","","29","","15","IEEE","3 Aug 2018","","","IEEE","IEEE Magazines"
"COVID-19 and Global Increases in Cybersecurity Attacks: Review of Possible Adverse Artificial Intelligence Attacks","A. N. Jaber; L. Fritsch","Artificial Intelligence Lab, Oslo Metropolitan University, Oslo, Norway; Artificial Intelligence Lab, Oslo Metropolitan University, Oslo, Norway",2021 25th International Computer Science and Engineering Conference (ICSEC),"27 Jan 2022","2021","","","434","442","The World Health Organization's (WHO) coronavirus disease dashboard has recorded over 207 million confirmed infections and over 4 million deaths. There has been an increasing vulnerability in cybersecurity amongst businesses, gov- ernments and individuals worldwide because the COVID-19 pandemic has led to additional online activities. Accordingly, many people have turned to online work whilst the world is locked down. Thus, warnings have been issued by cybersecurity agencies that the number of cyber threat actors is increasing, and that they are improving in terms of stealing money, personal information and intellectual property. Opportunities for cybercrimes have increased, and COVID-19 is an effective lure. New methods for adverse artificial intelligence (AI)-empowered cyberattacks have been developed, or will be in the near future, using various weaponisations of AI under the COVID-19 umbrella. For this reason, this study reviewed and summarised how and when the most recent cyberattack trends can successfully exploit COVID-19 as a context for attack. Additionally, a summary of the state of knowledge of adverse AI is given, and its potential within the COVID-themed security threats, including defenses, is discussed.","","978-1-6654-1197-4","10.1109/ICSEC53205.2021.9684603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684603","computer security;artificial intelligence;cyber-attack;COVID-19","COVID-19;Uniform resource locators;Pandemics;Phishing;Weapons;Malware;Servers","","6","","50","IEEE","27 Jan 2022","","","IEEE","IEEE Conferences"
"Application of Artificial Intelligence in the Creation of Web Content","M. Jovanić; M. Čarapina","Zagreb University of Applied Sciences, Zagreb, Croatia; Zagreb University of Applied Sciences, Zagreb, Croatia",2024 47th MIPRO ICT and Electronics Convention (MIPRO),"28 Jun 2024","2024","","","2063","2068","This paper delves into the application of artificial intelligence (AI) technology in the creation of web content, with a special emphasis on A/B testing as an optimization strategy. It analyses the way technical tools like Prisma, Next.js, Tailwind, and Clickhouse contribute to the development and analysis of web applications. The importance of Large Language Models (LLM) in developing interactive interfaces and providing application performance insights is also assessed, with a focus on user behavior analysis. The paper explores how AI, specifically tools like GPT-3.5-turbo, might enhance the process of creating content for the web. The usefulness and potential of AI in generating text are looked into in the context of the continued development of digital communication strategies. Methods such as A/B testing and performance monitoring have significance in evaluating the effectiveness of AI-generated content communicating with users. The work aims to provide an understanding of how the integration of AI and present-day technologies may increase user efficiency and satisfaction in web application development. The paper demonstrates the potential and constraints of applying AI in digital communication through theoretical and empirical research, emphasizing its relevance in shaping the future of web content.","2623-8764","979-8-3503-8250-1","10.1109/MIPRO60963.2024.10569691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10569691","artificial intelligence;content generation;web application development;A/B testing;LLM","Productivity;Ethics;Law;Transforms;Writing;Digital communication;Artificial intelligence","","","","6","IEEE","28 Jun 2024","","","IEEE","IEEE Conferences"
"A Large-Scale Survey on the Usability of AI Programming Assistants: Successes and Challenges","J. T. Liang; C. Yang; B. A. Myers","Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA",2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE),"14 Jun 2024","2024","","","616","628","The software engineering community recently has witnessed widespread deployment of AI programming assistants, such as GitHub Copilot. However, in practice, developers do not accept AI programming assistants' initial suggestions at a high frequency. This leaves a number of open questions related to the usability of these tools. To understand developers' practices while using these tools and the important usability challenges they face, we administered a survey to a large population of developers and received responses from a diverse set of 410 developers. Through a mix of qualitative and quantitative analyses, we found that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions. We also found the most important reasons why developers do not use these tools are because these tools do not output code that addresses certain functional or non-functional requirements and because developers have trouble controlling the tool to generate the desired output. Our findings have implications for both creators and users of AI programming assistants, such as designing minimal cognitive effort interactions with these tools to reduce distractions for users while they are programming.","1558-1225","979-8-4007-0217-4","","National Science Foundation(grant numbers:DGE1745016,DGE2140739,IIS-1856641); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548213","Software and its engineering → Software notations and tools;• Human-centered computing → Empirical studies in HCI;• Computing methodologies → Natural language processing;AI programming assistants;usability study","Surveys;Programming;Syntactics;Software;Artificial intelligence;Usability;Task analysis","","8","","62","CCBY","14 Jun 2024","","","IEEE","IEEE Conferences"
"Robust Image Watermarking Using Bidirection-Interactive and Context-Aware Networks","B. Yin; K. Yin","School of Computer and Communication Engineering, Changsha University of Science and Technology, Changsha, China; School of Computer and Communication Engineering, Changsha University of Science and Technology, Changsha, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Individuals can easily generate highly realistic images using artificial intelligence-generated content technologies, which complicates the verification of images’ ownership rights. This raises potential issues such as spreading misinformation, fraud, and copyright infringement. Digital watermarking is a promising solution to protect the copyright of a digital image by embedding watermarks within it. However, many existing deep learning-based watermarking approaches struggle to simultaneously resist multiple attacks effectively and maintain the quality of watermark images. In this paper, we propose a bidirectional-interactive and context-aware (BICA) deep network designed to enhance the robustness of the watermark while maintaining the quality of the encoded image. We propose a new attention module in the encoder to improve the invisibility and robustness of the watermarked images by implementing an adaptive two-way interaction between local and global features. Additionally, we employ fine-grained downsampling to enhance the attention module’s ability to capture comprehensive feature information. Extensive experimental results demonstrate that the BICA network can embed watermark information into an image without compromising image quality. For instance, BICA has an accuracy exceeding 95% against various moderate noise attacks, with average PSNR and SSIM values of 40.4021 dB and 0.9943, respectively.","1558-2205","","10.1109/TCSVT.2025.3543969","Natural Science Foundation of Hunan Province of China(grant numbers:2024JJ5043); Scientific Research Fund of Hunan Provincial Education Department of China(grant numbers:24A0230); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10896764","Image watermarking;deep learning;digital watermarking;noise attacks","Watermarking;Robustness;Transforms;Noise;Training;Accuracy;Feature extraction;Transform coding;Image coding;Discrete cosine transforms","","","","","IEEE","20 Feb 2025","","","IEEE","IEEE Early Access Articles"
"MultiSEAO-Mix: A Multimodal Multitask Framework for Sentiment, Emotion, Support, and Offensive Analysis in Code-Mixed Setting","G. V. Singh; Mamta; A. Verma; A. Ekbal","Department of Computer Science and Engineering, Indian Institute of Technology, Patna, India; Department of Computer Science and Engineering, Indian Institute of Technology, Patna, India; Department of Computer Science and Engineering, Indian Institute of Technology, Patna, India; School of Artificial Intelligence and Data Science, Indian Institute of Technology Jodhpur, Karwar, India",IEEE Transactions on Computational Social Systems,"3 Feb 2025","2025","12","1","101","112","Social media platforms have become an open door for users to share their views, resulting in a growing trend of offensive content being shared on social media. Detecting and addressing offensive content is crucial due to its significant impact on society. Although there has been extensive research on the detection of offensive content in the English language, there is a notable gap in detecting offensive content in multimodal settings involving code-mixed languages. In this article, we propose a large scale multimodal code-mixed dataset for Hinglish (Hindi+English) MultiSEAO-Mix focusing on women and children. The MultiSEAO-Mix is annotated with offensiveness, sentiment, emotion, and their respective intensities. Additionally, it is also annotated with author support. A multimodal, multitask framework is proposed that considers offensive detection, intensity prediction, and author support as the primary tasks and improves their performance using sentiment, emotion, and corresponding intensities as the auxiliary tasks. Further, we propose a fusion technique that captures the enhanced multimodal representation to improve the performance of our model. Experimental results demonstrate that the proposed multitask framework improves the model performance by more than 4.5 points compared to multitask system without sentiment and emotion as the auxiliary tasks.","2329-924X","","10.1109/TCSS.2024.3430821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10722034","Code-mixing;emotion;multitask;multimodal;offensive;sentiment","Social networking (online);Sentiment analysis;Annotations;Media;Hate speech;Feature extraction;Visualization;Vectors;Transformers;Stars","","","","53","IEEE","18 Oct 2024","","","IEEE","IEEE Journals"
"ProMark: Proactive Diffusion Watermarking for Causal Attribution","V. Asnani; J. Collomosse; T. Bui; X. Liu; S. Agarwal",Adobe Research; Adobe Research; University of Surrey; Michigan State University; Adobe Research,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","10802","10811","Generative AI (GenAI) is transforming creative work-flows through the capability to synthesize and manipulate images via high-level prompts. Yet creatives are not well supported to receive recognition or reward for the use of their content in GenAI training. To this end, we propose ProMark, a causal attribution technique to attribute a synthetically generated image to its training data concepts like objects, motifs, templates, artists, or styles. The concept information is proactively embedded into the input training images using imperceptible watermarks, and the diffusion models (unconditional or conditional) are trained to retain the corresponding watermarks in generated images. We show that we can embed as many as 216 unique water-marks into the training data, and each training image can contain more than one watermark. ProMark can maintain image quality whilst outperforming correlation-based attribution. Finally, several qualitative examples are presented, providing the confidence that the presence of the watermark conveys a causative relationship between training data and synthetic images.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.01027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10657140","watermarking;proactive learning;causal attribution;concept attribution;GenAI defense","Training;Image quality;Accuracy;PSNR;Sensitivity;Generative AI;Training data","","1","","39","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"Build a Large Language Model (From Scratch)","S. Raschka",Manning Publications,Build a Large Language Model (From Scratch),"","2024","","","","","Learn how to create, train, and tweak large language models (LLMs) by building one from the ground up! In Build a Large Language Model (from Scratch) bestselling author Sebastian Raschka guides you step by step through creating your own LLM. Each stage is explained with clear text, diagrams, and examples. You’ll go from the initial design and creation, to pretraining on a general corpus, and on to fine-tuning for specific tasks. Build a Large Language Model (from Scratch) teaches you how to:  Plan and code all the parts of an LLM Prepare a dataset suitable for LLM training Fine-tune LLMs for text classification and with your own data Use human feedback to ensure your LLM follows instructions Load pretrained weights into an LLM  Build a Large Language Model (from Scratch) takes you inside the AI black box to tinker with the internal systems that power generative AI. As you work through each key stage of LLM creation, you’ll develop an in-depth understanding of how LLMs work, their limitations, and their customization methods. Your LLM can be developed on an ordinary laptop, and used as your own personal assistant.","","9781633437166","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10745359.pdf&bkn=10745358&pdfType=book","GPT;LLM;personal assistant;text classification;chatbot;train;pretrained weights;Python;fine-tune;human feedback;embeddings;attention mechanisms;generative;AI;Top-k","","","","","","","6 Nov 2024","","","Manning","Manning eBooks"
"Trusted Deep Neural Execution—A Survey","M. F. Babar; M. Hasan","School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA; School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA",IEEE Access,"15 May 2023","2023","11","","45736","45748","The growing use of deep neural networks (DNNs) in various applications has raised concerns about the security and privacy of model parameters and runtime execution. To address these concerns, researchers have proposed using trusted execution environments (TEEs) to build trustworthy neural network execution. This paper comprehensively surveys the literature on trusted neural networks, viz., answering how to efficiently execute neural models inside trusted enclaves. We review the various TEE architectures and techniques employed to achieve secure neural network execution and provide a classification of existing work. Additionally, we discuss the challenges and present a few open issues. We intend that this review will assist researchers and practitioners in understanding the state-of-the-art and identifying research problems.","2169-3536","","10.1109/ACCESS.2023.3274190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10121428","Neural network;DNN;trusted execution;TEE;TrustZone;SGX","Neural networks;Program processors;Codes;Surveys;Computational modeling;Data models;Trust computing","","1","","55","CCBYNCND","8 May 2023","","","IEEE","IEEE Journals"
"Large Language Models in Education: A Systematic Review","B. Dong; J. Bai; T. Xu; Y. Zhou","Faculty of Education, Shaanxi Normal University, Xi'an, China; Faculty of Education, Shaanxi Normal University, Xi'an, China; School of Software, Northwestern Polytechnical University, Xi'an, China; Faculty of Education, Shaanxi Normal University, Xi'an, China",2024 6th International Conference on Computer Science and Technologies in Education (CSTE),"18 Jul 2024","2024","","","131","134","Large Language Models (LLMs) refer to a type of generative artificial intelligence model that produces responses to natural language input. The purpose of this study is to analyze the current application status of LLMs in the field of education through a systematic review of the literature. Data were sourced from three databases: Web of Science, ERIC, and Google Scholar. The study includes 94 documents, analyzed from both qualitative and quantitative perspectives. The results show that large language models have great potential in the field of education, specifically in generating medical content, serving as an English learning assistant, assisting academic research, and evaluating the quality of tests, etc. However, there are still potential dangers such as hindering the development of critical thinking, creating academic integrity crises, and ethical and moral challenges. These findings showed the current application status of LLMs in education, laying the groundwork to inspire future research.","","979-8-3503-5180-4","10.1109/CSTE62025.2024.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10589960","large language model;ChatGPT;artificial intelligence;education;systematic review","Ethics;Systematics;Reviews;Generative AI;Large language models;Education;Natural languages","","3","","36","IEEE","18 Jul 2024","","","IEEE","IEEE Conferences"
"Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion","S. Lee; B. Hoover; H. Strobelt; Z. J. Wang; S. Peng; A. Wright; K. Li; H. Park; H. Yang; D. H. P. Chau",Georgia Tech; Georgia Tech; IBM Research; Georgia Tech; Georgia Tech; Georgia Tech; Georgia Tech; Georgia Tech; Georgia Tech; Georgia Tech,2024 IEEE Visualization and Visual Analytics (VIS),"2 Dec 2024","2024","","","96","100","Diffusion-based generative models’ impressive ability to create convincing images has garnered global attention. However, their complex structures and operations often pose challenges for non-experts to grasp. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion’s complex structure with explanations of the underlying operations. By comparing image generation of prompt variants, users can discover the impact of keyword changes on image generation. A 56-participant user study demonstrates that Diffusion Explainer offers substantial learning benefits to non-experts. Our tool has been used by over 10,300 users from 124 countries at https://poloclub.github.io/diffusion-explainer/.","2771-9553","979-8-3503-5485-0","10.1109/VIS55277.2024.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771097","Generative AI;Machine Learning;Interactive visualization;Text-to-image generative AI;Artificial Intelligence;User study","Visualization;Image synthesis;Visual analytics;Blogs;Text to image;Transforms;Animation","","1","","50","IEEE","2 Dec 2024","","","IEEE","IEEE Conferences"
"Modelling of Firefly Algorithm with Densely Connected Networks for Near-Duplicate Image Detection System","S. Sundaram; K. Somasundaram; S. Jothilakshmi; S. Jayaraman; P. Dhanalakshmi","Department of Information Technology, Annamalai University, Annamalai Nagar; Department of Information Technology, School of Computing Sciences VISTAS (Vels Institute of Sciences Technology & Advanced Studies), Pallavaram, Chennai; Department of Information Technology, Annamalai University, Annamalai Nagar; Department of Information Technology, Annamalai University, Annamalai Nagar; Dept of Computer Science & Engineering, Annamalai University, Annamalai Nagar",2023 International Conference on Sustainable Communication Networks and Application (ICSCNA),"1 Jan 2024","2023","","","66","72","Near-duplicate image detection is the way of detecting and flagging images that are highly similar to each other but not identical. It is a crucial task in different fields, including search engines, content management, and copyright enforcement, as it helps in content and organization de duplication. To achieve this, techniques and algorithms are employed that calculate similarity metrics, compare features, or analyze image content to define the degree of resemblance between images. Near-duplicate image detection can include approaches such as feature extraction, machine learning, and perceptual hashing to effectively manage and identify similar images in large datasets, which offer benefits in content retrieval and storage optimization. Therefore, this study presents a new firefly algorithm with deep learning-based near-duplicate image detection (FFADL-NDID) technique. Initially, median filtering (MF) approach is used to preprocess the input images. The proposed FFADL-NDID technique exploits the robust feature extraction abilities of DenseNet, a pre-trained DL model for capturing complex visual patterns from database and query images. In addition, the FFA is applied to carry out the hyperparameter tuning, optimizing the system for superior performance. This synergistic fusion enhances the overall efficiency of near-duplicate image detection by successfully searching the hyperparameter space for optimal configurations. Finally, the FFADL-NDID framework applies Euclidean distance-based similarity matching processes, which detects the near-duplicate images significantly. The simulation analysis of the FFADL-NDID method is tested on multiple datasets and the outcomes show its promising performance over other DL models in terms of different measures.","","979-8-3503-1398-7","10.1109/ICSCNA58489.2023.10370117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10370117","Near duplicate image detection;Computer vision;Deep learning;Firefly algorithm;Hyperparameter tuning","Measurement;Visualization;Machine learning algorithms;Organizations;Search engines;Feature extraction;Visual databases","","1","","25","IEEE","1 Jan 2024","","","IEEE","IEEE Conferences"
"Semantics-Preserving Reinforcement Learning Attack Against Graph Neural Networks for Malware Detection","L. Zhang; P. Liu; Y. -H. Choi; P. Chen","Department of Information Science and Technology, Penn State University, State College, PA, USA; Department of Information Science and Technology, Penn State University, State College, PA, USA; Pusan National University, Kumjeong-ku, Korea; Fudan University, Shanghai, China",IEEE Transactions on Dependable and Secure Computing,"13 Mar 2023","2023","20","2","1390","1402","As an increasing number of deep-learning-based malware scanners have been proposed, the existing evasion techniques, including code obfuscation and polymorphic malware, are found to be less effective. In this work, we propose a reinforcement learning based semantics-preserving (i.e. functionality-preserving) attack against black-box GNNs (Graph Neural Networks) for malware detection. The key factor of adversarial malware generation via semantic Nops insertion is to select the appropriate semantic Nops and their corresponding basic blocks. The proposed attack uses reinforcement learning to automatically make these “how to select” decisions. To evaluate the attack, we have trained two kinds of GNNs with three types (e.g., Backdoor, Trojan, and Virus) of Windows malware samples and various benign Windows programs. The evaluation results have shown that the proposed attack can achieve a significantly higher evasion rate than four baseline attacks, namely the binary diversification attack, the semantics-preserving random instruction insertion attack, the semantics-preserving accumulative instruction insertion attack, and the semantics-preserving gradient-based instruction insertion attack.","1941-0018","","10.1109/TDSC.2022.3153844","ARO(grant numbers:W911NF-13-1-0421); National Science Foundation(grant numbers:CNS-1814679,CNS-2019340); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721612","Adversarial samples generation;graph neural networks;malware detection;reinforcement learning","Malware;Feature extraction;Codes;Semantics;Reinforcement learning;Graph neural networks;Viruses (medical)","","26","","47","IEEE","25 Feb 2022","","","IEEE","IEEE Journals"
"An Introduction to the JPEG Fake Media Initiative","F. Temmermans; D. Bhowmik; F. Pereira; T. Ebrahimi","Department of Electronics & Informatics (ETRO), Vrije Universiteit Brussel, Brussels, Belgium; Division of Computing Science & Mathematics, University of Stirling, Stirling, UK; Instituto Superior Técnico, Instituto de Telecomunicações, Lisboa, Portugal; Multimedia Signal Processing Group, Ecole Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland",2021 IEEE 4th International Conference on Multimedia Information Processing and Retrieval (MIPR),"19 Oct 2021","2021","","","406","411","Recent advances in media creation and modification allow to produce near realistic media assets that are almost indistinguishable from original assets to the human eye. These developments open opportunities for creative production of new media in the entertainment and art industry. However, the intentional or unintentional spread of manipulated media, i.e., modified media with the intention to induce misinterpretation, also imposes risks such as social unrest, spread of rumours for political gain or encouraging hate crimes. The clear and transparent annotation of media modifications is considered to be a crucial element in many usage scenarios bringing trust to the users. This has already triggered various organizations to develop mechanisms that can detect and annotate modified media assets when they are shared. However, these annotations should be attached to the media in a secure way to prevent them of being compromised. In addition, to achieve a wide adoption of such an annotation ecosystem, interoperability is essential and this clearly calls for a standard. This paper presents an initiative by the JPEG Committee called JPEG Fake Media. The scope of JPEG Fake Media is the creation of a standard that can facilitate the secure and reliable annotation of media asset creation and modifications. The standard shall support usage scenarios that are in good faith as well as those with malicious intent. This paper gives an overview of the current state of this initiative and introduces already identified use cases and requirements.","","978-1-6654-1865-2","10.1109/MIPR51284.2021.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565508","JPEG;fake media;standardisation;media creation and modification;deepfake;authenticity;media forensics","Annotations;Conferences;Standards organizations;Transform coding;Production;Media;Reliability","","","","25","IEEE","19 Oct 2021","","","IEEE","IEEE Conferences"
"Dataset Distillation: A Comprehensive Review","R. Yu; S. Liu; X. Wang","National University of Singapore, Singapore; National University of Singapore, Singapore; National University of Singapore, Singapore",IEEE Transactions on Pattern Analysis and Machine Intelligence,"5 Dec 2023","2024","46","1","150","170","Recent success of deep learning is largely attributed to the sheer amount of data used for training deep neural networks. Despite the unprecedented success, the massive data, unfortunately, significantly increases the burden on storage and transmission and further gives rise to a cumbersome model training process. Besides, relying on the raw data for training per se yields concerns about privacy and copyright. To alleviate these shortcomings, dataset distillation (DD), also known as dataset condensation (DC), was introduced and has recently attracted much research attention in the community. Given an original dataset, DD aims to derive a much smaller dataset containing synthetic samples, based on which the trained models yield performance comparable with those trained on the original dataset. In this paper, we give a comprehensive review and summary of recent advances in DD and its application. We first introduce the task formally and propose an overall algorithmic framework followed by all existing DD methods. Next, we provide a systematic taxonomy of current methodologies in this area, and discuss their theoretical interconnections. We also present current challenges in DD through extensive empirical studies and envision possible directions for future works.","1939-3539","","10.1109/TPAMI.2023.3323376","Ministry of Education Singapore(grant numbers:MOE-T2EP20122-0006); National Research Foundation Singapore(grant numbers:AISG2-RP-2021-023); Advanced Research and Technology Innovation Centre, College of Design and Engineering, National University of Singapore; National University of Singapore(grant numbers:A-0005947-21-00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275116","Dataset distillation;dataset condensation;data compression;efficient learning","Training;Synthetic data;Data models;Optimization;Data privacy;Knowledge engineering;Computer architecture","","27","","183","IEEE","10 Oct 2023","","","IEEE","IEEE Journals"
"The Impact of ChatGPT on Streaming Media: A Crowdsourced and Data-Driven Analysis using Twitter and Reddit","Y. Feng; P. Poralla; S. Dash; K. Li; V. Desai; M. Qiu",University of North Texas; University of North Texas; University of North Texas; Ohio State University; University of North Texas; Dakota State University,"2023 IEEE 9th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)","26 May 2023","2023","","","222","227","ChatGPT, a general-purpose text generation AI model, is reshaping various domains ranging from education and software development to legal defense and novel writing. Despite its potential impact, there is a lack of research on how ChatGPT might influence streaming media, which is an essential part of everyday entertainment. As a result, it remains unclear how ChatGPT is changing the future of streaming media. To bridge such a research gap, in this paper, we propose a crowdsourced, data-driven framework that leverages two social media platforms, Twitter and Reddit, to explore the impact of ChatGPT on streaming media. Through extensive analysis of social media data collected from Twitter and Reddit, we reveal how ChatGPT is transforming streaming media from diverse perspectives. Our data analytics demonstrates that ChatGPT is sparking both fear and excitement in the context of the streaming media and enhancing the downstream visual generative models, such as DALLE-2 and Stable Diffusion Videos. To the best of our knowledge, this study is the first large-scale and systematical investigation into the effects of ChatGPT on streaming media. Hope our findings will inspire further research and discussions on this topic across academia and industry.","","979-8-3503-1293-5","10.1109/BigDataSecurity-HPSC-IDS58521.2023.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10132120","ChatGPT;social networks;streaming media;data analysis;Twitter;Reddit","Analytical models;Visualization;Social networking (online);Blogs;Streaming media;Media;Writing","","9","","24","IEEE","26 May 2023","","","IEEE","IEEE Conferences"
"A Secure and Robust Knowledge Transfer Framework via Stratified-Causality Distribution Adjustment in Intelligent Collaborative Services","J. Jia; S. Ma; L. Wang; Y. Liu; R. H. Deng","School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of System and Computing, University of New South Wales, Canberra, ACT, Australia; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Information Systems, Singapore Management University, Singapore",IEEE Transactions on Computers,"22 Dec 2023","2024","73","1","58","72","The rapid development of device-edge-cloud collaborative computing techniques has actively contributed to the popularization and application of intelligent service models. The intensity of knowledge transfer plays a vital role in enhancing the performance of intelligent services. However, the existing knowledge transfer methods are mainly implemented through data fine-tuning and model distillation, which may cause the leakage of data privacy or model copyright in intelligent collaborative systems. To address this issue, we propose a secure and robust knowledge transfer framework through stratified-causality distribution adjustment (SCDA) for device-edge-cloud collaborative services. Specifically, a simple yet effective density-based estimation is first employed to obtain uncertainty scores that guide the space stratification, which is conducive to reconstructing low-density distribution regions from high-density distribution regions more adaptively and accurately. Subsequently, we devise a novel causality-aware generative model to generate synthetic features for the out-of-distribution domain by exploring the relationship between factors and variables. Ultimately, we introduce a cycle-consistent minimax optimization mechanism to ensure the effectiveness and dependability of knowledge transfer through the influence minimization and the diversity maximization. Furthermore, extensive experiments demonstrate that our scheme can protect the security of data privacy and model copyright in intelligent collaborative services through adaptive distribution adjustment.","1557-9956","","10.1109/TC.2023.3318403","National Natural Science Foundation of China(grant numbers:62372334); Start-up Research Fund of Southeast University(grant numbers:RF1028623129); National Research Foundation, SingaporeDSO National LaboratoriesAI Singapore Programme(grant numbers:AISG2-GC-2023-008); National Research Foundation, SingaporeCyber Security AgencyNational Cybersecurity R&D Programme(grant numbers:NCRP25-P04-TAICeN); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261341","Intelligent collaborative service;knowledge transfer;privacy preservation;copyright protection;adaptive distribution adjustment","Knowledge transfer;Collaboration;Task analysis;Data models;Adaptation models;Artificial intelligence;Robustness","","","","45","IEEE","22 Sep 2023","","","IEEE","IEEE Journals"
"A Roadmap Toward the Resilient Internet of Things for Cyber-Physical Systems","D. Ratasich; F. Khalid; F. Geissler; R. Grosu; M. Shafique; E. Bartocci","Institute of Computer Engineering, TU Wien, Vienna, Austria; Institute of Computer Engineering, TU Wien, Vienna, Austria; CPS Dependability Research Lab, Intel Deutschland GmbH, Neubiberg, Germany; Institute of Computer Engineering, TU Wien, Vienna, Austria; Institute of Computer Engineering, TU Wien, Vienna, Austria; Institute of Computer Engineering, TU Wien, Vienna, Austria",IEEE Access,"5 Feb 2019","2019","7","","13260","13283","The Internet of Things (IoT) is a ubiquitous system connecting many different devices - the things - which can be accessed from the distance. The cyber-physical systems (CPSs) monitor and control the things from the distance. As a result, the concepts of dependability and security get deeply intertwined. The increasing level of dynamicity, heterogeneity, and complexity adds to the system's vulnerability, and challenges its ability to react to faults. This paper summarizes the state of the art of existing work on anomaly detection, fault-tolerance, and self-healing, and adds a number of other methods applicable to achieve resilience in an IoT. We particularly focus on non-intrusive methods ensuring data integrity in the network. Furthermore, this paper presents the main challenges in building a resilient IoT for the CPS, which is crucial in the era of smart CPS with enhanced connectivity (an excellent example of such a system is connected autonomous vehicles). It further summarizes our solutions, work-in-progress and future work to this topic to enable “Trustworthy IoT for CPS”. Finally, this framework is illustrated on a selected use case: a smart sensor infrastructure in the transport domain.","2169-3536","","10.1109/ACCESS.2019.2891969","IoT4CPS; Österreichische Forschungsförderungsgesellschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606923","Anomaly detection;cyber-physical systems (CPS);Internet of Things (IoT);monitoring;resilience;long-term dependability and security;self-adaptation;self-healing","Resilience;Security;Internet of Things;Cyber-physical systems;Robustness;Safety;Monitoring","","89","","203","CCBY","10 Jan 2019","","","IEEE","IEEE Journals"
"Machine Learning for Healthcare-IoT Security: A Review and Risk Mitigation","M. A. Khatun; S. F. Memon; C. Eising; L. L. Dhirani","Department Electronic and Computer Engineering, University of Limerick, Limerick, Ireland; Department Electronic and Computer Engineering, University of Limerick, Limerick, Ireland; Department Electronic and Computer Engineering, University of Limerick, Limerick, Ireland; Department Electronic and Computer Engineering, University of Limerick, Limerick, Ireland",IEEE Access,"29 Dec 2023","2023","11","","145869","145896","The Healthcare Internet-of-Things (H-IoT), commonly known as Digital Healthcare, is a data-driven infrastructure that highly relies on smart sensing devices (i.e., blood pressure monitors, temperature sensors, etc.) for faster response time, treatments, and diagnosis. However, with the evolving cyber threat landscape, IoT devices have become more vulnerable to the broader risk surface (e.g., risks associated with generative AI, 5G-IoT, etc.), which, if exploited, may lead to data breaches, unauthorized access, and lack of command and control and potential harm. This paper reviews the fundamentals of healthcare IoT, its privacy, and data security challenges associated with machine learning and H-IoT devices. The paper further emphasizes the importance of monitoring healthcare IoT layers such as perception, network, cloud, and application. Detecting and responding to anomalies involves various cyber-attacks and protocols such as Wi-Fi 6, Narrowband Internet of Things (NB-IoT), Bluetooth, ZigBee, LoRa, and 5G New Radio (5G NR). A robust authentication mechanism based on machine learning and deep learning techniques is required to protect and mitigate H-IoT devices from increasing cybersecurity vulnerabilities. Hence, in this review paper, security and privacy challenges and risk mitigation strategies for building resilience in H-IoT are explored and reported.","2169-3536","","10.1109/ACCESS.2023.3346320","Science Foundation Ireland (SFI)(grant numbers:18/CRT/6049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10371310","Healthcare-IoT;generative AI;5G-IoT;security and privacy challenges;cybersecurity;attacks;anomaly detection;machine learning;deep learning;mitigation techniques;5G NR","Medical services;Internet of Things;Security;Sensors;Machine learning;Temperature sensors;Monitoring","","13","","280","CCBY","22 Dec 2023","","","IEEE","IEEE Journals"
"The Long and Winding Road Toward Efficient High-Performance Computing","W. Jalby; D. Kuck; A. D. Malony; M. Masella; A. Mazouz; M. Popov","Department of Computer Science, Université de Versailles Saint-Quentin en Yvelines UFR des Sciences, Versailles, France; SSG, INTEL, Austin, TX, USA; Department of Computer Science, University of Oregon, Eugene, OR, USA; DRF/DSV, CEA, Gif sur Yvette, France; HPC, BULL/ATOS, Bruyères le Chatel, France; Department of Computer Science, Université de Versailles Saint-Quentin en Yvelines UFR des Sciences, Versailles, France",Proceedings of the IEEE,"26 Oct 2018","2018","106","11","1985","2003","The major challenge to Exaflop computing, and more generally, efficient high-end computing, is in finding the best “matches” between advanced hardware capabilities and the software used to program applications, so that top performance will be achieved. Several benchmarks show very disappointing performance progress over the last decade, clearly indicating a mismatch between hardware and software. To remedy this problem, it is important that key performance enablers at the software level-autotuning, performance analysis tools, full application optimization-are understood. For each area, we highlight major limitations and most promising approaches to reaching better performance and energy levels. Finally, we conclude by analyzing hardware and software design, trying to pave the way for more tightly integrated hardware and software codesign.","1558-2256","","10.1109/JPROC.2018.2851190","Intel Corporation; UVSQ; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8430508","Autotuning;benchmarking;hardware design;molecular dynamics;performance evaluation tools","Benchmark testing;High performance computing;Performance evaluation;Hardware design languages;Program processors;Molecular computing","","3","","65","IEEE","9 Aug 2018","","","IEEE","IEEE Journals"
"MACE: Mass Concept Erasure in Diffusion Models","S. Lu; Z. Wang; L. Li; Y. Liu; A. W. -K. Kong","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Institute for Infocomm Research (IR) & Centre for Frontier AI Research (CFAR), A*STAR, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","6430","6440","The rapid expansion of large-scale text-to-image diffusion models has raised growing concerns regarding their potential misuse in creating harmful or misleading content. In this paper, we introduce MACE, a finetuning framework for the task of MAss Concept Erasure. This task aims to prevent models from generating images that embody unwanted concepts when prompted. Existing concept erasure methods are typically restricted to handling fewer than five concepts simultaneously and struggle to find a balance between erasing concept synonyms (generality) and maintaining unrelated concepts (specificity). In contrast, MACE differs by successfully scaling the erasure scope up to 100 concepts and by achieving an effective balance between generality and specificity. This is achieved by leveraging closed-form cross-attention refinement along with LoRA finetuning, collectively eliminating the information of undesirable concepts. Furthermore, MACE integrates multiple LoRAs without mutual interference. We conduct extensive evaluations of MACE against prior methods across four different tasks: object erasure, celebrity erasure, explicit content erasure, and artistic style erasure. Our results reveal that MACE surpasses prior methods in all evaluated tasks. Code is available at https://github.com/Shilin-LU/MACE.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.00615","National Research Foundation, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10657770","Generative AI;AI security;diffusion model;concept editing","Computer vision;Codes;Text to image;Interference;Diffusion models;Pattern recognition","","4","","77","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"Large Language Models and Artificial Intelligence Generated Content Technologies Meet Communication Networks","J. Guo; M. Wang; H. Yin; B. Song; Y. Chi; F. R. Yu; C. Yuen","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; School of Electrical and Electronics Engineering, Nanyang Technological University, Jurong West, Singapore",IEEE Internet of Things Journal,"10 Jan 2025","2025","12","2","1529","1553","Artificial intelligence generated content (AIGC) technologies, with a predominance of large language models (LLMs), have demonstrated remarkable performance improvements in various applications, which have attracted great interests from both academia and industry. Although some noteworthy advancements have been made in this area, a comprehensive exploration of the intricate relationship between AIGC and communication networks remains relatively limited. To address this issue, this article conducts an exhaustive survey from dual standpoints: first, it scrutinizes the integration of LLMs and AIGC technologies within the domain of communication networks and second, it investigates how the communication networks can further bolster the capabilities of LLMs and AIGC. Additionally, this research explores the promising applications along with the challenges encountered during the incorporation of these AI technologies into communication networks. Through these detailed analyses, our work aims to deepen the understanding of how LLMs and AIGC can synergize with and enhance the development of advanced intelligent communication networks, contributing to a more profound comprehension of next-generation intelligent communication networks.","2327-4662","","10.1109/JIOT.2024.3496491","National Natural Science Foundation of China(grant numbers:62471357,62372357,62201424); Key Research and Development Program of Shaanxi under Program 2023-YBGY-218; Fundamental Research Funds for the Central Universities(grant numbers:QTZX23072,ZYTS24100); Proof of Concept Foundation of Xidian University Hangzhou Institute of Technology(grant numbers:GNYZ2023XJ0301); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10750803","Artificial intelligence generated content (AIGC);communication networks;generative models;large language models (LLMs);novel network architecture","Communication networks;Training;Surveys;6G mobile communication;Noise;Data models;Chatbots;Adaptation models;Security;Reviews","","","","174","IEEE","12 Nov 2024","","","IEEE","IEEE Journals"
"Issues with Generic Large Language Models (GLLMs)","D. Dasgupta; A. Roy","Dept. of Computer Science, The University of Memphis, Memphis, TN, USA; Dept. of Computer Science, The University of Memphis, Memphis, TN, USA",2024 Artificial Intelligence for Business (AIxB),"3 Dec 2024","2024","","","47","50","Generic Large Language Models (GLLMs) are continuously being released with enhanced size and capabilities, promoting the abilities of these tools for different use. GLLMs excel in text, image, and video generation (assembling, summarizing, translating) with proper queries and prompts. However, the reliability of GLLMs’ responses is questionable in critical applications due to factual inaccuracies, and inappropriate or unrelated responses. Also there remain many open questions on the data collection-privacy, legal and ethical issues. This short report emphasizes the reliability and security aspects of GLLMs while recognizing significant benefits in a wide variety of applications.","","979-8-3503-9103-9","10.1109/AIxB62249.2024.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771198","Generative AI;Large Language Models (LLMs);Generative Pre-Trained Models (GPTs);Small Parameterized Data Models (SPDM)","Ethics;Law;Large language models;Data models;Reliability;Security;Business","","","","34","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"Trustworthy and Synergistic Artificial Intelligence for Software Engineering: Vision and Roadmaps","D. Lo","School of Computing and Information Systems, Singapore Management University, Singapore",2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE),"8 Mar 2024","2023","","","69","85","For decades, much software engineering research has been dedicated to devising automated solutions aimed at enhancing developer productivity and elevating software quality. The past two decades have witnessed an unparalleled surge in the development of intelligent solutions tailored for software engineering tasks. This momentum established the Artificial Intelligence for Software Engineering (AI4SE) area, which has swiftly become one of the most active and popular areas within the software engiueering field. This Future of Software Engineering (FoSE) paper navigates through several focal points. It commences with a succinct introduction and history of AI4SE. Thereafter, it underscores the core challenges inherent to AI4SE, particularly highlighting the need to realize trustworthy and synergistic AI4SE. Progressing, the paper paints a vision for the potential leaps achievable if AI4SE's key challenges are surmounted, suggesting a transition toward Software Engineering 2.0. Two strategic roadmaps are then laid out: one centered on realizing trustworthy AI4SE, and the other on fostering synergistic AI4SE. While this paper may not serve as a conclusive guide, its intent is to catalyze further progress. The ultimate aspiration is to position AI4SE as a linchpin in redefining the horizons of software engineering, propelling us toward Software Engineering 2.0.","","979-8-3503-2496-9","10.1109/ICSE-FoSE59343.2023.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10449668","AI4SE;Trustworthy AI;Human-AI Collaboration;Software Engineering 2.0;Vision;Roadmaps","Productivity;Navigation;Software quality;Propulsion;History;Artificial intelligence;Task analysis","","11","","152","IEEE","8 Mar 2024","","","IEEE","IEEE Conferences"
"RoSteALS: Robust Steganography using Autoencoder Latent Space","T. Bui; S. Agarwal; N. Yu; J. Collomosse",University of Surrey; Adobe Research; Salesforce Research; University of Surrey,2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),"14 Aug 2023","2023","","","933","942","Data hiding such as steganography and invisible watermarking has important applications in copyright protection, privacy-preserved communication and content provenance. Existing works often fall short in either preserving image quality, or robustness against perturbations or are too complex to train. We propose RoSteALS, a practical steganography technique leveraging frozen pretrained autoencoders to free the payload embedding from learning the distribution of cover images. RoSteALS has a lightweight secret encoder of just 300k parameters, is easy to train, has perfect secret recovery performance and comparable image quality on three benchmarks. Additionally, RoSteALS can be adapted for novel cover-less steganography applications in which the cover image can be sampled from noise or conditioned on text prompts via a denoising diffusion process. Our model and code are available at https://github.com/TuBui/RoSteALS.","2160-7516","979-8-3503-0249-3","10.1109/CVPRW59228.2023.00100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10208817","","Image quality;Measurement;Steganography;Adaptation models;Perturbation methods;Watermarking;Robustness","","14","","57","IEEE","14 Aug 2023","","","IEEE","IEEE Conferences"
"Deep Learning Applied to Steganalysis of Digital Images: A Systematic Review","T. -S. Reinel; R. -P. Raúl; I. Gustavo","Departamento de Electrónica y Automatización-Antigua Estación del Ferrocarril, Universidad Autónoma de Manizales, Manizales, Colombia; Departamento de Ingeniería de Sistemas, Universidad de Antioquia, Medellín, Colombia; Departamento de Sistemas e Informática, Universidad de Caldas, Manizales, Colombia",IEEE Access,"5 Jun 2019","2019","7","","68970","68990","Steganography consists of hiding messages inside some object known as a carrier in order to establish a covert communication channel so that the act of communication itself goes unnoticed by observers who have access to that channel. The steganalysis is dedicated to the detection of hidden messages using steganography; these messages can be implicit in different types of media, such as digital images, video files, audio files or plain text. Traditionally, steganalysis has been divided into two separate stages, the first stage consists of manual extraction of sophisticated features and the second stage is classification using Ensemble Classifiers or Support Vector Machines. In recent years, the development of Deep Learning has made it possible to unify and automate the two traditional stages into an end to end approach with promising results. This paper shows the evolution of steganalysis in recent years using the Deep Learning techniques. The results of these techniques have surpassed those obtained with conventional methods - Rich Models with Ensemble Classifiers - both in the spatial and frequency (JPEG) domains. Since 2014, researchers have used The Convolutional Neural Networks to solve this problem generating diverse architectures and strategies to improve the detection percentages of steganographic images on the last generation algorithms (WOW, S-UNIWARD, HUGO, J-UNIWARD, among others). The Deep Learning, being applied to steganalysis, is now in the process of construction and results so far are encouraging for researchers that are interested in the topic.","2169-3536","","10.1109/ACCESS.2019.2918086","Universidad Autonóma de Manizales under the project entitled “Application of Convolutional Neural Networks for Steganalysis”(grant numbers:589-089); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718661","Convolutional neural network;deep learning;steganalysis;steganography","Feature extraction;Transform coding;Deep learning;Databases;Frequency-domain analysis;Payloads;Manuals","","47","","90","OAPA","20 May 2019","","","IEEE","IEEE Journals"
"A Survey on Neural Trojans","Y. Liu; A. Mondal; A. Chakraborty; M. Zuzak; N. Jacobsen; D. Xing; A. Srivastava","University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park",2020 21st International Symposium on Quality Electronic Design (ISQED),"9 Jul 2020","2020","","","33","39","Neural networks have become increasingly prevalent in many real-world applications including security critical ones. Due to the high hardware requirement and time consumption to train high-performance neural network models, users often outsource training to a machine-learning-as-a-service (MLaaS) provider. This puts the integrity of the trained model at risk. In 2017, Liu et al. found that, by mixing the training data with a few malicious samples of a certain trigger pattern, hidden functionality can be embedded in the trained network which can be evoked by the trigger pattern [33]. We refer to this kind of hidden malicious functionality as neural Trojans. In this paper, we survey a myriad of neural Trojan attack and defense techniques that have been proposed over the last few years. In a neural Trojan insertion attack, the attacker can be the MLaaS provider itself or a third party capable of adding or tampering with training data. In most research on attacks, the attacker selects the Trojan's functionality and a set of input patterns that will trigger the Trojan. Training data poisoning is the most common way to make the neural network acquire the Trojan functionality. Trojan embedding methods that modify the training algorithm or directly interfere with the neural network's execution at the binary level have also been studied. Defense techniques include detecting neural Trojans in the model and/or Trojan trigger patterns, erasing the Trojan's functionality from the neural network model, and bypassing the Trojan. It was also shown that carefully crafted neural Trojans can be used to mitigate other types of attacks. We systematize the above attack and defense approaches in this paper.","1948-3287","978-1-7281-4207-4","10.1109/ISQED48828.2020.9137011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9137011","","Training;Neural networks;Training data;Hardware;Trojan horses;Security","","35","","53","IEEE","9 Jul 2020","","","IEEE","IEEE Conferences"
"The AI Companion in Education: Analyzing the Pedagogical Potential of ChatGPT in Computer Science and Engineering","Z. He; T. Nguyen; T. Miari; M. Aliasgari; S. Rafatirad; H. Sayadi","Departmentof Computer Engineering and Computer Science, California State University, Long Beach, CA, USA; Departmentof Computer Engineering and Computer Science, California State University, Long Beach, CA, USA; Center for Information Systems and Technology, Claremont Graduate University, CA, USA; Departmentof Computer Engineering and Computer Science, California State University, Long Beach, CA, USA; Department of Computer Science, University of California, Davis, CA, USA; Departmentof Computer Engineering and Computer Science, California State University, Long Beach, CA, USA",2024 IEEE Global Engineering Education Conference (EDUCON),"8 Jul 2024","2024","","","1","10","Artificial Intelligence (AI), with ChatGPT as a prominent example, has recently taken center stage in various domains including higher education, particularly in Computer Science and Engineering (CSE). The AI revolution brings both convenience and controversy, offering substantial benefits while lacking formal guidance on their application. The primary objective of this work is to comprehensively analyze the pedagogical potential of ChatGPT in CSE education, understanding its strengths and limitations from the perspectives of educators and learners. We employ a systematic approach, creating a diverse range of educational practice problems within CSE field, focusing on various subjects such as data science, programming, AI, machine learning, networks, and more. According to our examinations, certain question types, like conceptual knowledge queries, typically do not pose significant challenges to ChatGPT, and thus, are excluded from our analysis. Alternatively, we focus our efforts on developing more in-depth and personalized questions and project-based tasks. These questions are presented to ChatGPT, followed by interactions to assess its effectiveness in delivering complete and meaningful responses. To this end, we propose a comprehensive five-factor reliability analysis framework to evaluate the responses. This assessment aims to identify when ChatGPT excels and when it faces challenges. Our study concludes with a correlation analysis, delving into the relationships among subjects, task types, and limiting factors. This analysis offers valuable insights to enhance ChatGPT's utility in CSE education, providing guidance to educators and students regarding its reliability and efficacy.","2165-9567","979-8-3503-9402-3","10.1109/EDUCON60312.2024.10578820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578820","ChatGPT;Computer Science and Engineering;Education;Generative Artificial Intelligence;Reliability Analysis","Systematics;Limiting;Focusing;Machine learning;Chatbots;Reliability;Task analysis","","2","","34","IEEE","8 Jul 2024","","","IEEE","IEEE Conferences"
"Research Challenges for Legal Document Summarization","Nikita; D. P. Rana; R. G. Mehta","Department of Computer Science and Engineering, Sardar Vallabhbhai National Institute of Technology, Surat, India; Department of Computer Science and Engineering, Sardar Vallabhbhai National Institute of Technology, Surat, India; Department of Computer Science and Engineering, Sardar Vallabhbhai National Institute of Technology, Surat, India",2023 IEEE World Conference on Applied Intelligence and Computing (AIC),"2 Oct 2023","2023","","","307","312","Legal judgment documents are detailed and contains legal terms and codes. These characteristics of legal documents makes it complex to read and analyze, which makes processing legal documents a challenging task. This raises a need for generating automatic summaries. Several techniques have been used by researchers to summarize legal documents such as traditional methods, legal specific approaches and transformer models based approaches. This research focuses on role of summarization in legal domain and various methods for summary generation. We summarize Indian judgment documents with various state of art methods for comparative study. The analysis opened various research challenges.","","979-8-3503-1006-1","10.1109/AIC57670.2023.10263906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10263906","text summarization;legal documents;legal text processing","Codes;Art;Law;Transformers;Task analysis;Text processing","","1","","57","IEEE","2 Oct 2023","","","IEEE","IEEE Conferences"
"Engineering Safety Requirements for Autonomous Driving with Large Language Models","A. Nouri; B. Cabrero-Daniel; F. Törner; H. Sivencrona; C. Berger","Volvo Cars, Gothenburg, Sweden; Department of Computer Science and Engineering, University of Gothenburg, Gothenburg, Sweden; Volvo Cars, Gothenburg, Sweden; Zenseact, Gothenburg, Sweden; Department of Computer Science and Engineering, University of Gothenburg, Gothenburg, Sweden",2024 IEEE 32nd International Requirements Engineering Conference (RE),"21 Aug 2024","2024","","","218","228","Changes and updates in the requirement artifacts, which can be frequent in the automotive domain, are a challenge for SafetyOps. Large Language Models (LLMs), with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update. In this study, we propose a prototype of a pipeline of prompts and LLMs that receives an item definition and outputs solutions in the form of safety requirements. This pipeline also performs a review of the requirement dataset and identifies redundant or contradictory requirements. We first identified the necessary characteristics for performing HARA and then defined tests to assess an LLM's capability in meeting these criteria. We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively. Finally, the prototype was implemented at a case company and the responsible team evaluated its efficiency.","2332-6441","979-8-3503-9511-2","10.1109/RE59067.2024.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628478","Requirement Engineering;Hazard Analysis Risk Assessment;Autonomous Vehicles;DevOps;Safety;Large Language Model;Prompt Engineering;LLM;ChatGPT","Reviews;Large language models;Pipelines;Refining;Prototypes;Companies;Natural language processing","","","","20","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"UV-IDM: Identity-Conditioned Latent Diffusion Model for Face UV-Texture Generation","H. Li; Y. Feng; S. Xue; X. Liu; B. Zeng; S. Li; B. Liu; J. Liu; S. Han; B. Zhang","Beihang University; Beihang University; Baidu VIS; Beihang University; Beihang University; Beihang University; Beihang University; Shenzhen Institute of Advanced Technology, Shenzhen, China; Baidu VIS; Beihang University",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","10585","10595","3D face reconstruction aims at generating high-fidelity 3D face shapes and textures from single-view or multi-view images. However, current prevailing facial texture generation methods generally suffer from low-quality texture, identity information loss, and inadequate handling of occlusions. To solve these problems, we introduce an Identity-Conditioned Latent Diffusion Model for face UV-texture generation (UV-IDM) to generate photo-realistic textures based on the Basel Face Model (BFM). UV-IDM leverages the powerful texture generation capacity of a latent diffusion model (LDM) to obtain detailed facial textures. To preserve the identity during the reconstruction procedure, we design an identity-conditioned module that can utilize any in-the-wild image as a robust condition for the LDM to guide texture generation. UV-IDM can be easily adapted to different BFM-based methods as a high-fidelity texture generator. Furthermore, in light of the limited accessibility of most existing UV-texture datasets, we build a large-scale and publicly available UV-texture dataset based on BFM, termed BFM-UV. Extensive experiments show that our UV-IDM can generate high-fidelity textures in 3D face reconstruction within seconds while maintaining image consistency, bringing new state-of-the-art performance in facial texture generation.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.01007","National Key Research and Development Program of China(grant numbers:2023YFC3300029); Beijing Natural Science Foundation(grant numbers:L223024); National Natural Science Foundation of China(grant numbers:62076016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10657312","3D face reconstruction;Diffusion Model;UV-Texture","Hair;Three-dimensional displays;Shape;Face recognition;Diffusion models;Generators;Robustness","","2","","84","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"Cognitive Hexagon-Controlled Intelligent Speech Interaction System","H. Chaurasiya","Department of Electronics and Communication, J. K. Institute of Applied Physics and Technology, University of Allahabad, Prayagraj, India",IEEE Transactions on Cognitive and Developmental Systems,"8 Dec 2022","2022","14","4","1413","1439","Several intelligent speech interaction (ISI) systems have emerged over the past four decades that have served the human community. The research papers show that these systems are very well connected to the cognitive hexagon and the six hybrid approaches. Where this hexagon reveals six distinct cognitive areas, one of the six hybrid perspectives gives rise to the dimensions of speech quality. This survey has been undertaken to reveal the dimensions of speech quality and to discuss the role of cognitive hexagonal regions on these dimensions with hybrid approaches. Here, ISI systems support this discussion and follow them as cognitive machines. An overview of the state of the art related to ISI systems is also described here. Techniques, such as processing [natural language (NL)], speech synthesis [speech-to-text (STT) or text-to-speech (TTS)], computing (voice/mobile), and audio mining are presented in this overview. These are contributing well with technologies, such as the Internet of Things (IoT), Voice over Internet Protocol (VoIP), and cloud-based systems (CBSs). In addition, stochastic components, such as reliability, availability, and failure rate were discussed to analyze whether the Quality of Service (QoS) of these ISI systems is described. Additionally, after the discussion, some aspects of the applications are also discussed along with the essential advantages and significant drawbacks.","2379-8939","","10.1109/TCDS.2022.3168807","University Grants Commission (UGC) D.Phil. Fellowship Department of Electronics and Communication, University of Allahabad, Allahabad(grant numbers:235/RES/14/2404); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761245","Cognitive;hexagon;hybrid;intelligent speech interaction (ISI);system","Philosophical considerations;Psychology;Artificial intelligence;Linguistics;Cognitive systems;Quality of service;Cloud computing;Speech recognition","","4","","201","IEEE","21 Apr 2022","","","IEEE","IEEE Journals"
"Research on Keyword-Based Element Extraction for Chinese Patent Retrieval","Z. Jin; Z. Yang; G. Tang; T. Liu; E. Xun","School of Information Science, Beijing Language and Culture University, Beijing, China; School of Information Science, Beijing Language and Culture University, Beijing, China; School of Information Science, Beijing Language and Culture University, Beijing, China; School of Information Science, Beijing Language and Culture University, Beijing, China; Beijing Advanced Innovation Center for Language Resources, Beijing Language and Culture University, Beijing, China",2024 International Conference on Asian Language Processing (IALP),"10 Sep 2024","2024","","","67","73","Patent retrieval is a critical step in patent analysis. Retrievable elements play a key role in constructing search queries and performing accurate searches, and most retrievable elements are created manually. However, the increment of patent applications each year has brought a huge burden on manual extraction of retrievable elements and patent examination, raising the urgent need of automated solutions. As keywords serve as an effective way of expressing retrievable elements in patent retrieval, we explore the automatic extraction of keyword-based retrievable elements from Chinese patent application texts in this study. We employ various keyword extraction methods, including large language model based methods, to identify retrievable elements within these texts. Our experimental results have shown that these methods can effectively extract keywords as retrievable elements from Chinese patent applications, which benefits to manual patent searching and patent examinations.","2159-1970","979-8-3315-4085-2","10.1109/IALP63756.2024.10661116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10661116","patent retrieval;retrievable elements;keyword extraction","Patents;Accuracy;Large language models;Manuals;Complexity theory;Iterative methods","","","","20","IEEE","10 Sep 2024","","","IEEE","IEEE Conferences"
"Quality Assessment for DIBR-Synthesized Views Based on Wavelet Transform and Gradient Magnitude Similarity","H. Zhang; D. Zheng; Y. Zhang; J. Cao; W. Lin; W. -K. Ling","School of Information Engineering, Guangdong University of Technology, Guangzhou, China; School of Information Engineering, Guangdong University of Technology, Guangzhou, China; School of Electronics and Communication Engineering, Sun Yat-Sen University, Shenzhen, China; School of Information Engineering, Guangdong University of Technology, Guangzhou, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Information Engineering, Guangdong University of Technology, Guangzhou, China",IEEE Transactions on Multimedia,"24 Jul 2024","2024","26","","6834","6847","To drive upgrades of Depth-Image-Based Rendering (DIBR) algorithms, depth image refinement, etc., quality assessment models for DIBR-synthesized images in 3D video systems are developed. However, most of these models could not effectively evaluate distortion due to irregular stretching (e.g., crumbling), which is more complex and common than black holes and regular stretching (e.g., horizontal stretching) in synthesized images. To make an attempt at this issue, a new quality assessment method is proposed for DIBR views. First, feature point matching and affine transformation are adopted to remove and compensate for the global object shift between reference and synthesized view images. Second, multi-scale discrete wavelet transform is utilized to extract multi-scale structure distortion; gradient magnitude similarity is further integrated to highlight the distortion features; morphological open operation and median filtering are adopted to exclude perceptually unimportant features. Third, scores are obtained by standard deviation pooling on distortion feature maps for each wavelet scale and sub-band. Experimental results demonstrate that our proposed model outperforms the state-of-the-art handcrafted feature-based DIBR-synthesized image quality assessment models on IETR database, and performs the best on average on IETR and IRCCyN/IVC databases.","1941-0077","","10.1109/TMM.2024.3356029","National Natural Science Foundation of China(grant numbers:62302105,62172400); National Natural Science Foundation of China-Guangdong Joint Fund(grant numbers:U1701266); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515110031); Guangdong Provincial Key Laboratory of Intellectual Property & Big Data(grant numbers:2018B030322016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10409615","Depth image-based rendering (DIBR);synthesized views;image quality assessment;quality of experience (QoE);local distortion","Distortion;Feature extraction;Measurement;Three-dimensional displays;Databases;Image quality;Visualization","","2","","59","IEEE","19 Jan 2024","","","IEEE","IEEE Journals"
"TPE for JPEG images with Dynamic M-ary Decomposition and Adaptive Threshold Constraints","Y. Ma; X. Chai; G. Long; Z. Gan; Y. Zhang","Henan Engineering Research Center for Industrial Internet of Things, School of Artificial Intelligence, Henan University, Zhengzhou, China; Henan Engineering Research Center for Industrial Internet of Things, School of Artificial Intelligence, Henan University, Zhengzhou, China; Henan Engineering Research Center for Industrial Internet of Things, School of Artificial Intelligence, Henan University, Zhengzhou, China; Intelligent Data Processing Engineering Research Center of Henan Province, Institute of Intelligent Network System, School of Software, Henan University, Kaifeng, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Traditional JPEG image encryption that prioritizes solely confidentiality fails to account for the pressing usability requirements of cloud-based environments, thus boosting the boom in thumbnail-preserving encryption (TPE) to balance image privacy and usability. However, existing TPE schemes for JPEG images face numerous challenges, such as insufficient security, inability to achieve lossless decryption, and high file extension. To address these challenges, we propose a TPE scheme based on dynamic M-ary decomposition and adaptive threshold constraints (TPE-MDTC). First, the valid ranges of quantized DC coefficients for JPEG images are determined. Then, a sum-preserving encryption method for quantized DC coefficients with compliance threshold constraints is designed using the bit-plane permutation to preserve thumbnails with high accuracy. Next, the introduction of dynamic M-ary decomposition effectively changes bit statistical characteristics preserved by bit-plane permutation, enhancing the ciphertext security. Finally, a quantized AC encryption method with RV (Run/Value) pair global permutation is proposed, effectively modifying the unit block features, thereby significantly improving the security and attack resistance of encrypted images. Experimental results show that the proposed TPE-MDTC scheme can reconstruct the original JPEG images without loss, and the generated ciphertext images exhibit significant advantages over previous schemes regarding file extension and security.","1558-2205","","10.1109/TCSVT.2025.3553962","National Natural Science Foundation of China(grant numbers:61802111,61872125); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10937774","Privacy and usability;JPEG image encryption;thumbnail-preserving;sum-preserving;block features","Encryption;Transform coding;Cryptography;Usability;Privacy;Image coding;Encoding;Visualization;Image resolution;Image quality","","","","","IEEE","24 Mar 2025","","","IEEE","IEEE Early Access Articles"
"IEEE Approved Draft Standard for System, Software, and Hardware Verification and Validation","",,"IEEE P1012/D22, August 2024","6 Sep 2024","2024","","","1","324","Verification and validation (V&V) processes are used to determine whether the development products of a given activity conform to the requirements of that activity and whether the product satisfies its intended use and user needs. V&V life cycle process requirements are specified for different integrity levels. The scope of V&V processes encompasses systems, software, and hardware, and it includes their interfaces. This standard applies to systems, software, and hardware being developed, maintained, or reused [legacy, commercial off-the-shelf (COTS), non-developmental items]. The term software also includes firmware and microcode, and each of the terms system, software, and hardware includes related information or documentation. V&V processes include the analysis, evaluation, review, inspection, assessment, and testing of product","","979-8-8557-1176-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10669285","acceptance testing;architecture evaluation;adaptive;Agile;AI;artificial intelligence;component testing;concept documentation evaluation;COTS;criticality;criticality analysis;design evaluation;disposal plan evaluation;environmental verification and validation (V&V) factors;field programmable gate array;firmware;FPGA;hardware life cycle;hardware V&V;hardware verification and validation;hazard analysis;IEEE 1012;implementation evaluation;independent V&V;integration testing;integrity level;interface analysis;IV&V;machine learning;microcode;minimum V&V tasks;ML;nth of a kind;objective evidence;operating procedure evaluation;qualification testing;quality assurance;regression analysis;regression testing;requirements allocation analysis;requirements evaluation;reuse software;risk analysis;security analysis;software as a service;SaaS software life cycle;software quality assurance;software V&V;software verification and validation;source code documentation evaluation;source code evaluation;SQA;stakeholder needs and requirements evaluation;system element interaction analysis;system life cycle;system maintenance strategy assessment;system of interest;system requirements evaluation;system V&V;system verification and validation;testing;traceability analysis;V&V;V&V measures;validation;verification;vignette","IEEE Standards;Artificial intelligence;Testing;Performance evaluation;Hazards;Formal verification;Field programmable gate arrays;Microprogramming;Hardware;Life cycle assessment","","","","","","6 Sep 2024","","","IEEE","IEEE Standards"
"Exploring LLMs Applications in Law: A Literature Review on Current Legal NLP Approaches","M. Siino; M. Falco; D. Croce; P. Rosso","Department of Electrical, Electronics and Informatics Engineering, University of Catania, Catania, Italy; Department of Engineering, University of Palermo, Palermo, Italy; Department of Engineering, University of Palermo, Palermo, Italy; PRHLT Research Center, Universitat Politècnica de València, Valencia, Spain",IEEE Access,"29 Jan 2025","2025","13","","18253","18276","Artificial Intelligence (AI) is reshaping the legal landscape, with software tools now impacting various aspects of legal work. The intersection of Natural Language Processing (NLP) and law holds potential to transform how legal professionals, including lawyers and judges, operate, resolve disputes, and retrieve case information to formulate their decisions. To identify the current state of the applications of Transformers (also known as Large Language Models or LLMs) in the legal domain, we analysed the existing literature from 2017 to 2023 through a database search and snowballing method. From 61 selected publications, we identified key application categories such as legal document analysis, case prediction, and contract review, along with their main characteristics. We observed a discernible upsurge in the volume of scholarly publications, a diversification of tasks undertaken (e.g., legal research, contract analysis, and regulatory compliance), and an increased range of languages considered. There has been a notable enhancement in the methodological sophistication employed by researchers in practical applications. The performance of models grounded in the Generative Pre-trained Transformer (GPT) architecture has consistently improved across various legal domains, including contract review, legal document summarization, and case outcome prediction. This paper makes several significant contributions to the field. Firstly, it identifies emerging trends in the application of LLMs within the legal domain, highlighting the growing interest and investment in this area. Secondly, it pinpoints methodological gaps in current research, suggesting areas where further development and refinement are needed. Lastly, it discusses the broader implications of these advancements for real-world legal tasks, offering insights into how LLM-based AI can enhance legal practice while addressing the associated challenges.","2169-3536","","10.1109/ACCESS.2025.3533217","European Union through the Italian Ministero dell’Università e della Ricerca (MUR) National Recovery and Resilience Plan (NRRP) of NextGenerationEU, partnership on “Telecommunications of the Future” (program “Research and Innovation on Future Telecommunications Systems and Networks, to make Italy more Smart (RESTART)”), within the project Net4Future(grant numbers:PE0000001); Malicious Actors Profiling and Detection in Online Social Networks through Artificial Intelligence (MARTINI); Ministerio de Ciencia e Innovación (MCIN)/Agencia Estatal de Investigación (AEI)/10.13039/501100011033(grant numbers:PCI2022-135008-2); European Union NextGenerationEU/Plan de Recuperación, Transformación y Resiliencia (PRTR) and FairTransNLP-Stereotypes—Fairness and transparency for equitable NLP applications in social media: Identifying stereotypes and prejudices and developing equitable systems funded by MCIN/AEI/10.13039/501100011033(grant numbers:PID2021-124361OB-C31); European Regional Development Fund (ERDF), EU, A way of making Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10850911","Natural language processing;law;AI for law;legal NLP;legal tech;GPT;transformers;literature review","Law;Artificial intelligence;Transformers;Systematic literature review;Contracts;Attention mechanisms;Databases;Reliability;Question answering (information retrieval);Quality assessment","","","","112","CCBY","23 Jan 2025","","","IEEE","IEEE Journals"
"IEEE Draft Standard for System, Software, and Hardware Verification and Validation","",,"IEEE P1012/D20, May 2024","24 May 2024","2024","","","1","327","Verification and validation (V&V) processes are used to determine whether the development products of a given activity conform to the requirements of that activity and whether the product satisfies its intended use and user needs. V&V life cycle process requirements are specified for different integrity levels. The scope of V&V processes encompasses systems, software, and hardware, and it includes their interfaces. This standard applies to systems, software, and hardware being developed, maintained, or reused [legacy, commercial off-the-shelf (COTS), non-developmental items]. The term software also includes firmware and microcode, and each of the terms system, software, and hardware includes related information or documentation. V&V processes include the analysis, evaluation, review, inspection, assessment, and testing of product","","979-8-8557-0842-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10539079","acceptance testing;architecture evaluation;adaptive;Agile;AI;artificial intelligence;component testing;concept documentation evaluation;COTS;criticality;criticality analysis;design evaluation;disposal plan evaluation;environmental verification and validation (V&V) factors;field programmable gate array;firmware;FPGA;hardware life cycle;hardware V&V;hardware verification and validation;hazard analysis;IEEE 1012;implementation evaluation;independent V&V;integration testing;integrity level;interface analysis;IV&V;machine learning;microcode;minimum V&V tasks;ML;nth of a kind;objective evidence;operating procedure evaluation;qualification testing;quality assurance;regression analysis;regression testing;requirements allocation analysis;requirements evaluation;reuse software;risk analysis;security analysis;software as a service;SaaS software life cycle;software quality assurance;software V&V;software verification and validation;source code documentation evaluation;source code evaluation;SQA;stakeholder needs and requirements evaluation;system element interaction analysis;system life cycle;system maintenance strategy assessment;system of interest;system requirements evaluation;system V&V;system verification and validation;testing;traceability analysis;V&V;V&V measures;validation;verification;vignette","IEEE Standards;Hardware design languages;Software testing;Artificial intelligence;Software performance;Formal verification;Software reliability;Performance evaluation","","","","","","24 May 2024","","","IEEE","IEEE Standards"
"Dependability Improvement Depends on Dependable Measurement","L. Hatton","Kingston University, London, U.K.",Computer,"2 Apr 2024","2024","57","4","57","67","If computing is ever to be considered dependable by all its stakeholders, both computer scientists and computer users, we must address the most fundamental aspect of process improvement, that of measurement. As currently practiced, it is wholly and demonstrably inadequate.","1558-0814","","10.1109/MC.2023.3326947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10488899","","Stakeholders","","1","","56","IEEE","2 Apr 2024","","","IEEE","IEEE Magazines"
"Mathematical Model and Algorithm for Accurate Main Content Extraction From News Websites","H. Salem; H. Salloum; M. Mazzara","Department of Computer Science and Engineering, Innopolis University, Innopolis, Russia; Department of Computer Science and Engineering, Innopolis University, Innopolis, Russia; Department of Computer Science and Engineering, Innopolis University, Innopolis, Russia",IEEE Access,"27 Jan 2025","2025","13","","15694","15711","Irrelevant elements like ads, menus, and footers in web pages hinder data extraction and reduce the performance of Retrieval-Augmented Generation (RAG) systems in Large Language Models (LLMs). This paper tackles the challenge of accurately identifying and extracting the main content from web pages to enhance the efficiency of these systems. We present a novel mathematical model and algorithm that leverages the Document Object Model (DOM) structure, effectively isolating relevant content with high accuracy. Our approach is language-neutral and performs well across diverse languages, including those with complex tokenization, such as Arabic. To validate the model, we created a dataset from 500 websites, allowing for comprehensive evaluation and benchmarking. The algorithm’s practical application demonstrates a reduction in token usage for LLM tasks, contributing to cost-effectiveness. This work introduces a robust, open-source tool for the academic and commercial communities, fostering further innovation in web content extraction and information retrieval.","2169-3536","","10.1109/ACCESS.2024.3524656","Analytical Center for the Government of the Russian Federation(grant numbers:70-2021-00143 dd.01.11.2021,IGK 000000D730324P540002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10819347","Information extraction;document object model (DOM);retrieval-augmented generation (RAG);large language models (LLM);main content detection","Feature extraction;Data mining;Web pages;Accuracy;Hidden Markov models;Mathematical models;Layout;Heuristic algorithms;Convolutional neural networks;Focusing","","","","0","CCBY","31 Dec 2024","","","IEEE","IEEE Journals"
"Are Existing Large Language Models Robust Against Jailbreak Attacks?","B. Rababah; S. T. Wu; M. Kwiatkowski; C. K. Leung; C. G. Akcora","Department of Computer Science, University of Manitoba, Winnipeg, Canada; Department of Computer Science, University of Manitoba, Winnipeg, Canada; Department of Computer Science, University of Manitoba, Winnipeg, Canada; Department of Computer Science, University of Manitoba, Winnipeg, Canada; AI Initiative, University of Central Florida, Orlando, FL, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","5383","5391","The safety and robustness of Large Language Models (LLMs) are major challenges in developing generative AI applications. One key issue is the vulnerability to prompt jailbreak attacks, which pose a significant threat to building secure and resilient LLM-based applications. In this work, we present a framework for understanding and evaluating the behaviors of popular LLMs by categorizing their responses into five distinct exposure levels. Additionally, we introduce a novel language attack that circumvents LLMs’ defenses by translating jailbreak prompts into languages such as Arabic, Chinese, and Greek. Despite ongoing efforts to enhance LLMs’ safety, we find that nearly all popular LLMs can be jailbroken. Our findings offer detailed insights into LLMs’ behavior, improve diagnostic capabilities, and support targeted safety improvements.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825802","Natural Sciences and Engineering Research Council of Canada; University of Manitoba; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825802","LLMs;Prompt Jailbreak;Multilingual Jailbreak","Translation;Accuracy;Generative AI;Large language models;Buildings;Big Data;Robustness;Safety;Multilingual","","","","17","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Pareto Optimized Large Mask Approach for Efficient and Background Humanoid Shape Removal","R. Maskeliunas; R. Damaševičius; D. Vitkute-Adzgauskiene; S. Misra","Center of Excellence Forest 4.0, Faculty of Informatics, Kaunas University of Technology, Kaunas, Lithuania; Faculty of Applied Mathematics, Silesian University of Technology, Gliwice, Poland; Department of Applied Informatics, Center of Excellence Forest 4.0, Vytautas Magnus University, Kaunas, Lithuania; Department of Computer Science and Communication, Ostfold University College, Halden, Norway",IEEE Access,"10 Apr 2023","2023","11","","33900","33914","The purpose of automated video object removal is to not only detect and remove the object of interest automatically, but also to utilize background context to inpaint the foreground area. Video inpainting requires to fill spatiotemporal gaps in a video with convincing material, necessitating both temporal and spatial consistency; the inpainted part must seamlessly integrate into the background in a variety of scenes, and it must maintain a consistent appearance in subsequent frames even if its surroundings change noticeably. We introduce deep learning-based methodology for removing unwanted human-like shapes in videos. The method uses Pareto-optimized Generative Adversarial Networks (GANs) technology, which is a novel contribution. The system automatically selects the Region of Interest (ROI) for each humanoid shape and uses a skeleton detection module to determine which humanoid shape to retain. The semantic masks of human like shapes are created using a semantic-aware occlusion-robust model that has four primary components: feature extraction, and local, global, and semantic branches. The global branch encodes occlusion-aware information to make the extracted features resistant to occlusion, while the local branch retrieves fine-grained local characteristics. A modified big mask inpainting approach is employed to eliminate a person from the image, leveraging Fast Fourier convolutions and utilizing polygonal chains and rectangles with unpredictable aspect ratios. The inpainter network takes the input image and the mask to create an output image excluding the background humanoid shapes. The generator uses an encoder-decoder structure with included skip connections to recover spatial information and dilated convolution and squeeze and excitation blocks to make the regions behind the humanoid shapes consistent with their surroundings. The discriminator avoids dissimilar structure at the patch scale, and the refiner network catches features around the boundaries of each background humanoid shape. The efficiency was assessed using the Structural Learned Perceptual Image Patch Similarity, Frechet Inception Distance, and Similarity Index Measure metrics and showed promising results in fully automated background person removal task. The method is evaluated on two video object segmentation datasets (DAVIS indicating respective values of 0.02, FID of 5.01 and SSIM of 0.79 and YouTube-VOS, resulting in 0.03, 6.22, 0.78 respectively) as well a database of 66 distinct video sequences of people behind a desk in an office environment (0.02, 4.01, and 0.78 respectively).","2169-3536","","10.1109/ACCESS.2023.3253206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10061219","Semantic segmentation;occlusion-robust network;human shape extraction;background person removal;image inpainting","Videos;Shape measurement;Feature extraction;Humanoid robots;Semantics;Computer architecture;Image processing","","","","114","CCBYNCND","6 Mar 2023","","","IEEE","IEEE Journals"
"Enhancing Human Pose Estimation in the Internet of Things via Diffusion Generative Models","S. Wu; H. Zhang; Z. Liu; H. Chen; Y. Jiao","Ministry of Education, College of Computer Science and Technology, and the Key Laboratory of Symbolic Computation and Knowledge Engineering, Jilin University, Changchun, China; School of discipline inspection and supervision China University of Political Science and Law, Beijing, China; State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; Ministry of Education, College of Computer Science and Technology, and the Key Laboratory of Symbolic Computation and Knowledge Engineering, Jilin University, Changchun, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China",IEEE Internet of Things Journal,"","2025","PP","99","1","1","With the ongoing development of public video surveillance technology, accurate human pose estimation is becoming increasingly important in urban administration and law enforcement. However, existing methods rely on large-scale dense annotations, which are labor-intensive and time-consuming. To tackle this, we propose SparsePose which leverages training videos with sparse annotations (labeled every k frames) to learn to propagate temporal poses that help to estimate the poses in unlabeled frames. Technically, we engage in a novel dual-branch architecture that combines 1) pose forecasting of the consecutive neighboring frames with 2) visual clues of the current frame and the nearest labeled frames. We theoretically derive the intra-branch and inter-branch mutual information loss to supervise that maximized pose-relevant features are extracted from the current frame and different branches complement each other to approach precise pose estimation. Additionally, we propose a diffusion generative enhancement, which improves the robustness of the model to challenging scenes from the perspective of diversity. Empirical results show that our method significantly outperforms the state-of-the-art methods in sparsely labeled pose estimation on three benchmark datasets.","2327-4662","","10.1109/JIOT.2025.3529917","the Key R&D Program of Zhejiang Province(grant numbers:No. 2023C01217); Graduate Innovation Fund of Jilin University(grant numbers:No. 2024CX089); Key Projects of Science and Technology Development Plan of Jilin Province(grant numbers:No. 20230201088GX); National Natural Science Foundation of China(grant numbers:No. 62276112,No. 62372402); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10869359","Human pose estimation;multi-person;sparsely-labeled;mutual information","Pose estimation;Internet of Things;Heating systems;Annotations;Mutual information;Cameras;Visualization;Streaming media;Behavioral sciences;Accuracy","","","","","IEEE","31 Jan 2025","","","IEEE","IEEE Early Access Articles"
"ChatPhishDetector: Detecting Phishing Sites Using Large Language Models","T. Koide; H. Nakano; D. Chiba","NTT Security Holdings Corporation & NTT Corporation, Tokyo, Japan; NTT Security Holdings Corporation & NTT Corporation, Tokyo, Japan; NTT Security Holdings Corporation & NTT Corporation, Tokyo, Japan",IEEE Access,"28 Oct 2024","2024","12","","154381","154400","Large Language Models (LLMs), such as ChatGPT, are significantly impacting various fields. While LLMs have been extensively studied for code generation and text synthesis, their application in detecting malicious web content, particularly phishing sites, remains largely unexplored. To counter the increasing cyber-attacks that leverage LLMs for creating more sophisticated and convincing phishing content, it is crucial to automate detection by harnessing LLMs’ advanced capabilities. This paper introduces ChatPhishDetector, a novel system that employs LLMs to identify phishing sites. Our approach involves using a web crawler to collect website information, generating prompts for LLMs based on the gathered data, and extracting detection results from LLM responses. This system enables accurate detection of multilingual phishing sites by identifying impersonated brands and social engineering techniques within the entire website context, without requiring machine learning model training. We evaluated our system’s performance using our own dataset and compared it with baseline systems and several LLMs. Experiments using GPT-4V showed exceptional results, achieving 98.7% precision and 99.6% recall, surpassing the detection performance of other LLMs and existing systems. These findings highlight the potential of LLMs for protecting users from online fraudulent activities and provide crucial insights for strengthening defenses against phishing attacks.","2169-3536","","10.1109/ACCESS.2024.3483905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10723311","Large language models;phishing sites;social engineering","Phishing;Uniform resource locators;Large language models;Crawlers;Codes;Web pages;Security;Accuracy;Visualization;Cognition","","1","","61","CCBY","21 Oct 2024","","","IEEE","IEEE Journals"
"Book of Abstracts","",,2018 48th European Microwave Conference (EuMC),"22 Nov 2018","2018","","","1","75","Presents abstracts for the articles comprising the conference proceedings.","","978-2-87487-051-4","10.23919/EuMC.2018.8541769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8541769","","Microwave filters;Microwave theory and techniques;Microwave communication;Microwave circuits;Microwave photonics;Microwave antenna arrays;Radar antennas","","","","","","22 Nov 2018","","","IEEE","IEEE Conferences"
"Too Large; Data Reduction for Vision-Language Pre-Training","A. J. Wang; K. Q. Lin; D. J. Zhang; S. W. Lei; M. Z. Shou","Show Lab, National University of Singapore; Show Lab, National University of Singapore; Show Lab, National University of Singapore; Show Lab, National University of Singapore; Show Lab, National University of Singapore",2023 IEEE/CVF International Conference on Computer Vision (ICCV),"15 Jan 2024","2023","","","3124","3134","This paper examines the problems of severe image-text misalignment and high redundancy in the widely-used large-scale Vision-Language Pre-Training (VLP) datasets. To address these issues, we propose an efficient and straightforward Vision-Language learning algorithm called ${\color {Purple}{TL;DR}}$, which aims to compress the existing large VLP data into a small, high-quality set. Our approach consists of two major steps. First, a codebook-based encoder-decoder captioner is developed to select representative samples. Second, a new caption is generated to complement the original captions for selected samples, mitigating the text-image misalignment problem while maintaining uniqueness. As the result, ${\color {Purple}{TL;DR}}$ enables us to reduce the large dataset into a small set of high-quality data, which can serve as an alternative pre-training dataset. This algorithm significantly speeds up the time-consuming pretraining process. Specifically, ${\color {Purple}{TL;DR}}$ can compress the mainstream VLP datasets at a high ratio, e.g., reduce well-cleaned CC3M dataset from 2.82M to 0.67M (~24%) and noisy YFCC15M from 15M to 2.5M (~16.7%). Extensive experiments with three popular VLP models over seven downstream tasks show that VLP model trained on the compressed dataset provided by ${\color {Purple}{TL;DR}}$ can perform similar or even better results compared with training on the full-scale dataset1.","2380-7504","979-8-3503-0718-4","10.1109/ICCV51070.2023.00292","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10378495","","Training;Computer vision;Image coding;Computational modeling;Redundancy;Noise measurement;Task analysis","","6","","61","IEEE","15 Jan 2024","","","IEEE","IEEE Conferences"
"Deformable few-shot face cartoonization via local to global translation","Y. Zhou; S. Li; H. Huang","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen 518060, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen 518060, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen 518060, China",Computational Visual Media,"","2025","PP","99","1","19","Cartoonizing portrait images is a stylish and eye-catching application in both computer vision and graphics. We aimed to train a face cartoonization model using very few (e.g., 5-10) style examples. The main difficulty in this challenging task lies in producing stylizations of high quality while preserving the identity of the input, particularly when the style examples contain strong exaggerations. To address this, we propose a novel cross-domain center loss for few-shot generative adversarial network (GAN) adaptation, which forces the distribution of the target domain to be similar to that of the source. We then employ it to solve this few-shot problem along with a two-stage strategy. Stage I generates an intermediate cartoonization for the input, where we first stylize the individual facial components locally and then deform them to mimic the desired exaggeration under the guidance of landmarks. Stage II focuses on global refinement of the intermediate image. First, we adapt a pretrained StyleGAN model using the proposed cross-domain center loss to the target domain defined by a few examples. Subsequently, the intermediate cartoonization from Stage I can be holistically refined through GAN inversion. The generative power of StyleGAN guarantees high image quality, while the local translation and landmark-guided deformation applied to facial components provide high identity fidelity. Experiments show that the proposed method outperforms state-of-the-art few-shot stylization approaches both qualitatively and quantitatively.","2096-0662","","10.26599/CVM.2025.9450348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903675","face stylization;cartoonization;few-shot learning;domain adaptation","Translation;Faces;Generative adversarial networks;Deformation;Shape;Feature extraction;Visualization;Neural style transfer;Generators;Adaptation models","","","","","","25 Feb 2025","","","TUP","TUP Early Access Articles"
"Artificial Intelligence Research in Management: A Computational Literature Review","J. Arsenyan; A. Piepenbrink","Rennes School of Business, Rennes, France; Rennes School of Business, Rennes, France",IEEE Transactions on Engineering Management,"14 Feb 2024","2024","71","","5088","5100","Artificial intelligence (AI) spring of the past decade created an increased interest into the topic in business as well as in academia. This resulted in an upward trend in academic publications, not only in computer science but also in management. This article presents a computational literature review with an abstract-based sampling approach to investigate the status of the management literature to take stock of academic research of the past two decades. We analyze 6324 papers from 1990 to 2020 published in five management-related domains and identify 41 distinct topics. We present the evolution of research pre and post AI spring, emerging topics as well as saturated areas. The findings show that the previously disjointed topic network structure is fully connected by early 2010s and the upward trend in management research starts in the period of 2014–2015. The results provide a comprehensive insight into the potential of AI in management versus underdeveloped areas, and presents, for management scholars and practitioners, suggestions about effective adoption of AI practices.","1558-0040","","10.1109/TEM.2022.3229821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015581","Artificial intelligence (AI);computational literature review (CLR);latent Dirichlet allocation (LDA);management research","Artificial intelligence;Business;Machine learning;Inspection;Bibliographies;Vocabulary;Computational modeling","","14","","55","IEEE","11 Jan 2023","","","IEEE","IEEE Journals"
"IEEE Draft Standard for Health Informatics—Point-of-Care Medical Device Communication—Part 10101: Nomenclature","",,"IEEE P11073-10101/D1, June 2018","3 Jul 2018","2018","","","1","704","Within the context of the ISO/IEEE 11073 family of standards for point-of-care (POC) and personal health devices (PHD) medical device communication (MDC), this standard provides the nomenclature that supports both the domain information model and service model components of the standards family, as well as the semantic content exchanged with medical devices. The nomenclature is specialized for patient vital signs information representation and medical device informatics, with major areas including concepts for electrocardiograph (ECG), haemodynamics, respiration, blood gas, urine, fluid-related metrics, and neurology, as well as specialized units of measurement, general device events, alarms, and body sites. The standard defines both the architecture and major components of the nomenclature, along with extensive definitions for each conceptual area.","","978-1-5044-5040-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8402266","codes;IEEE 11073-10101;IHE PCD-01;independent living;information model;medical device communication;nomenclature;ontology;patient;personal health devices;PHD;POC;point-of-care;semantics;service model;terminology","IEEE Standards;Medica devices;Point of care;Patient monitoring;Smart healthcare;Terminology","","","","","","3 Jul 2018","","","IEEE","IEEE Standards"
"ISO/IEEE International Standard - Health informatics-Device interoperability-Part 10101: Point-of-care medical device communication-Nomenclature","",,ISO/IEEE 11073-10101:2020(E),"1 Sep 2020","2020","","","1","1064","Within the context of the ISO/IEEE 11073 family of standards for point-of-care (POC) and personal health devices (PHD) medical device communication (MDC), this standard provides the nomenclature that supports both the domain information model and service model components of the standards family, as well as the semantic content exchanged with medical devices. The nomenclature is specialized for patient vital signs information representation and medical device informatics, with major areas including concepts for electrocardiograph (ECG), haemodynamics, respiration, blood gas, urine, fluid-related metrics, and neurology, as well as specialized units of measurement, general device events, alarms, and body sites. The standard defines both the architecture and major components of the nomenclature, along with extensive definitions for each conceptual area.;Within the context of the ISO/IEEE 11073 family of standards for point-of-care (POC) and personal health devices (PHD) medical device communication (MDC), this standard provides the nomenclature that supports both the domain information model and service model components of the standards family, as well as the semantic content exchanged with medical devices. The nomenclature is specialized for patient vital signs information representation and medical device informatics, with major areas including concepts for electrocardiograph (ECG), haemodynamics, respiration, blood gas, urine, fluid-related metrics, and neurology, as well as specialized units of measurement, general device events, alarms, and body sites. The standard defines both the architecture and major components of the nomenclature, along with extensive definitions for each conceptual area.","","978-1-5044-6959-3","10.1109/IEEESTD.2020.9184351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9184351","codes;IEEE 11073-10101™;IHE PCD-01;independent living;information model;medical device communication;nomenclature;ontology;patient;personal health devices;PHD;POC;point-of-care;semantics;service model;terminology","IEEE Standards;Medical devices;Biomedical monitoring;Point of care;Bioinformatics","","","","","","1 Sep 2020","","","IEEE","IEEE Standards"
"SCF-Stega: Controllable Linguistic Steganography Based on Semantic Communications Framework","Y. Long; Z. Yang; Z. Wang; Z. Zhou; Y. Huang; L. Zhou","School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China; School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China; School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China; School of Artificial Intelligence, Guangzhou University, Guangzhou, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Linguistic steganography is a key information hiding technique but faces challenges like abrupt content shifts, detection risks, and high training resource demands. To address these, this paper introduces SCF-Stega, a controllable method based on Semantic Communications Framework. By using a knowledge graph to guide secret encoding and dynamically adjusting large language model outputs, SCF-Stega enhances text imperceptibility and semantic coherence. Experiments show improved text quality and strong resistance to steganalysis, without needing additional training data.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10888762","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10888762","Linguistic steganography;semantic communications;knowledge graph;large language models","Training;Resistance;Steganography;Training data;Linguistics;Signal processing;Semantic communication;Encoding;Security;Speech processing","","","","33","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Large Language Models and Security","M. Bezzi","SAP Security Research, Mougins, France",IEEE Security & Privacy,"2 Apr 2024","2024","22","2","60","68","We analyze the security implications of large language models (LLMs) from their use as security tools for both attackers and defenders and the security of LLMs. We discuss how LLMs increase the scale of traditional threats such as social engineering and add new ones such as prompt injections.","1558-4046","","10.1109/MSEC.2023.3345568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10413528","","Security;Phishing;Malware;Fake news;Electronic mail;Costs;Computational modeling;Large language models","","","","14","IEEE","24 Jan 2024","","","IEEE","IEEE Magazines"
"Research on English Data Intelligent Retrieval Terminal System Based on Computer 3D Image Vision Technology","C. Liu; L. Yang","Xi'an Eurasia University, Xi'an, China; Xi'an Eurasia University, Xi'an, China",2023 IEEE 3rd International Conference on Data Science and Computer Application (ICDSCA),"23 Jan 2024","2023","","","1097","1102","This paper introduces an embedded software for English information query based on ARM single chip microcomputer LPC2290 and Mini GUI. Integrate the Mini GUI with network communication. Remote access to MySQL using C/S architecture. The hardware and software architecture of English data intelligent retrieval terminal system is presented. The graphical interface of the system and the data extraction of the system are completed based on Mini GUI. Several main implementation links of the system are also explained. This paper presents an intelligent query terminal based on English data, which improves the user interface very well. It can not only realize the intelligent search of English data, but also solve the expensive problem of the traditional intelligent search terminal of English data.","","979-8-3503-4154-6","10.1109/ICDSCA59871.2023.10392411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10392411","3D image;visual technology;English materials;intelligent retrieval;terminal system","Three-dimensional displays;Software architecture;Operating systems;Computer architecture;Search problems;Hardware;Graphical user interfaces","","","","7","IEEE","23 Jan 2024","","","IEEE","IEEE Conferences"
"SoK: Prompt Hacking of Large Language Models","B. Rababah; S. T. Wu; M. Kwiatkowski; C. K. Leung; C. G. Akcora","Department of Computer Science, University of Manitoba, Winnipeg, Canada; Department of Computer Science, University of Manitoba, Winnipeg, Canada; Department of Computer Science, University of Manitoba, Winnipeg, Canada; Department of Computer Science, University of Manitoba, Winnipeg, Canada; AI Initiative, University of Central Florida, Orlando, FL, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","5392","5401","The safety and robustness of large language models (LLMs) based applications remain critical challenges in artificial intelligence. Among the key threats to these applications are prompt hacking attacks, which can significantly undermine the security and reliability of LLM-based systems. In this work, we offer a comprehensive and systematic overview of three distinct types of prompt hacking: jailbreaking, leaking, and injection, addressing the nuances that differentiate them despite their overlapping characteristics. To enhance the evaluation of LLM-based applications, we propose a novel framework that categorizes LLM responses into five distinct classes, moving beyond the traditional binary classification. This approach provides more granular insights into the AI’s behavior, improving diagnostic precision and enabling more targeted enhancements to the system’s safety and robustness.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825103","LLMs","Hands;Systematics;Large language models;Refining;Big Data;Robustness;Safety;Security;Computer crime;Usability","","","","42","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Economic, Societal, Legal, and Ethical Considerations for Large Language Models","J. Lofstead","Center for Computing Research, Sandia National Laboratories, Albuquerque, USA",2023 Fifth International Conference on Transdisciplinary AI (TransAI),"16 Jan 2024","2023","","","155","162","Systems like ChatGPT-3 captured the imagination in late 2022. The quality of the systems surprised and delighted many with both a flurry of lawsuits, predictions of the collapse of many white collar careers, and decimation of artist incomes. The potential to have an interactive tool that can work like an interactive textbook is compelling and potentially transformative for the betterment of humanity offering low cost access to something to talk with about just about anything. This can enable learning to whole new audiences. With this potential upside, the current status of these tools in terms of the economics of the tools themselves, how their costs compare to human labor, the impacts on human jobs both white collar and creative, and in the current legal environment must be considered to best see the future these tools may offer. This work examines what these tools are, how they work, and the implications of how they are created through all of these lenses to give a holistic view of the current state and potential future possibilities. While this is a changing landscape, the current status can offer a strong position to talk about these tools and help guide what the future will be like.","","979-8-3503-5801-8","10.1109/TransAI60598.2023.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10387642","artificial intelligence;machine learning;ethics;society;economics;law","Ethics;Art;Costs;Law;Biological system modeling;Training data;Companies","","","","42","USGov","16 Jan 2024","","","IEEE","IEEE Conferences"
"Databricks ML in Action: Learn how Databricks supports the entire ML lifecycle end to end from data ingestion to the model deployment","S. Rivera; A. Prokaieva; A. Baker; H. Horn",NA; NA; NA; NA,Databricks ML in Action: Learn how Databricks supports the entire ML lifecycle end to end from data ingestion to the model deployment,"","2024","","","","","Get to grips with autogenerating code, deploying ML algorithms, and leveraging various ML lifecycle features on the Databricks Platform, guided by best practices and reusable code for you to try, alter, and build onKey FeaturesBuild machine learning solutions faster than peers only using documentationEnhance or refine your expertise with tribal knowledge and concise explanationsFollow along with code projects provided in GitHub to accelerate your projectsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionDiscover what makes the Databricks Data Intelligence Platform the go-to choice for top-tier machine learning solutions. Written by a team of industry experts at Databricks with decades of combined experience in big data, machine learning, and data science, Databricks ML in Action presents cloud-agnostic, end-to-end examples with hands-on illustrations of executing data science, machine learning, and generative AI projects on the Databricks Platform. You’ll develop expertise in Databricks' managed MLflow, Vector Search, AutoML, Unity Catalog, and Model Serving as you learn to apply them practically in everyday workflows. This Databricks book not only offers detailed code explanations but also facilitates seamless code importation for practical use. You’ll discover how to leverage the open-source Databricks platform to enhance learning, boost skills, and elevate productivity with supplemental resources. By the end of this book, you'll have mastered the use of Databricks for data science, machine learning, and generative AI, enabling you to deliver outstanding data products.What you will learnSet up a workspace for a data team planning to perform data scienceMonitor data quality and detect driftUse autogenerated code for ML modeling and data explorationOperationalize ML with feature engineering client, AutoML, VectorSearch, Delta Live Tables, AutoLoader, and WorkflowsIntegrate open-source and third-party applications, such as OpenAI's ChatGPT, into your AI projectsCommunicate insights through Databricks SQL dashboards and Delta SharingExplore data and models through the Databricks marketplaceWho this book is forThis book is for machine learning engineers, data scientists, and technical managers seeking hands-on expertise in implementing and leveraging the Databricks Data Intelligence Platform and its Lakehouse architecture to create data products.","","9781800564008","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769322.pdf&bkn=10769321&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"On the use of automatically generated synthetic image datasets for benchmarking face recognition","L. Colbois; T. d. Freitas Pereira; S. Marcel","Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland",2021 IEEE International Joint Conference on Biometrics (IJCB),"20 Jul 2021","2021","","","1","8","The availability of large-scale face datasets has been key in the progress of face recognition. However, due to licensing issues or copyright infringement, some datasets are not available anymore (e.g. MS-Celeb-1M). Recent advances in Generative Adversarial Networks (GANs), to synthesize realistic face images, provide a pathway to replace real datasets by synthetic datasets, both to train and benchmark face recognition (FR) systems. The work presented in this paper provides a study on benchmarking FR systems using a synthetic dataset. First, we introduce the proposed methodology to generate a synthetic dataset, without the need for human intervention, by exploiting the latent structure of a StyleGAN2 model with multiple controlled factors of variation. Then, we confirm that (i) the generated synthetic identities are not data subjects from the GAN’s training dataset, which is verified on a synthetic dataset with 10K+ identities; (ii) benchmarking results on the synthetic dataset are a good substitution, often providing error rates and system ranking similar to the benchmarking on the real dataset.","2474-9699","978-1-6654-3780-6","10.1109/IJCB52358.2021.9484363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484363","","Training;Visualization;Databases;Error analysis;Face recognition;Semantics;Benchmark testing","","29","","33","IEEE","20 Jul 2021","","","IEEE","IEEE Conferences"
"Unlocking the Secrets of Prompt Engineering: Master the art of creative language generation to accelerate your journey from novice to pro","G. Mizrahi; D. Serfaty",NA; NA,Unlocking the Secrets of Prompt Engineering: Master the art of creative language generation to accelerate your journey from novice to pro,"","2024","","","","","Enhance your writing with AI by mastering prompt engineering techniques and become an expert in developing and utilizing LLM prompts across applicationsKey FeaturesMaster prompt engineering techniques to harness AI's writing potentialDiscover diverse LLM applications for content creation and beyondLearn through practical examples, use cases, and hands-on guidancePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionUnlocking the Secrets of Prompt Engineering is your key to mastering the art of AI-driven writing. This book propels you into the world of large language models (LLMs), empowering you to create and apply prompts effectively for diverse applications, from revolutionizing content creation and chatbots to coding assistance. Starting with the fundamentals of prompt engineering, this guide provides a solid foundation in LLM prompts, their components, and applications. Through practical examples and use cases, you'll discover how LLMs can be used for generating product descriptions, personalized emails, social media posts, and even creative writing projects like fiction and poetry. The book covers advanced use cases such as creating and promoting podcasts, integrating LLMs with other tools, and using AI for chatbot development. But that’s not all. You'll also delve into the ethical considerations, best practices, and limitations of using LLM prompts as you experiment and optimize your approach for best results. By the end of this book, you'll have unlocked the full potential of AI in writing and content creation to generate ideas, overcome writer's block, boost productivity, and improve communication skills.What you will learnExplore the different types of prompts, their strengths, and weaknessesUnderstand the AI agent's knowledge and mental modelEnhance your creative writing with AI insights for fiction and poetryDevelop advanced skills in AI chatbot creation and deploymentDiscover how AI will transform industries such as education, legal, and othersIntegrate LLMs with various tools to boost productivityUnderstand AI ethics and best practices, and navigate limitations effectivelyExperiment and optimize AI techniques for best resultsWho this book is forThis book is for a wide audience, including writers, marketing and business professionals, researchers, students, tech enthusiasts, and creative individuals. Anyone looking for strategies and examples for using AI co-writing tools like ChatGPT effectively in domains such as content creation, drafting emails, and inspiring artistic works, will find this book especially useful. If you are interested in AI, NLP, and innovative software for personal or professional use, this is the book for you.","","9781835088265","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10460891.pdf&bkn=10460890&pdfType=book","","","","","","","","6 Mar 2024","","","Packt Publishing","Packt Publishing eBooks"
"Federated Learning Attacks and Defenses: A Survey","Y. Chen; Y. Gui; H. Lin; W. Gan; Y. Wu","Jinan University, Guangzhou, China; Jinan University, Guangzhou, China; Jinan University, Guangzhou, China; Jinan University, Guangzhou, China; Jinan University, Guangzhou, China",2022 IEEE International Conference on Big Data (Big Data),"26 Jan 2023","2022","","","4256","4265","In terms of artificial intelligence, there are several security and privacy deficiencies in the traditional centralized training methods of machine learning models by a server. To address this limitation, federated learning (FL) has been proposed and is known for breaking down ""data silos"" and protecting the privacy of users. However, FL has not yet gained popularity in the industry, mainly due to its security, privacy, and high cost of communication. For the purpose of advancing the research in this field, building a robust FL system, and realizing the wide application of FL, this paper sorts out the possible attacks and corresponding defenses of the current FL system systematically. Firstly, this paper briefly introduces the basic workflow of FL and related knowledge of attacks and defenses. It reviews a great deal of research about privacy theft and malicious attacks that have been studied in recent years. Most importantly, in view of the current three classification criteria, namely the three stages of machine learning, the three different roles in federated learning, and the CIA (Confidentiality, Integrity, and Availability) guidelines on privacy protection, we divide attack approaches into two categories according to the training stage and the prediction stage in machine learning. Furthermore, we also identify the CIA property violated for each attack method and potential attack role. Various defense mechanisms are then analyzed separately from the level of privacy and security. Finally, we summarize the possible challenges in the application of FL from the aspect of attacks and defenses and discuss the future development direction of FL systems. In this way, the designed FL system has the ability to resist different attacks and is more secure and stable.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10020431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020431","federated learning;attacks;defenses;challenges;opportunities","Training;Privacy;Pediatrics;Systematics;Costs;Federated learning;Resists","","16","","102","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models on the Traditional Scientific Writing Workflow","T. T. Procko; A. Davidoff; T. Elvira; O. Ochoa","Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University, Daytona Beach, FL, USA; Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University, Daytona Beach, FL, USA; Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University, Daytona Beach, FL, USA; Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University, Daytona Beach, FL, USA","2024 Conference on AI, Science, Engineering, and Technology (AIxSET)","3 Dec 2024","2024","","","154","161","Technological advances in Natural Language Processing have brought forth language models capable of advanced response delivery. Scientific papers are traditionally written manually by human researchers, but with the advent of mainstream Large Language Models, e.g., OpenAI's ChatGPT, it is of increasing concern to scientists and academics that content in scientific papers may be generated by Artificial Intelligence (AI). Wishing to stop this is a losing attitude, as large-scale generative AI only becomes more powerful and accessible. Taking the more tenable position of cautious adaptation, this paper argues that there exists a taxonomy in the structure of scientific papers, and that language models can be used by scientific researchers to bootstrap scientific writing. Furthermore, AI can augment their own writing workflow to more efficiently traverse the academic publishing pipeline. Despite the shortcomings of language models, e.g., hallucination, when prompted appropriately with sufficient constraints, language models are extremely accurate and efficient content providers. In this work, the canonical scientific paper is broken down into its taxonomy of parts, where it is then considered how each part can benefit from language models, e.g., in generating abstracts and keywords, reformatting sections, theorizing titles, etc. Finally, a call for consensus among the academic and scientific communities regarding the use of language models in the scientific writing workflow is established.","","979-8-3503-9099-5","10.1109/AIxSET62544.2024.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10770985","literature review;metascience;scientific workflow system;language model;scientific writing;academia;GPT","Adaptation models;Generative AI;Publishing;Large language models;Taxonomy;Pipelines;Writing;Horses;Throughput;Automobiles","","","","68","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"Systematic Literature Review of Data Quality in Open Government Data: Trend, Methods, and Applications","Z. Zainuddin; E. A. P. Akhir","Computer and Information Science Department, Universiti Teknologi PETRONAS, Seri Iskandar, Malaysia; Computer and Information Science Department, Universiti Teknologi PETRONAS, Seri Iskandar, Malaysia",IEEE Access,"18 Oct 2024","2024","12","","148466","148487","An open government data (OGD) is an assortment of datasets dumped by government agencies into a portal with the goal of promoting the legitimacy and openness of government operations and processes to citizens. OGD also assist citizens in obtaining other forms of information to aid in decision-making or planning. However, poor data quality (DQ) leads to users losing interest and trust in using the data. DQ is a type of measurement that determines the usefulness of data. It is a vast issue for discussion, with uncertainty arising from the diversity across a range of descriptions and dimensions. Hence, this study explored the concept of DQ and found the completeness dimension as a vital component in OGD. According to an analysis of 37 articles, incomplete, missing, and unknown values all had a direct effect on completeness. This study illustrated recent ways of addressing the issue, as well as real life examples. Finally, this study offers recommendations for novices and other intellectuals for improvement in future research.","2169-3536","","10.1109/ACCESS.2024.3475577","Murata Science Foundation(grant numbers:015ME0-347); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10706921","Completeness;data quality;dimension;missing value;open government data","Accuracy;Government;Data integrity;Economics;Portals;Open data;Data visualization;Technological innovation;Standards;Metadata","","1","","88","CCBYNCND","7 Oct 2024","","","IEEE","IEEE Journals"
"Deep image synthesis from intuitive user input: A review and perspectives","Y. Xue; Y. -C. Guo; H. Zhang; T. Xu; S. -H. Zhang; X. Huang","College of Information Sciences and Technology, the Pennsylvania State University, University Park, PA, USA; Department of Computer Science and Technology, Tsinghua University, Beijing, China, and Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Google Brain, Mountain View, CA, USA; Facebook, Menlo Park, CA, USA; Department of Computer Science and Technology, Tsinghua University, Beijing, China, and Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; College of Information Sciences and Technology, the Pennsylvania State University, University Park, PA, USA",Computational Visual Media,"20 Feb 2025","2022","8","1","3","31","In many applications of computer graphics, art, and design, it is desirable for a user to provide intuitive non-image input, such as text, sketch, stroke, graph, or layout, and have a computer system automatically generate photo-realistic images according to that input. While classically, works that allow such automatic image content generation have followed a framework of image retrieval and composition, recent advances in deep generative models such as generative adversarial networks (GANs), variational autoencoders (VAEs), and flow-based methods have enabled more powerful and versatile image generation approaches. This paper reviews recent works for image synthesis given intuitive user input, covering advances in input versatility, image generation methodology, benchmark datasets, and evaluation metrics. This motivates new perspectives on input representation and interactivity, cross fertilization between major image generation paradigms, and evaluation and comparison of generation methods.","2096-0662","","10.1007/s41095-021-0234-8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897565","image synthesis;intuitive user input;deep generative models;synthesized image quality evaluation","Image synthesis;Measurement;Reviews;Semantics;Feature extraction;Visualization;Vectors;Noise;Electronic mail;Computational modeling","","2","","","","20 Feb 2025","","","TUP","TUP Journals"
"The Book Proposal Book: A Guide for Scholarly Authors","L. Portwood-Stacer",NA,The Book Proposal Book: A Guide for Scholarly Authors,"","2021","","","","","A step-by-step guide to crafting a compelling scholarly book proposal—and seeing your book through to successful publicationThe scholarly book proposal may be academia’s most mysterious genre. You have to write one to get published, but most scholars receive no training on how to do so—and you may have never even seen a proposal before you’re expected to produce your own. The Book Proposal Book cuts through the mystery and guides prospective authors step by step through the process of crafting a compelling proposal and pitching it to university presses and other academic publishers.Laura Portwood-Stacer, an experienced developmental editor and publishing consultant for academic authors, shows how to select the right presses to target, identify audiences and competing titles, and write a project description that will grab the attention of editors—breaking the entire process into discrete, manageable tasks. The book features over fifty time-tested tips to make your proposal stand out; sample prospectuses, a letter of inquiry, and a response to reader reports from real authors; optional worksheets and checklists; answers to dozens of the most common questions about the scholarly publishing process; and much, much more.Whether you’re hoping to publish your first book or you’re a seasoned author with an unfinished proposal languishing on your hard drive, The Book Proposal Book provides honest, empathetic, and invaluable advice on how to overcome common sticking points and get your book published. It also shows why, far from being merely a hurdle to clear, a well-conceived proposal can help lead to an outstanding book.","","9780691216621","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9647707.pdf&bkn=9647706&pdfType=book","Publishing;Publication;Author;Writing;Academic publishing;Paragraph;Copy editing;Princeton University Press;Suggestion;Guideline;Manuscript;Target audience;Table of contents;Narrative;Marketing;Credential;Handbook;Career;Bibliography;Website;Word count;Finding;Writing style;Understanding;Email;Publicist;Editorial board;Literature review;Article (publishing);Proofreading;Editorial;Case study;Literary agent;Peer review;Copyright;Gaze;Quality assurance;Illustration;First Book;Editing;Rhetorical question;Phenomenon;Passive voice;Writing process;Wildlife conservation;Funding;Academic journal;Citizen science;Technology;Book;Recommendation (European Union);Edition (book);Designer;Developmental editing;Writer;Rhetoric;Environmental protection;Search engine optimization;Ecosystem;Institution;Monograph;Calculation;Marketing plan;Price point;Academic writing;Social media;Publicity;Consideration;Documents (magazine);Sociology;Brand management;Femininity;Librarian;Style guide;Symbolic capital;On Royalty;Neoliberalism;Qualitative research;Verb;Infrastructure;Thesis statement;Routledge;Criticism;Decision-making;Book design;Textbook;Creative director;Methodology;Sensibility;Brand culture;University of Illinois Press;Sentence (linguistics);Newspaper;Hardcover;Description;Public talks;Empowerment;Sexism;Globalization;One Laptop per Child","","","","","","","13 Dec 2021","","","Princeton University Press","Princeton University Press eBooks"
"What Is So Deep About Deepfakes? A Multi-Disciplinary Thematic Analysis of Academic Narratives About Deepfake Technology","J. Twomey; D. Ching; M. Peter Aylett; M. Quayle; C. Linehan; G. Murphy","School of Applied Psychology, University College Cork, Cork, Ireland; School of Applied Psychology, University College Cork, Cork, Ireland; Mathematics and Computer Science Department, Heriot Watt University, Edinburgh, U.K.; Department of Psychology, Centre for Social Issues Research, University of Limerick, Limerick, Ireland; School of Applied Psychology, University College Cork, Cork, Ireland; School of Applied Psychology, University College Cork, Cork, Ireland",IEEE Transactions on Technology and Society,"28 Feb 2025","2025","6","1","64","79","Deepfakes are a form of synthetic media that uses deep-learning technology to create fake images, video, and audio. The emergence of this technology has inspired much commentary and speculation from academics across a range of disciplines, who have contributed expert opinions regarding the implications of deepfake proliferation on fields such as law, politics, and entertainment. A systematic scoping review was carried out to identify, assemble, and critically analyze those academic narratives. The aim is to build on and critique previous attempts at defining the technology and categorizing the harms and benefits of deepfake technology. A range of databases were searched for relevant articles from 2017 to 2023, resulting in a large multi-disciplinary dataset of 102 papers, 181,659 words long, which were analyzed qualitatively through thematic analysis. Implications for future research include questioning the lack of research evidence for the supposed positives of deepfakes, recognizing the role that identity plays in deepfake technology, challenging the perceived accessibility/ believability of deepfakes, and proposing a more nuanced approach to the dichotomous “positive and negatives” of deepfakes. Furthermore, we show how definitional issues around what a deepfake is versus other forms of fake media feeds confusion around the novelty and impacts of deepfakes.","2637-6415","","10.1109/TTS.2024.3493465","Science Foundation Ireland(grant numbers:13/RC/2094_2); European Regional Development Fund through the Southern and Eastern Regional Operational Programme to Lero—The Science Foundation Ireland Research Centre for Software; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10756226","Deepfakes;social implications of technology;media","Deepfakes;Reviews;Generative AI;Software;Psychology;Social networking (online);Organizations;Lenses;Law;Internet","","","","75","CCBYNCND","18 Nov 2024","","","IEEE","IEEE Journals"
"Privacy Protection Against Reverse Image Search","R. Pratik; R. Sendhil","School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India; School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India",2023 Third International Conference on Artificial Intelligence and Smart Energy (ICAIS),"27 Mar 2023","2023","","","1207","1214","The user wants to include their image in their résumé, LinkedIn profile, or any other relevant place and doesn't want their personal life to be viewed using a reverse image search, which can trace back their professional image to their social ID based on facial image. They may not want to be evaluated as social media account may expose sensitive information or personal life. Various techniques such as synthetic modification exist to protect the privacy of an individual but they mostly tend to distort the image to such an extent that figure is not identifiable as a human or some other person is represented.This article provides a technique that enables people to protect their images against the misuse of facial recognition software enabling humans to identify the person but machines fail to recognize the person. The proposed method would help the users to edit their images at the pixel level and removing the metadata before sharing them. These ""cloaked"" images yield functional models that repeatedly lead to incorrect identification of typical images of the user when used to train facial recognition models. When combined with metadata removal, it highly reduces the possibility of image traceback to a person’s social ID.","","978-1-6654-6216-7","10.1109/ICAIS56108.2023.10073803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10073803","Privacy protection;image recognition;metadata;cloaking;reverse search;google lens","Privacy;Image recognition;Social networking (online);Face recognition;Multimedia Web sites;Organizations;Metadata","","1","","34","IEEE","27 Mar 2023","","","IEEE","IEEE Conferences"
"Physical Inspection & Attacks: New Frontier in Hardware Security","M. T. Rahman; Q. Shi; S. Tajik; H. Shen; D. L. Woodard; M. Tehranipoor; N. Asadizanjani","Electrical & Computer Engineering Department, University of Florida, Gainesville, FL, USA; Electrical & Computer Engineering Department, University of Florida, Gainesville, FL, USA; Electrical & Computer Engineering Department, University of Florida, Gainesville, FL, USA; Electrical & Computer Engineering Department, University of Florida, Gainesville, FL, USA; Electrical & Computer Engineering Department, University of Florida, Gainesville, FL, USA; Electrical & Computer Engineering Department, University of Florida, Gainesville, FL, USA; Electrical & Computer Engineering Department, University of Florida, Gainesville, FL, USA",2018 IEEE 3rd International Verification and Security Workshop (IVSW),"18 Oct 2018","2018","","","93","102","Due to globalization, the semiconductor industry is becoming more susceptible to trust and security issues. Hardware Trojans, i.e., malicious modification to integrated circuits (ICs), can violate the root of trust when the devices are fabricated in untrusted facilities. Literature shows as the microscopy and failure analysis tools excel in the resolution and capability, physical inspection methods like reverse engineering and photonic emission become attractive in helping verify such trust issues. On the contrary, such physical inspection methods are opening new capabilities for an adversary to extract sensitive information like secret keys, memory content or intellectual property (IP) from the chip compromising confidentiality and integrity. Different countermeasures have been proposed, however, there are still many unanswered questions. In this paper, we discuss physical inspection/attack methods using failure analysis tools and analyze the existing countermeasures and security/trust issues related to them. Next, we will introduce challenges related to the development of new countermeasures and trust verification. Finally, we present research roadmap for this emerging field.","","978-1-5386-6544-2","10.1109/IVSW.2018.8494856","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8494856","Physical Inspection/attacks;Invasive attacks;Reverse Engineering;Probing;Optical Attacks","Reverse engineering;Hardware;Integrated circuits;Trojan horses;Security;Inspection;Tools","","51","","67","IEEE","18 Oct 2018","","","IEEE","IEEE Conferences"
"An Infrared and Visible Image Fusion Method Based on Semantic-Sensitive Mask Selection and Bidirectional-Collaboration Region Fusion","X. Li; G. Zhang; W. Chen; L. Cheng; Y. Xie; J. Ma","School of Electrical and Information Engineering, Wuhan Institute of Technology, Wuhan, China; School of Electrical and Information Engineering, Wuhan Institute of Technology, Wuhan, China; Fiberhome Telecommunication Technologies Co., Ltd, Wuhan, China; School of Electrical and Information Engineering, Wuhan Institute of Technology, Wuhan, China; College of Mechanical and Electrical Engineering, Northeast Forestry University, Harbin, China; Electronic Information School, Wuhan University, Wuhan, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2024","PP","99","1","1","Mask is considered as an important prior for fusion, which could selectively enhance specific regions to generate ideal fused images. However, masks used in the existing methods exhibit limitations in the precise representation of targets, and more importantly, these masks are generated from a single modality, which restricts the effective integration of multi-modal information. To address this issue, we propose a competitive mask-guidance fusion method for infrared and visible images. A multi-modal semantic-sensitive mask selection network is proposed to generate complementary-mask maps, which organically integrate advantageous target regions of different modalities by competitively comparing the qualities of masks. In this network, a pseudosiamese architecture is designed to obtain respective target masks, and specifically, a spatial-aligned-based feature aggregation module is devised to produce high-quality pseudo-labels which are served as references for the generation of the complementary-mask maps. Furthermore, we propose a bidirectional-collaboration region fusion strategy, which enhances the expression of advantageous target regions from each modality in foreground while suppressing the contribution of corresponding regions from the other modality in background. Compared to methods on public datasets, the results show that our method significantly enhances the description of semantic-sensitive targets in fused images, including the saliency and the integrity of structural information. Code are available at https://github.com/9DunDun9/FusionNet.git.","1558-2205","","10.1109/TCSVT.2024.3520252","Fundamental Research Funds for the Central Universities(grant numbers:2042024kf0038); National Natural Science Foundation of China(grant numbers:No.62401411); Wuhan Municipal Natural Science Foundation(grant numbers:No.2024040801020323); Natural Science Foundation of Hubei Province(grant numbers:No.2022CFB776); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10810321","Image fusion;Semantic-sensitive mask selection;Complementary-mask map;Bidirectional-collaboration fusion","Feature extraction;Image fusion;Circuits and systems;Deep learning;Generators;Network architecture;Measurement;Image reconstruction;Generative adversarial networks;Visual effects","","","","","IEEE","20 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Multi-modal Long-Short Distance Attention-based Transformer-GAN for PET Reconstruction with Auxiliary MRI","P. Zeng; X. Zeng; Y. Wang; L. Zhou; C. Zu; X. Wu; J. Zhou; D. Shen","School of Computer Science, Sichuan University, China; School of Computer Science, Sichuan University, China; School of Computer Science, Sichuan University, China; School of Electrical and Information Engineering, University of Sydney, Australia; Department of Risk Controlling Research, JD.COM, China; School of Computer Science, Chengdu University of Information Technology, China; School of Computer Science, Sichuan University, China; School of Biomedical Engineering & State Key Laboratory of Advanced Medical Materials and Devices, ShanghaiTech University, Shanghai, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","To obtain high-quality PET scans while minimizing potential radiation hazards for patients, various GAN-based methods have been developed to reconstruct high-quality standard-count PET (SPET) images from low-count PET (LPET) ones. While recent efforts try to integrate MRI or CT to enhance reconstruction in a multi-modal way, current architectures mainly face two limitations: (1) CNN backbones or simple Transformer bottleneck layers are insufficient for robust semantic understanding, and (2) the identical strategies for multi-modal feature extraction and fusion overlook each modality’s respective importance for the reconstruction task. In this work, we propose the Multi-modal Long-Short Distance Attention-based Transformer-GAN (MLSDA-GAN), a novel network combining 3D transformer and CNN architecture for PET image reconstruction. Specifically, to extract fine-grained features with a small number of parameters, our MLSDA-GAN integrates multi-scale convolution into the embedding part of the transformer. As for our multi-modal design, given the strong correlation between LPET and SPET in structural characteristics, we treat MRI as an auxiliary modality to LPET and achieve effective multi-modal extraction and fusion strategies. These strategies include (1) a PET-specific Self-attention Extraction (PSE) block for comprehensive feature extraction of the primary LPET and (2) a Multi-modality Cross-attention Fusion (MCF) block for effective multi-modal interaction and fusion, enabling us to more efficiently model both long- and short-range relationships in the corresponding feature extraction and fusion processes. Experiments demonstrate superiority of our method quantitatively and qualitatively. Code is available at https://github.com/Aru321/MLSDA-GAN.","1558-2205","","10.1109/TCSVT.2025.3545911","National Natural Science Foundation of China(grant numbers:62131015,62371325,82394432,82441023,U23A20295); Sichuan Science and Technology Program(grant numbers:2023YFG0025); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10904905","Attention Mechanism;Feature Extraction and Fusion;Multi-modal PET Reconstruction","Image reconstruction;Feature extraction;Transformers;Magnetic resonance imaging;Three-dimensional displays;Convolutional neural networks;Convolution;Generative adversarial networks;Solid modeling;Positron emission tomography","","","","","IEEE","26 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Transferable Presynthesis PPA Estimation for RTL Designs With Data Augmentation Techniques","W. Fang; Y. Lu; S. Liu; Q. Zhang; C. Xu; L. Wu Wills; H. Zhang; Z. Xie","Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, SAR; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, SAR; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, SAR; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, SAR; Department of Computer Science and Electrical and Computer Engineering, Duke University, Durham, NC, USA; Department of Computer Science and Electrical and Computer Engineering, Duke University, Durham, NC, USA; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, SAR; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, SAR",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,"24 Dec 2024","2025","44","1","200","213","In modern VLSI design flow, evaluating the quality of register-transfer level (RTL) designs involves time-consuming logic synthesis using electronic design automation tools, a process that often slows down early optimization. While recent machine learning (ML) solutions offer some advancements, they typically struggle with maintaining high accuracy across any given RTL design. In this work, we propose an innovative transferable presynthesis power, performance, and area (PPA) estimation framework named MasterRTL. It first converts the hardware description language code to a new bit-level design representation named the simple operator graph (SOG). By only adopting single-bit simple operators, this SOG proves to be a general representation that unifies different design types and styles. The SOG is also more similar to the target gate-level netlist, reducing the gap between the RTL representation and netlist. In addition to the new SOG representation, MasterRTL proposes new ML methods for the RTL-stage modeling of timing, power, and area separately. Compared with the state-of-the-art solutions, the experiment on a comprehensive dataset with 90 different designs shows accuracy improvement by 0.33, 0.22, and 0.15 in correlation for total negative slack (TNS), worst negative slack (WNS), and power, respectively. Besides the prediction of the synthesis results, MasterRTL also excels in accurately predicting layout-stage PPA based on the RTL designs and in adapting across different technology nodes and process corners. Furthermore, we investigate two effective data augmentation techniques: 1) a graph generation method and 2) a large language model (LLM)-based approach. Our results validate the effectiveness of the generated RTL designs in mitigating the data shortage challenges.","1937-4151","","10.1109/TCAD.2024.3420904","Hong Kong Research Grants Council ECS(grant numbers:26208723); National Natural Science Foundation of China(grant numbers:62304192); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10577671","Data augmentation;power modeling;register-transfer level (RTL);timing analysis","Data models;Layout;Timing;Data augmentation;Codes;Predictive models;Logic gates","","","","45","IEEE","1 Jul 2024","","","IEEE","IEEE Journals"
"An Enhanced Loss Function for Semantic Road Segmentation in Remote Sensing Images","L. Nanni; S. Brahnam; A. Loreggia","Department of Information Engineering (DEI), University of Padova, Padua, Italy; Information Technology and Cybersecurity, Missouri State University, Springfield, MO, USA; Department of Information Engineering (DII), University of Brescia, Brescia, Italy",IEEE Access,"30 May 2024","2024","12","","74218","74229","The analysis of road continuity in satellite images is a complex challenge. This is due to the difficulty in identifying the directional vector of road sections, especially when the satellite view of roads is obstructed by trees or other structures. Today, most research focuses on optimizing the deep learning network topology, however, the accuracy of segmentation is affected by the loss function used in training; currently, little research has been published on ad-hoc loss functions for road segmentation. To solve this problem, we proposed loss functions based on topological pixel analysis, in which more weight is given to problematic pixels representing non-real road breaks. We report the results of different tests, obtaining state-of-the-art performance among convolution neural network-based approaches. For instance, on the Massachusetts Roads dataset, our method achieved a Dice score of 75.34% and an IoU of 60.44%, compared to the best baseline scores of 74.64% and 59.51% achieved by GapLoss. Similarly, on the DeepGlobe Roads dataset, our method obtained a Dice score of 79.78% and an IoU of 66.36%, outperforming the best baseline scores of 78.62% and 64.47% by GapLoss. Both the code and information for replicating our experiments are available at https://github.com/LorisNanni/An-Enhanced-Loss-Function-for-Semantic-Road-Segmentation-in-Remote-Sensing-Images, so as to enable future reliable comparisons.","2169-3536","","10.1109/ACCESS.2024.3405559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10539125","Convolutional neural networks;road segmentation;optimization;ensemble","Roads;Image segmentation;Remote sensing;Task analysis;Convolution;Feature extraction;Deep learning","","","","71","CCBY","27 May 2024","","","IEEE","IEEE Journals"
"A Systematic Review of Adversarial Machine Learning Attacks, Defensive Controls, and Technologies","J. Malik; R. Muthalagu; P. M. Pawar","Department of Computer Science, Birla Institute of Technology and Science, Pilani, Dubai Campus, Dubai, United Arab Emirates; Department of Computer Science, Birla Institute of Technology and Science, Pilani, Dubai Campus, Dubai, United Arab Emirates; Department of Computer Science, Birla Institute of Technology and Science, Pilani, Dubai Campus, Dubai, United Arab Emirates",IEEE Access,"25 Jul 2024","2024","12","","99382","99421","Adversarial machine learning (AML) attacks have become a major concern for organizations in recent years, as AI has become the industry’s focal point and GenAI applications have grown in popularity around the world. Organizations are eager to invest in GenAI applications and develop their own large language models, but they face numerous security and data privacy issues, particularly AML attacks. AML attacks have jeopardized numerous large-scale machine learning models. If carried out successfully, AML attacks can significantly reduce the efficiency and precision of machine learning models. They have far-reaching negative consequences in the context of critical healthcare and autonomous transportation systems. In this paper, AML attacks are identified, analyzed, and classified using adversarial tactics and techniques. This research also recommends open-source tools for testing AI and ML models against AML attacks. Furthermore, this research suggests specific mitigating measures against each attack. It aims to serve as a guidance for organizations to defend against AML attacks and gain assurance in the security of ML models.","2169-3536","","10.1109/ACCESS.2024.3423323","Birla Institute of Technology and Science, Pilani for paying the Article Processing Charges (APC) of this publication; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10584534","Adversarial machine learning;AI assurance;cybersecurity;data privacy;secure software development lifecycle","Artificial intelligence;Security;Organizations;Testing;Reviews;Data models;Computational modeling;Adversarial machine learning;Data privacy;Software development management;Life cycle assessment","","1","","159","CCBYNCND","4 Jul 2024","","","IEEE","IEEE Journals"
"Controllable multi-domain semantic artwork synthesis","Y. Huang; S. Iizuka; E. Simo-Serra; K. Fukui","Department of Computer Science, University of Tsukuba, Tsukuba 305-8577, Japan; Department of Computer Science, University of Tsukuba, Tsukuba 305-8577, Japan; Department of Computer Science and Engineering, Waseda University, Tokyo 169-8050, Japan; Department of Computer Science, University of Tsukuba, Tsukuba 305-8577, Japan",Computational Visual Media,"20 Feb 2025","2024","10","2","355","373","We present a novel framework for the multidomain synthesis of artworks from semantic layouts. One of the main limitations of this challenging task is the lack of publicly available segmentation datasets for art synthesis. To address this problem, we propose a dataset called ArtSem that contains 40,000 images of artwork from four different domains, with their corresponding semantic label maps. We first extracted semantic maps from landscape photography and used a conditional generative adversarial network (GAN)-based approach for generating high-quality artwork from semantic maps without requiring paired training data. Furthermore, we propose an artwork-synthesis model using domain-dependent variational encoders for high-quality multi-domain synthesis. Subsequently, the model was improved and complemented with a simple but effective normalization method based on jointly normalizing semantics and style, which we call spatially style-adaptive normalization (SSTAN). Compared to the previous methods, which only take semantic layout as the input, our model jointly learns style and semantic information representation, improving the generation quality of artistic images. These results indicate that our model learned to separate the domains in the latent space. Thus, we can perform fine-grained control of the synthesized artwork by identifying hyperplanes that separate the different domains. Moreover, by combining the proposed dataset and approach, we generated user-controllable artworks of higher quality than that of existing approaches, as corroborated by quantitative metrics and a user study.","2096-0662","","10.1007/s41095-023-0356-2","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897652","semantic artwork synthesis;generative adversarial network (GAN);datasets;non-photorealistic rendering","Semantics;Painting;Art;Training;Oils;Vectors;Translation;Layout;Image synthesis;Computational modeling","","","","","","20 Feb 2025","","","TUP","TUP Journals"
"HTrans: Transformer-Based Method for Hardware Trojan Detection and Localization","Y. Li; S. Li; H. Shen","School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China",2023 IEEE 32nd Asian Test Symposium (ATS),"20 Nov 2023","2023","","","1","6","Hardware Trojan (HT) is a malicious code intentionally inserted into the original circuit design to modify the original function, leak information or decrease the performance. Circuit fabrications have increased Third-Party Intellectual Property (3PIP) usage with market pressure and the increasing global economy. Consequently, hardware may become vulnerable to a wide range of attacks at some stage of the manufacturing process, making detecting HT a necessary procedure. HT detection in the early stage is crucial because removing HT and re-designing the circuit later or after fabrication could be expensive. In this work, we propose a novel Transformer-based Method for pre-silicon HT detection and localization called HTrans. We innovatively use Graph Convolutional Network (GCN) as a preprocessing stage before the Transformer, giving our model the scalability to any design size. Experiments on the Trusthub benchmark show that our model achieves an average of 96.7% Fl score on HT detection and 91.7% accuracy on HT localization. In addition, HTrans can quickly complete the detection on the Register Transfer Level (RTL) within a second.","2377-5386","979-8-3503-0310-0","10.1109/ATS59501.2023.10317971","National Key R&D Program of China(grant numbers:2022YFB4500402); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10317971","hardware Trojan detection;security;deep learning","Location awareness;Fabrication;Knowledge engineering;Manufacturing processes;Scalability;Transformers;Hardware","","2","","20","IEEE","20 Nov 2023","","","IEEE","IEEE Conferences"
"Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects","N. Li; C. Zhou; Y. Gao; H. Chen; Z. Zhang; B. Kuang; A. Fu","School of Cyber Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Department of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Department of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia; School of Cyber Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Cyber Science and Engineering, Nanjing University of Science and Technology, Nanjing, China",IEEE Transactions on Neural Networks and Learning Systems,"","2025","PP","99","1","21","Personal digital data is a critical asset, and governments worldwide have enforced laws and regulations to protect data privacy. Data users have been endowed with the “right to be forgotten” (RTBF) of their data. In the course of machine learning (ML), the forgotten right requires a model provider to delete user data and its subsequent impact on ML models upon user requests. Machine unlearning (MU) emerges to address this, which has garnered ever-increasing attention from both industry and academia. Specifically, MU allows model providers to eliminate the influence of unlearned data without retraining the model from scratch, ensuring the model behaves as if it never encountered this data. While the area has developed rapidly, there is a lack of comprehensive surveys to capture the latest advancements. Recognizing this shortage, we conduct an extensive exploration to map the landscape of MU including the (fine-grained) taxonomy of unlearning algorithms under centralized and distributed settings, debate on approximate unlearning, verification and evaluation metrics, and challenges and solutions across various applications. We also focus on the motivations, challenges, and specific methods for deploying unlearning in large language models (LLMs), as well as the potential attacks targeting unlearning processes. The survey concludes by outlining potential directions for future research, hoping to serve as a beacon for interested scholars.","2162-2388","","10.1109/TNNLS.2025.3530988","thee National Natural Science Foundation of China(grant numbers:62072239,62372236); Open Foundation of the State Key Laboratory of Integrated Services Networks(grant numbers:ISN24-15); Qing Lan Project of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10880482","Data privacy;federated learning (FL);large language model (LLM);machine learning (ML);machine unlearning (MU)","Data models;Surveys;Measurement;Training;Electronic mail;Data privacy;Taxonomy;General Data Protection Regulation;Computational modeling;Approximation algorithms","","1","","","IEEE","11 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Threat Modeling AI/ML With the Attack Tree","S. V. Hoseini; J. Suutala; J. Partala; K. Halunen","Faculty of Information and Electrical Engineering, University of Oulu, Oulu, Finland; Faculty of Information and Electrical Engineering, University of Oulu, Oulu, Finland; Faculty of Information and Electrical Engineering, University of Oulu, Oulu, Finland; Faculty of Information and Electrical Engineering, University of Oulu, Oulu, Finland",IEEE Access,"22 Nov 2024","2024","12","","172610","172637","The pervasive use of AI assistant systems and machine learning-based applications in various fields and everyday life has significantly shifted. However, this shift is not without its challenges. The emergence of security threats, various attacks, and vulnerabilities in this domain has not only questioned their use but also sparked the interest of security experts and researchers, underlining the urgency and importance of this topic. However, a comprehensive and systematic research endeavor is yet to be undertaken on threat modeling based on violating basic tenets of information security on the various components of a machine learning system and evaluating their security risks. This lack of comprehensive threat modeling for each violation of a machine learning system’s confidentiality, integrity, availability, and privacy for various attacks and their risk analysis is a significant gap in the field. This article aims to bridge this gap by proposing a simple, efficient, and time-saving approach to evaluate potential attacks and their security risks by utilizing the attack tree and a risk analysis method in the Adversarial Machine Learning (AML) field. One of the most important steps in determining the overall risk of the attack is evaluating the risk attached to each node in an attack tree. A systematic approach that includes describing the system architecture and identifying its assets under various operational environment scenarios is also outlined in this paper. This approach can also offer crucial insights to security experts, aiding them in understanding and mitigating potential threats and risk analysis in AML systems. To ensure the validity and reliability of our findings, we have conducted a thorough and rigorous review of academic papers, summarizing different threats and attacks and their root cause analysis.","2169-3536","","10.1109/ACCESS.2024.3497011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10752529","Machine learning;adversarial machine learning;attack tree;security;privacy;integrity;confidentiality;availability","Threat modeling;Root cause analysis;Systematics;Reviews;Roads;Information security;Systems architecture;Vectors;Risk analysis;Reliability","","","","152","CCBYNCND","13 Nov 2024","","","IEEE","IEEE Journals"
"The Artificial Intelligence Revolution in New-Product Development","R. G. Cooper","McMaster University, Hamilton, ON, Canada",IEEE Engineering Management Review,"18 Apr 2024","2024","52","1","195","211","Artificial Intelligence (AI) is poised to revolutionize all aspects of business, particularly new-product development (NPD). Currently, our approach to NPD has remained largely unchanged for decades, yielding stubbornly poor results: only 30% of NP development projects become commercial successes. However, the AI revolution is set to alter this landscape significantly! Leading early adopter firms demonstrate that AI not only finds many applications in NPD but also offers substantial payoffs, such as 50% reductions in development times. This article provides an outline of the diverse and powerful applications of AI in NPD, offering numerous examples from leading companies. Examples include GE's use of digital models and twins to quickly test product designs in turbine development; BASFs use of AI to identify new molecules for use in customer formulations; and AI to generate new-product ideas, identify new-product opportunities, and even create new-product concepts. Our exploratory journey begins at the idea stage and traverses the entire new-product process to the postlaunch period. While AI might still resemble science fiction to many, that future is no longer fiction—it is here now. AI has arrived in full force! With an adoption window of about 13 years, the time is now to embrace AI in NPD in your business. AI will become a major milestone in NPD, perhaps the most important, within the decade.","1937-4178","","10.1109/EMR.2023.3336834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10330569","AI for new-product development (NPD);artificial intelligence (AI);generative AI;new-product development;new-product process;product innovation","Artificial intelligence;Business;Technological innovation;Product development;Prediction algorithms;Software algorithms;Chatbots","","11","","69","IEEE","28 Nov 2023","","","IEEE","IEEE Journals"
"Understanding Open Source Large Language Models: An Exploratory Study","S. Sowe; Y. Mou; D. Cheng; L. Kong; A. T. Neumann; S. Decker","Chair of Computer Science 5, RWTH Aachen University, Aachen, Germany; Chair of Computer Science 5, RWTH Aachen University, Aachen, Germany; Chair of Computer Science 5, RWTH Aachen University, Aachen, Germany; Data Science and AI, Fraunhofer (FIT), Sankt Augustin, Germany; Chair of Computer Science 5, RWTH Aachen University, Aachen, Germany; Chair of Computer Science 5, RWTH Aachen University, Aachen, Germany",2024 2nd International Conference on Foundation and Large Language Models (FLLM),"28 Jan 2025","2024","","","132","140","Prompted by the increasing dominance of proprietary Large Language Models (LLMs), such as OpenAI’s GPT-4 and Google’s Gemini, concerns about data privacy, accessibility and bias have led to a growing advocacy for OSLLMs. This study investigates Open Source Large Language Models (OSLLMs), exploring their characteristics, openness, and community interactions. Our research aims to define OSLLMs (license, openness, community engagement). Utilizing data from the Hugging Face platform, we examine the popularity metrics, license distribution, artefact accessibilities and community engagement of LLM projects. Findings reveal a skewed distribution of model usage, with a few models dominating downloads and likes. Apache 2.0 and MIT are the most common licenses among top models, highlighting a preference for flexible usage terms. However, a significant portion of models lack specified licenses, posing potential legal challenges. Openness analysis shows that nearly half of the examined models share their training code and datasets, with standardized evaluation metrics common across repositories. Community engagement analysis indicates that engineer users are more active than general users, contributing significantly to technical discussions. Sentiment analysis of forum interactions reveals varying user attitudes, with licensed models generally receiving more positive feedback. This study underscores the potential of OSLLMs to democratize AI access and foster innovation, while also highlighting areas for improvement in community engagement and model openness.","","979-8-3503-5479-9","10.1109/FLLM63129.2024.10852438","Ministry of Education; Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10852438","Artificial intelligence (AI);Open Source Large Language Models (OSLLMs);LLMs Licenses","Measurement;Training;Analytical models;Technological innovation;Sentiment analysis;Data privacy;Law;Large language models;Licenses;Faces","","","","33","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"Contagion Source Detection in Epidemic and Infodemic Outbreaks: Mathematical Analysis and Network Algorithms","C. W. Tan; P. -D. Yu",NA; NA,Contagion Source Detection in Epidemic and Infodemic Outbreaks: Mathematical Analysis and Network Algorithms,"","2023","","","","","The rapid spread of infectious diseases and online rumors share similarities in terms of their speed, scale, and patterns of contagion. Although these two phenomena have historically been studied separately, the COVID-19 pandemic has highlighted the devastating consequences that simultaneous crises of epidemics and misinformation can have on the world. Soon after the outbreak of COVID-19, the World Health Organization launched a campaign against the COVID-19 Infodemic, which refers to the dissemination of pandemic-related false information online that causes widespread panic and hinders recovery efforts. Undoubtedly, nothing spreads faster than fear. Networks serve as a crucial platform for viral spreading, as the actions of highly influential users can quickly render others susceptible to the same. The potential for contagion in epidemics and rumors hinges on the initial source, underscoring the need for rapid and efficient digital contact tracing algorithms to identify super-spreaders or Patient Zero. Similarly, detecting and removing rumor mongers is essential for preventing the proliferation of harmful information in online social networks. Identifying the source of large-scale contagions requires solving complex optimization problems on expansive graphs. Accurate source identification and understanding the dynamic spreading process requires a comprehensive understanding of surveillance in massive networks, including topological structures and spreading veracity. Ultimately, the efficacy of algorithms for digital contact tracing and rumor source detection relies on this understanding. This monograph provides an overview of the mathematical theories and computational algorithm design for contagion source detection in large networks. By leveraging network centrality as a tool for statistical inference, we can accurately identify the source of contagions, trace their spread, and predict future trajectories. This approach provides fundamental insights into surveillance capability and asymptotic behavior of contagion spreading in networks. Mathematical theory and computational algorithms are vital to understanding contagion dynamics, improving surveillance capabilities, and developing effective strategies to prevent the spread of infectious diseases and misinformation.","","9781638282518","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10173717.pdf&bkn=10173716&pdfType=book","","","","","","","","6 Jul 2023","","","now","Now Foundations and Trends Books"
"A Survey on Learning to Reject","X. -Y. Zhang; G. -S. Xie; X. Li; T. Mei; C. -L. Liu","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Deepwise Artificial Intelligence Lab, Beijing, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China",Proceedings of the IEEE,"6 Feb 2023","2023","111","2","185","215","Learning to reject is a special kind of self-awareness (the ability to know what you do not know), which is an essential factor for humans to become smarter. Although machine intelligence has become very accurate nowadays, it lacks such kind of self-awareness and usually acts as omniscient, resulting in overconfident errors. This article presents a comprehensive overview of this topic from three perspectives: confidence, calibration, and discrimination. Confidence is an important measurement for the reliability of model predictions. Rejection can be realized by setting thresholds on confidence. However, most models, especially modern deep neural networks, are usually overconfident. Therefore, calibration is a process to ensure confidence matching the actual likelihood of correctness, including two approaches: post-calibration and self-calibration. Calibration reflects the global characteristic of confidence, and the local distinguishing property of confidence is also important. In light of this, discrimination focuses on the performance of accepting positive samples while rejecting negative samples. As a binary classification problem, the challenge of discrimination comes from the missing and nonrepresentativeness of the negative data. Three discrimination tasks are comprehensively analyzed and discussed: failure rejection, unknown rejection, and fake rejection. By rejecting failures, the risk could be controlled especially for mission-critical applications. By rejecting unknowns, the awareness of the knowledge blind zone would be enhanced. By rejecting fakes, security and privacy could be protected. We provide a general taxonomy, organization, and discussion of the methods for solving these problems, which are studied separately in the literature. The connections between different approaches and future directions that are worth further investigation are also presented. With a discriminative and calibrated confidence, learning to reject will let the decision-making process be more practical, reliable, and secure.","1558-2256","","10.1109/JPROC.2023.3238024","National Key Research and Development Program(grant numbers:2018AAA0100400); National Natural Science Foundation of China(grant numbers:61836014,62222609,62076236,61721004,62276134); Key Research Program of Frontier Sciences of Chinese Academy of Sciences(grant numbers:ZDBS-LY-7004); Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2019141); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10028760","Calibration;confidence;discrimination;failure;fake;rejection;unknown","Predictive models;Failure analysis;Estimation;Calibration;Sorting;Predictive methods;Probabilistic logic","","12","","337","IEEE","27 Jan 2023","","","IEEE","IEEE Journals"
"Advanced Deep Learning Models for 6G: Overview, Opportunities, and Challenges","L. Jiao; Y. Shao; L. Sun; F. Liu; S. Yang; W. Ma; L. Li; X. Liu; B. Hou; X. Zhang; R. Shang; Y. Li; S. Wang; X. Tang; Y. Guo","Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China",IEEE Access,"1 Oct 2024","2024","12","","133245","133314","The advent of the sixth generation of mobile communications (6G) ushers in an era of heightened demand for advanced network intelligence to tackle the challenges of an expanding network landscape and increasing service demands. Deep Learning (DL), as a crucial technique for instilling intelligence into 6G, has demonstrated powerful and promising development. This paper provides a comprehensive overview of the pivotal role of DL in 6G, exploring the myriad opportunities and challenges that arise. Firstly, we present a detailed vision for DL in 6G, emphasizing areas such as adaptive resource allocation, intelligent network management, robust signal processing, ubiquitous edge intelligence, and endogenous security. Secondly, this paper reviews how DL models leverage their unique learning capabilities to solve complex service demands in 6G. The models discussed include Convolutional Neural Networks (CNN), Generative Adversarial Networks (GAN), Graph Neural Networks (GNN), Deep Reinforcement Learning (DRL), Transformer, Federated Learning (FL), and Meta Learning. Additionally, we examine the specific challenges each DL model faces within the 6G context. Moreover, we delve into the rapidly evolving field of Artificial Intelligence Generated Content (AIGC), examining its development and impact within the 6G framework. Finally, this paper culminates in a detailed discussion of ten critical open problems in integrating DL with 6G, setting the stage for future research and development in this field.","2169-3536","","10.1109/ACCESS.2024.3418900","Key Scientific Technological Innovation Research Project by Ministry of Education; National Natural Science Foundation of China(grant numbers:U22B2054); National Natural Science Foundation of China(grant numbers:62076192,61902298,61573267,61906150,62276199); Higher Education Discipline Innovation Project; Program for Cheung Kong Scholars and Innovative Research Team in University(grant numbers:IRT 15R53); ST Innovation Project from the Chinese Ministry of Education, the Key Research and Development Program in Shaanxi Province of China(grant numbers:2019ZDLGY03-06); China Postdoctoral Fund(grant numbers:2022T150506); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10570412","Deep learning;6G;network intelligence;artificial intelligence generated content (AIGC);open problems","6G mobile communication;Resource management;Adaptation models;Artificial intelligence;Transformers;Deep learning;Intelligent networks;Content management","","3","","660","CCBYNCND","25 Jun 2024","","","IEEE","IEEE Journals"
"Non-parallel Many-to-many Singing Voice Conversion by Adversarial Learning","J. Hu; C. Yu; F. Guan","College of Mathematics and Computer Science, Fuzhou University, China; College of Mathematics and Computer Science, Fuzhou University, China; College of Mathematics and Computer Science, Fuzhou University, China",2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),"5 Mar 2020","2019","","","125","132","With the rapid development of deep learning, although speech conversion had made great progress, there are still rare researches in deep learning to model on singing voice conversion, which is mainly based on statistical methods at present and can only achieve one-to-one conversion with parallel training datasets. So far, its application is limited. This paper proposes a generative adversarial learning model, MSVC-GAN, for many-to-many singing voice conversion using non-parallel datasets. First, the generator of our model is concatenated by the singer label, which denotes domain constraint. Furthermore, the model integrates self-attention mechanism to capture long-term dependence on the spectral features. Finally, switchable normalization is employed to stabilize network training. Both the objective and subjective evaluation results show that our model achieves the highest similarity and naturalness not only on the parallel speech dataset but also on the non-parallel singing dataset.","2640-0103","978-1-7281-3248-8","10.1109/APSIPAASC47483.2019.9023357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9023357","","5G mobile communication","","2","","33","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Exploiting Deep Neural Networks as Covert Channels","H. S. Pishbin; A. J. Bidgoly","Department of Information Technology, Computer Engineering, University of Qom, Qom, Iran; Department of Information Technology, Computer Engineering, University of Qom, Qom, Iran",IEEE Transactions on Dependable and Secure Computing,"10 Jul 2024","2024","21","4","2115","2126","With the increasing development of deep learning models, the security of these models has become more important. In this work, for the first time, we have investigated the possibility of abusing the deep model as a covert channel. The concept of a covert channel is to use a channel that is not designed for information exchange for transmitting a covert message. This work studies how a deep model can be used by an adversary as a covert channel. The proposed approach is using an end-to-end training deep model called the covert model to produce artificial data which includes some covert messages. This artificial data is the input of the deep model, which is aimed at being exploited as a covert channel, in such a way that the signal will be covered in the output of this model. To achieve indistinguishability of concealment, generative adversarial networks are used. The results show that it is possible to have a covert channel with an acceptable message transmission power in well-known deep models such as the ResNet and InceptionV3 models. Results of case studies indicate the signal-to-noise ratio (SNR) of 12.67, the bit error rate (BER) of 0.08, and the accuracy of the deep model used to hide the signal reaches 92%.","1941-0018","","10.1109/TDSC.2023.3300072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10197462","Trustworthy machine learning;deep neural network;covert channel;deep learning attack;concealment","Data models;Computational modeling;Deep learning;Receivers;Training;Artificial neural networks;Malware","","","","35","IEEE","31 Jul 2023","","","IEEE","IEEE Journals"
"Multi-Label Lifelong Machine Learning: A Scoping Review of Algorithms, Techniques, and Applications","M. Awal Kassim; H. Viktor; W. Michalowski","School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada; Telfer School of Management, University of Ottawa, Ottawa, ON, Canada",IEEE Access,"30 May 2024","2024","12","","74539","74557","Lifelong machine learning concerns the development of systems that continuously learn from diverse tasks, incorporating new knowledge without forgetting the knowledge they have previously acquired. Multi-label classification is a supervised learning process in which each instance is assigned multiple non-exclusive labels, with each label denoted as a binary value. One of the main challenges within the lifelong learning paradigm is the stability-plasticity dilemma, which entails balancing a model’s adaptability in terms of incorporating new knowledge with its stability in terms of retaining previously acquired knowledge. When faced with multi-label data, the lifelong learning challenge becomes even more pronounced, as it becomes essential to preserve relations between multiple labels across sequential tasks. This scoping review explores the intersection of lifelong learning and multi-label classification, an emerging domain that integrates continual adaptation with intricate multi-label datasets. By analyzing the existing literature, we establish connections, identify gaps in the existing research, and propose new directions for research to improve the efficacy of multi-label lifelong learning algorithms. Our review unearths a growing number of algorithms and underscores the need for specialized evaluation metrics and methodologies for the accurate assessment of their performance. We also highlight the need for strategies that incorporate real-world data from varying contexts into the learning process to fully capture the nuances of real-world environments.","2169-3536","","10.1109/ACCESS.2024.3403569","Natural Sciences and Engineering Research Council of Canada(grant numbers:GR000540); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10535494","Continual learning;lifelong learning;machine learning;multi-label classification","Classification algorithms;Reviews;Machine learning;Training;Object recognition;Continuing education;Algorithm design and analysis","","","","80","CCBYNCND","21 May 2024","","","IEEE","IEEE Journals"
"To Write or Not to Write as a Machine? That's the Question","R. Sepúlveda-Torres; I. Martínez-Murillo; E. Saquete; E. Lloret; M. Palomar",NA; NA; NA; NA; NA,IEEE Transactions on Big Data,"","2025","PP","99","1","12","Considering the potential of tools such as ChatGPT or Gemini to generate texts in a similar way to a human would do, having reliable detectors of AI –AI-generated content (AIGC)– is vital to combat the misuse and the surrounding negative consequences of those tools. Most research on AIGC detection has focused on the English language, often overlooking other languages that also have tools capable of generating human-like texts, such is the case of the Spanish language. This paper proposes a novel multilingual and multi-task approach for detecting machine vs. human-generated text. The first task classifies whether a text is written by a machine or by a human, which is the research objective of this paper. The second task consists in detect the language of the text. To evaluate the results of our approach, this study has framed the scope of the AuTexTification shared task and also we have collected a different dataset in Spanish. The experiments carried out in Spanish and English show that our approach is very competitive concerning the state of the art, as well as it can generalize better, thus being able to detect an AI-generated text in multiple domains.","2332-7790","","10.1109/TBDATA.2025.3536938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10858399","Multi-task Learning;Multilingual;Natural Language Processing;Large Language Models;AI-Generated Content","Multitasking;Multilingual;Text detection;Detectors;Art;Transformers;Reliability;Generative AI;Feature extraction;Chatbots","","","","","CCBY","30 Jan 2025","","","IEEE","IEEE Early Access Articles"
"Membership Inference Attacks With Token-Level Deduplication on Korean Language Models","M. G. Oh; L. Hyun Park; J. Kim; J. Park; T. Kwon","Graduate School of Information, Yonsei University, Seoul, South Korea; Graduate School of Information, Yonsei University, Seoul, South Korea; Graduate School of Information, Yonsei University, Seoul, South Korea; Graduate School of Information, Yonsei University, Seoul, South Korea; Graduate School of Information, Yonsei University, Seoul, South Korea",IEEE Access,"3 Feb 2023","2023","11","","10207","10217","The confidentiality threat against training data has become a significant security problem in neural language models. Recent studies have shown that memorized training data can be extracted by injecting well-chosen prompts into generative language models. While these attacks have achieved remarkable success in the English-based Transformer architecture, it is unclear whether they are still effective in other language domains. This paper studies the effectiveness of attacks against Korean models and the potential for attack improvements that might be beneficial for future defense studies. The contribution of this study is two-fold. First, we perform a membership inference attack against the state-of-the-art Korean GPT model. We found approximate training data with 20% to 90% precision in the top-100 samples and confirmed that the proposed attack technique for naive GPT is valid across the language domains. Second, in this process, we observed that the redundancy of the selected sentences could hardly be detected with the existing attack method. Since the information appearing in a few documents is more likely to be meaningful, it is desirable to increase the uniqueness of the sentences to improve the effectiveness of the attack. Thus, we propose a deduplication strategy to replace the traditional word-level similarity metric with the BPE token level. Our proposed strategy reduces 6% to 22% of the underestimated samples among selected ones, thereby improving precision by up to 7%p. As a result, we show that considering both language- and model-specific characteristics is essential to improve the effectiveness of attack strategies. We also discuss possible mitigations against the MI attacks on the general language models.","2169-3536","","10.1109/ACCESS.2023.3239668","National Research Foundation of Korea (NRF); Korea government (MSIT)(grant numbers:NRF-2019R1A2C1088802); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10025743","Confidentiality;deep learning;generative language model;Korean-based GPT;membership inference;training data extraction attack","Training data;Deep learning;Measurement;Natural language processing;Data models;Entropy;Data mining","","5","","73","CCBY","25 Jan 2023","","","IEEE","IEEE Journals"
"MasterRTL: A Pre-Synthesis PPA Estimation Framework for Any RTL Design","W. Fang; Y. Lu; S. Liu; Q. Zhang; C. Xu; L. W. Wills; H. Zhang; Z. Xie",Hong Kong University of Science and Technology (Guangzhou); 2Hong Kong University of Science and Technology; 2Hong Kong University of Science and Technology; 2Hong Kong University of Science and Technology; Duke University; Duke University; Hong Kong University of Science and Technology (Guangzhou); 2Hong Kong University of Science and Technology,2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD),"30 Nov 2023","2023","","","1","9","In modern VLSI design flow, the register-transfer level (RTL) stage is a critical point, where designers define precise design behavior with hardware description languages (HDLs) like Verilog. Since the RTL design is in the format of HDL code, the standard way to evaluate its quality requires time-consuming subsequent synthesis steps with EDA tools. This time-consuming process significantly impedes design optimization at the early RTL stage. Despite the emergence of some recent ML-based solutions, they fail to maintain high accuracy for any given RTL design. In this work, we propose an innovative pre-synthesis PPA estimation framework named MasterRTL. It first converts the HDL code to a new bit-level design representation named the simple operator graph (SOG). By only adopting single-bit simple operators, this SOG proves to be a general representation that unifies different design types and styles. The SOG is also more similar to the target gate-level netlist, reducing the gap between RTL representation and netlist. In addition to the new SOG representation, MasterRTL proposes new ML methods for the RTL-stage modeling of timing, power, and area separately. Compared with state-of-the-art solutions, the experiment on a comprehensive dataset with 90 different designs shows accuracy improvement by 0.33, 0.22, and 0.15 in correlation for total negative slack (TNS), worst negative slack (WNS), and power, respectively.","1558-2434","979-8-3503-2225-5","10.1109/ICCAD57390.2023.10323951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10323951","","Codes;Design automation;Estimation;Very large scale integration;Logic gates;Hardware;Timing","","2","","41","IEEE","30 Nov 2023","","","IEEE","IEEE Conferences"
"A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges","M. A. K. Raiaan; M. S. H. Mukta; K. Fatema; N. M. Fahad; S. Sakib; M. M. J. Mim; J. Ahmad; M. E. Ali; S. Azam","Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; LUT School of Engineering Sciences, Lappeenranta-Lahti University of Technology, Lappeenranta, Finland; Faculty of Science and Technology, Charles Darwin University, Casuarina, NT, Australia; Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Department of CSE, Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh; Faculty of Science and Technology, Charles Darwin University, Casuarina, NT, Australia",IEEE Access,"23 Feb 2024","2024","12","","26839","26874","Large Language Models (LLMs) recently demonstrated extraordinary capability in various natural language processing (NLP) tasks including language translation, text generation, question answering, etc. Moreover, LLMs are new and essential part of computerized language processing, having the ability to understand complex verbal patterns and generate coherent and appropriate replies in a given context. Though this success of LLMs has prompted a substantial increase in research contributions, rapid growth has made it difficult to understand the overall impact of these improvements. Since a plethora of research on LLMs have been appeared within a short time, it is quite impossible to track all of these and get an overview of the current state of research in this area. Consequently, the research community would benefit from a short but thorough review of the recent changes in this area. This article thoroughly overviews LLMs, including their history, architectures, transformers, resources, training methods, applications, impacts, challenges, etc. This paper begins by discussing the fundamental concepts of LLMs with its traditional pipeline of the LLMs training phase. Then the paper provides an overview of the existing works, the history of LLMs, their evolution over time, the architecture of transformers in LLMs, the different resources of LLMs, and the different training methods that have been used to train them. The paper also demonstrates the datasets utilized in the studies. After that, the paper discusses the wide range of applications of LLMs, including biomedical and healthcare, education, social, business, and agriculture. The study also illustrates how LLMs create an impact on society and shape the future of AI and how they can be used to solve real-world problems. Finally, the paper also explores open issues and challenges to deploy LLMs in real-world scenario. Our review paper aims to help practitioners, researchers, and experts thoroughly understand the evolution of LLMs, pre-trained architectures, applications, challenges, and future goals.","2169-3536","","10.1109/ACCESS.2024.3365742","Institute of Advance Research, United International University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10433480","Large language models (LLM);natural language processing (NLP);artificial intelligence;transformer;pre-trained models;taxonomy;application","Cognition;Artificial intelligence;Transformers;Training;Taxonomy;Task analysis;Surveys;Natural language processing;Question answering (information retrieval);Information analysis;Linguistics","","89","","187","CCBYNCND","13 Feb 2024","","","IEEE","IEEE Journals"
"The Recent Large Language Models in NLP","N. T. K. Le; N. Hadiprodjo; H. El-Alfy; A. Kerimzhanov; A. Teshebaev","S P Jain School Of Global Management, Sydney, Australia; S P Jain School Of Global Management, Sydney, Australia; S P Jain School Of Global Management, Sydney, Australia; S P Jain School Of Global Management, Sydney, Australia; S P Jain School Of Global Management, Sydney, Australia",2023 22nd International Symposium on Communications and Information Technologies (ISCIT),"3 Jan 2024","2023","","","1","6","Over the past few years, Natural Language Processing (NLP) has evolved significantly thanks to the development of large Language Models (LMs). In this paper, we present a survey of four recent language models that we believe have had a significant importance in the NLP field lately: BERT (Google), ELMo (Allen Institute), GPT-3 (OpenAI), and LLaMA (Meta AI). For each model, we analyse its architecture, the dataset on which it was trained, its performance evaluation, as well as the strengths and challenges faced by each. Our paper compares the recent Language Models and their contributions to the field of NLP, and discusses future extensions.","2643-6175","978-1-6654-5731-6","10.1109/ISCIT57293.2023.10376050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10376050","NLP;LLM(s);pre-trained;data;model;architecture;performance;state-of-art;BERT;ELMo;GPT-3;LLaMA;application.","Surveys;Performance evaluation;Training data;Market research;Chatbots;Internet;Information and communication technology","","3","","22","IEEE","3 Jan 2024","","","IEEE","IEEE Conferences"
"R-HTDetector: Robust Hardware-Trojan Detection Based on Adversarial Training","K. Hasegawa; S. Hidano; K. Nozawa; S. Kiyomoto; N. Togawa","KDDI Research, Inc., Fujimino, Japan; KDDI Research, Inc., Fujimino, Japan; Waseda University, Tokyo, Japan; KDDI Research, Inc., Fujimino, Japan; Waseda University, Tokyo, Japan",IEEE Transactions on Computers,"18 Jan 2023","2023","72","2","333","345","Hardware Trojans (HTs) have become a serious problem, and extermination of them is strongly required for enhancing the security and safety of integrated circuits. An effective solution is to identify HTs at the gate level via machine learning techniques. However, machine learning has specific vulnerabilities, such as adversarial examples. In reality, it has been reported that adversarial modified HTs greatly degrade the performance of a machine learning-based HT detection method. Therefore, we propose a robust HT detection method using adversarial training (R-HTDetector). We formally describe the robustness of R-HTDetector in modifying HTs. Our work gives the world-first adversarial training for HT detection with theoretical backgrounds. We show through experiments with Trust-HUB benchmarks that R-HTDetector overcomes adversarial examples while maintaining its original accuracy.","1557-9956","","10.1109/TC.2022.3222090","National Institute of Information and Communications Technology(grant numbers:05201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950314","Adversarial examples;adversarial training;hardware Trojans;machine learning;gate-level netlists","Logic gates;Integrated circuits;Feature extraction;Trojan horses;Hardware;Training;Machine learning","","15","","33","CCBYNCND","14 Nov 2022","","","IEEE","IEEE Journals"
"A Systematic Review on Multimodal Emotion Recognition: Building Blocks, Current State, Applications, and Challenges","S. Kalateh; L. A. Estrada-Jimenez; S. Nikghadam-Hojjati; J. Barata","Centre of Technology and Systems (CTS-UNINOVA), Caparica, Portugal; Centre of Technology and Systems (CTS-UNINOVA), Caparica, Portugal; Centre of Technology and Systems (CTS-UNINOVA), Caparica, Portugal; Centre of Technology and Systems (CTS-UNINOVA), Caparica, Portugal",IEEE Access,"5 Aug 2024","2024","12","","103976","104019","Emotion recognition involves accurately interpreting human emotions from various sources and modalities, including questionnaires, verbal, and physiological signals. With its broad applications in affective computing, computational creativity, human-robot interactions, and market research, the field has seen a surge in interest in recent years. This paper presents a systematic review of multimodal emotion recognition (MER) techniques developed from 2014 to 2024, encompassing verbal, physiological signals, facial, body gesture, and speech as well as emerging methods like sketches emotion recognition. The review explores various emotion models, distinguishing between emotions, feelings, sentiments, and moods, along with human emotional expression, categorized in both artistic and non-verbal ways. It also discusses the background of automated emotion recognition systems and introduces seven criteria for evaluating modalities alongside a current state analysis of MER, drawn from the human-centric perspective of this field. By selecting the PRISMA guidelines and carefully analyzing 45 selected articles, this review provides comprehensive perspectives into existing studies, datasets, technical approaches, identified gaps, and future directions in MER. It also highlights existing challenges and current applications of the MER.","2169-3536","","10.1109/ACCESS.2024.3430850","Fundação para a Ciência e Tecnologia(grant numbers:UIDB/00066/2020); Center of Technology and Systems (CTS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10602503","Multimodal emotion recognition;artificial intelligence;affective computing;emotion recognition;deep learning;machine learning;emotion expression","Emotion recognition;Physiology;Mood;Feature extraction;Cultural differences;Guidelines;Multimodal sensors;Artificial intelligence;Affective computing;Deep learning;Machine learning","","2","","232","CCBYNCND","18 Jul 2024","","","IEEE","IEEE Journals"
"What Makes a High-Quality Training Dataset for Large Language Models: A Practitioners’ Perspective","X. Yu; Z. Zhang; F. Niu; X. Hu; X. Xia; J. Grundy","Huawei, Hangzhou, China; School of Computer Science and Artificial Intelligence, Wuhan University of Technology, Wuhan, China; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada; The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; Huawei, Hangzhou, China; Faculty of Information Technology, Monash University, Victoria, Australia",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","656","668","Large Language Models (LLMs) have demonstrated remarkable performance in various application domains, largely due to their self-supervised pre-training on extensive high-quality text datasets. However, despite the importance of constructing such datasets, many leading LLMs lack documentation of their dataset construction and training procedures, leaving LLM practitioners with a limited understanding of what makes a high-quality training dataset for LLMs. To fill this gap, we initially identified 18 characteristics of high-quality LLM training datasets, as well as 10 potential data pre-processing methods and 6 data quality assessment methods, through detailed interviews with 13 experienced LLM professionals. We then surveyed 219 LLM practitioners from 23 countries across 5 continents. We asked our survey respondents to rate the importance of these characteristics, provide a rationale for their ratings, specify the key data pre-processing and data quality assessment methods they used, and highlight the challenges encountered during these processes. From our analysis, we identified 13 crucial characteristics of high-quality LLM datasets that receive a high rating, accompanied by key rationale provided by respondents. We also identified some widely-used data pre-processing and data quality assessment methods, along with 7 challenges encountered during these processes. Based on our findings, we discuss the implications for researchers and practitioners aiming to construct high-quality training datasets for optimizing LLMs.CCS CONCEPTS• Software and its engineering → Software implementation planning.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764812","Large Language Models;High-Quality Data;Practitioners’ Perspective;Empirical Study","Training;Surveys;Data integrity;Large language models;Documentation;Software;Planning;Continents;Interviews;Software engineering","","","","85","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Ethical Challenges and Frameworks in AI-Driven Software Development and Testing","A. Donvir; G. Sharma","Application Development, Wayne, NJ, USA; AI Test Automation Solution Architect, Atlanta, GA, USA",2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC),"5 Mar 2025","2025","","","00569","00576","Artificial Intelligence (AI) has revolutionized and transformed the landscape of software development and testing by introducing new efficiencies and capabilities through advancements like Generative AI (GenAI) and Large Language Models (LLMs). While these technologies bring major benefits in terms of productivity, personalization, and innovation, they also raise critical ethical challenges, such as biases, lack of transparency, data privacy concerns, and potential negative societal impacts. This paper examines the ethical considerations involved in developing such advanced AI systems as well using AI systems within software development and testing. It explores existing ethical frameworks and principles provided by leading organizations, emphasizing core concepts like human-centered design, accountability, transparency, fairness, and privacy. Practical strategies for integrating ethical practices throughout the AI development lifecycle are discussed, with a strong emphasis on the need for continuous ethical evaluation. The paper explores the ethical landscape of AI in software development, addressing challenges like algorithmic bias, data security, and broader societal impacts. Real-world case studies presented in the paper demonstrate the consequences of neglecting ethical considerations. Looking forward, the paper suggests future directions, including the development of unified ethical standards, collaborative ethical auditing, regulatory advancements, and higher societal engagement.","","979-8-3315-0769-5","10.1109/CCWC62904.2025.10903892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903892","Artificial Intelligence (AI);Ethical AI;Software Development;Software Testing;Generative AI (GenAI);Large Language Models (LLMs);Ethical Frameworks;Human-Centered Design;Accountability;Transparency;Fairness and Non-Discrimination;Data Privacy;Responsible AI;Bias Detection;Explainable AI (XAI);Ethical Auditing;Regulatory Frameworks;Societal Engagement;Case Studies in AI Ethics","Ethics;Technological innovation;Data privacy;Standards organizations;Software algorithms;Collaboration;Stakeholders;Artificial intelligence;Software development management;Testing","","","","26","IEEE","5 Mar 2025","","","IEEE","IEEE Conferences"
"Deep Learning for IoT Big Data and Streaming Analytics: A Survey","M. Mohammadi; A. Al-Fuqaha; S. Sorour; M. Guizani","Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA; Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA; Department of Electrical and Computer Engineering, University of Idaho, Moscow, ID, USA; Department of Electrical and Computer Engineering, University of Idaho, Moscow, ID, USA",IEEE Communications Surveys & Tutorials,"20 Nov 2018","2018","20","4","2923","2960","In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely deep learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.","1553-877X","","10.1109/COMST.2018.2844341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8373692","Deep learning;deep neural network;Internet of Things;on-device intelligence;IoT big data;fast data analytics;cloud-based analytics","Machine learning;Big Data;Data analysis;Economics;Internet of Things;Data mining;Tutorials","","979","","229","IEEE","6 Jun 2018","","","IEEE","IEEE Journals"
"The Missing Link in Network Intrusion Detection: Taking AI/ML Research Efforts to Users","K. Dietz; M. Mühlhauser; J. Kögel; S. Schwinger; M. Sichermann; M. Seufert; D. Herrmann; T. Hoßfeld","Chair of Communication Networks, University of Würzburg, Würzburg, Germany; Privacy and Security in Information Systems Group, University of Bamberg, Bamberg, Germany; IsarNet Software Solutions GmbH, Munich, Germany; genua GmbH, Kirchheim, Germany; Chair of Communication Networks, University of Würzburg, Würzburg, Germany; Chair of Networked Embedded Systems and Communication Systems, University of Augsburg, Augsburg, Germany; Privacy and Security in Information Systems Group, University of Bamberg, Bamberg, Germany; Chair of Communication Networks, University of Würzburg, Würzburg, Germany",IEEE Access,"10 Jun 2024","2024","12","","79815","79837","Intrusion Detection Systems (IDS) tackle the challenging task of detecting network attacks as fast as possible. As this is getting more complex in modern enterprise networks, Artificial Intelligence (AI) and Machine Learning (ML) have gained substantial popularity in research. However, their adoption into real-world IDS solutions remains poor. Academic research often overlooks the interconnection of users and technical aspects. This leads to less explainable AI/ML models that hinder trust among AI/ML non-experts. Additionally, research often neglects secondary concerns such as usability and privacy. If IDS approaches conflict with current regulations or if administrators cannot deal with attacks more effectively, enterprises will not adopt the IDS in practice. To identify those problems systematically, our literature survey takes a user-centric approach; we examine IDS research from the perspective of stakeholders by applying the concept of personas. Further, we investigate multiple factors limiting the adoption of AI/ML in security and suggest technical, non-technical, and user-related considerations to enhance the adoption in practice. Our key contributions are threefold. (i) We derive personas from realistic enterprise scenarios, (ii) we provide a set of relevant hypotheses in the form of a review template, and (iii), based on our reviews, we derive design guidelines for practical implementations. To the best of our knowledge, this is the first paper that analyzes practical adoption barriers of AI/ML-based intrusion detection solutions concerning appropriateness of data, reproducibility, explainability, practicability, usability, and privacy. Our guidelines may help researchers to holistically evaluate their AI/ML-based IDS approaches to increase practical adoption.","2169-3536","","10.1109/ACCESS.2024.3406939","German Federal Ministry of Education and Research (BMBF); WINTERMUTE(grant numbers:16KIS1128,16KIS1129,16KIS1131,16KIS1132); BMBF(grant numbers:16KISA079K); Deutsche Forschungsgemeinschaft (DFG)(grant numbers:SE 3163/3-1,500105691); Bavarian Ministry of Economics, Regional Development and Energy (StMWI); VIPNANO(grant numbers:DIK-2307-0005,DIK-2307-0006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10540566","Anomaly detection;artificial intelligence;intrusion detection;machine learning;network monitoring;privacy;security;usability","Surveys;Security;Monitoring;Network intrusion detection;Usability;Privacy;Anomaly detection;Artificial intelligence;Intrusion detection;Machine learning","","3","","221","CCBYNCND","29 May 2024","","","IEEE","IEEE Journals"
"AI-Assisted Programming for Web and Machine Learning: Improve your development workflow with ChatGPT and GitHub Copilot","C. Noring; A. Jain; M. Fernandez; A. Mutlu; A. Jaokar",NA; NA; NA; NA; NA,AI-Assisted Programming for Web and Machine Learning: Improve your development workflow with ChatGPT and GitHub Copilot,"","2024","","","","","Speed up your development processes and improve your productivity by writing practical and relevant prompts to build web applications and Machine Learning (ML) models Purchase of the print or Kindle book includes a free PDF copyKey FeaturesUtilize prompts to enhance frontend and backend web developmentDevelop prompt strategies to build robust machine learning modelsUse GitHub Copilot for data exploration, maintaining existing code bases, and augmenting ML models into web applicationsBook DescriptionAI-Assisted Programming for Web and Machine Learning shows you how to build applications and machine learning models and automate repetitive tasks. Part 1 focuses on coding, from building a user interface to the backend. You’ll use prompts to create the appearance of an app using HTML, styling with CSS, adding behavior with JavaScript, and working with multiple viewports. Next, you’ll build a web API with Python and Flask and refactor the code to improve code readability. Part 1 ends with using GitHub Copilot to improve the maintainability and performance of existing code. Part 2 provides a prompting toolkit for data science from data checking (inspecting data and creating distribution graphs and correlation matrices) to building and optimizing a neural network. You’ll use different prompt strategies for data preprocessing, feature engineering, model selection, training, hyperparameter optimization, and model evaluation for various machine learning models and use cases. The book closes with chapters on advanced techniques on GitHub Copilot and software agents. There are tips on code generation, debugging, and troubleshooting code. You’ll see how simpler and AI-powered agents work and discover tool calling.What you will learnSpeed up your coding and machine learning workflows with GitHub Copilot and ChatGPTUse an AI-assisted approach across the development lifecycle Implement prompt engineering techniques in the data science lifecycleDevelop the frontend and backend of a web application with AI assistance Build machine learning models with GitHub Copilot and ChatGPT Refactor code and fix faults for better efficiency and readability Improve your codebase with rich documentation and enhanced workflows Who this book is forExperienced developers new to GitHub Copilot and ChatGPT can discover the best strategies to improve productivity and deliver projects quicker than traditional methods. This book is ideal for software engineers working on web or machine learning projects. It is also a useful resource for web developers, data scientists, and analysts who want to improve their efficiency with the help of prompting. This book does not teach web development or how different machine learning models work.","","9781835083895","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769375.pdf&bkn=10769374&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Key Considerations to be Applied While Leveraging Machine Learning for Financial Statement Fraud Detection: A Review","D. Lin","School of Public Finance and Taxation, Central University of Finance and Economics, Beijing, China",IEEE Access,"18 Nov 2024","2024","12","","168213","168228","Financial statement fraud (FSF) is a challenging issue in capital markets and severely affects their overall health and stability. The effective prediction of FSF has become an urgent need for investors. In recent years, scholars have developed several machine learning-based FSF prediction models. This study conducted a systematic review of such models to facilitate an understanding of the latest developments in this field. First, sample and data preprocessing were analyzed, focusing on key aspects such as data sources, splitting training and testing sets, imbalanced samples, cross-period FSF, and handling of zero and missing values. Second, existing research on FSF prediction models was reviewed considering structured and unstructured data. The existing studies exhibited two significant characteristics: expansion if data from structured to unstructured formats and the evolution of methodologies from traditional machine learning to deep learning approaches. Third, the effectiveness of FSF prediction models was evaluated. Indiscriminately pursuing higher recall rates is not advisable. Rather, the effectiveness of the model in terms of predicting FSF must be scientific assessed. Finally, the challenges and opportunities in current research were summarized. The core challenges were identified as the development of robust prediction models and the incorporation of unstructured data into these models. Moreover, leveraging diverse data, deep learning, and large language models can significantly enhance the performance of prediction models. Furthermore, to advance research in this field, this study advocates the construction of a shared and open FSF database.","2169-3536","","10.1109/ACCESS.2024.3488832","Youth Program of the National Natural Science Foundation of China(grant numbers:72002234); Ministry of Education Humanities and Social Sciences Research Project(grant numbers:19YJC790072); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740285","Financial statement fraud;machine learning;deep learning;literature review","Fraud;Predictive models;Data models;Biological system modeling;Training;Companies;Deep learning;Databases;Testing;Data preprocessing;Financial management;Deep learning","","","","73","CCBYNCND","31 Oct 2024","","","IEEE","IEEE Journals"
"Empowering Caregivers of Alzheimer's Disease and Related Dementias (ADRD) with a GPT-Powered Voice Assistant: Leveraging Peer Insights from Social Media","K. T. Zaman; W. U. Hasan; J. Li; C. Tao","Department of Computer Science, North Dakota State University, Fargo, USA; Department of Computer Science, North Dakota State University, Fargo, USA; Department of Computer Science, North Dakota State University, Fargo, USA; School of Biomedical Informatics, The University of Texas Health Science Center, Houston, USA",2023 IEEE Symposium on Computers and Communications (ISCC),"28 Aug 2023","2023","","","1","7","Caring for individuals with Alzheimer's Disease and Related Dementias (ADRD) is a complex and challenging task, especially for unprofessional caregivers who often lack the necessary training and resources. While online peer support groups have been shown to be useful in providing caregivers with information and emotional support, many caregivers are unable to benefit from them due to time constraints and limited knowledge of social media platforms. To address this issue, we propose the development of a voice assistant app that can collect relevant information and discussions from online peer support groups on social media. This app will use the collected information as a knowledge base and fine-tune a Generative Pre-trained Transformers (GPT) model to facilitate caregivers in accessing shared experiences and practical tips from peers. Initial evaluation of the app has shown promising results in terms of feasibility and potential impact on caregivers.","2642-7389","979-8-3503-0048-2","10.1109/ISCC58397.2023.10218142","National Science Foundation(grant numbers:2218046); Cancer Prevention and Research Institute of Texas(grant numbers:RP220244); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10218142","Alzheimer's Disease and Related Dementias (ADRD);voice assistant;caregiving;Generative Pre-trained Transformers (GPT);natural language processing","Training;Computers;Social networking (online);Knowledge based systems;Transformers;User experience;Question answering (information retrieval)","","2","","23","IEEE","28 Aug 2023","","","IEEE","IEEE Conferences"
"Chinese Ancient Painting Figure Face Restoration and its Application in a Q&A Interaction System","R. Li; Y. Wei; H. Lu; S. Ma; Z. Liu; H. Liu; Q. Wang; Y. Wu; J. Tan","Peking University; Peking University; Shanghai Jiaotong University; Peking University; Zhejiang University; Zhejiang University; Lenovo (Beijing) Co., Ltd.,; Lenovo (Beijing) Co., Ltd.,; Zhejiang University",2024 IEEE International Conference on Multimedia and Expo Workshops (ICMEW),"29 Aug 2024","2024","","","1","6","We design a complete technical chain for developing a Q&A interaction system between Chinese ancient figures and modern users. The system is built on end-cloud collaboration, and all the users need to do is uploading a Chinese ancient painting figure face image. The interactive ability is realized by a large language model. Chinese ancient paintings often emphasize vivid expression and lack realism. Therefore, to enhance the time-travel experience, we restore the Chinese ancient painting figure faces as modern faces with realistic style, which is also the most critical and challenging part in the entire technical chain. We solve this problem by the StyleGAN2 generator in the encoder4editing (e4e) algorithm. Our system is very expected to be deployed in application scenarios such as museums.","2995-1429","979-8-3503-7981-5","10.1109/ICMEW63481.2024.10645456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10645456","Talking face animation;ancient painting;StyleGAN;face restoration;large language model","Humanities;Large language models;Multimedia systems;Conferences;Collaboration;Museums;Generators","","","","21","IEEE","29 Aug 2024","","","IEEE","IEEE Conferences"
"An Introduction to Neural Data Compression","Y. Yang; S. Mandt; L. Theis",NA; NA; NA,An Introduction to Neural Data Compression,"","2023","","","","","The goal of data compression is to reduce the number of bits needed to represent useful information. Neural, or learned compression, is the application of neural networks and related machine learning techniques to this task. This monograph aims to serve as an entry point for machine learning researchers interested in compression by reviewing the prerequisite background and representative methods in neural compression. Neural compression is the application of neural networks and other machine learning methods to data compression. Recent advances in statistical machine learning have opened up new possibilities for data compression, allowing compression algorithms to be learned end-to-end from data using powerful generative models such as normalizing flows, variational autoencoders, diffusion probabilistic models, and generative adversarial networks. This monograph introduces this field of research to a broader machine learning audience by reviewing the necessary background in information theory (e.g., entropy coding, rate-distortion theory) and computer vision (e.g., image quality assessment, perceptual metrics), and providing a curated guide through the essential ideas and methods in the literature thus far. Instead of surveying the vast literature, essential concepts and methods in neural compression are covered, with a reader in mind who is versed in machine learning but not necessarily data compression.","","9781638281757","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10109624.pdf&bkn=10109623&pdfType=book","","","","","","","","27 Apr 2023","","","now","Now Foundations and Trends Books"
"AI-Powered IoT: A Survey on Integrating Artificial Intelligence With IoT for Enhanced Security, Efficiency, and Smart Applications","I. M. U; V. Babu Kumaravelu; V. K. C; R. A; S. Chinnadurai; R. Venkatesan; H. Hai; P. Selvaprabhu","Department of Engineering “Enzo Ferrari,”, University of Modena and Reggio Emilia, Modena, Italy; Department of Communication Engineering, School of Electronics Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu, India; Department of Communication Engineering, School of Electronics Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu, India; Automotive Research Centre, Vellore Institute of Technology, Vellore, Tamil Nadu, India; Department of Electronics and Communication Engineering, School of Engineering and Science, SRM University AP, Guntur, Andhra Pradesh, India; Department of Communication Engineering, School of Electronics Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu, India; College of Information Science and Technology, Donghua University, Shanghai, China; Department of Communication Engineering, School of Electronics Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu, India",IEEE Access,"25 Mar 2025","2025","13","","50296","50339","The Internet of Things (IoT) and artificial intelligence (AI) enabled IoT is a significant paradigm that has been proliferating to new heights in recent years. IoT is a smart technology in which the physical objects or the things that are ubiquitously around us are networked and linked to the internet to deliver new services and enhance efficiency. The primary objective of the IoT is to connect all the physical objects or the things of the world under a common infrastructure, allowing humans to control them and get timely, frequent updates on their status. These things or devices connected to IoT generate, gather and process a massive volume of binary data. This massive volume of data generated from these devices is analyzed and learned by AI algorithms and techniques that aid in providing users with better services. Thus, AI-enabled IoT or artificial IoT (AIoT) is a hybrid technology that merges AI with IoT and is capable of simplifying complicated and strenuous tasks with ease and efficiency. The various machine learning (ML) and deep learning (DL) algorithms in IoT are necessary to ensure the IoT network’s improved security and confidentiality. Furthermore, this paper also surveys the various architectures that form the backbone of IoT and AIoT. Moreover, the myriad state-of-the-art ML and DL-based approaches for securing IoT, including detecting anomalies/intrusions, authentication and access control, attack detection and mitigation, preventing distributed denial of service (DDoS) attacks, and analyzing malware in IoT, are also enlightened. In addition, this work also reviews the role of AIoT in optimizing network efficiency, securing IoT infrastructures, and addressing key challenges. Furthermore, it explores cutting-edge technologies like blockchain, 6G-enabled AIoT, federated learning (FL), and hyperdimensional (HD) computing, indicating their potential in advancing IoT and AIoT-driven applications within sectors like healthcare, autonomous systems, and industrial automation. Therefore, based on the plethora of prevailing significant works, the objective of this manuscript is to provide a comprehensive survey that expounds on AIoT in terms of security, architecture, applications, emerging technologies, and challenges.","2169-3536","","10.1109/ACCESS.2025.3551750","School of Electronics Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu, India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10929047","Internet of Things;artificial intelligence;blockchain;machine learning;IoT security;6G;AIoT","Internet of Things;Artificial intelligence;Security;Surveys;Wireless sensor networks;Scalability;Explainable AI;Computer architecture;Wireless fidelity;Radiofrequency identification","","","","314","CCBY","17 Mar 2025","","","IEEE","IEEE Journals"
"Machine Learning Techniques for Text: Apply modern techniques with Python for text processing, dimensionality reduction, classification, and evaluation","N. Tsourakis",NA,"Machine Learning Techniques for Text: Apply modern techniques with Python for text processing, dimensionality reduction, classification, and evaluation","","2022","","","","","Take your Python text processing skills to another level by learning about the latest natural language processing and machine learning techniques with this full color guideKey FeaturesLearn how to acquire and process textual data and visualize the key findingsObtain deeper insight into the most commonly used algorithms and techniques and understand their tradeoffsImplement models for solving real-world problems and evaluate their performanceBook DescriptionWith the ever-increasing demand for machine learning and programming professionals, it's prime time to invest in the field. This book will help you in this endeavor, focusing specifically on text data and human language by steering a middle path among the various textbooks that present complicated theoretical concepts or focus disproportionately on Python code. A good metaphor this work builds upon is the relationship between an experienced craftsperson and their trainee. Based on the current problem, the former picks a tool from the toolbox, explains its utility, and puts it into action. This approach will help you to identify at least one practical use for each method or technique presented. The content unfolds in ten chapters, each discussing one specific case study. For this reason, the book is solution-oriented. It's accompanied by Python code in the form of Jupyter notebooks to help you obtain hands-on experience. A recurring pattern in the chapters of this book is helping you get some intuition on the data and then implement and contrast various solutions. By the end of this book, you'll be able to understand and apply various techniques with Python for text preprocessing, text representation, dimensionality reduction, machine learning, language modeling, visualization, and evaluation.What you will learnUnderstand fundamental concepts of machine learning for textDiscover how text data can be represented and build language modelsPerform exploratory data analysis on text corporaUse text preprocessing techniques and understand their trade-offsApply dimensionality reduction for visualization and classificationIncorporate and fine-tune algorithms and models for machine learningEvaluate the performance of the implemented systemsKnow the tools for retrieving text data and visualizing the machine learning workflowWho this book is forThis book is for professionals in the area of computer science, programming, data science, informatics, business analytics, statistics, language technology, and more who aim for a gentle career shift in machine learning for text. Students in relevant disciplines that seek a textbook in the field will benefit from the practical aspects of the content and how the theory is presented. Finally, professors teaching a similar course will be able to pick pertinent topics in terms of content and difficulty. Beginner-level knowledge of Python programming is needed to get started with this book.","","9781803236292","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162969.pdf&bkn=10162968&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"IEEE Recommended Practice for Privacy and Security for Federated Machine Learning","",,IEEE Std 2986-2023,"23 Apr 2024","2024","","","1","57","Privacy and security issues pose great challenges to the federated machine leaning (FML) community. A general view on privacy and security risks while meeting applicable privacy and security requirements in FML is provided. This recommended practice is provided in four parts: malicious failure and non-malicious failure in FML, privacy and security requirements from the perspective of system and FML participants, defensive methods and fault recovery methods, and the privacy and security risks evaluation. It also provides some guidance for typical FML scenarios in different industry areas, which can facilitate practitioners to use FML in a better way.","","979-8-8557-0705-2","10.1109/IEEESTD.2024.10507779","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10507779","federated machine learning;FML;IEEE 2986™;machine learning;privacy;security","IEEE Standards;Federated learning;Machine learning;Privacy;Security","","","","49","","23 Apr 2024","","","IEEE","IEEE Standards"
"Social Computing Unhinged","J. Evans","Santa Fe Institute, Santa Fe, NM, USA",Journal of Social Computing,"28 Oct 2020","2020","1","1","1","13","Social computing is ubiquitous and intensifying in the 21st Century. Originally used to reference computational augmentation of social interaction through collaborative filtering, social media, wikis, and crowdsourcing, here I propose to expand the concept to cover the complete dynamic interface between social interaction and computation, including computationally enhanced sociality and social science, socially enhanced computing and computer science, and their increasingly complex combination for mutual enhancement. This recommends that we reimagine Computational Social Science as Social Computing, not merely using computational tools to make sense of the contemporary explosion of social data, but also recognizing societies as emergent computers of more or less collective intelligence, innovation and flourishing. It further proposes we imagine a socially inspired computer science that takes these insights into account as we build machines not merely to substitute for human cognition, but radically complement it. This leads to a vision of social computing as an extreme form of human computer interaction, whereby machines and persons recursively combine to augment one another in generating collective intelligence, enhanced knowledge, and other social goods unattainable without each other. Using the example of science and technology, I illustrate how progress in each of these areas unleash advances in the others and the beneficial relationship between the technology and science of social computing, which reveals limits of sociality and computation, and stimulates our imagination about how they can reach past those limits together.","2688-5255","","10.23919/JSC.2020.0002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9241509","social computing;complex systems;computer supported cooperative work;computational social science;artificial intelligence;human computer interaction;human-centered computing","Social computing;Cognition;Collaboration;Social networking (online);Artificial intelligence;Encyclopedias","","22","","103","","28 Oct 2020","","","TUP","TUP Journals"
"A State-of-the-Art Review on Phishing Website Detection Techniques","W. Li; S. Manickam; Y. -W. Chong; W. Leng; P. Nanda","Cybersecurity Research Centre, Universiti Sains Malaysia, George Town, Penang, Malaysia; Cybersecurity Research Centre, Universiti Sains Malaysia, George Town, Penang, Malaysia; School of Computer Sciences, Universiti Sains Malaysia, George Town, Penang, Malaysia; Research Institute of Drilling and Production Engineering Technology, Chuanqing Drilling Engineering Company Ltd., CNPC, Guanghan, China; Faculty of Engineering and IT, University of Technology Sydney, Sydney, NSW, Australia",IEEE Access,"17 Dec 2024","2024","12","","187976","188012","Phishing attacks remain a significant cybersecurity threat, with phishing websites serving as a primary tool for attackers to deceive users and steal sensitive information. The rapid evolution of phishing tactics has spurred the development of increasingly sophisticated detection mechanisms. This paper provides a comprehensive review of state-of-the-art techniques for phishing website detection, highlighting recent advancements in the field. In particular, it addresses emerging methods for detection, such as graph-based, large language model (LLM)-based approaches and phishing kit-based detection methods, which have not been extensively covered in previous surveys. By critically reviewing recent works from reliable databases, this study constructs a new taxonomy for phishing detection techniques. This review offers a comparison of these techniques, highlighting their strengths and limitations, and explores the challenges of real-world applications of these detection systems. Furthermore, the role of artificial intelligence (AI) in phishing website detection is discussed, and future research directions to improve detection capabilities are suggested. This work addresses emerging and uncovered phishing website detection methods in previous review papers and provides valuable insights for both researchers and practitioners working to develop more robust phishing website detection systems.","2169-3536","","10.1109/ACCESS.2024.3514972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10788671","Cybersecurity;deep learning;machine learning;phishing website detection","Phishing;Reviews;Surveys;Feature extraction;Visualization;Uniform resource locators;Convolutional neural networks;Blocklists;Analytical models;Organizations","","","","0","CCBYNCND","11 Dec 2024","","","IEEE","IEEE Journals"
"Investigating the Effectiveness of Artificial Intelligence in Watermarking and Steganography for Digital Media Security","S. M. J. a. Abdalwahid; W. A. Hashim; M. G. Saeed; S. A. Altaie; S. W. Kareem","Information System Engineering Dept., Engineering College, Erbil Polytechnic University, Erbil, Iraq; Medical Instruments techniques Dept, AIQAlam University College, Kirkuk, Iraq; Department of Information Technology, Dohuk Polytechnic University, Dohuk, Iraq; Computer Engineering Department, University of Technology, Baghdad, Iraq; Information System Engineering Dept., Erbil Technical Engineering College, Erbil Polytechnic University, Erbil, Iraq","2024 21st International Multi-Conference on Systems, Signals & Devices (SSD)","12 Jun 2024","2024","","","552","561","Watermarking and Steganography are methods of embedding digital information within images or other media, such as text or audio, for the purpose of Copyright Protection or covert communication. This field of study is not recent and has been ongoing for several years, culminating in its current advanced stage. The utilization of Artificial Intelligence algorithms has played a pivotal role in revolutionizing various aspects, including security concerns and the precision of outcomes, as compared to traditional methods. This paper focuses on doing an in-depth analysis of cutting-edge research, techniques, and methodologies employed in the domain of Watermarking and Steganography, specifically in conjunction with Artificial Intelligence. By thoroughly examining and evaluating a collection of recent studies in this domain, we have scrutinized the outcomes of each study with respect to its research goal, the acquired results, the employed algorithm, and the research's robustness in terms of susceptibility to various forms of attacks and the technique of data embedding. Our findings indicate that the use of Artificial Intelligence algorithms has a substantial influence on enhancing result precision, system resilience, and establishing data ownership. Deep Neural Networks (DNN) are essential in Watermarking and Steganography due to their robustness, effectiveness, and accuracy. Novel methodologies and systems improve security, incorporation rates, precision of detection, and speed of convergence. Deep learning is being investigated in techniques such as data concealment, information hiding, and steganography to improve security.","2474-0446","979-8-3503-7413-1","10.1109/SSD61670.2024.10549272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10549272","Watermarking;Steganography;Artificial intelligence;Key Embedding","Deep learning;Steganography;Fault tolerance;Reviews;Measurement standards;Watermarking;Artificial neural networks","","","","53","IEEE","12 Jun 2024","","","IEEE","IEEE Conferences"
"Understanding Deep Learning Techniques for Recognition of Human Emotions Using Facial Expressions: A Comprehensive Survey","M. Karnati; A. Seal; D. Bhattacharjee; A. Yazidi; O. Krejcar","Pandit Dwarka Prasad Mishra Indian Institute of Information Technology, Design and Manufacturing Jabalpur, Jabalpur, India; Pandit Dwarka Prasad Mishra Indian Institute of Information Technology, Design and Manufacturing Jabalpur, Jabalpur, India; Department of Computer Science and Engineering, Jadavpur University, Kolkata, India; Research Group in Applied Artificial Intelligence, Oslo Metropolitan University, Oslo, Norway; Center for Basic and Applied Science, Faculty of Informatics and Management, University of Hradec Králové, Hradec Králové, Czech Republic",IEEE Transactions on Instrumentation and Measurement,"23 Feb 2023","2023","72","","1","31","Emotion recognition plays a significant role in cognitive psychology research. However, measuring emotions is a challenging task. Thus, several approaches have been designed for facial expression recognition (FER). Although, the challenges increase further as the data transit from the laboratory-controlled environment to in-the-wild circumstances, nowadays, applications are overwhelmed by a profusion of deep learning (DL) techniques in real-world problems. DL networks have steadily led to a better understanding of low-dimensional discriminative features from high-dimensional complex face patterns for automatic FER. The modern FER systems based on deep neural networks mainly suffer from two problems: overfitting due to the inadequate availability of training data and complications unassociated with the expressions, such as occlusion, posture, illumination, and identity bias. This study aims to provide a comprehensive survey of the significant DL-based methods that have made a notable contribution to the field of FER. Different components of the methods, such as preprocessing, feature extraction, and classification of facial expressions, are described systematically. Moreover, the discussed approaches are analyzed to compare their performance along with their advantages and limitations. Furthermore, different databases relevant to FER are also explored in this study. Essentially, the main aim of this survey is twofold. The former is to discuss the current scenario of FER approaches and the latter is to present some thoughts on the future directions of facial emotion recognition by machines: what are the obstacles and prospects for FER researchers?","1557-9662","","10.1109/TIM.2023.3243661","project “Smart Solutions in Ubiquitous Computing Environments”, Grant Agency of Excellence (under ID: UHKFIM-GE-2023), University of Hradec Kralove, Faculty of Informatics and Management, Czech Republic; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10041168","Deep learning (DL);facial expression databases;facial expression recognition (FER);FER challenges;FER future directions;overfitting;survey","Face recognition;Databases;Feature extraction;Emotion recognition;Task analysis;Lighting;Deep learning","","54","","260","IEEE","9 Feb 2023","","","IEEE","IEEE Journals"
"Explainable AI for Cyber-Physical Systems: Issues and Challenges","A. Hoenig; K. Roy; Y. T. Acquaah; S. Yi; S. S. Desai","Department of Computational Data Science and Engineering, North Carolina Agricultural and Technical State University, Greensboro, NC, USA; Department of Computer Science, North Carolina Agricultural and Technical State University, Greensboro, NC, USA; Department of Computer Science, North Carolina Agricultural and Technical State University, Greensboro, NC, USA; Department of Mechanical Engineering, North Carolina Agricultural and Technical State University, Greensboro, NC, USA; Department of Industrial and Systems Engineering, North Carolina Agricultural and Technical State University, Greensboro, NC, USA",IEEE Access,"30 May 2024","2024","12","","73113","73140","Artificial intelligence and cyber-physical systems (CPS) are two of the key technologies of the future that are enabling major global shifts. However, most of the current implementations of AI in CPS are not explainable, which creates serious problems in ethical, legal, regulatory, and other domains. Therefore, it is necessary for explainable artificial intelligence (XAI) to be integrated with cyber-physical systems to meet the vital needs for control, fairness, accountability, safety, cyber-resilience, and cybersecurity. The goal of this review is to demonstrate the need, benefits, challenges, and implementation of XAI for CPS. We review the existing literature about XAI and CPS, discuss the current state of the art, examine applications in different domains, and make recommendations for future research directions. To the best of our knowledge, this is the first peer-reviewed academic article to provide a comprehensive review of general XAI for CPS. We also contribute new research ideas including development of multisensory explanations and outputs for these systems, application of XAI to CPS to decrease occupational burnout and increase employee engagement, and enumeration of the multidisciplinary goals and benefits of XAI as applied to cyber-physical systems.","2169-3536","","10.1109/ACCESS.2024.3395444","Office of Naval Research (ONR)(grant numbers:N00014-22-1-2724); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516690","Cyber-physical systems (CPS);cyber-resilience;cybersecurity;explainable artificial intelligence (XAI);industrial CPS;Industry 5.0","Artificial intelligence;Cyber-physical systems;Explainable AI;Biological system modeling;Reviews;Safety;Computer security;Fifth Industrial Revolution","","5","","77","CCBYNCND","1 May 2024","","","IEEE","IEEE Journals"
"Deep Learning for Predictive Analytics in Reversible Steganography","C. -C. Chang; X. Wang; S. Chen; I. Echizen; V. Sanchez; C. -T. Li","National Institute of Informatics, Tokyo, Japan; Department of Information Engineering and Computer Science, Feng Chia University, Taichung, Taiwan; Department of Information Engineering and Computer Science, Feng Chia University, Taichung, Taiwan; National Institute of Informatics, Tokyo, Japan; Department of Computer Science, University of Warwick, Coventry, U.K.; School of Information Technology, Deakin University, Geelong, VIC, Australia",IEEE Access,"12 Jan 2023","2023","11","","3494","3510","Deep learning is regarded as a promising solution for reversible steganography. There is an accelerating trend of representing a reversible steo-system by monolithic neural networks, which bypass intermediate operations in traditional pipelines of reversible steganography. This end-to-end paradigm, however, suffers from imperfect reversibility. By contrast, the modular paradigm that incorporates neural networks into modules of traditional pipelines can stably guarantee reversibility with mathematical explainability. Prediction-error modulation is a well-established reversible steganography pipeline for digital images. It consists of a predictive analytics module and a reversible coding module. Given that reversibility is governed independently by the coding module, we narrow our focus to the incorporation of neural networks into the analytics module, which serves the purpose of predicting pixel intensities and a pivotal role in determining capacity and imperceptibility. The objective of this study is to evaluate the impacts of different training configurations upon predictive accuracy of neural networks and provide practical insights. In particular, we investigate how different initialisation strategies for input images may affect the learning process and how different training strategies for dual-layer prediction respond to the problem of distributional shift. Furthermore, we compare steganographic performance of various model architectures with different loss functions.","2169-3536","","10.1109/ACCESS.2023.3233976","Japan Society for the Promotion of Science (JSPS) through KAKENHI(grant numbers:JP16H06302,JP18H04120,JP20K23355,JP21H04907,JP21K18023); Japan Science and Technology Agency (JST) through CREST(grant numbers:JPMJCR18A6,JPMJCR20D3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10005178","Deep learning;modularity;predictive analytics;reversible steganography","Neural networks;Deep learning;Steganography;Training data;Distortion;Encoding;Watermarking;Predictive models","","3","","63","CCBY","3 Jan 2023","","","IEEE","IEEE Journals"
"The NeRF Signature: Codebook-Aided Watermarking for Neural Radiance Fields","Z. Luo; A. Rocha; B. Shi; Q. Guo; H. Li; R. Wan","Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China; Artificial Intelligence Lab., Recod.ai, Institute of Computing, University of Campinas, Brazil; State Key Laboratory of Multimedia Information Processing and National Engineering Research Center of Visual Technology, School of Computer Science, Peking University, Beijing, China; IHPC and CFAR, Agency for Science, Technology and Research (A*STAR), Singapore; Department of Electrical Engineering, City University of Hong Kong, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China",IEEE Transactions on Pattern Analysis and Machine Intelligence,"","2025","PP","99","1","16","Neural Radiance Fields (NeRF) have been gaining attention as a significant form of 3D content representation. With the proliferation of NeRF-based creations, the need for copyright protection has emerged as a critical issue. Although some approaches have been proposed to embed digital watermarks into NeRF, they often neglect essential model-level considerations and incur substantial time overheads, resulting in reduced imperceptibility and robustness, along with user inconvenience. In this paper, we extend the previous criteria for image watermarking to the model level and propose NeRF Signature, a novel watermarking method for NeRF. We employ a Codebook-aided Signature Embedding (CSE) that does not alter the model structure, thereby maintaining imperceptibility and enhancing robustness at the model level. Furthermore, after optimization, any desired signatures can be embedded through the CSE, and no fine-tuning is required when NeRF owners want to use new binary signatures. Then, we introduce a joint pose-patch encryption watermarking strategy to hide signatures into patches rendered from a specific viewpoint for higher robustness. In addition, we explore a Complexity-Aware Key Selection (CAKS) scheme to embed signatures in high visual complexity patches to enhance imperceptibility. The experimental results demonstrate that our method outperforms other baseline methods in terms of imperceptibility and robustness.","1939-3539","","10.1109/TPAMI.2025.3550166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10922145","3D reconstruction;digital watermarking;neural radiance fields","Neural radiance field;Watermarking;Three-dimensional displays;Training;Robustness;Pipelines;Computational modeling;Visualization;Solid modeling;Rendering (computer graphics)","","","","","IEEE","11 Mar 2025","","","IEEE","IEEE Early Access Articles"
"A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI","E. Tjoa; C. Guan","HealthTech Division, Alibaba Group Holding Ltd., Hangzhou, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore",IEEE Transactions on Neural Networks and Learning Systems,"27 Oct 2021","2021","32","11","4793","4813","Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.","2162-2388","","10.1109/TNNLS.2020.3027314","Health-AI; DAMO Academy; Alibaba Group Holding Ltd.; Alibaba-NTU Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233366","Explainable artificial intelligence (XAI);interpretability;machine learning (ML);medical information system;survey","Artificial intelligence;Machine learning;Medical information systems;Machine learning algorithms","Artificial Intelligence;Humans;Image Processing, Computer-Assisted;Machine Learning;Neural Networks, Computer;Pattern Recognition, Automated;Surveys and Questionnaires","929","","169","CCBY","20 Oct 2020","","","IEEE","IEEE Journals"
"On Interpretability of Artificial Neural Networks: A Survey","F. -L. Fan; J. Xiong; M. Li; G. Wang","Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA; AI and Hybrid Clouds Systems, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA; Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA",IEEE Transactions on Radiation and Plasma Medical Sciences,"2 Nov 2021","2021","5","6","741","760","Deep learning as performed by artificial deep neural networks (DNNs) has achieved great successes recently in many important areas that deal with text, images, videos, graphs, and so on. However, the black-box nature of DNNs has become one of the primary obstacles for their wide adoption in mission-critical applications such as medical diagnosis and therapy. Because of the huge potentials of deep learning, the interpretability of DNNs has recently attracted much research attention. In this article, we propose a simple but comprehensive taxonomy for interpretability, systematically review recent studies on interpretability of neural networks, describe applications of interpretability in medicine, and discuss future research directions, such as in relation to fuzzy logic and brain science.","2469-7303","","10.1109/TRPMS.2021.3066428","Rensselaer-IBM AI Research Collaboration Program; IBM AI Horizons Network; NIH(grant numbers:R01 EB026646,R01 CA233888,R01 CA237267,R01 HL151561); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380482","Deep learning;interpretability;neural networks;survey","Deep learning;Neural networks;Taxonomy;Data models;Training","","264","","211","IEEE","17 Mar 2021","","","IEEE","IEEE Journals"
"Subsurface Structure Analysis Using Computational Interpretation and Learning: A Visual Signal Processing Perspective","G. AlRegib; M. Deriche; Z. Long; H. Di; Z. Wang; Y. Alaudah; M. A. Shafiq; M. Alfarraj","Electrical engineering, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Queensland University of Technology, Australia; Electrical engineering, Mississippi State University; Geophysics, Ocean University of China; Information and communication engineering, Shanghai Jiao Tong University, China; Electrical engineering, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Georgia Institute of Technology, Atlanta, GA, US; Electrical engineering, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia",IEEE Signal Processing Magazine,"12 Mar 2018","2018","35","2","82","98","Understanding Earth's subsurface structures has been and continues to be an essential component of various applications such as environmental monitoring, carbon sequestration, and oil and gas exploration. By viewing the seismic volumes that are generated through the processing of recorded seismic traces, researchers were able to learn from applying advanced image processing and computer vision algorithms to effectively analyze and understand Earth's subsurface structures. In this article, we first summarize the recent advances in this direction that relied heavily on the fields of image processing and computer vision. Second, we discuss the challenges in seismic interpretation and provide insights and some directions to address such challenges using emerging machine-learning algorithms.","1558-0792","","10.1109/MSP.2017.2785979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8312469","","Earth;Image processing;Computer vision;Environmental monitoring;Carbon;Oil drilling;Gas industry","","62","","82","IEEE","12 Mar 2018","","","IEEE","IEEE Magazines"
"BSFR-SH: Blockchain-Enabled Security Framework Against Ransomware Attacks for Smart Healthcare","M. Wazid; A. Kumar Das; S. Shetty","Department of Computer Science and Engineering, Graphic Era Deemed to be University, Dehradun, India; Center for Security, Theory and Algorithmic Research, International Institute of Information Technology Hyderabad, Hyderabad, India; Department of Computational Modeling and Simulation Engineering, Virginia Modeling, Analysis and Simulation Center, Old Dominion University, Suffolk, VA, USA",IEEE Transactions on Consumer Electronics,"26 Jan 2023","2023","69","1","18","28","Ransomware is a type of malicious program or software that encrypts the contents on a hard disc and prevents the users from accessing them unless they pay an amount (called a ransom). Most of the organizations, such as financial institutes and healthcare sectors (i.e., smart healthcare) are targeted by ransomware attacks. Ransomware assaults are among the most frightening types of cyber-attacks, and they are not confined to a specific sector or the countries. Blockchain is a tamper-proof technology, which is more secure, robust and decentralized in nature. Features of blockchain can add more security for detection and mitigation of ransomware more effectively. In this paper, we propose a new blockchain-enabled security framework to detect and defend the ransomware attacks for smart healthcare (in short, BSFR-SH). The conducted security analysis proves the security of the proposed BSFR-SH against the ransomware attacks. The performance of BSFR-SH is significantly better than the other similar existing mechanisms as it achieves better accuracy and F1-score than other compared mechanisms. Furthermore, the practical demonstration of BSFR-SH is provided to estimate the impact on important performance parameters.","1558-4127","","10.1109/TCE.2022.3208795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9899708","Ransomware;smart healthcare;intrusion detection;machine learning;blockchain","Ransomware;Medical services;Blockchains;Malware;Smart healthcare;Intrusion detection;Machine learning","","50","","26","IEEE","22 Sep 2022","","","IEEE","IEEE Journals"
"RANK: AI-Assisted End-to-End Architecture for Detecting Persistent Attacks in Enterprise Networks","H. M. Soliman; D. Sovilj; G. Salmon; M. Rao; N. Mayya","Arctic Wolf Networks, Waterloo, ON, Canada; Arctic Wolf Networks, Waterloo, ON, Canada; Arctic Wolf Networks, Waterloo, ON, Canada; Arctic Wolf Networks, Waterloo, ON, Canada; Arctic Wolf Networks, Waterloo, ON, Canada",IEEE Transactions on Dependable and Secure Computing,"11 Jul 2024","2024","21","4","3834","3850","Modern government and enterprise networks are the target of sophisticated multi-step attacks called Advanced Persistent Threats (APTs), designed and carried out by expert adversaries. The prolonged nature of APTs results in overwhelming the analyst with an increasingly impractical number of alerts. As a result, the challenge of APT detection is ideal for automation through artificial intelligence (AI). In this paper, we propose the first, up to our knowledge, end-to-end AI-assisted architecture for detecting APTs – RANK. We propose advanced algorithms and solutions for four consecutive sub-problems: 1) alert templating and merging, 2) alert graph construction, 3) alert graph partitioning into incidents, and 4) incident scoring and prioritization. Additionally, we discuss the necessary optimizations and techniques enabling the system to operate in a real-time fashion. We evaluate our architecture against the 2000 DARPA, Mordor, as well as a large number of real-world datasets from enterprise networks. Extensive results are provided showing four orders-of-magnitude reduction in the amount of data to be reviewed, innovative extraction and security-aware scoring of incidents. The extracted incidents can be further used for downstream tasks. In our experiments where we have access to a portion of alert labels, we are able achieve 87% balanced accuracy.","1941-0018","","10.1109/TDSC.2023.3338136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10337612","Advanced persistent threats;enterprise networks;intrusion detection;machine learning;mathematical optimization;security management architecture","Security;Detectors;Buildings;Correlation;Computer architecture;Merging;Deep learning","","3","","52","IEEE","1 Dec 2023","","","IEEE","IEEE Journals"
"Privacy-Preserving Federated Learning for Intrusion Detection in IoT Environments: A Survey","A. Vyas; P. -C. Lin; R. -H. Hwang; M. Tripathi","Computer Science and Information Engineering Department, National Chung Cheng University, Minxiong, Chiayi, Taiwan; Computer Science and Information Engineering Department, National Chung Cheng University, Minxiong, Chiayi, Taiwan; College of Artificial Intelligence, National Yang Ming Chiao Tung University, Tainan, Taiwan; Computer Science and Engineering Department, Malaviya National Institute of Technology Jaipur, Jaipur, Rajasthan, India",IEEE Access,"17 Sep 2024","2024","12","","127018","127050","With the rapid development of artificial intelligence and a new generation of network technologies, the Internet of Things (IoT) is expanding worldwide. Malicious agents consistently exploit new technical vulnerabilities to access the various IoT systems used in critical industries, medical diagnosis, military, and defense systems. To mitigate these threats, IoT networks should be equipped with intrusion detection systems capable of detecting threat vectors in an attempt to compromise the systems. Moreover, many researchers have integrated privacy-preserving technologies such as homomorphic encryption, differential privacy, and secure multiparty computation with machine learning algorithms. Furthermore, federated learning, which shares only model parameters rather than data, provides distributed privacy-preserving learning; therefore, federated learning is secure and reliable for the implementation of intrusion detection systems in IoT environments. This survey examined the utilization and applications of privacy-preserving mechanisms, explicitly focusing on privacy-preserving federated learning for intrusion detection systems in IoT environments. This survey also highlights future research directions and open research questions. Privacy-preserving federated learning can significantly contribute to the rapid and efficient detection and prevention of various threat vectors that target IoT ecosystems.","2169-3536","","10.1109/ACCESS.2024.3454211","National Science and Technology Council (NSTC), Taiwan, through the National Yang-Ming Chiao Tung University, Taiwan(grant numbers:NSTC 113-2634-F-A49-001-MBK); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664537","Privacy preservation;federated learning;intrusion detection system;Internet of Things","Surveys;Internet of Things;Intrusion detection;Federated learning;Vectors;Privacy;Data privacy","","1","","186","CCBY","4 Sep 2024","","","IEEE","IEEE Journals"
"MR${}^{2}$ 2-KG: A Multi-Relation Multi-Rationale Knowledge Graph for Modeling Software Engineering Knowledge on Stack Overflow","L. Gong; H. Zhang","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen’s University, Kingston, Canada",IEEE Transactions on Software Engineering,"17 Jul 2024","2024","50","7","1867","1887","Stack Overflow is a knowledge sharing platform where its users create and share informative content from both inside and outside the site. Prior studies have leveraged the relation across Stack Overflow posts through internal links to build services and applications to enhance the accessibility of knowledge. However, they focused on studying a knowledge unit that consists of a question post and all the associated answer posts to represent the relation. It is unknown whether such representation of knowledge on Stack Overflow could comprehensively model various complex relations among webpages, such as questions, answers, internal and external links. In addition, the rationales behind sharing knowledge on Stack Overflow have yet to be explored among distinct user groups, such as askers, answerers, readers who wish to learn. Thus, in this study, we first investigate the real-world characteristics of Stack Overflow knowledge by abstracting the complex knowledge representation into relations among its building blocks. We observe that a question thread includes three basic knowledge relations to reassemble into complex knowledge, that is, the hierarchy relation within the associated answers in a question, the coupling relation between knowledge artifacts (i.e., question or answer posts) through internal links, and the complimentary relation between Stack Overflow posts and external websites. All these three basic knowledge relations are informative and could be caused by different rationales when the crowdsourced knowledge is shared on Stack Overflow. Our findings highlight that it is necessary to propose a comprehensive knowledge graph to represent the real-world knowledge on Stack Overflow. Therefore, we further propose a Multi-Relation Multi-Rationale Knowledge Graph (MR${}^{2}$ 2-KG), whose nodes represent questions, answers, and external webpages. Edges in the MR${}^{2}$ 2-KG represent the rationales included in the three structures (i.e., question answering, duplicate, priori, posterior, parallelism, containment, and working examples knowledge). In addition, we develop an automated approach to model the nodes and edges to represent Stack Overflow knowledge associated with a question thread. Our case study shows that the automated knowledge representation generation can achieve an ROC AUC of 96% and MCC of 89% to identify edges in the MR${}^{2}$ 2-KG. To further evaluate the applicability of MR${}^{2}$ 2-KG, we develop an answer generator to help developers efficiently identify the answers that meet their intent. Our user study of 100 real-world Java questions indicates the usefulness of MR${}^{2}$ 2-KG. Finally, we discuss the implications of our findings for developers, researchers, and Stack Overflow moderators.","1939-3520","","10.1109/TSE.2024.3403108","National Natural Science Foundation of China(grant numbers:62202223); Natural Science Foundation of Jiangsu Province, China(grant numbers:BK20220881); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10535119","Empirical software engineering;stack overflow;crowdsourced knowledge sharing;knowledge graph","Knowledge graphs;Message systems;Java;Couplings;Question answering (information retrieval);Software engineering;Parallel processing","","","","59","IEEE","20 May 2024","","","IEEE","IEEE Journals"
"A Brief Overview of ChatGPT: The History, Status Quo and Potential Future Development","T. Wu; S. He; J. Liu; S. Sun; K. Liu; Q. -L. Han; Y. Tang","Key Laboratory of Smart Manufacturing in Energy Chemical Process, East China University of Science and Technology, Shanghai, China; Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; Research Institute of Intelligent Complex Systems, Fudan University, Shanghai, China; Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; Key Laboratory of Smart Manufacturing in Energy Chemical Process, East China University of Science and Technology, Shanghai, China",IEEE/CAA Journal of Automatica Sinica,"1 May 2023","2023","10","5","1122","1136","ChatGPT, an artificial intelligence generated content (AIGC) model developed by OpenAI, has attracted world-wide attention for its capability of dealing with challenging language understanding and generation tasks in the form of conversations. This paper briefly provides an overview on the history, status quo and potential future development of ChatGPT, helping to provide an entry point to think about ChatGPT. Specifically, from the limited open-accessed resources, we conclude the core techniques of ChatGPT, mainly including large-scale language models, in-context learning, reinforcement learning from human feedback and the key technical steps for developing Chat-GPT. We further analyze the pros and cons of ChatGPT and we rethink the duality of ChatGPT in various fields. Although it has been widely acknowledged that ChatGPT brings plenty of opportunities for various fields, mankind should still treat and use ChatGPT properly to avoid the potential threat, e.g., academic integrity and safety challenge. Finally, we discuss several open problems as the potential development of ChatGPT.","2329-9274","","10.1109/JAS.2023.123618","National Key Research and Development Program of China(grant numbers:2021YFB1714300); National Natural Science Foundation of China(grant numbers:62293502,61831022,61976211); Youth Innovation Promotion Association CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113601","AIGC;ChatGPT;GPT-3;GPT-4;human feedback;large language models","Three-dimensional displays;Web and internet services;Reinforcement learning;Chatbots;Robot sensing systems;Transformers;History","","400","","128","","1 May 2023","","","IEEE","IEEE Journals"
"Privacy and Security in Distributed Learning: A Review of Challenges, Solutions, and Open Research Issues","M. U. Afzal; A. A. Abdellatif; M. Zubair; M. Q. Mehmood; Y. Massoud","Department of Electrical Engineering, MicroNano Laboratory, Information Technology University (ITU), Punjab, Lahore, Pakistan; Department of Computer Science and Engineering, Qatar University, Doha, Qatar; Innovative Technologies Laboratories (ITL), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Department of Electrical Engineering, MicroNano Laboratory, Information Technology University (ITU), Punjab, Lahore, Pakistan; Innovative Technologies Laboratories (ITL), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia",IEEE Access,"20 Oct 2023","2023","11","","114562","114581","In recent years, the way that machine learning is used has undergone a paradigm shift driven by distributed and collaborative learning. Several approaches have emerged to enable pervasive computing and distributed learning in ubiquitous Internet of Things (IoT) systems. Numerous decentralized strategies have been proposed to deal with the limitations of centralized learning, including privacy and latency due to sharing local data, while utilizing distributed computations as a promising substitute to centralized learning. However, such distributed learning schemes come with new security and privacy concerns that should be addressed. Thus, in this paper, we first provide an overview for the emerging paradigms developed for distributed learning. Then, we performed a comprehensive survey for the privacy and security challenges associated with distributed learning along with the presented solutions to overcome them. Furthermore, we highlight key challenges and open future research directions toward implementing more robust distributed systems.","2169-3536","","10.1109/ACCESS.2023.3323932","Innovative Technologies Laboratories (ITL), King Abdullah University of Science and Technology (KAUST); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10278413","Data privacy and security;Internet of Things (IoT);deep learning;adversarial attacks","Distance learning;Computer aided instruction;Security;Privacy;Artificial intelligence;Data privacy;Servers;Internet of Things;Adversarial machine learning","","1","","136","CCBYNCND","11 Oct 2023","","","IEEE","IEEE Journals"
"Draft Recommended Practice for Improving Generalizability of Artificial Intelligence for Medical Imaging","",,"IEEE P3350/D1, March 2023","25 Sep 2024","2024","","","1","43","This recommended practice delineates an architecture and offers suggestions for enhancing the generalizability of artificial intelligence (AI) models in medical imaging.","","979-8-8557-1289-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10694768","Generalizability;dataset;modeling;training;evaluation","IEEE Standards;Biomedical imaging;Artificial intelligence;Data models;Training data;Performance evaluation","","","","","","25 Sep 2024","","","IEEE","IEEE Standards"
"IEEE Draft Recommended Practice for Improving Generalizability of Artificial Intelligence for Medical Imaging","",,"IEEE P3350/D2, November 2024","18 Nov 2024","2024","","","1","38","This recommended practice delineates an architecture and offers suggestions for enhancing the generalizability of artificial intelligence (AI) models in medical imaging.","","979-8-8557-1444-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10756189","Generalizability;dataset;modeling;training;evaluation","IEEE Standards;Artificial intelligence;Biomedical monitoring;Data models;Training;Performance evaluation","","","","","","18 Nov 2024","","","IEEE","IEEE Standards"
"C-HIDE: A Steganographic Framework for Robust Data Hiding and Advanced Security Using Coverless Hybrid Image Encryption With AES and ECC","S. A. El-Rahman; A. E. Mansour; L. Jamel; M. Abdullah Alohali; M. Seifeldin; Y. Alkady","Computer Systems Program-Electrical Engineering Department, Faculty of Engineering-Shoubra, Benha University, Cairo, Egypt; Faculty of Computer Science, Misr International University, Cairo, Egypt; Department of Information Systems, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; Department of Information Systems, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; Faculty of Computer Science, Misr International University, Cairo, Egypt; Faculty of Computer Science, Misr International University, Cairo, Egypt",IEEE Access,"10 Mar 2025","2025","13","","41367","41381","Coverless image steganography conceals information without modifying the carrier image, addressing vulnerabilities in traditional methods. However, existing approaches often require transmitting metadata, raising suspicion and security risks. To overcome these limitations, we propose Coverless Hybrid Image Data Encryption (C-HIDE), a robust steganographic method integrating Advanced Encryption Standard (AES) for data confidentiality and Elliptic Curve Cryptography (ECC) for secure key exchange. The system ensures secure transmission without altering cover images, making embedded data harder to detect. C-HIDE eliminates metadata transmission by enabling both sender and receiver to independently generate synchronized coverless image datasets (CIDs) using random seeds. Encrypted secret data is mapped to images whose hash sequences correspond to segments of the message, with Speeded-Up Robust Features (SURF) ensuring reliable image matching. At the receiver’s end, ECC-decrypted AES keys recover the original message while SURF retrieves relevant images. Experimental results demonstrate that C-HIDE achieves an embedding capacity of 574 bits per image, significantly surpassing DCT (256 bits) and DWT (128 bits) techniques. The system maintains 98.5% accuracy under attacks such as noise addition, cropping, and geometric transformations. Furthermore, it enhances security by eliminating metadata transmission, achieving a zero additional information ratio, unlike conventional methods requiring up to 25% extra data. By integrating encryption, minimizing detection, and removing metadata transmission, C-HIDE provides a secure, efficient, and scalable solution for covert communication in real-world applications.","2169-3536","","10.1109/ACCESS.2025.3546255","Deanship of Scientific Research, Princess Nourah bint Abdulrahman University, through the Program of Research Project Funding After Publication(grant numbers:44-PRFA-P-114); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10906583","Steganography;coverless image steganography;information hiding;information security;concealed communications;cryptography;embedding;encryption technique","Steganography;Security;Robustness;Encryption;Feature extraction;Metadata;Convolutional neural networks;Receivers;Elliptic curve cryptography;Discrete cosine transforms","","","","55","CCBY","27 Feb 2025","","","IEEE","IEEE Journals"
"Customizing Synthetic Data for Data-Free Student Learning","S. Luo; D. Chen; C. Wang","Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China",2023 IEEE International Conference on Multimedia and Expo (ICME),"25 Aug 2023","2023","","","1817","1822","Data-free knowledge distillation (DFKD) aims to obtain a lightweight student model without original training data. Existing works generally synthesize data from the pretrained teacher model to replace the original training data for student learning. To more effectively train the student model, the synthetic data shall be customized to the current student learning ability. However, this is ignored in the existing DFKD methods and thus negatively affects the student training. To address this issue, we propose Customizing Synthetic Data for Data-Free Student Learning (CSD) in this paper, which achieves adaptive data synthesis using a self-supervised augmented auxiliary task to estimate the student learning ability. That is, data synthesis is dynamically adjusted to enlarge the cross entropy between the labels and the predictions from the self-supervised augmented task, thus generating the hard samples for the student model. The experiments on various datasets and teacher-student models show the effectiveness of our proposed method. Code is available at: https://github.com/luoshiya/CSD","1945-788X","978-1-6654-6891-6","10.1109/ICME55011.2023.00312","Institute for Advanced Study; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10219901","data-free knowledge distillation;self-supervision;model compression","Training;Adaptation models;Codes;Training data;Predictive models;Data models;Entropy","","","","23","IEEE","25 Aug 2023","","","IEEE","IEEE Conferences"
"Adversarial Machine Learning-Industry Perspectives","R. S. Siva Kumar; M. Nyström; J. Lambert; A. Marshall; M. Goertzel; A. Comissoneru; M. Swann; S. Xia","Microsoft, Redmond, USA; Microsoft, Redmond, USA; Microsoft, Redmond, USA; Microsoft, Redmond, USA; Microsoft, Redmond, USA; Microsoft, Redmond, USA; Microsoft, Redmond, USA; Microsoft, Redmond, USA",2020 IEEE Security and Privacy Workshops (SPW),"18 Dec 2020","2020","","","69","75","Based on interviews with 28 organizations, we found that industry practitioners are not equipped with tactical and strategic tools to protect, detect and respond to attacks on their Machine Learning (ML) systems. We leverage the insights from the interviews and enumerate the gaps in securing machine learning systems when viewed in the context of traditional software security development. We write this paper from the perspective of two personas: developers/ML engineers and security incident responders. The goal of this paper is to layout the research agenda to amend the Security Development Lifecycle for industrial-grade software in the adversarial ML era.","","978-1-7281-9346-5","10.1109/SPW50608.2020.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283867","adversarial machine learning;software security;engineering","Organizations;Machine learning;Tools;Software;Software reliability;Security;Interviews","","73","","71","IEEE","18 Dec 2020","","","IEEE","IEEE Conferences"
"Social Metaverse: Challenges and Solutions","Y. Wang; Z. Su; M. Yan","Xi'an Jiaotong University, China; Xi'an Jiaotong University, China; Xi'an Jiaotong University, China",IEEE Internet of Things Magazine,"20 Sep 2023","2023","6","3","144","150","Social metaverse is a shared digital space combining a series of interconnected virtual worlds for users to play, shop, work, and socialize. In parallel with the advances of artificial intelligence (AI) and growing awareness of data privacy concerns, federated learning (FL) is promoted as a paradigm shift towards privacy-preserving AI-empowered social metaverse. However, challenges including privacy-utility tradeoff, learning reliability, and AI model thefts hinder the deployment of FL in real metaverse applications. In this article, we exploit the pervasive social ties among users/avatars to advance a social-aware hierarchical FL framework, i.e., SocialFL for a better privacy-utility tradeoff in the social metaverse. Then, an aggregator-free robust FL mechanism based on blockchain is devised with a new block structure and an improved consensus protocol featured with on/off-chain collaboration. Furthermore, based on digital watermarks, an automatic federated AI (FedAI) model ownership provenance mechanism is designed to prevent AI model thefts and collusive avatars in social metaverse. Experimental findings validate the feasibility and effectiveness of proposed framework. Finally, we envision promising future research directions in this emerging area.","2576-3199","","10.1109/IOTM.001.2200266","NSFC(grant numbers:U22A2029,U20A20175); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255769","","Training;Privacy;Metaverse;Collaboration;Watermarking;Reliability engineering;Robustness","","18","","15","IEEE","20 Sep 2023","","","IEEE","IEEE Magazines"
"AI Needs You: How We Can Change AI's Future and Save Our Own","V. Harding",NA,AI Needs You: How We Can Change AI's Future and Save Our Own,"","2024","","","","","A humanist manifesto for the age of AIArtificial intelligence may be the most transformative technology of our time. As AI’s power grows, so does the need to figure out what—and who—this technology is really for. AI Needs You argues that it is critical for society to take the lead in answering this urgent question and ensuring that AI fulfills its promise.Verity Harding draws inspiring lessons from the histories of three twentieth-century tech revolutions—the space race, in vitro fertilization, and the internet—to empower each of us to join the conversation about AI and its possible futures. Sharing her perspective as a leading insider in technology and politics, she rejects the dominant narrative, which often likens AI’s advent to that of the atomic bomb. History points the way to an achievable future in which democratically determined values guide AI to be peaceful in its intent; to embrace limitations; to serve purpose, not profit; and to be firmly rooted in societal trust.AI Needs You gives us hope that we, the people, can imbue AI with a deep intentionality that reflects our best values, ideals, and interests, and that serves the public good. AI will permeate our lives in unforeseeable ways, but it is clear that the shape of AI’s future—and of our own—cannot be left only to those building it. It is up to us to guide this technology away from our worst fears and toward a future that we can trust and believe in.","","9780691244907","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10614683.pdf&bkn=10614682&pdfType=book","Artificial intelligence;science;technology;history;space race;satellites;biotech;life sciences;politics;history of science;internet;AI governance;AI ethics;democracy geopolitics;chatgpt;generative ai;AI Needs You: How We Can Change AI's Future and Save Our Own;verity harding;Times 100 AI;how we can safeguard AI’s future for the public good;safe AI: is AI good: is AI bad;how can AI help: AI policy;AI education: AI public good;AI fear: Artificial intelligence;transformative technology;AI’s power grows;technology;society;critical;urgent: social responsibility;better society;harmful;better education;future of AI;societal trust;Silicon Valley;Large Language Models (LLMs);ChatGPT;Bing;Google;Space Race;United Nations Outer Space Treaty 1967;Cold War;IVF (in vitro fertilization);Louise Joy Brown;Roe V. Wade;Embryo research/human embryology;Chatbot;DeepMind;Online security;OpenAI;Atomic Bomb;Oppenheimer;AI Bill of Rights;Warnock Commission;AI regulation","","","","","","","30 Jul 2024","","","Princeton University Press","Princeton University Press eBooks"
"Data Management Strategy at Microsoft: Best practices from a tech giant's decade-long data transformation journey","A. Plotnikovs",NA,Data Management Strategy at Microsoft: Best practices from a tech giant's decade-long data transformation journey,"","2024","","","","","Leverage your data as a business asset, from readiness to actionable insights, and drive exceptional performanceKey FeaturesLearn strategies to create a data-driven culture and align data initiatives with business goalsNavigate the ever-evolving business landscape with a modern data platform and unique Data IPSurpass competitors by harnessing the true value of data and fostering data literacy in your organizationPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionMicrosoft pioneered data innovation and investment ahead of many in the industry, setting a remarkable standard for data maturity. Written by a data leader with over 15 years of experience following Microsoft’s data journey, this book delves into every crucial aspect of this journey, including change management, aligning with business needs, enhancing data value, and cultivating a data-driven culture. This book emphasizes that success in a data-driven enterprise goes beyond relying solely on modern technology and highlights the importance of prioritizing genuine business needs to propel necessary modernizations through change management practices. You’ll see how data-driven innovation does not solely reside within central IT engineering teams but also among the data's business owners who rely on data daily for their operational needs. This guide empower these professionals with clean, easily discoverable, and business-ready data, marking a significant breakthrough in how data is perceived and utilized throughout an enterprise. You’ll also discover advanced techniques to nurture the value of data as unique intellectual property, and differentiate your organization with the power of data. Its storytelling approach and summary of essential insights at the end of each chapter make this book invaluable for business and data leaders to advocate for crucial data investments.What you will learnDevelop a data-driven roadmap to achieve significant and quantifiable business goalsDiscover the ties between data management and change managementExplore the data maturity curve with essential technology investmentsBuild, safeguard, and amplify your organization's unique Data Intellectual PropertyEquip business leaders with trustworthy and high value data for informed decision-makingUnleash the value of data management and data governance to uplift your data investmentsWho this book is forThis book is for data leaders, CDOs, CDAOs, data practitioners, data stewards, and enthusiasts, as well as modern business leaders intrigued by the transformative potential of data. While a technical background isn't essential, a basic understanding of data management and quality concepts will be helpful. The book avoids twisted technical, engineering, or data science aspects, making it accessible and insightful for data engineers and data scientists to gain a wider understanding of enterprise data needs and challenges.","","9781835466933","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769273.pdf&bkn=10769272&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Friend or Foe? AI and the Evolving Landscape of Ransomware-as-a-Service (RaaS)","F. O. Gyimah; E. Ofori-Mensah; H. Boowuo; S. Aggrawal","Interdisciplinary Information Security, Purdue University, West Lafayette, U.S.A.; Computer and Information Technology, Purdue University, West Lafayette, U.S.A.; Computer and Information Technology, Purdue University, West Lafayette, U.S.A.; Computer and Information Technology, Purdue University, West Lafayette, U.S.A.",2024 Cyber Awareness and Research Symposium (CARS),"13 Dec 2024","2024","","","1","5","This paper examines Ransomware-as-a-Service (RaaS) and its impact on cybercrime. RaaS has made sophisticated attacks accessible to a wider range of criminals, increasing the number of ransomware attacks. The paper explores how Artificial Intelligence (AI) is being used by both attackers and defenders in this evolving landscape. AI empowers RaaS attackers by improving target selection, vulnerability identification, and social engineering tactics. It also automates attack processes, making them more efficient. For defenders, AI offers potential in threat detection, vulnerability assessment, and incident response through real-time data analysis. However, challenges like model development complexity, false positives, and the need for explainable AI models exist for both sides. The paper concludes that AI use by both attackers and defenders creates an ""AI arms race"" in cybersecurity. It further aim(s) to illuminate future cybersecurity strategies and equip defenders with proactive measures against evolving cyber threats.","","979-8-3503-8641-7","10.1109/CARS61786.2024.10778711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10778711","Ransomware;AI in Cybersecurity;Artificial Intelligence;Cybersecurity;Ransomware-as-a-Service;RaaS;Attackers;Defenders;Cybercrime","Data analysis;Explainable AI;Weapons;Fasteners;Threat assessment;Real-time systems;Ransomware;Artificial intelligence;Computer crime;Research and development","","","","46","IEEE","13 Dec 2024","","","IEEE","IEEE Conferences"
"ConStruct-VL: Data-Free Continual Structured VL Concepts Learning*","J. S. Smith; P. Cascante-Bonilla; A. Arbelle; D. Kim; R. Panda; D. Cox; D. Yang; Z. Kira; R. Feris; L. Karlinsky",MIT-IBM Watson AI Lab; MIT-IBM Watson AI Lab; IBM Research; MIT-IBM Watson AI Lab; MIT-IBM Watson AI Lab; MIT-IBM Watson AI Lab; Stanford University; Georgia Institute of Technology; MIT-IBM Watson AI Lab; MIT-IBM Watson AI Lab,2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"22 Aug 2023","2023","","","14994","15004","Recently, large-scale pre-trained Vision-and-Language (VL) foundation models have demonstrated remarkable capabilities in many zero-shot downstream tasks, achieving competitive results for recognizing objects defined by as little as short text prompts. However, it has also been shown that VL models are still brittle in Structured VL Concept (SVLC) reasoning, such as the ability to recognize object attributes, states, and inter-object relations. This leads to reasoning mistakes, which need to be corrected as they occur by teaching VL models the missing SVLC skills; often this must be done using private data where the issue was found, which naturally leads to a data-free continual (no task-id) VL learning setting. In this work, we introduce the first Continual Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark11Our code is publicly available at https://github.com/jamessealesmith/ConStruct-VL and show it is challenging for many existing data-free CL strategies. We, therefore, propose a data-free method comprised of a new approach of Adversarial Pseudo-Replay (APR) which generates adversarial reminders of past tasks from past task models. To use this method efficiently, we also propose a continual parameter-efficient Layered-LoRA (LaLo) neural architecture allowing no-memory-cost access to all past models at train time. We show this approach outperforms all data-free methods by as much as ~ 7% while even matching some levels of experience-replay (prohibitive for applications where data-privacy must be preserved).","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01440","Defense Advanced Research Projects Agency(grant numbers:FA8750-19-C-1001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205003","Transfer;meta;low-shot;continual;or long-tail learning","Computer vision;Codes;Text recognition;Education;Computer architecture;Benchmark testing;Cognition","","6","","68","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Metaverse Security and Privacy: An Overview","Z. Chen; J. Wu; W. Gan; Z. Qi","Jinan University, Guangzhou, China; Guangdong Ocean University, Zhanjiang, China; Jinan University, Guangzhou, China; Guangdong Eco-Engineering Polytechnic, Guangzhou, China",2022 IEEE International Conference on Big Data (Big Data),"26 Jan 2023","2022","","","2950","2959","Metaverse is a living space and cyberspace that realizes the process of virtualizing and digitizing the real world. It integrates a plethora of existing technologies with the goal of being able to map the real world, even beyond the real world. Metaverse has a bright future and is expected to have many applications in various scenarios. The support of the Metaverse is based on numerous related technologies becoming mature. Hence, there is no doubt that the security risks of the development of the Metaverse may be more prominent and more complex. We present some Metaverse-related technologies and some potential security and privacy issues in the Metaverse. We present current solutions for Metaverse security and privacy derived from these technologies. In addition, we also raise some unresolved questions about the potential Metaverse. To summarize, this survey provides an in-depth review of the security and privacy issues raised by key technologies in Metaverse applications. We hope that this survey will provide insightful research directions and prospects for the Metaverse's development, particularly in terms of security and privacy protection in the Metaverse.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10021112","National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021112","Metaverse;cyber;security;privacy;overview","Industries;Privacy;Metaverse;Cyberspace;Big Data;Internet;Security","","65","","93","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"FReTAL: Generalizing Deepfake Detection using Knowledge Distillation and Representation Learning","M. Kim; S. Tariq; S. S. Woo","College of Computing and Informatics, Sungkyunkwan University, South Korea; College of Computing and Informatics, Sungkyunkwan University, South Korea; Department of Applied Data Science, Sungkyunkwan University, South Korea",2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),"1 Sep 2021","2021","","","1001","1012","As GAN-based video and image manipulation technologies become more sophisticated and easily accessible, there is an urgent need for effective deepfake detection technologies. Moreover, various deepfake generation techniques have emerged over the past few years. While many deepfake detection methods have been proposed, their performance suffers from new types of deepfake methods on which they are not sufficiently trained. To detect new types of deepfakes, the model should learn from additional data without losing its prior knowledge about deepfakes (catastrophic forgetting), especially when new deepfakes are significantly different. In this work, we employ the Representation Learning (ReL) and Knowledge Distillation (KD) paradigms to introduce a transfer learning-based Feature Representation Transfer Adaptation Learning (FReTAL) method. We use FReTAL to perform domain adaptation tasks on new deepfake datasets, while minimizing the catastrophic forgetting. Our student model can quickly adapt to new types of deepfake by distilling knowledge from a pre-trained teacher model and applying transfer learning without using source domain data during domain adaptation. Through experiments on FaceForensics++ datasets, we demonstrate that FReTAL outperforms all baselines on the domain adaptation task with up to 86.97% accuracy on low-quality deepfakes.","2160-7516","978-1-6654-4899-4","10.1109/CVPRW53098.2021.00111","Sungkyunkwan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9523015","","Adaptation models;Computer vision;Conferences;Computational modeling;Transfer learning;Data models;Pattern recognition","","53","","89","IEEE","1 Sep 2021","","","IEEE","IEEE Conferences"
"On Hallucinating Context and Background Pixels from a Face Mask using Multi-scale GANs","S. Banerjee; W. J. Scheirer; K. W. Bowyer; P. J. Flynn","Affectiva, USA; Department of Computer Science & Engineering, University of Notre Dame, USA; Department of Computer Science & Engineering, University of Notre Dame, USA; Department of Computer Science & Engineering, University of Notre Dame, USA",2020 IEEE Winter Conference on Applications of Computer Vision (WACV),"14 May 2020","2020","","","289","298","We propose a multi-scale GAN model to hallucinate realistic context (forehead, hair, neck, clothes) and background pixels automatically from a single input face mask, without any user supervision. Instead of swapping a face on to an existing picture, our model directly generates realistic context and background pixels based on the features of the provided face mask. Unlike facial inpainting algorithms, it can generate realistic hallucinations even for a large number of missing pixels. Our model is composed of a cascaded network of GAN blocks, each tasked with hallucination of missing pixels at a particular resolution while guiding the synthesis process of the next GAN block. The hallucinated full face image is made photo-realistic by using a combination of reconstruction, perceptual, adversarial and identity preserving losses at each block of the network. With a set of extensive experiments, we demonstrate the effectiveness of our model in hallucinating context and background pixels from face masks varying in facial pose, expression and lighting, collected from multiple datasets subject disjoint with our training data. We also compare our method with popular face inpainting and face swapping models in terms of visual quality, realism and identity preservation. Additionally, we analyze our cascaded pipeline and compare it with the progressive growing of GANs, and explore its usage as a data augmentation module for training CNNs.","2642-9381","978-1-7281-6553-0","10.1109/WACV45572.2020.9093568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093568","","Face;Gallium nitride;Training;Context modeling;Image resolution;Data models;Image reconstruction","","13","","79","IEEE","14 May 2020","","","IEEE","IEEE Conferences"
"Deep Learning Approaches in Topics of Singing Information Processing","C. Gupta; H. Li; M. Goto","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Chinese University of Hong Kong, Shenzhen, China; National Institute of Advanced Industrial Science and Technology (AIST), Ibaraki, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","29 Jul 2022","2022","30","","2422","2451","Singing, the vocal productionof musical tones, is one of the most important elements of music. Addressing the needs of real-world applications, the study of technologies related to singing voices has become an increasingly active area of research. In this paper, we provide a comprehensive overview of the recent developments in the field of singing information processing, specifically in the topics of singing skill evaluation, singing voice synthesis, singing voice separation, and lyrics synchronization and transcription. We will especially focus on deep learning approaches including modern representation learning techniques for singing voices. We will also provide an overview of contributions in public datasets for singing voice research.","2329-9304","","10.1109/TASLP.2022.3190732","Academic Research Council; Ministry of Education - Singapore(grant numbers:MOE2018-T2-2-127); Guangdong Provincial Key Laboratory of Big Data Computing; The Chinese University of Hong Kong, Shenzhen, China(grant numbers:B10120210117-KP02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829265","Singing information processing;singing voice;singing skill evaluation;singing voice synthesis;singing voice separation;lyrics synchronization;lyrics transcription","Music;Information processing;Synchronization;Speech processing;Rhythm;Instruments;Deep learning","","10","","242","CCBY","13 Jul 2022","","","IEEE","IEEE Journals"
"Sustainable Computing Based Deep Learning Framework for Writing Research Manuscripts","G. S. Mahalakshmi; G. Muthu Selvi; S. Sendhilkumar; P. Vijayakumar; Y. Zhu; V. Chang","Department of Computer Science and Engineering, CEG, Anna University, Chennai, Tamil Nadu, India; Department of Computer Science and Engineering, CEG, Anna University, Chennai, Tamil Nadu, India; Department of Information Science and Technology, CEG, Anna University, Chennai, Tamil Nadu, India; University College of Engineering Tindivanam Melpakkam, Villupuram, Tamil Nadu, India; Shanghai Advanced Research Institute, Chinese Academy of Sciences, RoadShanghai, China; Department of Information Management and Information Systems, Xi'an Jiaoton University, Liver-pool, Suzhou, China",IEEE Transactions on Sustainable Computing,"7 Mar 2019","2019","4","1","4","16","Writing research manuscripts is always a tough task at the eleventh hour. Often researchers do not find time to rewrite the manuscript to satisfaction, which is not quantifiable though. This paper proposes a sustainable computing based deep learning framework for iterated accumulation of ideas while writing research manuscripts. The framework suggests Deep Author Topic Models (DATM) where every author of the manuscript is modeled. For this, we have assumed time based sustainable computing as a measure of evaluation for research manuscript effectiveness. Using respective DATM, the region contributed by every author in the manuscriptis analyzed and fine-tuned semantically such that the manuscript is made to perfection in least time.","2377-3782","","10.1109/TSUSC.2018.2829196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8344521","Author contribution analysis;deep learning;stacked auto encoder;sustainable computing;topic models","Writing;Gold;Semantics;Computational modeling;Machine learning;Analytical models;History","","8","","86","Crown","20 Apr 2018","","","IEEE","IEEE Journals"
"Sensorimotor Skill Communication: A Literature Review","V. Babushkin; M. H. Jamil; W. Park; M. Eid","NYU Tandon School of Engineering, New York University, Brooklyn, NYC, USA; Division of Engineering, New York University at Abu Dhabi, Abu Dhabi, United Arab Emirates; Division of Engineering, New York University at Abu Dhabi, Abu Dhabi, United Arab Emirates; Division of Engineering, New York University at Abu Dhabi, Abu Dhabi, United Arab Emirates",IEEE Access,"25 May 2021","2021","9","","75132","75149","A sensorimotor skill is a sequence of motions generated in response to external stimuli and aiming to accomplish a particular task. It can be communicated to reproduce the task in a distant environment with similar settings. In this work, we conceptualize a multi-modal sensorimotor skill communication system that incorporates modeling, simulation, and evaluation of the sensorimotor skill. The proposed sensorimotor skill communication system can be applied for learning a specific style of human sensorimotor skill and teaching the skill to distant learners, which can be implemented in a variety of applications such as Tele-consultation, Tele-diagnosis, Tele-treatment, Tele-monitoring, and Tele-support. To understand the processes behind the communication of sensorimotor skill we review the representation of a human sensorimotor system from the neurobiological perspective. Then we analyze the existing literature on sensorimotor skill communication systems and propose a taxonomy of currently available methods for sensorimotor skill modeling, simulation, and evaluation. Furthermore, we propose a benchmark for evaluating the quality of the sensorimotor skill communication system. We present a case study aiming to demonstrate modeling the dental sensorimotor skill of periodontal probing. Lastly, we discuss challenges and limitations and provide perspectives for future research in developing sensorimotor skill communication systems.","2169-3536","","10.1109/ACCESS.2021.3081449","ADEK Award for Research Excellence (AARE) 2019 Program(grant numbers:AARE19-159); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9433546","Haptics and haptic interfaces;learning from demonstration;sensorimotor learning;virtual reality and interfaces","Robot sensing systems;Solid modeling;Task analysis;Internet;Communication systems;Visualization;Tactile Internet","","1","","167","CCBY","17 May 2021","","","IEEE","IEEE Journals"
"Programme","",,"2021 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","25 Oct 2021","2021","","","1","102","Presents abstracts for the articles comprising the conference proceedings.","","978-1-6654-2918-4","10.1109/ICSPCC52875.2021.9564992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564992","","","","","","","IEEE","25 Oct 2021","","","IEEE","IEEE Conferences"
"Music Emotion Recognition Based on Deep Learning: A Review","X. Jiang; Y. Zhang; G. Lin; L. Yu","School of Automation and Information Engineering, Sichuan University of Science and Engineering, Zigong, China; School of Automation and Information Engineering, Sichuan University of Science and Engineering, Zigong, China; School of Automation and Information Engineering, Sichuan University of Science and Engineering, Zigong, China; School of Automation and Information Engineering, Sichuan University of Science and Engineering, Zigong, China",IEEE Access,"1 Nov 2024","2024","12","","157716","157745","In recent years, with the development of the digital era, music emotion recognition technology has been widely used in the fields of music recommendation system, music classification, psychotherapy, music visualization, background music generation, smart home, and other applications of music emotion recognition, and has received attention from all walks of life. Especially the rapid development of artificial intelligence and deep learning, the music emotion recognition model using efficient deep neural network composition has become the mainstream model. This paper provides a more detailed overview of music emotion recognition, first introducing the background of music and emotion, and briefly summarizing the content of related works as well as the content framework. In the process, we also compare the similarities and differences in the content of other researchers’ reviews of related research areas. And in the middle section, we provide a detailed account of datasets, emotion models, feature extraction, and emotion recognition algorithms. Finally, we discuss the current challenges in music emotion recognition and explore future research priorities.","2169-3536","","10.1109/ACCESS.2024.3484470","Scientific Research Foundation of Sichuan University of Science and Engineering(grant numbers:2019RC12); Sichuan Science and Technology Program(grant numbers:2024YFHZ0026); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10729262","Music emotion recognition;deep learning;artificial intelligence;music emotion datasets","Emotion recognition;Reviews;Music;Feature extraction;Deep learning;Brain modeling;Speech recognition;Machine learning algorithms;Visualization;Support vector machines;Artificial intelligence","","","","120","CCBYNCND","22 Oct 2024","","","IEEE","IEEE Journals"
"ICSSES 2024 Book of Abstracts","",,2024 International Conference on Smart Systems for applications in Electrical Sciences (ICSSES),"21 Jun 2024","2024","","","i","cxv","","","979-8-3503-6404-0","10.1109/ICSSES62373.2024.10561413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10561413","","Education;Recording;Instruments;ISO Standards;Diamonds;Communications technology","","","","","IEEE","21 Jun 2024","","","IEEE","IEEE Conferences"
"IEEE Draft Guide for an Architectural Framework for Explainable Artificial Intelligence","",,"IEEE P2894/D8, August 2023","19 Sep 2023","2023","","","1","51","A new wave of artificial intelligence applications that offer extensive benefits to our daily lives has been led to by dramatic success in machine learning. The loss of explainability during this transition, however, means vulnerability to vicious data, poor model structure design, and suspicion of stakeholders and the general public--all with a range of legal implications. The study of explainable AI (XAI), which is an active research field that aims to make AI systems results more understandable to humans, has been called for by this dilemma. This is a field with great hopes for improving the trust and transparency of AI-based systems and is considered a necessary route for AI to move forward. A technological blueprint for building, deploying, and managing machine learning models, while meeting the requirements of transparent and trustworthy AI by adopting a variety of XAI methodologies, is provided by this guide. It defines the architectural framework and application guidelines for explainable AI, including: description and definition of XAI; the types of XAI methods and the application scenarios to which each type applies; and performance evaluation of XAI.","","978-1-5044-9689-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255703","AI;architectural framework;artificial intelligence;explainable AI;explainable artificial intelligence;IEEE 2894™;machine learning;XAI","IEEE Standards;Artificial intelligence;Software architecture","","","","","","19 Sep 2023","","","IEEE","IEEE Standards"
"Navigating The Battleground: An Analysis Of Adversarial Threats And Protections In Deep Neural Networks","V. Sehgal; S. Sharma; S. Pathak; K. Ahuja","Mahakal Institute of Technology, Ujjain, India; Mahakal Institute of Technology, Ujjain, India; Mahakal Institute of Technology, Ujjain, India; Mahakal Institute of Technology, Ujjain, India",2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG),"13 Mar 2025","2024","","","1","9","Deep learning techniques find broad applications in important areas such as malware detection systems, self-driving cars, and health care. However, it is still possible to attack deep learning soft intelligent models using adversarial example approaches. This study investigates the works and findings of recent research, whose central focus has been on adversarial machine learning, its weaknesses, and its application. Some of the focus has been on malware attacks that use deep learning frameworks, such as neural network based Jacobian saliency map attacks and anatomy of the Carlini and Wagner attacks. This has again posed a limitation to the present-day research on the increasing range of the actors and their role within the range of types of attack as well as strategies and development of countermeasures, including making biases and models to mitigate the threats. Furthermore, there is also a deficiency in the evaluation of defence mechanisms, particularly in developing appropriate parameters that would demonstrate the efficacy of the existing models. In conclusion, this paper presents communication and deliberations on areas that offer an interesting promise for future research in which the challenges experienced in applying deep learning systems will be satisfactorily addressed.","","979-8-3315-1898-1","10.1109/ICTBIG64922.2024.10911402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10911402","Adversarial Defence;Adversarial Examples;Adversarial Machine Learning;Adversarial Threats;Deep Learning;Model Robustness;Neural Network","Deep learning;Jacobian matrices;Navigation;Government;Medical services;Malware;Adversarial machine learning;Robustness;Autonomous automobiles;Protection","","","","31","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"Exploring ChatGPT App Ecosystem: Distribution, Deployment and Security","C. Yan; R. Ren; M. H. Meng; L. Wan; T. Y. Ooi; G. Bai","University of Queensland, Australia; University of Queensland, Australia; Technical University of Munich, Germany; University of Queensland, Australia; University of Queensland, Australia; University of Queensland, Australia",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","1370","1382","ChatGPT has enabled third-party developers to create plugins to expand ChatGPT’s capabilities. These plugins are distributed through OpenAI’s plugin store, making them easily accessible to users. With ChatGPT as the backbone, this app ecosystem has illustrated great business potential by offering users personalized services in a conversational manner. Nonetheless, many crucial aspects regarding app development, deployment, and security of this ecosystem have yet to be thoroughly studied in the research community, potentially hindering a broader adoption by both developers and users. In this work, we conduct the first comprehensive study of the Chat-GPT app ecosystem, aiming to illuminate its landscape for our research community. Our study examines the distribution and deployment models in the integration of LLMs and third-party apps, and assesses their security and privacy implications. We uncover an uneven distribution of functionality among ChatGPT plugins, highlighting prevalent and emerging topics. We also identify severe flaws in the authentication and user data protection for third-party app APIs integrated within LLMs, revealing a concerning status quo of security and privacy in this app ecosystem. Our work provides insights for the secure and sustainable development of this rapidly evolving ecosystem.CCS CONCEPTS• Security and privacy → Software and application security.","2643-1572","979-8-4007-1248-7","","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765055","Large Language Model;Testing;Security;Deployment","Privacy;Biological system modeling;Source coding;Ecosystems;Reverse engineering;Chatbots;Software;Security;Sustainable development;Software engineering","","","","59","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Abstract Book","",,"2020 International Conference on Computer, Control, Electrical, and Electronics Engineering (ICCCEEE)","17 May 2021","2021","","","1","105","Presents abstracts for the articles comprising the conference proceedings.","","978-1-7281-9111-9","10.1109/ICCCEEE49695.2021.9429649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9429649","","","","","","","IEEE","17 May 2021","","","IEEE","IEEE Conferences"
"Secure Continuous Delivery on Google Cloud: Implement an automated and secure software delivery pipeline on Google Cloud using native services","G. Galloro; N. Avery; D. Dorbin; R. Seroter",NA; NA; NA; NA,Secure Continuous Delivery on Google Cloud: Implement an automated and secure software delivery pipeline on Google Cloud using native services,"","2024","","","","","Build an end-to-end continuous delivery pipeline on Google Cloud and secure your software supply chain using GCP tools and services including Cloud Code, Cloud Workstations, Cloud Build, Artifact Registry, and Cloud DeployKey FeaturesGain hands-on experience building an end-to-end software delivery pipeline using Google Cloud servicesDeploy your applications on GKE, Cloud Run, and across hybrid and multi-cloud environmentsSecure pipelines with artifact scanning, dependency vulnerability checks, signed provenance, and admission controlPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionContinuous delivery, a cornerstone of modern software engineering, facilitates quick and secure software delivery using a robust toolkit encompassing automated builds, testing, source code management, artifact storage, and deployment. Whether you integrate tools from different providers or use a set of managed services from a single cloud provider, the goal is to streamline setup, integration, and management. This book focuses on continuous delivery on Google Cloud. Starting with an introduction to continuous delivery and secure software supply chain concepts, this book uses hands-on exercises to demonstrate how to continuously test your application with Skaffold and Cloud Code, leverage AI-assisted code generation with Cloud Code and Cloud Workstations, and automate your continuous integration with Cloud Build. You’ll see how to store and scan your software artifacts on Artifact Registry, orchestrate deployments with Cloud Deploy, and release your software on GKE and Cloud Run, configured to admit only trusted code. Using an example application, you’ll implement tools for creating an end-to-end delivery pipeline using Google Cloud services. By the end of this book, you’ll be able to build a secure software delivery pipeline from development to production using Google Cloud managed services and best practices.What you will learnCreate an end-to-end continuous delivery pipeline using Cloud Build, Artifact Registry, and Cloud DeployDevelop, build, and deploy container-based applications with Skaffold and Cloud CodeExperiment with AI-assisted code generation in Cloud CodeAutomate continuous integration with Cloud Build triggersAutomate deployment on GKE and Cloud Run through Cloud DeployEnhance pipeline security with Artifact Analysis, Binary Authorization, and SLSAApply best practices, including logging and monitoringWho this book is forThis book is for DevOps, Platform, and Cloud Engineers tasked with managing application deployment and creating continuous delivery pipelines who want to automate workflows in a fully managed, scalable, and secure platform. Software developers involved in application delivery and interested in harnessing Google Cloud tools to optimize development flow status and feedback loop will also find this book useful. Prior knowledge of Google Cloud fundamentals (including Cloud APIs and IAM), software delivery, containerization, and Kubernetes will enhance the reading experience.","","9781805127642","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769300.pdf&bkn=10769299&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"LLMs and the Future of Chip Design: Unveiling Security Risks and Building Trust","Z. Wang; L. Alrahis; L. Mankali; J. Knechtel; O. Sinanoglu","New York University, USA; New York University Abu Dhabi, UAE; New York University, USA; New York University Abu Dhabi, UAE; New York University Abu Dhabi, UAE",2024 IEEE Computer Society Annual Symposium on VLSI (ISVLSI),"25 Sep 2024","2024","","","385","390","Chip design is about to be revolutionized by the integration of large language, multimodal, and circuit models (collectively LxMs). While exploring this exciting frontier with tremendous potential, the community must also carefully consider the related security risks and the need for building trust into using LxMs for chip design. First, we review the recent surge of using LxMs for chip design in general. We cover state-of-the-art works for the automation of hardware description language code generation and for scripting and guidance of essential but cumbersome tasks for electronic design automation tools, e.g., design-space exploration, tuning, or designer training. Second, we raise and provide initial answers to novel research questions on critical issues for security and trustworthiness of LxM-powered chip design from both the attack and defense perspectives.","2159-3477","979-8-3503-5411-9","10.1109/ISVLSI61997.2024.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10682661","Electronic Design Automation;Integrated Circuits;Large Language Models;Hardware Security","Training;Reviews;Hardware security;Buildings;Very large scale integration;Chip scale packaging;Integrated circuit modeling","","2","","68","IEEE","25 Sep 2024","","","IEEE","IEEE Conferences"
"Create Your World: Lifelong Text-to-Image Diffusion","G. Sun; W. Liang; J. Dong; J. Li; Z. Ding; Y. Cong","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, China; Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE; School of Computer Science and Engineering, Nanjing University of Science and Technology, Jiangsu, China; Department of Computer Science, Tulane University, New Orleans, LA, USA; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China",IEEE Transactions on Pattern Analysis and Machine Intelligence,"6 Aug 2024","2024","46","9","6454","6470","Text-to-image generative models can produce diverse high-quality images of concepts with a text prompt, which have demonstrated excellent ability in image generation, image translation, etc. We in this work study the problem of synthesizing instantiations of a user's own concepts in a never-ending manner, i.e., create your world, where the new concepts from user are quickly learned with a few examples. To achieve this goal, we propose a Lifelong text-to-image Diffusion Model (L $^{2}$2 DM), which intends to overcome knowledge “catastrophic forgetting” for the past encountered concepts, and semantic “catastrophic neglecting” for one or more concepts in the text prompt. In respect of knowledge “catastrophic forgetting”, our L $^{2}$2 DM framework devises a task-aware memory enhancement module and an elastic-concept distillation module, which could respectively safeguard the knowledge of both prior concepts and each past personalized concept. When generating images with a user text prompt, the solution to semantic “catastrophic neglecting” is that a concept attention artist module can alleviate the semantic neglecting from concept aspect, and an orthogonal attention module can reduce the semantic binding from attribute aspect. To the end, our model can generate more faithful image across a range of continual text prompts in terms of both qualitative and quantitative metrics, when comparing with the related state-of-the-art models.","1939-3539","","10.1109/TPAMI.2024.3382753","National Natural Science Foundation of China(grant numbers:62225310,62127807,62273333,62133005); CAS-Youth Innovation Promotion Association Scholarship(grant numbers:2023207); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489849","Continual learning;image generation;lifelong machine learning;stable diffusion","Task analysis;Dogs;Computational modeling;Semantics;Training;Neural networks;Electronic mail","","7","","44","IEEE","4 Apr 2024","","","IEEE","IEEE Journals"
"Confidential Machine Learning Computation in Untrusted Environments: A Systems Security Perspective","K. D. Duy; T. Noh; S. Huh; H. Lee","Department of Computer Science and Engineering, Sungkyunkwan University, Natural Sciences Campus, Suwon-si, Gyeonggi-do, South Korea; Department of Computer Science and Engineering, Sungkyunkwan University, Natural Sciences Campus, Suwon-si, Gyeonggi-do, South Korea; Department of Computer Science and Engineering, Sungkyunkwan University, Natural Sciences Campus, Suwon-si, Gyeonggi-do, South Korea; Department of Computer Science and Engineering, Sungkyunkwan University, Natural Sciences Campus, Suwon-si, Gyeonggi-do, South Korea",IEEE Access,"30 Dec 2021","2021","9","","168656","168677","As machine learning (ML) technologies and applications are rapidly changing many computing domains, security issues associated with ML are also emerging. In the domain of systems security, many endeavors have been made to ensure ML model and data confidentiality. ML computations are often inevitably performed in untrusted environments and entail complex multi-party security requirements. Hence, researchers have leveraged the Trusted Execution Environments (TEEs) to build confidential ML computation systems. We conduct a systematic and comprehensive survey by classifying attack vectors and mitigation in confidential ML computation in untrusted environments, analyzing the complex security requirements in multi-party scenarios, and summarizing engineering challenges in confidential ML implementation. Lastly, we suggest future research directions based on our study.","2169-3536","","10.1109/ACCESS.2021.3136889","National Research Foundation of Korea (NRF); Korea government (Ministry of Science and ICT (MSIT))(grant numbers:NRF-2020R1C1C1011980); Institute for Information & communication Technology Promotion (IITP); Korea government (MSIT)(grant numbers:2019-0-01343); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9656734","Confidential machine learning computation;trusted execution;side-channel attacks;multi-party ML computation","Security;Computational modeling;Hardware;Data models;Software;Machine learning;Codes","","5","","149","CCBY","20 Dec 2021","","","IEEE","IEEE Journals"
"A RAG based Personal Placement Assistant System using Large Language Models for Customized Interview Preparation","S. Patel; J. Patel; D. Shah; P. Goel; B. Patel","Computer Science & Engineering Department, Devang Patel Institute of Advance Technology and Research (DEPSTAR), Charotar University of Science and Technology (CHARUSAT), Anand, India; Computer Science & Engineering Department, Devang Patel Institute of Advance Technology and Research (DEPSTAR), Charotar University of Science and Technology (CHARUSAT), Anand, India; Computer Science & Engineering Department, Devang Patel Institute of Advance Technology and Research (DEPSTAR), Charotar University of Science and Technology (CHARUSAT), Anand, India; Computer Science & Engineering Department, Devang Patel Institute of Advance Technology and Research (DEPSTAR), Charotar University of Science and Technology (CHARUSAT), Anand, India; Computer Science & Engineering Department, Devang Patel Institute of Advance Technology and Research (DEPSTAR), Charotar University of Science and Technology (CHARUSAT), Anand, India",2024 5th International Conference on Data Intelligence and Cognitive Informatics (ICDICI),"31 Dec 2024","2024","","","1468","1475","This paper introduces a Personal Placement Assistant (PPA) framework that utilizes advanced Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) to automate and personalize job placement preparation. The system integrates Natural Language Processing (NLP) techniques, including text embedding using the all-MiniLM-L6-v2 transformer model and semantic retrieval using ChromaDB for accurate resume analysis and context-aware question generation. The PPA is structured into three core components: the Retriever, using PyMuPDF for resume parsing and recursive text chunking for efficient vector storage and search; the Analyzer, employing the Google Gemini-1.5-flash model for domain extraction and percentage-based content profiling; and the Generator, which produces domain-specific MCQs, coding challenges, and interview questions aligned with Bloom’s Taxonomy. RAG enhances the system’s ability to integrate external knowledge, improving the contextual relevance of the generated content. Evaluation results demonstrate an 83.77% accuracy in domainspecific extraction and question generation, confirming the PPA’s effectiveness in automating personalized job preparation across industries.","","979-8-3503-8960-9","10.1109/ICDICI62993.2024.10810811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10810811","Retrieval Augmented Generation (RAG);Large Language Models (LLMs);Transformers;Personal Placement Assistant (PPA);ChromaDB;Interview Preparation","Analytical models;Accuracy;Large language models;Retrieval augmented generation;Resumes;Transformer cores;Transformers;Question generation;Vectors;Interviews","","","","25","IEEE","31 Dec 2024","","","IEEE","IEEE Conferences"
"Crafting Secure Software: An engineering leader's guide to security by design","G. Bulmash; T. Segura",NA; NA,Crafting Secure Software: An engineering leader's guide to security by design,"","2024","","","","","Gain a solid understanding of the threat landscape and discover best practices to protect your software factory throughout the SDLC, with valuable insights from security experts at GitGuardianKey FeaturesDevelop a strong security posture by grasping key attack vectors in the SDLCImplement industry-leading best practices to protect software from evolving threatsUtilize legislative and regulatory landscapes to mitigate compliance-related costsBook DescriptionDrawing from GitGuardian's extensive experience in securing millions of lines of code for organizations worldwide, Crafting Secure Software takes you on an exhaustive journey through the complex world of software security and prepares you to face current and emerging security challenges confidently. Authored by security experts, this book provides unique insights into the software development lifecycle (SDLC) and delivers actionable advice to help you mitigate and prevent risks. From securing code-writing tools and secrets to ensuring the integrity of the source code and delivery pipelines, you’ll get a good grasp on the threat landscape, uncover best practices for protecting your software, and craft recommendations for future-proofing against upcoming security regulations and legislation. By the end of this book, you’ll have gained a clear vision of the improvements needed in your security posture, along with concrete steps to implement them, empowering you to make informed decisions and take decisive action in safeguarding your software assets.What you will learnGet to grips with security trends and GitGuardian's role in modern softwareAnalyze major security breaches and their impact on the industryDevelop a threat model tailored to your business and risk appetiteImplement security measures across your entire SDLCSecure secrets within codebases, configurations, and artifactsDesign and maintain secure build pipelines and deployment setupsNavigate security compliance, including current and future lawsPrepare for future security with AI-generated code integrationWho this book is forThis book is an essential read for security and IT leaders navigating the complexities of modern software development. The book is also useful for chief security officers (CSOs), chief information security officers (CISOs), security architects, DevOps professionals, and IT decision makers. A basic understanding of software engineering, version control, and build and delivery mechanisms is needed. This guide will empower you to comprehend and mitigate threats in today's dynamic software factories, regardless of your technical depth.","","9781835885079","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769314.pdf&bkn=10769313&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Class-conditional domain adaptation for semantic segmentation","Y. Wang; Y. Li; J. H. Elder; R. Wu; H. Lu","School of Information and Communication Engineering, Dalian University of Technology, Dalian 116024, China; School of Computer Science, Wuhan University, Wuhan 430072, China; Department of Electrical Engineering and Computer Science, York University, Toronto M3J 1P3, Canada; Department of Computer Science, the University of Hong Kong, Hong Kong 999077, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian 116024, China",Computational Visual Media,"13 Feb 2025","2024","10","5","1013","1030","Semantic segmentation is an important sub-task for many applications. However, pixel-level ground-truth labeling is costly, and there is a tendency to overfit to training data, thereby limiting the generalization ability. Unsupervised domain adaptation can potentially address these problems by allowing systems trained on labelled datasets from the source domain (including less expensive synthetic domain) to be adapted to a novel target domain. The conventional approach involves automatic extraction and alignment of the representations of source and target domains globally. One limitation of this approach is that it tends to neglect the differences between classes: representations of certain classes can be more easily extracted and aligned between the source and target domains than others, limiting the adaptation over all classes. Here, we address this problem by introducing a Class-Conditional Domain Adaptation (CCDA) method. This incorporates a class-conditional multi-scale discriminator and class-conditional losses for both segmentation and adaptation. Together, they measure the segmentation, shift the domain in a class-conditional manner, and equalize the loss over classes. Experimental results demonstrate that the performance of our CCDA method matches, and in some cases, surpasses that of state-of-the-art methods.","2096-0662","","10.1007/s41095-023-0362-4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10885014","domain adaptation;generative adversarial networks;semantic segmentation;cityscapes","Semantic segmentation;Loss measurement;Training;Semantics;Feature extraction;Decoding;Translation;Adversarial machine learning;Visualization;Limiting","","","","","","13 Feb 2025","","","TUP","TUP Journals"
"Threat Intelligence and Advanced Threat Hunting","A. Basta; N. Basta; W. Anwar; M. I. Essar","NA; Reinhardt University, GA, USA; NA; NA","Open-Source Security Operations Center (SOC): A Complete Guide to Establishing, Managing, and Maintaining a Modern SOC","","2025","","","361","393","Summary <p>Threat intelligence and proactive hunting empower security teams to uncover sophisticated threats that evade traditional protective controls. Intelligence provides context to focus hunts while hunting informs intelligence analysis to drive detection engineering. Advanced methodologies fuse an outside perspective from threat intelligence with inside––out defenses structured across high‐value data, identities, and systems. Scaling beyond manual processes, purpose‐built threat intelligence also directly fuels automated incident response by keeping pace with attacker innovation through continually updated countermeasures applied instantly against emergent infrastructure linked to known campaigns. In today's interconnected world, cyberattacks aimed at financial gain rather than ideological or geopolitical motives are an ever‐present threat to businesses. Cloud environments require updated threat‐hunting techniques addressing expanded attack surfaces across fragmented infrastructure sprawl, accessing sensitive data through managed services with restricted monitoring increasingly provisioned outside security team visibility.</p>","","9781394201617","10.1002/9781394201631.ch13","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10896990.pdf&bkn=10896967&pdfType=chapter","","Security;Malware;Codes;Telemetry;Vectors;Training;Surface reconstruction;Real-time systems;Prevention and mitigation;Phishing","","","","","","20 Feb 2025","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications","Y. Zhang; Y. Zhao; Z. Li; X. Cheng; Y. Wang; O. Kotevska; P. S. Yu; T. Derr","Vanderbilt University, Nashville, TN, USA; Vanderbilt University, Nashville, TN, USA; The Chinese University of Hong Kong, Hong Kong, China; Vanderbilt University, Nashville, TN, USA; Vanderbilt University, Nashville, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; University of Illinois Chicago, Chicago, IL, USA; Vanderbilt University, Nashville, TN, USA",IEEE Transactions on Knowledge and Data Engineering,"12 Nov 2024","2024","36","12","7497","7515","Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.","1558-2191","","10.1109/TKDE.2024.3454328","National Science Foundation(grant numbers:IIS2239881); Home Depot; Snap; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10693287","Deep learning on graphs;graph neural networks;privacy attacks;privacy preservation","Privacy;Data privacy;Deep learning;Aggregates;Training;Surveys;Electronic mail","","1","","172","CCBY","24 Sep 2024","","","IEEE","IEEE Journals"
"DevSecOps for Azure: End-to-end supply chain security for GitHub, Azure DevOps, and the Azure cloud","D. Okeyode; J. Kirui; S. Hanselman",NA; NA; NA,"DevSecOps for Azure: End-to-end supply chain security for GitHub, Azure DevOps, and the Azure cloud","","2024","","","","","Gain holistic insights and practical expertise in embedding security within the DevOps pipeline, specifically tailored for Azure cloud environments Key FeaturesLearn how to integrate security into Azure DevOps workflows for cloud infrastructureFind out how to integrate secure practices across all phases of the Azure DevOps workflow, from planning to monitoringHarden the entire DevOps workflow, from planning and coding to source control, CI, and cloud workload deploymentPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionBusinesses must prioritize security, especially when working in the constantly evolving Azure cloud. However, many organizations struggle to maintain security and compliance. Attackers are increasingly targeting software development processes, making software supply chain security crucial. This includes source control systems, build systems, CI/CD platforms, and various artifacts. With the help of this book, you’ll be able to enhance security and compliance in Azure software development processes. Starting with an overview of DevOps and its relationship with Agile methodologies and cloud computing, you'll gain a solid foundation in DevSecOps principles. The book then delves into the security challenges specific to DevOps workflows and how to address them effectively. You'll learn how to implement security measures in the planning phase, including threat modeling and secure coding practices. You'll also explore pre-commit security controls, source control security, and the integration of various security tools in the build and test phases. The book covers crucial aspects of securing the release and deploy phases, focusing on artifact integrity, infrastructure as code security, and runtime protection. By the end of this book, you’ll have the knowledge and skills to implement a secure code-to-cloud process for the Azure cloud.What you will learnUnderstand the relationship between Agile, DevOps, and the cloudSecure the use of containers in a CI/CD workflowImplement a continuous and automated threat modeling processSecure development toolchains such as GitHub Codespaces, Microsoft Dev Box, and GitHubIntegrate continuous security throughout the code development workflow, pre-source and post-source control contributionIntegrate SCA, SAST, and secret scanning into the build process to ensure code safetyImplement security in release and deploy phases for artifact and environment complianceWho this book is forThis book is for security professionals and developers transitioning to a public cloud environment or moving towards a DevSecOps paradigm. It's also designed for DevOps engineers, or anyone looking to master the implementation of DevSecOps in a practical manner. Individuals who want to understand how to integrate security checks, testing, and other controls into Azure cloud continuous delivery pipelines will also find this book invaluable. Prior knowledge of DevOps principles and practices, as well as an understanding of security fundamentals will be beneficial. ","","9781837633333","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10803971.pdf&bkn=10803970&pdfType=book","","","","","","","","16 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"Practical Data Science with Python: Learn tools and techniques from hands-on examples to extract insights from data","N. George",NA,Practical Data Science with Python: Learn tools and techniques from hands-on examples to extract insights from data,"","2021","","","","","Learn to effectively manage data and execute data science projects from start to finish using PythonKey FeaturesUnderstand and utilize data science tools in Python, such as specialized machine learning algorithms and statistical modelingBuild a strong data science foundation with the best data science tools available in PythonAdd value to yourself, your organization, and society by extracting actionable insights from raw dataBook DescriptionPractical Data Science with Python teaches you core data science concepts, with real-world and realistic examples, and strengthens your grip on the basic as well as advanced principles of data preparation and storage, statistics, probability theory, machine learning, and Python programming, helping you build a solid foundation to gain proficiency in data science. The book starts with an overview of basic Python skills and then introduces foundational data science techniques, followed by a thorough explanation of the Python code needed to execute the techniques. You'll understand the code by working through the examples. The code has been broken down into small chunks (a few lines or a function at a time) to enable thorough discussion. As you progress, you will learn how to perform data analysis while exploring the functionalities of key data science Python packages, including pandas, SciPy, and scikit-learn. Finally, the book covers ethics and privacy concerns in data science and suggests resources for improving data science skills, as well as ways to stay up to date on new data science developments. By the end of the book, you should be able to comfortably use Python for basic data science projects and should have the skills to execute the data science process on any data source.What you will learnUse Python data science packages effectivelyClean and prepare data for data science work, including feature engineering and feature selectionData modeling, including classic statistical models (such as t-tests), and essential machine learning algorithms, such as random forests and boosted modelsEvaluate model performanceCompare and understand different machine learning methodsInteract with Excel spreadsheets through PythonCreate automated data science reports through PythonGet to grips with text analytics techniquesWho this book is forThe book is intended for beginners, including students starting or about to start a data science, analytics, or related program (e.g. Bachelor’s, Master’s, bootcamp, online courses), recent college graduates who want to learn new skills to set them apart in the job market, professionals who want to learn hands-on data science techniques in Python, and those who want to shift their career to data science. The book requires basic familiarity with Python. A ""getting started with Python"" section has been included to get complete novices up to speed.","","9781801076654","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162807.pdf&bkn=10162806&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Detecting Latent Topics and Trends in Software Engineering Research Since 1980 Using Probabilistic Topic Modeling","F. Gurcan; G. G. M. Dalveren; N. E. Cagiltay; A. Soylu","Department of Computer Engineering, Faculty of Engineering, Karadeniz Technical University, Trabzon, Turkey; Department of Software Engineering, Faculty of Engineering, Atilim University, Ankara, Turkey; Department of Software Engineering, Faculty of Engineering, Atilim University, Ankara, Turkey; Department of Computer Science, Norwegian University of Science and Technology, Gjøvik, Norway",IEEE Access,"20 Jul 2022","2022","10","","74638","74654","The landscape of software engineering research has changed significantly from one year to the next in line with industrial needs and trends. Therefore, today’s research literature on software engineering has a rich and multidisciplinary content that includes a large number of studies; however, not many of them demonstrate a holistic view of the field. From this perspective, this study aimed to reveal a holistic view that reflects topics, trends, and trajectories in software engineering research by analyzing the majority of domain-specific articles published over the last 40 years. This study first presents an objective and systematic method for corpus creation through major publication sources in the field. A corpus was then created using this method, which includes 44 domain-specific conferences and journals and 57,174 articles published between 1980 and 2019. Next, this corpus was analyzed using an automated text-mining methodology based on a probabilistic topic-modeling approach. As a result of this analysis, 24 main topics were found. In addition, topical trends in the field were revealed. Finally, three main developmental stages of the field were identified as: the programming age, the software development age, and the software optimization age.","2169-3536","","10.1109/ACCESS.2022.3190632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9828025","Corpus creation;research trends and topics;software engineering;text mining;topic model","Market research;Systematics;Software engineering;Bibliometrics;Text mining","","15","","64","CCBY","13 Jul 2022","","","IEEE","IEEE Journals"
"Towards More Realistic Membership Inference Attacks on Large Diffusion Models","J. Dubiński; A. Kowalczuk; S. Pawlak; P. Rokita; T. Trzcinski; P. Morawiecki",Warsaw University of Technology; Warsaw University of Technology; Warsaw University of Technology; Warsaw University of Technology; Warsaw University of Technology; Polish Academy of Sciences,2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),"9 Apr 2024","2024","","","4848","4857","Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known in the cybersecurity community as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a new dataset to establish a fair evaluation setup and apply it to Stable Diffusion, also applicable to other generative models. With the proposed dataset, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not provide a full understanding of the effectiveness of membership inference attacks. We conclude that the membership inference attack remains a significant challenge for large diffusion models (often deployed as black-box systems), indicating that related privacy and copyright issues will persist in the foreseeable future.","2642-9381","979-8-3503-1892-0","10.1109/WACV57701.2024.00479","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483613","Algorithms;Explainable;fair;accountable;privacy-preserving;ethical computer vision","Training;Privacy;Data privacy;Computer vision;Computational modeling;Closed box;Reliability","","3","","36","IEEE","9 Apr 2024","","","IEEE","IEEE Conferences"
"Quantitative Biosciences Companion in MATLAB: Dynamics across Cells, Organisms, and Populations","J. S. Weitz; B. Taylor",NA; NA,"Quantitative Biosciences Companion in MATLAB: Dynamics across Cells, Organisms, and Populations","","2024","","","","","A hands-on lab guide in the MATLAB programming language that enables students in the life sciences to reason quantitatively about living systems across scalesThis lab guide accompanies the textbook Quantitative Biosciences, providing students with the skills they need to translate biological principles and mathematical concepts into computational models of living systems. This hands-on guide uses a case study approach organized around central questions in the life sciences, introducing landmark advances in the field while teaching students—whether from the life sciences, physics, computational sciences, engineering, or mathematics—how to reason quantitatively in the face of uncertainty.Draws on real-world case studies in molecular and cellular biosciences, organismal behavior and physiology, and populations and ecological communitiesEncourages good coding practices, clear and understandable modeling, and accessible presentation of resultsHelps students to develop a diverse repertoire of simulation approaches, enabling them to model at the appropriate scaleBuilds practical expertise in a range of methods, including sampling from probability distributions, stochastic branching processes, continuous time modeling, Markov chains, bifurcation analysis, partial differential equations, and agent-based simulationsBridges the gap between the classroom and research discovery, helping students to think independently, troubleshoot and resolve problems, and embark on research of their ownStand-alone computational lab guides for Quantitative Biosciences also available in Python and R","","9780691259628","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10614675.pdf&bkn=10614674&pdfType=book","Pars;Dynamics;Code;Distribution;Probability;Function;Tmph;Random;Parameters;Protein;Cells;Prey;Population;Simulation;Event;Hawks;Stochastic;Predator;Variables;Mean;Equations;Numbers;Equilibrium;Models;Transition;Game;Space;Plot;= pars;Dove;Mrna;Data;Rates;Laboratory;Lab;Simulate;Matrix;Rand;Systems;Position;Trajectories;Exponential;Payoff;Direction;Fraction;Solution;Growth;Zeros;Dynamical;Command;Link;Length;Basis;Phase;Core;Note;Sum;Voltage;Player;Equal;Focal;Gca;Generation;Trajectory;= zeros;Poisson;Line;Gene;Points;Cdf","","","","","","","30 Jul 2024","","","Princeton University Press","Princeton University Press eBooks"
"Quantitative Biosciences Companion in Python: Dynamics across Cells, Organisms, and Populations","J. S. Weitz; N. English; A. B. Lee; A. Zamani",NA; NA; NA; NA,"Quantitative Biosciences Companion in Python: Dynamics across Cells, Organisms, and Populations","","2024","","","","","A hands-on lab guide in the Python programming language that enables students in the life sciences to reason quantitatively about living systems across scalesThis lab guide accompanies the textbook Quantitative Biosciences, providing students with the skills they need to translate biological principles and mathematical concepts into computational models of living systems. This hands-on guide uses a case study approach organized around central questions in the life sciences, introducing landmark advances in the field while teaching students—whether from the life sciences, physics, computational sciences, engineering, or mathematics—how to reason quantitatively in the face of uncertainty.Draws on real-world case studies in molecular and cellular biosciences, organismal behavior and physiology, and populations and ecological communitiesEncourages good coding practices, clear and understandable modeling, and accessible presentation of resultsHelps students to develop a diverse repertoire of simulation approaches, enabling them to model at the appropriate scaleBuilds practical expertise in a range of methods, including sampling from probability distributions, stochastic branching processes, continuous time modeling, Markov chains, bifurcation analysis, partial differential equations, and agent-based simulationsBridges the gap between the classroom and research discovery, helping students to think independently, troubleshoot and resolve problems, and embark on research of their ownStand-alone computational lab guides for Quantitative Biosciences also available in R and MATLAB","","9780691259611","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10614681.pdf&bkn=10614680&pdfType=book","Initial;Array;Zeros;Variables;= pars;Mean;Result;Numbers;Current;Equilibrium;Models;Exponential;Transition;Space;Dove;Boid;Integrate;Mrna;Matrix;Equations;Color=;Uniform;Rates;Run;Len;Laboratory;Lab;Simulate;Odeint;Data;Direction;Systems;Trajectories;Stochastic;Hawk;Event;Predator;Np random;Simulation;Plt;Population;Range;Cells;Prey;Protein;Values;Probability;Parameters;Function;Distribution;Random;Plot;Code;Dynamics;= np;Pars;Np;Payoff;= plt;Points;Fraction;Lambda;Gamma;Link;Dyn;Np array;Growth;Game;Dynamical;Note","","","","","","","30 Jul 2024","","","Princeton University Press","Princeton University Press eBooks"
"Quantitative Biosciences Companion in R: Dynamics across Cells, Organisms, and Populations","J. S. Weitz; M. Domínguez-Mirazo",NA; NA,"Quantitative Biosciences Companion in R: Dynamics across Cells, Organisms, and Populations","","2024","","","","","A hands-on lab guide in the R programming language that enables students in the life sciences to reason quantitatively about living systems across scalesThis lab guide accompanies the textbook Quantitative Biosciences, providing students with the skills they need to translate biological principles and mathematical concepts into computational models of living systems. This hands-on guide uses a case study approach organized around central questions in the life sciences, introducing landmark advances in the field while teaching students—whether from the life sciences, physics, computational sciences, engineering, or mathematics—how to reason quantitatively in the face of uncertainty.Draws on real-world case studies in molecular and cellular biosciences, organismal behavior and physiology, and populations and ecological communitiesEncourages good coding practices, clear and understandable modeling, and accessible presentation of resultsHelps students to develop a diverse repertoire of simulation approaches, enabling them to model at the appropriate scaleBuilds practical expertise in a range of methods, including sampling from probability distributions, stochastic branching processes, continuous time modeling, Markov chains, bifurcation analysis, partial differential equations, and agent-based simulationsBridges the gap between the classroom and research discovery, helping students to think independently, troubleshoot and resolve problems, and embark on research of their ownStand-alone computational lab guides for Quantitative Biosciences also available in Python and MATLAB","","9780691259604","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10614699.pdf&bkn=10614698&pdfType=book","Matrix;Result;Equations;Mean;Numbers;Current;Denotes;Mrna;Equilibrium;Models;Transition;Pars;Length;Space;Dove;= element_text;Element_text;Rates;Title =;Title;Lab;Laboratory;Simulate;Systems;Color=;Axis;Fraction;Line;Trajectories;Exponential;Ggplot;Hawks;Plot;Frame;Predator;Stochastic;Event;Variables;Initial;Simulation;Values;Population;Data;Cells;Prey;= function;Protein;Random;Parameters;Probability;Aes;Distribution;Function;Code;Dynamics;Data frame;Df;Behavior;Focal;Variance;= data;Markov;Sum;Exponentially;Element;Snippet;Gait;Transition matrix;Production;Positive","","","","","","","30 Jul 2024","","","Princeton University Press","Princeton University Press eBooks"
"Hiding Images within Images","S. Baluja","Google-AI, Google, Inc., San Diego, USA",IEEE Transactions on Pattern Analysis and Machine Intelligence,"4 Jun 2020","2020","42","7","1685","1697","We present a system to hide a full color image inside another of the same size with minimal quality loss to either image. Deep neural networks are simultaneously trained to create the hiding and revealing processes and are designed to specifically work as a pair. The system is trained on images drawn randomly from the ImageNet database, and works well on natural images from a wide variety of sources. Beyond demonstrating the successful application of deep learning to hiding images, we examine how the result is achieved and apply numerous transformations to analyze if image quality in the host and hidden image can be maintained. These transformation range from simple image manipulations to sophisticated machine learning-based adversaries. Two extensions to the basic system are presented that mitigate the possibility of discovering the content of the hidden image. With these extensions, not only can the hidden information be kept secure, but the system can be used to hide even more than a single image. Applications for this technology include image authentication, digital watermarks, finding exact regions of image manipulation, and storing meta-information about image rendering and content.","1939-3539","","10.1109/TPAMI.2019.2901877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8654686","Information hiding;image verification;image trust","Containers;Neural networks;Image coding;Image reconstruction;Image color analysis;Training;Receivers","","150","","66","CCBY","28 Feb 2019","","","IEEE","IEEE Journals"
"A Survey and Tutorial on Security and Resilience of Quantum Computing","A. A. Saki; M. Alam; K. Phalak; A. Suresh; R. O. Topaloglu; S. Ghosh","School of EECS, Pennsylvania State University, PA; School of EECS, Pennsylvania State University, PA; School of EECS, Pennsylvania State University, PA; School of EECS, Pennsylvania State University, PA; IBM Corporation, Hopewell Junction, NY; School of EECS, Pennsylvania State University, PA",2021 IEEE European Test Symposium (ETS),"29 Jun 2021","2021","","","1","10","Present-day quantum computers suffer from various noises or errors such as, gate error, relaxation, dephasing, readout error, and crosstalk. Besides, they offer a limited number of qubits with restrictive connectivity. Therefore, quantum programs running these computers face resilience issues and low output fidelities. The noise in the cloud-based access of quantum computers also introduce new modes of security and privacy issues. Furthermore, quantum computers face several threat models from insider and outsider adversaries including input tampering, program misallocation, fault injection, Reverse Engineering (RE) and Cloning. This paper provides an overview of various assets embedded in quantum computers and programs, vulnerabilities and attack models and the relation between resilience and security. We also cover countermeasures against the reliability and security issues and present future outlook for security of quantum computing.","1558-1780","978-1-6654-1849-2","10.1109/ETS50041.2021.9465397","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9465397","Quantum Computing;Security;Privacy;Resilience;Fault Injection;Reverse Engineering","Computers;Privacy;Quantum computing;Computational modeling;Reverse engineering;Tutorials;Security","","18","","69","IEEE","29 Jun 2021","","","IEEE","IEEE Conferences"
"Recent advances in 3D Gaussian splatting","T. Wu; Y. -J. Yuan; L. -X. Zhang; J. Yang; Y. -P. Cao; L. -Q. Yan; L. Gao","Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Tencent AI Lab, Beijing 100089, China; VAST, Beijing 100000, China; Department of Computer Science, University of California, Santa Barbara, CA 93106, USA; Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China",Computational Visual Media,"20 Feb 2025","2024","10","4","613","642","The emergence of 3D Gaussian splatting (3DGS) has greatly accelerated rendering in novel view synthesis. Unlike neural implicit representations like neural radiance fields (NeRFs) that represent a 3D scene with position and viewpoint-conditioned neural networks, 3D Gaussian splatting utilizes a set of Gaussian ellipsoids to model the scene so that efficient rendering can be accomplished by rasterizing Gaussian ellipsoids into images. Apart from fast rendering, the explicit representation of 3D Gaussian splatting also facilitates downstream tasks like dynamic reconstruction, geometry editing, and physical simulation. Considering the rapid changes and growing number of works in this field, we present a literature review of recent 3D Gaussian splatting methods, which can be roughly classified by functionality into 3D reconstruction, 3D editing, and other downstream applications. Traditional point-based rendering methods and the rendering formulation of 3D Gaussian splatting are also covered to aid understanding of this technique. This survey aims to help beginners to quickly get started in this field and to provide experienced researchers with a comprehensive overview, aiming to stimulate future development of the 3D Gaussian splatting representation.","2096-0662","","10.1007/s41095-024-0436-y","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897713","3D Gaussian splatting (3DGS);radiance field;novel view synthesis;3D editing;scene generation","Three-dimensional displays;Rendering (computer graphics);Neural radiance field;Image color analysis;Image coding;Ellipsoids;Solid modeling;Training;Geometry;Image reconstruction","","5","","","","20 Feb 2025","","","TUP","TUP Journals"
"Average Sparse Attention for Dense Video Captioning From Multiperspective Edge-Computing Cameras","L. -H. Huang; C. -H. Lu","Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan",IEEE Systems Journal,"4 Dec 2024","2024","18","4","1939","1950","In recent years, the artificial intelligence of things (AIoT) has accelerated the development of edge computing. Since existing edge computing for dense video captioning has only explored single-camera decision-making, we propose a lightweight image stitching model that uses a proposed inverted pruned residual model to realize multicamera decision-making to generate more accurate captions. Existing dense video captioning uses an intensive attention mechanism, which readily results in the loss of important information. Thus, our study proposes an average sparse attention mechanism such that the resultant dense video-captioning model is better able to focus on important information and improve the quality of its generated captions. The experiments show that the lightweight video stitching model can reduce model parameters by 13.40% and increase frames per second by 28.96% on an edge platform when compared to the latest studies. Furthermore, a dense video caption network with the average sparse attention mechanism yielded improvements of 22.97% for BLEU3, 35.04% for BLEU4, and 7.51% for METEOR.","1937-9234","","10.1109/JSYST.2024.3456864","National Science and Technology Council(grant numbers:NSTC 113-2221-E-011-094-MY2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10703164","Dense video captioning;edge computing;image stitching;Internet of Things;lightweight neural networks;sparse attention","Cameras;Computational modeling;Attention mechanisms;Image edge detection;Accuracy;Image coding;Detectors;Synthesizers;Streaming media;Image annotation;Neural networks","","","","34","IEEE","2 Oct 2024","","","IEEE","IEEE Journals"
"Title Page","",,"2023 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","29 Jan 2024","2023","","","1","136","","2837-116X","979-8-3503-1672-8","10.1109/ICSPCC59353.2023.10400363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10400363","","Signal processing;Speech processing;Codes;Wireless communication;Urban areas;Uncertainty;Schedules","","","","","IEEE","29 Jan 2024","","","IEEE","IEEE Conferences"
"Python Data Cleaning Cookbook: Prepare your data for analysis with pandas, NumPy, Matplotlib, scikit-learn, and OpenAI","M. Walker",NA,"Python Data Cleaning Cookbook: Prepare your data for analysis with pandas, NumPy, Matplotlib, scikit-learn, and OpenAI","","2024","","","","","Learn the intricacies of data description, issue identification, and practical problem-solving, armed with essential techniques and expert tips.Key FeaturesGet to grips with new techniques for data preprocessing and cleaning for machine learning and NLP modelsUse new and updated AI tools and techniques for data cleaning tasksClean, monitor, and validate large data volumes to diagnose problems using cutting-edge methodologies including Machine learning and AIBook DescriptionJumping into data analysis without proper data cleaning will certainly lead to incorrect results. The Python Data Cleaning Cookbook - Second Edition will show you tools and techniques for cleaning and handling data with Python for better outcomes. Fully updated to the latest version of Python and all relevant tools, this book will teach you how to manipulate and clean data to get it into a useful form. he current edition focuses on advanced techniques like machine learning and AI-specific approaches and tools for data cleaning along with the conventional ones. The book also delves into tips and techniques to process and clean data for ML, AI, and NLP models. You will learn how to filter and summarize data to gain insights and better understand what makes sense and what does not, along with discovering how to operate on data to address the issues you've identified. Next, you’ll cover recipes for using supervised learning and Naive Bayes analysis to identify unexpected values and classification errors and generate visualizations for exploratory data analysis (EDA) to identify unexpected values. Finally, you’ll build functions and classes that you can reuse without modification when you have new data. By the end of this Data Cleaning book, you'll know how to clean data and diagnose problems within it.What you will learnUsing OpenAI tools for various data cleaning tasksProducing summaries of the attributes of datasets, columns, and rowsAnticipating data-cleaning issues when importing tabular data into pandasApplying validation techniques for imported tabular dataImproving your productivity in pandas by using method chainingRecognizing and resolving common issues like dates and IDsSetting up indexes to streamline data issue identificationUsing data cleaning to prepare your data for ML and AI modelsWho this book is forThis book is for anyone looking for ways to handle messy, duplicate, and poor data using different Python tools and techniques. The book takes a recipe-based approach to help you to learn how to clean and manage data with practical examples. Working knowledge of Python programming is all you need to get the most out of the book.","","9781803246291","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769288.pdf&bkn=10769287&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"API Security for White Hat Hackers: Uncover offensive defense strategies and get up to speed with secure API implementation","C. Staveley; C. Romeo",NA; NA,API Security for White Hat Hackers: Uncover offensive defense strategies and get up to speed with secure API implementation,"","2024","","","","","Become an API security professional and safeguard your applications against threats with this comprehensive guide Key FeaturesGain hands-on experience in testing and fixing API security flaws through practical exercisesDevelop a deep understanding of API security to better protect your organization's dataIntegrate API security into your company's culture and strategy, ensuring data protectionPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionAPIs have evolved into an essential part of modern applications, making them an attractive target for cybercriminals. Written by a multi-award-winning cybersecurity leader , this comprehensive guide offers practical insights into testing APIs, identifying vulnerabilities, and fixing them. With a focus on hands-on learning, this book guides you through securing your APIs in a step-by-step manner. You'll learn how to bypass authentication controls, circumvent authorization controls, and identify vulnerabilities in APIs using open-source and commercial tools. Moreover, you'll gain the skills you need to write comprehensive vulnerability reports and recommend and implement effective mitigation strategies to address the identified vulnerabilities. This book isn't just about hacking APIs; it's also about understanding how to defend them. You'll explore various API security management strategies and understand how to use them to safeguard APIs against emerging threats. By the end of this book, you'll have a profound understanding of API security and how to defend against the latest threats. Whether you're a developer, security professional, or ethical hacker, this book will ensure that your APIs are secure and your organization's data is protected.What you will learnImplement API security best practices and industry standardsConduct effective API penetration testing and vulnerability assessmentsImplement security measures for API security managementUnderstand threat modeling and risk assessment in API securityGain proficiency in defending against emerging API security threatsBecome well-versed in evasion techniques and defend your APIs against themIntegrate API security into your DevOps workflowImplement API governance and risk management initiatives like a proWho this book is forIf you’re a cybersecurity professional, web developer, or software engineer looking to gain a comprehensive understanding of API security, this book is for you. The book is ideal for those who have beginner to advanced-level knowledge of cybersecurity and API programming concepts. Professionals involved in designing, developing, or maintaining APIs will also benefit from the topics covered in this book. ","","9781800569355","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769218.pdf&bkn=10769217&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"ICSPCC 2022 Proceedings","",,"2022 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","23 Dec 2022","2022","","","i","cxiv","Presents the front cover, title page, cover page, or splash screen of the proceedings record.","","978-1-6654-6972-2","10.1109/ICSPCC55723.2022.9984366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9984366","","Signal processing;Faces;Codes;COVID-19;Writing;Wireless communication;Urban areas","","","","","IEEE","23 Dec 2022","","","IEEE","IEEE Conferences"
"A Practical Guide to Quantum Machine Learning and Quantum Optimization: Hands-on Approach to Modern Quantum Algorithms","A. D. Meglio; E. F. Combarro; S. González-Castillo; A. D. Meglio",NA; NA; NA; NA,A Practical Guide to Quantum Machine Learning and Quantum Optimization: Hands-on Approach to Modern Quantum Algorithms,"","2023","","","","","Work with fully explained algorithms and ready-to-use examples that can be run on quantum simulators and actual quantum computers with this comprehensive guideKey FeaturesGet a solid grasp of the principles behind quantum algorithms and optimization with minimal mathematical prerequisitesLearn the process of implementing the algorithms on simulators and actual quantum computersSolve real-world problems using practical examples of methodsBook DescriptionThis book provides deep coverage of modern quantum algorithms that can be used to solve real-world problems. You’ll be introduced to quantum computing using a hands-on approach with minimal prerequisites. You’ll discover many algorithms, tools, and methods to model optimization problems with the QUBO and Ising formalisms, and you will find out how to solve optimization problems with quantum annealing, QAOA, Grover Adaptive Search (GAS), and VQE. This book also shows you how to train quantum machine learning models, such as quantum support vector machines, quantum neural networks, and quantum generative adversarial networks. The book takes a straightforward path to help you learn about quantum algorithms, illustrating them with code that’s ready to be run on quantum simulators and actual quantum computers. You’ll also learn how to utilize programming frameworks such as IBM’s Qiskit, Xanadu’s PennyLane, and D-Wave’s Leap. Through reading this book, you will not only build a solid foundation of the fundamentals of quantum computing, but you will also become familiar with a wide variety of modern quantum algorithms. Moreover, this book will give you the programming skills that will enable you to start applying quantum methods to solve practical problems right away.What you will learnReview the basics of quantum computingGain a solid understanding of modern quantum algorithmsUnderstand how to formulate optimization problems with QUBOSolve optimization problems with quantum annealing, QAOA, GAS, and VQEFind out how to create quantum machine learning modelsExplore how quantum support vector machines and quantum neural networks work using Qiskit and PennyLaneDiscover how to implement hybrid architectures using Qiskit and PennyLane and its PyTorch interfaceWho this book is forThis book is for professionals from a wide variety of backgrounds, including computer scientists and programmers, engineers, physicists, chemists, and mathematicians. Basic knowledge of linear algebra and some programming skills (for instance, in Python) are assumed, although all mathematical prerequisites will be covered in the appendices.","","9781804618301","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162422.pdf&bkn=10162421&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Grammar-based Game Description Generation using Large Language Models","T. Tanaka; E. Simo-Serra","Waseda University, Tokyo, Japan; Waseda University, Tokyo, Japan",IEEE Transactions on Games,"","2024","PP","99","1","14","Game Description Language (GDL) provides a standardized way to express diverse games in a machine-readable format, enabling automated game simulation, and evaluation. While previous research has explored game description generation using search-based methods, generating GDL descriptions from natural language remains a challenging task. This paper presents a novel framework that leverages Large Language Models (LLMs) to generate grammatically accurate game descriptions from natural language. Our approach consists of two stages: first, we gradually generate a minimal grammar based on GDL specifications; second, we iteratively improve the game description through grammar-guided generation. Our framework employs a specialized parser that identifies valid subsequences and candidate symbols from LLM responses, enabling gradual refinement of the output to ensure grammatical correctness. Experimental results demonstrate that our iterative improvement approach significantly outperforms baseline methods that directly use LLM outputs. Our code is available at https://github.com/tsunehiko/ggdg","2475-1510","","10.1109/TG.2024.3520214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10807354","Large Language Model;Ludii;Game Description Language;Grammar;Game Description Generation","Games;Grammar;Natural languages;Iterative decoding;Accuracy;Large language models;Decoding;Symbols;Evolutionary computation;Semantics","","","","","IEEE","19 Dec 2024","","","IEEE","IEEE Early Access Articles"
"AI-Powered Developer: Build software with ChatGPT and Copilot","N. Crocker",Manning Publications,AI-Powered Developer: Build software with ChatGPT and Copilot,"","2024","","","","","Use groundbreaking generative AI tools to increase your productivity, efficiency, and code quality. AI coding tools like ChatGPT and GitHub Copilot are changing the way we write code and build software. AI-Powered Developer reveals the practical best practices you need to deliver reliable results with AI. It cuts through the hype, showcasing real-world examples of how these tools ease and enhance your everyday tasks, and make you more creative. In AI-Powered Developer you’ll discover how to get the most out of AI:  Harness AI to help you design and plan software Use AI for code generation, debugging, and documentation Improve your code quality assessments with the help of AI Articulate complex problems to prompt an AI solution Develop a continuous learning mindset that keeps you up to date Adapt your development skills to almost any language  AI coding tools give you a smart and reliable junior developer that’s fast and keen to help out with your every task and query. AI-Powered Developer helps you put your new assistant to work. You’ll learn to use AI for everything from writing boilerplate, to testing and quality assessment, managing infrastructure, delivering security, and even assisting with software design.","","9781633437616","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10745291.pdf&bkn=10745290&pdfType=book","Copilot;test;ChatGPT;solutions;software design;adaptive;CodeWhisperer;fast;code generation;debugging;documentation;quality assessment;hands-on","","","","","","","6 Nov 2024","","","Manning","Manning eBooks"
"SRSA: A Cost-Efficient Strategy-Router Search Agent for Real-world Human-Machine Interactions","Y. Wang; H. Xu","School of Computer Science, Carnegie Mellon University, Pittsburgh, USA; Graduate School of Art and Science, Columbia University, New York City, USA",2024 IEEE International Conference on Data Mining Workshops (ICDMW),"18 Mar 2025","2024","","","307","316","Recently, as Large Language Models (LLMs) have shown impressive emerging capabilities and gained widespread popularity, research on LLM-based search agents has proliferated. In real-world situations, users often input contextual and highly personalized queries to chatbots, challenging LLMs to capture context and generate appropriate answers. However, much of the prior research has not focused specifically on authentic human-machine dialogue scenarios. It also ignores the important balance between response quality and computational cost by forcing all queries to follow the same agent process. To address these gaps, we propose a Strategy-Router Search Agent (SRSA), routing different queries to appropriate search strategies and enabling fine-grained serial searches to obtain high-quality results at a relatively low cost. To evaluate our work, we introduce a new dataset, Contextual Query Enhancement Dataset (CQED), comprising contextual queries to simulate authentic and daily interactions between humans and chatbots. Using LLM-based automatic evaluation metrics, we assessed SRSA’s performance in terms of informativeness, completeness, novelty, and actionability. To conclude, SRSA provides an approach that resolves the issue of simple serial searches leading to degenerate answers for lengthy and contextual queries, effectively and efficiently parses complex user queries, and generates more comprehensive and informative responses without fine-tuning an LLM. The code is available at https://anonymous.4open.science/r/SRSA-3A04/.","2375-9259","979-8-3315-3063-1","10.1109/ICDMW65004.2024.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10917407","Retrieval-Augmented Generation;Large Language Models;Search Agent;Human-Computer Interaction;Information Retrieval","Measurement;Large language models;Human-machine systems;Retrieval augmented generation;Search problems;Routing;Chatbots;Information retrieval;Computational efficiency;Data mining","","","","41","IEEE","18 Mar 2025","","","IEEE","IEEE Conferences"
"Position, Posture, and Pose Definitions for 3D Body Processing","C. McDonald; B. Carmicino; K. Schildmeyer; E. Scott; I. Dabolina",NA; NA; NA; NA; NA,"Position, Posture, and Pose Definitions for 3D Body Processing","29 Mar 2024","2024","","","1","47","The interchangeable use of the terms position, posture, and pose causes confusion for 3D body processing (3DBP) applications. This paper reviews current definitions and contextual use of these terms to suggest standardized nomenclature for posture and pose. This paper also reviews and discusses possible standard definitions for location, body regions, landmarks, and anatomical relationships. With large language models being central to artificial intelligence (AI), standardized terminology is imperative to all digitization efforts. Replicating, or cloning of, actual posture is a known challenge inhibiting cloned human body models, optimized virtual try-on, and critical fit assessment. 3DBP applications need to be sophisticated enough to accept and utilize unique actual postures, in a given pose, which may vary drastically from a predetermined norm. The discussion builds off of current ISO standards to an open discussion for standards toward posture-improved human body models inclusive of widely varying morphology.","","979-8-8557-0628-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486845","3DBP;3D body processing;body models;definitions;pose;posture;standardized terminology;terminology","","","","","","","29 Mar 2024","","","IEEE","IEEE Standards"
"Learn OpenAI Whisper: Transform your understanding of GenAI through robust and accurate speech processing solutions","J. R. Batista; C. Papile",NA; NA,Learn OpenAI Whisper: Transform your understanding of GenAI through robust and accurate speech processing solutions,"","2024","","","","","Master automatic speech recognition (ASR) with groundbreaking generative AI for unrivaled accuracy and versatility in audio processing Key FeaturesUncover the intricate architecture and mechanics behind Whisper's robust speech recognitionApply Whisper's technology in innovative projects, from audio transcription to voice synthesisNavigate the practical use of Whisper in real-world scenarios for achieving dynamic tech solutionsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionAs the field of generative AI evolves, so does the demand for intelligent systems that can understand human speech. Navigating the complexities of automatic speech recognition (ASR) technology is a significant challenge for many professionals. This book offers a comprehensive solution that guides you through OpenAI's advanced ASR system. You’ll begin your journey with Whisper's foundational concepts, gradually progressing to its sophisticated functionalities. Next, you’ll explore the transformer model, understand its multilingual capabilities, and grasp training techniques using weak supervision. The book helps you customize Whisper for different contexts and optimize its performance for specific needs. You’ll also focus on the vast potential of Whisper in real-world scenarios, including its transcription services, voice-based search, and the ability to enhance customer engagement. Advanced chapters delve into voice synthesis and diarization while addressing ethical considerations. By the end of this book, you'll have an understanding of ASR technology and have the skills to implement Whisper. Moreover, Python coding examples will equip you to apply ASR technologies in your projects as well as prepare you to tackle challenges and seize opportunities in the rapidly evolving world of voice recognition and processing.What you will learnIntegrate Whisper into voice assistants and chatbotsUse Whisper for efficient, accurate transcription servicesUnderstand Whisper's transformer model structure and nuancesFine-tune Whisper for specific language requirements globallyImplement Whisper in real-time translation scenariosExplore voice synthesis capabilities using Whisper's robust techExecute voice diarization with Whisper and NVIDIA's NeMoNavigate ethical considerations in advanced voice technologyWho this book is forLearn OpenAI Whisper is designed for a diverse audience, including AI engineers, tech professionals, and students. It's ideal for those with a basic understanding of machine learning and Python programming, and an interest in voice technology, from developers integrating ASR in applications to researchers exploring the cutting-edge possibilities in artificial intelligence. ","","9781835087497","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769391.pdf&bkn=10769390&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"A Survey on Side-Channel-based Reverse Engineering Attacks on Deep Neural Networks","Y. Liu; M. Zuzak; D. Xing; I. McDaniel; P. Mittu; O. Ozbay; A. Akib; A. Srivastava","University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park",2022 IEEE 4th International Conference on Artificial Intelligence Circuits and Systems (AICAS),"5 Sep 2022","2022","","","312","315","Hardware side-channels have been exploited to leak sensitive information. With the emergence of deep learning, their hardware platforms have also been scrutinized for side-channel information leakage. It has been shown that the structure, weights, and input samples of deep neural networks (DNN) can all be the victim of reverse engineering attacks that rely on side-channel information leakage. In this paper, we survey existing work on hardware side-channel-based reverse engineering attacks on DNNs as well as the countermeasures.","","978-1-6654-0996-4","10.1109/AICAS54282.2022.9869995","National Science Foundation(grant numbers:1953285); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869995","Reverse Engineering;Side-Channel Attacks;Deep Neural Networks","Deep learning;Circuits and systems;Reverse engineering;Neural networks;Side-channel attacks;Hardware;Resource management","","1","","34","IEEE","5 Sep 2022","","","IEEE","IEEE Conferences"
"Exploring GPT-3: An unofficial first look at the general-purpose language processing API from OpenAI","S. Tingiris; B. Kinsella",NA; NA,Exploring GPT-3: An unofficial first look at the general-purpose language processing API from OpenAI,"","2021","","","","","Get started with GPT-3 and the OpenAI API for natural language processing using JavaScript and PythonKey FeaturesUnderstand the power of potential GPT-3 language models and the risks involvedExplore core GPT-3 use cases such as text generation, classification, and semantic search using engaging examplesPlan and prepare a GPT-3 application for the OpenAI review process required for publishing a live applicationBook DescriptionGenerative Pre-trained Transformer 3 (GPT-3) is a highly advanced language model from OpenAI that can generate written text that is virtually indistinguishable from text written by humans. Whether you have a technical or non-technical background, this book will help you understand and start working with GPT-3 and the OpenAI API. If you want to get hands-on with leveraging artificial intelligence for natural language processing (NLP) tasks, this easy-to-follow book will help you get started. Beginning with a high-level introduction to NLP and GPT-3, the book takes you through practical examples that show how to leverage the OpenAI API and GPT-3 for text generation, classification, and semantic search. You'll explore the capabilities of the OpenAI API and GPT-3 and find out which NLP use cases GPT-3 is best suited for. You’ll also learn how to use the API and optimize requests for the best possible results. With examples focusing on the OpenAI Playground and easy-to-follow JavaScript and Python code samples, the book illustrates the possible applications of GPT-3 in production. By the end of this book, you'll understand the best use cases for GPT-3 and how to integrate the OpenAI API in your applications for a wide array of NLP tasks.What you will learnUnderstand what GPT-3 is and how it can be used for various NLP tasksGet a high-level introduction to GPT-3 and the OpenAI APIImplement JavaScript and Python code examples that call the OpenAI APIStructure GPT-3 prompts and options to get the best possible resultsSelect the right GPT-3 engine or model to optimize for speed and cost-efficiencyFind out which use cases would not be suitable for GPT-3Create a GPT-3-powered knowledge base application that follows OpenAI guidelinesWho this book is forExploring GPT-3 is for anyone interested in natural language processing or learning GPT-3 with or without a technical background. Developers, product managers, entrepreneurs, and hobbyists looking to get to grips with NLP, AI, and GPT-3 will find this book useful. Basic computer skills are all you need to get the most out of this book.","","9781800565494","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163180.pdf&bkn=10163179&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Poirot: Causal Correlation Aided Semantic Analysis for Advanced Persistent Threat Detection","J. Yang; Q. Zhang; X. Jiang; S. Chen; F. Yang","School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China",IEEE Transactions on Dependable and Secure Computing,"31 Aug 2022","2022","19","5","3546","3563","The volatile, covert and slow multistage attack patterns of Advanced Persistent Threat (APT) present a tricky challenge of APT detection, which are vital for organisations to protect their critical assets. In this article, we aim to develop system that aggregates and uses existing systems’ alerts to detect APTs. In order to achieve this, we propose a causal correlation aided semantic analysis system, called Poirot, for detecting the multi-stage threats over a long-time span from existing systems’ alerts. Poirot is capable of autonomously mining causality between anomalous events, which instructs us in reorganizing the original alerts and in constructing alert-chains. The system further exploits the Latent Dirichlet Allocation (LDA) to model the semantic context of the alert-chains. This LDA model facilitates us to carry out the semantic analysis for capturing the latent attack intent as well as for reconstructing the APT scenario. We use an alert dataset provided by a cyber security company to verify the proposed Poirot in terms of the detection accuracy and the capability of attack scenario reconstruction. The experiment results are presented to show the achievable performance of the proposed semantic analysis based APT detection.","1941-0018","","10.1109/TDSC.2021.3101649","National Key Research and Development Program of China(grant numbers:2018YFF01012200); Key Science and Technology Project of Anhui(grant numbers:201903c08020001); Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:CX2100107001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506866","Advanced persistent threat;alert-chain;causality analysis;average causal effect;latent Dirichlet allocation","Semantics;Correlation;Inference algorithms;Tools;Support vector machines;Resource management;Integrated circuit modeling","","22","","49","IEEE","4 Aug 2021","","","IEEE","IEEE Journals"
"Can Adam Smith’s Invisible Hand phenomenon be used for the analysis of Fourth Estate’s impact and behavior?","T. Szuba; D. Sztuba","Institute of Journalism and Social Communication Faculty of Social Sciences, UPJP2 University, Cracow, Poland; Faculty of Social Sciences, Ph. D. student, UPJP2 University, Cracow, Poland",2020 International Joint Conference on Neural Networks (IJCNN),"28 Sep 2020","2020","","","1","8","Paper presents research conducted in order to understand why neural networks from Evolution point of view are developing in such a strange way. When observing development of species, development of consistent neural network is always stopped on certain level and is continued as development of distributed, cooperating neural networks. Such networks are organized into social structures. Adam Smith's Invisible Hand (ASIH) phenomena emerges here as key factor to understand this. ASIH is perceived here as meta-computational process on platform of local neural networks, hosted by agents. ASIH theoretically is able to provide self-regulation for social structures, better than any centralized structure (dictator, government, authority) can do. Contrary to deterministic computational processes in today's digital computers, the computational processes that are behind Invisible Hand are: unconscious, nondeterministic, multithread, chaotic and non-continuous. This research methodology has provided astonishing results: . Understanding of Elementary Invisible Hand, which is ruling so efficiently anthill. On this basis Artificial Invisible Hand can be derived to provide self-control of teams of AI mobile robots for situations when human-supervisor cannot assist them or management is too complicated; . Invisible Hand applied to problem of understanding the Fourth (4th) Estate allowed, for the first time, to point to very clear, well visible real (not abstract) case of Invisible Hand; . The 4th Estate on the platform of modern electronic media (MEM) emerges as a new worldwide governing superpower.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9207576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207576","Evolution of neural networks;Adam Smith’s Invisible Hand (ASIH);Fourth (4th) Estate;self-regulation;meta-computational process;modern electronic media (MEM)","Bridges;Media;Biological neural networks;Economics;Companies;Artificial intelligence","","2","","20","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
"Mastering PyTorch: Build powerful neural network architectures using advanced PyTorch 1.x features","A. R. Jha; D. G. Pillai",NA; NA,Mastering PyTorch: Build powerful neural network architectures using advanced PyTorch 1.x features,"","2021","","","","","Master advanced techniques and algorithms for deep learning with PyTorch using real-world examplesKey FeaturesUnderstand how to use PyTorch 1.x to build advanced neural network modelsLearn to perform a wide range of tasks by implementing deep learning algorithms and techniquesGain expertise in domains such as computer vision, NLP, Deep RL, Explainable AI, and much moreBook DescriptionDeep learning is driving the AI revolution, and PyTorch is making it easier than ever before for anyone to build deep learning applications. This PyTorch book will help you uncover expert techniques to get the most out of your data and build complex neural network models. The book starts with a quick overview of PyTorch and explores using convolutional neural network (CNN) architectures for image classification. You'll then work with recurrent neural network (RNN) architectures and transformers for sentiment analysis. As you advance, you'll apply deep learning across different domains, such as music, text, and image generation using generative models and explore the world of generative adversarial networks (GANs). You'll not only build and train your own deep reinforcement learning models in PyTorch but also deploy PyTorch models to production using expert tips and techniques. Finally, you'll get to grips with training large models efficiently in a distributed manner, searching neural architectures effectively with AutoML, and rapidly prototyping models using PyTorch and fast.ai. By the end of this PyTorch book, you'll be able to perform complex deep learning tasks using PyTorch to build smart artificial intelligence models.What you will learnImplement text and music generating models using PyTorchBuild a deep Q-network (DQN) model in PyTorchExport universal PyTorch models using Open Neural Network Exchange (ONNX)Become well-versed with rapid prototyping using PyTorch with fast.aiPerform neural architecture search effectively using AutoMLEasily interpret machine learning (ML) models written in PyTorch using CaptumDesign ResNets, LSTMs, Transformers, and more using PyTorchFind out how to use PyTorch for distributed training using the torch.distributed APIWho this book is forThis book is for data scientists, machine learning researchers, and deep learning practitioners looking to implement advanced deep learning paradigms using PyTorch 1.x. Working knowledge of deep learning with Python programming is required.","","9781789616408","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162714.pdf&bkn=10162713&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Pretrain Vision and Large Language Models in Python: End-to-end techniques for building and deploying foundation models on AWS","E. Webber; A. Olgiati",NA; NA,Pretrain Vision and Large Language Models in Python: End-to-end techniques for building and deploying foundation models on AWS,"","2023","","","","","Master the art of training vision and large language models with conceptual fundaments and industry-expert guidance. Learn about AWS services and design patterns, with relevant coding examplesKey FeaturesLearn to develop, train, tune, and apply foundation models with optimized end-to-end pipelinesExplore large-scale distributed training for models and datasets with AWS and SageMaker examplesEvaluate, deploy, and operationalize your custom models with bias detection and pipeline monitoringBook DescriptionFoundation models have forever changed machine learning. From BERT to ChatGPT, CLIP to Stable Diffusion, when billions of parameters are combined with large datasets and hundreds to thousands of GPUs, the result is nothing short of record-breaking. The recommendations, advice, and code samples in this book will help you pretrain and fine-tune your own foundation models from scratch on AWS and Amazon SageMaker, while applying them to hundreds of use cases across your organization. With advice from seasoned AWS and machine learning expert Emily Webber, this book helps you learn everything you need to go from project ideation to dataset preparation, training, evaluation, and deployment for large language, vision, and multimodal models. With step-by-step explanations of essential concepts and practical examples, you’ll go from mastering the concept of pretraining to preparing your dataset and model, configuring your environment, training, fine-tuning, evaluating, deploying, and optimizing your foundation models. You will learn how to apply the scaling laws to distributing your model and dataset over multiple GPUs, remove bias, achieve high throughput, and build deployment pipelines. By the end of this book, you’ll be well equipped to embark on your own project to pretrain and fine-tune the foundation models of the future.What you will learnFind the right use cases and datasets for pretraining and fine-tuningPrepare for large-scale training with custom accelerators and GPUsConfigure environments on AWS and SageMaker to maximize performanceSelect hyperparameters based on your model and constraintsDistribute your model and dataset using many types of parallelismAvoid pitfalls with job restarts, intermittent health checks, and moreEvaluate your model with quantitative and qualitative insightsDeploy your models with runtime improvements and monitoring pipelinesWho this book is forIf you’re a machine learning researcher or enthusiast who wants to start a foundation modelling project, this book is for you. Applied scientists, data scientists, machine learning engineers, solution architects, product managers, and students will all benefit from this book. Intermediate Python is a must, along with introductory concepts of cloud computing. A strong understanding of deep learning fundamentals is needed, while advanced topics will be explained. The content covers advanced machine learning and cloud techniques, explaining them in an actionable, easy-to-understand way.","","9781804612545","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251210.pdf&bkn=10251209&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"LiSum: Open Source Software License Summarization with Multi-Task Learning","L. Li; S. Xu; Y. Liu; Y. Gao; X. Cai; J. Wu; W. Song; Z. Liu","DISSec, NDST, College of Computer Science, Nankai University, Tianjin, China; DISSec, NDST, College of Cyber Science, Nankai University, Tianjin, China; DISSec, NDST, College of Computer Science, Nankai University, Tianjin, China; DISSec, NDST, College of Computer Science, Nankai University, Tianjin, China; DISSec, NDST, College of Computer Science, Nankai University, Tianjin, China; DISSec, NDST, College of Computer Science, Nankai University, Tianjin, China; Information Security Evaluation Center, Civil Aviation University of China, Tianjin, China; DISSec, NDST, College of Cyber Science, Nankai University, Tianjin, China",2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE),"8 Nov 2023","2023","","","787","799","Open source software (OSS) licenses regulate the conditions under which users can reuse, modify, and distribute the software legally. However, there exist various OSS licenses in the community, written in a formal language, which are typically long and complicated to understand. In this paper, we conducted a 661-participants online survey to investigate the perspectives and practices of developers towards OSS licenses. The user study revealed an indeed need for an automated tool to facilitate license understanding. Motivated by the user study and the fast growth of licenses in the community, we propose the first study towards automated license summarization. Specifically, we released the first high quality text summarization dataset and designed two tasks, i.e., license text summarization (LTS), aiming at generating a relatively short summary for an arbitrary license, and license term classification (LTC), focusing on the attitude inference towards a predefined set of key license terms (e.g., Distribute). Aiming at the two tasks, we present LiSum, a multi-task learning method to help developers overcome the obstacles of understanding OSS licenses. Comprehensive experiments demonstrated that the proposed jointly training objective boosted the performance on both tasks, surpassing state-of-the-art baselines with gains of at least 5 points w.r.t. F1 scores of four summarization metrics and achieving 95.13% micro average F1 score for classification simultaneously. We released all the datasets, the replication package, and the questionnaires for the community.","2643-1572","979-8-3503-2996-4","10.1109/ASE56229.2023.00150","National Natural Science Foundation of China(grant numbers:62202245,62002178); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298592","Open Source Software Licenses;Multi-Task Learning;License comprehension","Training;Measurement;Learning systems;Surveys;Formal languages;Licenses;Multitasking","","","","50","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"Machine Learning Security Principles: Keep data, networks, users, and applications safe from prying eyes","J. P. Mueller; R. Stephens",NA; NA,"Machine Learning Security Principles: Keep data, networks, users, and applications safe from prying eyes","","2022","","","","","Thwart hackers by preventing, detecting, and misdirecting access before they can plant malware, obtain credentials, engage in fraud, modify data, poison models, corrupt users, eavesdrop, and otherwise ruin your dayKey FeaturesDiscover how hackers rely on misdirection and deep fakes to fool even the best security systemsRetain the usefulness of your data by detecting unwanted and invalid modificationsDevelop application code to meet the security requirements related to machine learningBook DescriptionBusinesses are leveraging the power of AI to make undertakings that used to be complicated and pricy much easier, faster, and cheaper. The first part of this book will explore these processes in more depth, which will help you in understanding the role security plays in machine learning. As you progress to the second part, you’ll learn more about the environments where ML is commonly used and dive into the security threats that plague them using code, graphics, and real-world references. The next part of the book will guide you through the process of detecting hacker behaviors in the modern computing environment, where fraud takes many forms in ML, from gaining sales through fake reviews to destroying an adversary’s reputation. Once you’ve understood hacker goals and detection techniques, you’ll learn about the ramifications of deep fakes, followed by mitigation strategies. This book also takes you through best practices for embracing ethical data sourcing, which reduces the security risk associated with data. You’ll see how the simple act of removing personally identifiable information (PII) from a dataset lowers the risk of social engineering attacks. By the end of this machine learning book, you'll have an increased awareness of the various attacks and the techniques to secure your ML systems effectively.What you will learnExplore methods to detect and prevent illegal access to your systemImplement detection techniques when access does occurEmploy machine learning techniques to determine motivationsMitigate hacker access once security is breachedPerform statistical measurement and behavior analysisRepair damage to your data and applicationsUse ethical data collection methods to reduce security risksWho this book is forWhether you’re a data scientist, researcher, or manager working with machine learning techniques in any aspect, this security book is a must-have. While most resources available on this topic are written in a language more suitable for experts, this guide presents security in an easy-to-understand way, employing a host of diagrams to explain concepts to visual learners. While familiarity with machine learning concepts is assumed, knowledge of Python and programming in general will be useful.","","9781804615409","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163571.pdf&bkn=10163570&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Hands-On Python for DevOps: Leverage Python's native libraries to streamline your workflow and save time with automation","A. Roy",NA,Hands-On Python for DevOps: Leverage Python's native libraries to streamline your workflow and save time with automation,"","2024","","","","","Unleash DevOps excellence with Python and its ecosystem of tools for seamless orchestration on both local and cloud platforms, such as GCP, AWS, and AzureKey FeaturesIntegrate Python into DevOps for streamlined workflows, task automation, and improved collaborationCombine the principles of Python and DevOps into a unified approach for problem solvingLearn about Python’s role in Infrastructure as Code (IaC), MLOps, networking, and other domainsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionPython stands out as a powerhouse in DevOps, boasting unparalleled libraries and support, which makes it the preferred programming language for problem solvers worldwide. This book will help you understand the true flexibility of Python, demonstrating how it can be integrated into incredibly useful DevOps workflows and workloads, through practical examples. You'll start by understanding the symbiotic relation between Python and DevOps philosophies and then explore the applications of Python for provisioning and manipulating VMs and other cloud resources to facilitate DevOps activities. With illustrated examples, you’ll become familiar with automating DevOps tasks and learn where and how Python can be used to enhance CI/CD pipelines. Further, the book highlights Python’s role in the Infrastructure as Code (IaC) process development, including its connections with tools like Ansible, SaltStack, and Terraform. The concluding chapters cover advanced concepts such as MLOps, DataOps, and Python’s integration with generative AI, offering a glimpse into the areas of monitoring, logging, Kubernetes, and more. By the end of this book, you’ll know how to leverage Python in your DevOps-based workloads to make your life easier and save time.What you will learnImplement DevOps practices and principles using PythonEnhance your DevOps workloads with PythonCreate Python-based DevOps solutions to improve your workload efficiencyUnderstand DevOps objectives and the mindset needed to achieve themUse Python to automate DevOps tasks and increase productivityExplore the concepts of DevSecOps, MLOps, DataOps, and moreUse Python for containerized workloads in Docker and KubernetesWho this book is forThis book is for IT professionals venturing into DevOps, particularly programmers seeking to apply their existing programming knowledge to excel in this field. For DevOps professionals without a coding background, this book serves as a resource to enhance their understanding of development practices and communicate more effectively with developers. Solutions architects, programmers, and anyone regularly working with DevOps solutions and Python will also benefit from this hands-on guide.","","9781835081495","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10522531.pdf&bkn=10522530&pdfType=book","","","","","","","","8 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"The Roles and Modes of Human Interactions with Automated Machine Learning Systems: A Critical Review and Perspectives","T. T. Khuat; D. J. Kedziora; B. Gabrys",NA; NA; NA,The Roles and Modes of Human Interactions with Automated Machine Learning Systems: A Critical Review and Perspectives,"","2023","","","","","Recent years have seen an unprecedented level of technological uptake and engagement by the mainstream. From deepfakes for memes to recommendation systems for commerce, machine learning (ML) has become a regular fixture in society. This ongoing transition from purely academic confines to the general public is not smooth as the public does not have the extensive expertise in data science required to fully exploit the capabilities of ML. As automated machine learning (AutoML) systems continue to progress in both sophistication and performance, it becomes important to understand the ‘how’ and ‘why’ of human-computer interaction (HCI) within these frameworks. This is necessary for optimal system design and leveraging advanced data-processing capabilities to support decision-making involving humans. It is also key to identifying the opportunities and risks presented by ever-increasing levels of machine autonomy. In this monograph, the authors focus on the following questions: (i) What does HCI currently look like for state-of-the-art AutoML algorithms? (ii) Do the expectations of HCI within AutoML frameworks vary for different types of users and stakeholders? (iii) How can HCI be managed so that AutoML solutions acquire human trust and broad acceptance? (iv) As AutoML systems become more autonomous and capable of learning from complex open-ended environments, will the fundamental nature of HCI evolve? To consider these questions, the authors project existing literature in HCI into the space of AutoML and review topics such as user-interface design, human-bias mitigation, and trust in artificial intelligence (AI). Additionally, to rigorously gauge the future of HCI, they contemplate how AutoML may manifest in effectively open-ended environments. Ultimately, this review serves to identify key research directions aimed at better facilitating the roles and modes of human interactions with both current and future AutoML systems.","","9781638282693","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10260290.pdf&bkn=10260289&pdfType=book","","","","","","","","22 Sep 2023","","","now","Now Foundations and Trends Books"
"Towards Federated Large Language Models: Motivations, Methods, and Future Directions","Y. Cheng; W. Zhang; Z. Zhang; C. Zhang; S. Wang; S. Mao","Department of Electronic Information Engineering, Tsinghua University, Beijing; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; Department of Electronic Information Engineering, Tsinghua University, Beijing; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; Department of Electronic Information Engineering, Tsinghua University, Beijing; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA",IEEE Communications Surveys & Tutorials,"","2024","PP","99","1","1","Large Language Models (LLMs), such as LLaMA and GPT-4, have transformed the paradigm of natural language comprehension and generation. Despite their impressive performance, these models still face certain challenges, including the need for extensive data, high computational resources, and privacy concerns related to their data sources. Recently, Federated Learning (FL) has surfaced as a cooperative AI methodology that enables AI training across distributed computation entities while maintaining decentralized data. Integrating FL with LLMs presents an encouraging solution for privacy-preserving and collaborative LLM learning across multiple end-users, thus addressing the aforementioned challenges. In this paper, we provide an exhaustive review of federated Large Language Models, starting from an overview of the latest progress in FL and LLMs, and proceeding to a discourse on their motivation and challenges for integration. We then conduct a thorough review of the existing federated LLM research from the perspective of the entire lifespan, from pre-training to fine-tuning and practical applications. Moreover, we address the threats and issues arising from this integration, shedding light on the delicate balance between privacy and robustness, and introduce existing approaches and potential strategies for enhancing federated LLM privacy and resilience. Finally, we conclude this survey by outlining promising avenues for future research in this emerging field.","1553-877X","","10.1109/COMST.2024.3503680","the state key development program in 14th Five-Year(grant numbers:2021QY1702); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10759678","Federated Learning;Large Language Model;Foundation model;Privacy","Privacy;Surveys;Data models;Computational modeling;Training;Artificial intelligence;Security;Robustness;Reviews;Federated learning","","","","","IEEE","21 Nov 2024","","","IEEE","IEEE Early Access Articles"
"Pentesting APIs: A practical guide to discovering, fingerprinting, and exploiting APIs","M. Harley",NA,"Pentesting APIs: A practical guide to discovering, fingerprinting, and exploiting APIs","","2024","","","","","Learn the essential steps to successfully identify and leverage API endpoints with a sequenced and structured approachKey FeaturesGain detailed insights into vulnerabilities and attack vectors for RESTful and GraphQL APIsFollow practical advice and best practices for securing APIs against potential threatsExplore essential security topics, potential vulnerabilities, common attack vectors, and the overall API security landscapePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionUnderstanding API security is crucial as APIs form the backbone of modern interconnected applications, making them prime targets for cyberattacks. Drawing on nearly 30 years of cybersecurity experience and an extensive background in network security and forensic analysis, this book provides the knowledge and tools to strengthen your API security practices and protect against cyber threats comprehensively. This book begins by establishing a foundational understanding of APIs, particularly focusing on REST and GraphQL, emphasizing their critical role and potential security vulnerabilities. It guides you through setting up a penetration testing environment to ensure the practical application of concepts. You’ll learn reconnaissance techniques, information-gathering strategies, and the discovery of API vulnerabilities. Authentication and authorization testing are thoroughly explored, covering mechanisms, weaknesses, and methods to bypass security controls. By comprehensively addressing these aspects, the book equips you to understand, identify, and mitigate risks, strengthening API security and effectively minimizing potential attack surfaces. By the end of this book, you’ll have developed practical skills to identify, exploit, and secure APIs against various vulnerabilities and attacks.What you will learnGet an introduction to APIs and their relationship with securitySet up an effective pentesting lab for API intrusionConduct API reconnaissance and information gathering in the discovery phaseExecute basic attacks such as injection, exception handling, and DoSPerform advanced attacks, including data exposure and business logic abuseBenefit from expert security recommendations to protect APIs against attacksWho this book is forThis book is for security engineers, particularly those focused on application security, as well as security analysts, application owners, web developers, pentesters, and all curious enthusiasts who want to learn about APIs, effective testing methods for their robustness, and how to protect them against cyber attacks. Basic knowledge of web development, familiarity with API concepts, and a foundational understanding of cybersecurity principles will help you get started with this book.","","9781837639731","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769281.pdf&bkn=10769280&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Unlocking the Emotional World of Visual Media: An Overview of the Science, Research, and Impact of Understanding Emotion","J. Z. Wang; S. Zhao; C. Wu; R. B. Adams; M. G. Newman; T. Shafir; R. Tsachor","College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA; Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA; Department of Psychology, The Pennsylvania State University, University Park, PA, USA; Department of Psychology, The Pennsylvania State University, University Park, PA, USA; Emily Sagol Creative Arts Therapies Research Center, University of Haifa, Haifa, Israel; School of Theatre and Music, University of Illinois at Chicago, Chicago, IL, USA",Proceedings of the IEEE,"12 Oct 2023","2023","111","10","1236","1286","The emergence of artificial emotional intelligence technology is revolutionizing the fields of computers and robotics, allowing for a new level of communication and understanding of human behavior that was once thought impossible. While recent advancements in deep learning have transformed the field of computer vision, automated understanding of evoked or expressed emotions in visual media remains in its infancy. This foundering stems from the absence of a universally accepted definition of “emotion,” coupled with the inherently subjective nature of emotions and their intricate nuances. In this article, we provide a comprehensive, multidisciplinary overview of the field of emotion analysis in visual media, drawing on insights from psychology, engineering, and the arts. We begin by exploring the psychological foundations of emotion and the computational principles that underpin the understanding of emotions from images and videos. We then review the latest research and systems within the field, accentuating the most promising approaches. We also discuss the current technological challenges and limitations of emotion analysis, underscoring the necessity for continued investigation and innovation. We contend that this represents a “Holy Grail” research problem in computing and delineate pivotal directions for future inquiry. Finally, we examine the ethical ramifications of emotion-understanding technologies and contemplate their potential societal impacts. Overall, this article endeavors to equip readers with a deeper understanding of the domain of emotion analysis in visual media and to inspire further research and development in this captivating and rapidly evolving field.","1558-2256","","10.1109/JPROC.2023.3273517","Amazon Research Awards Program; National Science Foundation (NSF)(grant numbers:IIS-1110970,CNS-1921783); NSF(grant numbers:CNS-2234195,CNS-2234197); NSF(grant numbers:CIF-2205004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10132377","Artificial emotional intelligence (AEI);bodily expressed emotion understanding (BEEU);deep learning;ethics;evoked emotion;expressed emotion;human behavior;intelligent robots;movement analysis;psychology","Visualization;Emotion recognition;Behavioral sciences;Psychology;Media;Videos;Ethics;Affective computing;Deep learning;Human factors","","15","","497","CCBYNCND","23 May 2023","","","IEEE","IEEE Journals"
"Finding Trends in Software Research","G. Mathew; A. Agrawal; T. Menzies","Department of Computer Science (CS), North Carolina State University (NCSU), Raleigh, NC, USA; Department of Computer Science (CS), North Carolina State University (NCSU), Raleigh, NC, USA; Department of Computer Science (CS), North Carolina State University (NCSU), Raleigh, NC, USA",IEEE Transactions on Software Engineering,"18 Apr 2023","2023","49","4","1397","1410","Text mining methods can find large scale trends within research communities. For example, using stable Latent Dirichlet Allocation (a topic modeling algorithm) this study found 10 major topics in 35,391 SE research papers from 34 leading SE venues over the last 25 years (divided, evenly, between conferences and journals). Out study also shows how those topics have changed over recent years. Also, we note that (in the historical record) mono-focusing on a single topic can lead to fewer citations than otherwise. Further, while we find no overall gender bias in SE authorship, we note that women are under-represented in the top-most cited papers in our field. Lastly, we show a previously unreported dichotomy between software conferences and journals (so research topics that succeed at conferences might not succeed at journals, and vice versa). An important aspect of this work is that it is automatic and quickly repeatable (unlike prior SE bibliometric studies that used tediously slow and labor intensive methods). Automation is important since, like any data mining study, its conclusions are skewed by the data used in the analysis. The automatic methods of this paper make it far easier for other researchers to re-apply the analysis to new data, or if they want to use different modeling assumptions.","1939-3520","","10.1109/TSE.2018.2870388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8465996","Software engineering;bibliometrics;topic modeling;text mining","Software engineering;Conferences;Software;Analytical models;Data models;Predictive models;Testing","","15","","58","IEEE","14 Sep 2018","","","IEEE","IEEE Journals"
"Framework for Prioritization of Open Data Publication: An Application to Smart Cities","A. E. PRIETO; J. -N. Mazón; A. Lozano-Tello","Quercus Software Engineering Group, Escuela Politécnica, Universidad de Extremadura, Cáceres, Spain; WaKe research group, DLSI & IUII, Universidad de Alicante, Alicante, Spain; Quercus Software Engineering Group, Escuela Politécnica, Universidad de Extremadura, Cáceres, Spain",IEEE Transactions on Emerging Topics in Computing,"8 Mar 2021","2021","9","1","131","143","Public Sector Information is considered to play a fundamental role in the growth of the knowledge economy and improvements in society. Given the difficulty in publishing and maintaining all available data, due to budget constraints, institutions need to select which data to publish, giving priority to data most likely to generate social and economic impact. Priority of publication could become an even more significant problem in Smart Cities: as huge amounts of information are generated from different domains, the way data is prioritized and thus reused, could be a determining factor in promoting, among others, new and sustainable business opportunities for local entrepreneurs, and to improve citizen quality of life. However, people in charge of prioritizing which data to publish through open data portals (such as Chief Data Officers, or CDOs) do not have available any specific support in their decision-making process. In this work, a proposal of a framework for prioritization of open data publication as well as its application to Smart Cities is presented. This specific application of the framework relies on OSS (Open Source Software) indicators to help making decisions on the most relevant data to publish focused on developers and businesses operating within the Smart City context.","2168-6750","","10.1109/TETC.2019.2893016","Ministerio de Economía e Innovación(grant numbers:TIN2015-69957-R (MINECO/ERDF, EU),TIN2016-78103-C2-2-R (MINECO/ERDF, EU)); POCTEP 4IE(grant numbers:0045-4IE-4-P); Consejería de Economía e Infraestructuras/Junta de Extremadura (Spain) - European Regional Development Fund (ERDF)(grant numbers:GR18112,IB16055); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611359","Decision support;dataset reuse indicators;open data;smart city application","Smart cities;Portals;II-VI semiconductor materials;Cadmium compounds;Decision making;Economics;Publishing","","12","","61","IEEE","13 Jan 2019","","","IEEE","IEEE Journals"
"IEEE Draft Standard for DevOps: Building Reliable and Secure Systems Including Application Build, Package and Deployment","",,"IEEE P2675/D2, October 2020","14 Oct 2020","2020","","","1","93","Technical principles and processes to build, package, and deploy systems and applications in a reliable and secure way are specified. Establishing effective compliance and information technology (IT) controls is the focus. DevOps principles presented include mission first, customer focus, left-shift, continuous everything, and systems thinking. How stakeholders, including developers and operations staff, can collaborate and communicate effectively is described. The process outcomes and activities herein are aligned with the process model specified in ISO/IEC/IEEE 12207:2017 and ISO/IEC/IEEE 15288:2015.","","978-1-5044-7130-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9224250","agile;continuous delivery;continuous deployment;continuous integration;DevOps;IEEE 2675;left-shift","IEEE Standards;IEC Standards;ISO Standards;Software engineering;Product life cycle management;Information technology;Stakeholders;Security;Systems thinking","","","","","","14 Oct 2020","","","IEEE","IEEE Standards"
"Coverless Steganography for Face Recognition Based on Diffusion Model","Y. Guo; Z. Liu","School of Computer Science and Technology, Heilongjiang University, Harbin, China; School of Computer Science and Technology, Heilongjiang University, Harbin, China",IEEE Access,"18 Oct 2024","2024","12","","148770","148782","As a highly recognizable biometric face recognition technology, it has been widely used in many identity verification systems. In order to enhance the protection of personal privacy and ensure the safe transmission and sharing of sensitive information without affecting the user experience, this paper proposes an innovative coverless steganography framework for face recognition images based on diffusion model. The framework firstly extracts face features and generates masks containing these features. Then, combined with conditional diffusion model and text key, a deterministic Denoising Diffusion Implicit Model (DDIM) is used to sample coverless steganography images. Secret images can also be recovered in high quality with DDIM Inversion technology. A large number of experiments show that compared with the existing methods, this approach has markedly enhanced the quality of steganographic and restored images. The face recognition rate of the restored image is more than 96%, which can effectively replace the original image for face recognition. The detection accuracy of this method is 55.25% on the steganographic detection tool, which is closer to random guessing and can resist steganographic analysis. It ensures the higher security of hidden images and solves the limitation of existing methods in protecting the privacy of face images. Moreover, it is shown how to achieve controlled local steganography with a custom mask, which enhances the controllability and flexibility of the method. In conclusion, the proposed method outperforms traditional steganography in security, controllability and robustness, and provides an effective technical scheme for steganography protection of face recognition images without additional training.","2169-3536","","10.1109/ACCESS.2024.3477469","National Natural Science Foundation of China(grant numbers:62473278); Natural Science Foundation of Heilongjiang Province(grant numbers:LH2021F056); Heilongjiang Provincial Education Department(grant numbers:1355091130); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10713347","Face recognition;coverless steganography;diffusion model;DDIM","Steganography;Face recognition;Diffusion models;Security;Noise measurement;Robustness;Image restoration;Image recognition;Visualization;Noise reduction;Biometrics (access control)","","","","77","CCBYNCND","10 Oct 2024","","","IEEE","IEEE Journals"
"Random Pixel Embedding: A Novel Approach to Image Steganography","J. Kanjalkar; A. Talele; P. Kanjalkar; H. Dhobale; S. Gavali; A. Pimple; N. Waghmare","Vishwakarma Institute of Technology, Pune, India; Vishwakarma Institute of Technology, Pune, India; Vishwakarma Institute of Technology, Pune, India; Vishwakarma Institute of Technology, Pune, India; Vishwakarma Institute of Technology, Pune, India; Vishwakarma Institute of Technology, Pune, India; Vishwakarma Institute of Technology, Pune, India",2024 2nd World Conference on Communication & Computing (WCONF),"4 Oct 2024","2024","","","1","10","This project presents a novel approach to image steganography titled “Random Pixel Embedding: A Novel Approach to Image Steganography.” Steganography, the art of concealing information within an innocuous carrier, finds widespread applications in secure communication and data protection. Our method employs a combination of LSB (Least Significant Bit) steganography, random pixel selection and AES (Advanced Encryption Standard) encryption to embed secret messages into digital images securely. The Tkinter library was used to create the system's user-friendly graphical user interface, which makes encoding and decoding processes easier. Additionally, it enhances the security of message encryption by incorporating PBKDF2 (Password-Based Key Derivation Function 2) for deriving cryptographic keys. By using this method, the project hopes to advance the fields of secure communication and data security by providing a reliable and effective way to conceal important information from view. The experimental results show that the suggested algorithm is reliable and effective, underscoring its potential for practical uses in data security and secure communication channels.","","979-8-3503-9532-7","10.1109/WCONF61366.2024.10692304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10692304","image steganography;random pixel;embedding algorithm;lsb steganography;aes encryption;pbkdf2;data security;secure communication;cryptography;information hiding","Steganography;Visualization;Digital images;Inspection;Libraries;Encryption;Reliability;Standards;Resilience;Graphical user interfaces","","","","22","IEEE","4 Oct 2024","","","IEEE","IEEE Conferences"
"Ensuring Dataset Accountability in Machine Learning: Insights from Software Engineering","V. Kotagi; V. K. Nassa; D. Patil; R. Gadhave; S. B. K. Adusumilli; P. P. Kumar","Information Science and Engineering, NITTE Meenakshi Institute Of Technology, Bangalore; Computer Science Engineering, Bharat institute of Engg. & Technology, Hyderabad, Telangana; Computer Engineering, Pillai HOC College of Engineering and Technology, Rasayani, Maharashtra; Computer Engineering, Pillai HOC college of Engineering and Technology, University of Mumbai, Mumbai, Maharashtra; Computer Science Engineering, San Francisco Bay University, Fremont, CA, USA; Electronic and Communication Engineering, Anurag University, Hyderabad, Telangana",2024 7th International Conference on Contemporary Computing and Informatics (IC3I),"15 Jan 2025","2024","7","","385","389","Machine learning is facing a crisis of accountability. On some tasks, deep learning can match or surpass human performance. It is extensive This work may be reproduced in whole or in part, for educational or personal purposes, in hard copy or digital format without monetary compensation as long as copies are made or distributed for non-commercial purposes and carry this notice and the whole citation on the first page. Conversely, the datasets that are crucial to machine learning (ML) are typically produced using opaque creation processes, poor maintenance, poor documentation, and a lack of answerability, which frequently results in errors. The study focused on several problems related to software engineering tasks, including performance evaluation metrics, software metrics, failure prediction, and problems with data quality. The paper draws attention to many methodological problems and difficulties associated with these software fault prediction tasks. Feature extraction and classification are commonly used to explore the excessive dimensionality of data and data class imbalance linked to software quality issues.","","979-8-3503-5006-7","10.1109/IC3I61595.2024.10829177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10829177","Dataset;Accountability;Machine Learning;Software Engineering","Support vector machines;Performance evaluation;Electric breakdown;Software quality;Documentation;Feature extraction;Maintenance;Software measurement;Informatics;Software engineering","","","","20","IEEE","15 Jan 2025","","","IEEE","IEEE Conferences"
"IEEE Approved Draft Standard for Wireless Smart Utility Network Field Area Network (FAN)","",,"IEEE P2857/D3, October 2020","3 Feb 2021","2021","","","1","177","This document describes a complete communications specification, encompassing layers 1 to 4 of the Open Systems Integration (OSI) network model, for a secure, wireless mesh communications network, using open standards communications and cybersecurity standards from standards organizations including Institute of Electrical and Electronics Engineers (IEEE) and Internet Engineering Task Force (IETF). The specification describes the functionality of the physical (PHY layer), medium access control (MAC layer), the network layer, transport layer and security parameters including certificate format for a highly scaleable and secure wireless mesh network for critical infrastructure ipv6 wireless communications networks.","","978-1-5044-7350-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9346094","adoption;FAN;field area network;IEEE 2857;Internet of Things;IoT;OSI;smart city;smart utility network;Wi-SUN;wireless mesh","IEEE Standards;Wireless communication;Open systems;Communication system security;Wireless mesh networks;Wireless networks;Physical layer","","","","","","3 Feb 2021","","","IEEE","IEEE Standards"
"IEEE Approved Draft Standard for DevOps: Building Reliable and Secure Systems Including Application Build, Package and Deployment","",,"IEEE P2675/D2, October 2020","11 Feb 2021","2021","","","1","93","Technical principles and processes to build, package, and deploy systems and applications in a reliable and secure way are specified. Establishing effective compliance and information technology (IT) controls is the focus. DevOps principles presented include mission first, customer focus, left-shift, continuous everything, and systems thinking. How stakeholders, including developers and operations staff, can collaborate and communicate effectively is described. The process outcomes and activities herein are aligned with the process model specified in ISO/IEC/IEEE 12207:2017 and ISO/IEC/IEEE 15288:2015.","","978-1-5044-7130-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9353440","agile;continuous delivery;continuous deployment;continuous integration;DevOps;IEEE 2675;left-shift","IEEE Standards;Security;Reliability;Packaging","","","","","","11 Feb 2021","","","IEEE","IEEE Standards"
"Generic Objects as Pose Probes for Few-shot View Synthesis","Z. Gao; R. Yi; C. Zhu; K. Zhuang; W. Chen; K. Xu","School of Computer, National University of Defense Technology, Changsha, China; School of Computer, National University of Defense Technology, Changsha, China; School of Computer, National University of Defense Technology, Changsha, China; School of Computer, National University of Defense Technology, Changsha, China; School of Computer, National University of Defense Technology, Changsha, China; School of Computer, National University of Defense Technology, Changsha, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Radiance fields, including NeRFs and 3D Gaussians, demonstrate great potential in high-fidelity rendering and scene reconstruction, while they require a substantial number of posed images as input. COLMAP is frequently employed for preprocessing to estimate poses. However, COLMAP necessitates a large number of feature matches to operate effectively, and struggles with scenes characterized by sparse features, large baselines, or few-view images. We aim to tackle few-view NeRF reconstruction using only 3 to 6 unposed scene images, freeing from COLMAP initializations. Inspired by the idea of calibration boards in traditional pose calibration, we propose a novel approach of utilizing everyday objects, commonly found in both images and real life, as “pose probes”. By initializing the probe object as a cube shape, we apply a dual-branch volume rendering optimization (object NeRF and scene NeRF) to constrain the pose optimization and jointly refine the geometry. PnP matching is used to initialize poses between images incrementally, where only a few feature matches are enough. PoseProbe achieves state-of-the-art performance in pose estimation and novel view synthesis across multiple datasets in experiments. We demonstrate its effectiveness, particularly in few-view and large-baseline scenes where COLMAP struggles. In ablations, using different objects in a scene yields comparable performance, showing that PoseProbe is robust to the choice of probe objects. Our project page is available at: https://zhirui-gao.github.io/PoseProbe.github.io/.","1558-2205","","10.1109/TCSVT.2025.3551303","Natural Science Foundation of Hainan Province(grant numbers:2021RC3071,2022RC1104); National University of Defense Technology Research Grants(grant numbers:ZK22-52); National Natural Science Foundation of China(grant numbers:62132021,62325221,62372457); Young Elite Scientists Sponsorship Program by CAST(grant numbers:2023ONRC001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10926495","Neural radiance fields;few-view reconstruction;pose optimization;pose probe","Probes;Cameras;Rendering (computer graphics);Neural radiance field;Optimization;Geometry;Pose estimation;Image reconstruction;Accuracy;Training","","","","","IEEE","14 Mar 2025","","","IEEE","IEEE Early Access Articles"
"PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software","W. Jiang; J. Yasmin; J. Jones; N. Synovic; J. Kuo; N. Bielanski; Y. Tian; G. K. Thiruvathukal; J. C. Davis","Purdue University, W Lafayette, IN, USA; Queen’s University, Kingston, CA; Purdue University, W Lafayette, IN, USA; Purdue University, W Lafayette, IN, USA; Purdue University, W Lafayette, IN, USA; Purdue University, W Lafayette, IN, USA; Queen’s University, Kingston, CA; Loyola University Chicago, Chicago, IL, USA; Purdue University, W Lafayette, IN, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),"18 Jun 2024","2024","","","431","443","The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse.This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset’s comprehensiveness, we developed prompts for a large language model to automatically extract model metadata, including the model’s training datasets, parameters, and evaluation metrics. Our analysis of this dataset provides the first summary statistics for the PTM supply chain, showing the trend of PTM development and common shortcomings of PTM package documentation. Our example application reveals inconsistencies in software licenses across PTMs and their dependent projects. PeaTMOSS lays the foundation for future research, offering rich opportunities to investigate the PTM supply chain. We outline mining opportunities on PTMs, their downstream usage, and cross-cutting questions.Our artifact is available at https://github.com/PurdueDualityLab/PeaTMOSS-Artifact. Our dataset is available at https://transfer.rcac.purdue.edu/file-manager?origin_id=ff978999-16c2-4b50-ac7a-947ffdc3eb1d&origin_path=%2F.CCS Concepts• Computing methodologies → Artificial intelligence; Information extraction; • Information systems → Database design and models; • Software and its engineering → Software libraries and repositories.","2574-3864","979-8-4007-0587-8","","Google; Argonne National Laboratory; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555612","Datasets;Machine learning;Deep neural networks;Model zoos;Package registries;Open-source;Empirical software engineering","Training;Analytical models;Software libraries;Computational modeling;Supply chains;Metadata;Licenses","","","","112","","18 Jun 2024","","","IEEE","IEEE Conferences"
"An Efficient and Flexible Black-Box Watermarking Framework for Large Language Models","S. Li; X. Xiong; Y. Yu; J. Lu; Z. Li","School of Cybersecurity, Chengdu University of Information Technology, Chengdu, China; School of Cybersecurity, Chengdu University of Information Technology, Chengdu, China; School of Cybersecurity, Chengdu University of Information Technology, Chengdu, China; School of Cybersecurity, Chengdu University of Information Technology, Chengdu, China; School of Cybersecurity, Chengdu University of Information Technology, Chengdu, China",2024 4th International Conference on Communication Technology and Information Technology (ICCTIT),"24 Mar 2025","2024","","","235","242","To prevent misuse of text generated by large language models (LLMs), watermarking technology offers a straightforward and effective means of verification. Existing approaches, however, suffer from several limitations, including a high dependency on the underlying logic of LLMs, inadequate performance against sophisticated watermarking attacks, and poor stealth, making them less suitable for complex and dynamic real-world applications. To address these challenges, we propose an efficient and flexible black-box watermarking framework for large language models. This framework enhances robustness and stealth by embedding watermarks in key areas of the text through the identification and sampling of high-entropy words. Furthermore, we introduce an encryption-based encoding algorithm that allows users to verify watermarks with their personal keys, supporting the customization of watermarking algorithms. Comparative experiments on four datasets with five baseline methods demonstrate that the proposed method excels in accuracy while significantly improving robustness and concealability with minimal semantic changes.","","979-8-3315-2897-3","10.1109/ICCTIT64404.2024.10928654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10928654","Large Language Models;Watermarking;HighEntropy Words;Key-Based Encryption;Black-Box Framework","Accuracy;Large language models;Semantics;Closed box;Watermarking;Robustness;Encoding;Encryption;Logic;Information technology","","","","28","IEEE","24 Mar 2025","","","IEEE","IEEE Conferences"
"""SQLSynthGen: Generating Synthetic Data for Healthcare Databases""","R. S. Raja",THREAD Research,2025 IEEE 4th International Conference on AI in Cybersecurity (ICAIC),"29 Jan 2025","2025","","","1","11","Synthetic data generation is crucial for leveraging machine learning in healthcare without compromising patient privacy. SQLSynthGen (SSG) offers a solution by generating synthetic datasets from relational databases, preserving data structure and integrity. By extracting statistical properties via SQL queries and applying differential privacy, SSG ensures data utility while protecting individual privacy. Transparency is maintained through configurable YAML files, facilitating data auditing and stakeholder trust. Despite its advancements, SSG faces challenges in relational data handling and privacy explainability, necessitating future improvements in these areas to enhance adoption in healthcare and other sensitive domains.","","979-8-3315-1888-2","10.1109/ICAIC63015.2025.10849122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10849122","Synthetic Data Generation;Differential Privacy;Data Fidelity;Relational Databases;Statistical Refinement","Sensitivity;Time series analysis;Focusing;Medical services;Relational databases;Data structures;Generators;Stakeholders;Faces;Synthetic data","","","","30","IEEE","29 Jan 2025","","","IEEE","IEEE Conferences"
"A Text Similarity Measurement Based on Semantic Fingerprint of Characteristic Phrases","S. Pang; J. Yao; T. Liu; H. Zhao; H. Chen","College of Computer and Communication Engineering, China University of Petroleum, Qingdao, China; College of Computer and Communication Engineering, China University of Petroleum, Qingdao, China; Weihai Science and Technology Bureau, Weihai, China; College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",Chinese Journal of Electronics,"21 Jan 2025","2020","29","2","233","241","Text similarity measurements are the basis for measuring the degree of matching between two or more texts. Traditional large-scale similarity detection methods based on a digital fingerprint have the advantage of high detection speed, which are only suitable for accurate detection. We propose a method of Chinese text similarity measurement based on feature phrase semantics. Natural language processing (NLP) technology is used to pre-process text and extract the keywords by the Term frequency-Inverse document frequency (TF-IDF) model and further screen out the feature words. We get the exact meaning of a word and semantic similarities between words and a HowNet semantic dictionary. We substitute concepts to get the feature phrases and generate a semantic fingerprint and calculate similarity. The experimental results indicate that the method proposed is superior in similarity detection in terms of its accuracy rate, recall rate, and $F$-value to the traditional and digital fingerprinting method.","2075-5597","","10.1049/cje.2019.12.011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10848260","Term frequency-Inverse document frequency(TF-IDF) model;Semantic fingerprint;Similarity;Characteristic phrases","Accuracy;Dictionaries;Semantics;Fingerprint recognition;Feature extraction;Natural language processing;Frequency measurement","","","","32","","21 Jan 2025","","","CIE","CIE Journals"
"NeuralSanitizer: Detecting Backdoors in Neural Networks","H. Zhu; Y. Zhao; S. Zhang; K. Chen","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, Metropolitan College, Boston University, Boston, MA, USA; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Information Forensics and Security,"8 May 2024","2024","19","","4970","4985","Deep neural networks (DNNs) have been pervasively used in many areas, e.g., computer vision, speech recognition, natural language processing, etc. However, recent works show that they are vulnerable to backdoor/Trojan attacks, severely restricting their usage in various scenarios. In this paper, we propose NeuralSanitizer, a novel approach to detect and remove backdoors in DNNs, capable of capturing various triggers with better accuracy and higher efficiency. In particular, we identify two fundamental properties of triggers, i.e., their effectiveness in the backdoored model and ineffectiveness in other clean models, and design a novel objective function to reconstruct triggers based on them. Then we present a new approach that leverages transferability to identify adversarial patches that could be generated during trigger reconstruction, thus detecting backdoors more accurately. We evaluate NeuralSanitizer on real-world backdoored DNNs and achieve 2.1% FNR and 0.9% FPR on average, significantly outperforming the state-of-the-art works by 1~14 times. In addition, NeuralSanitizer can reconstruct triggers up to 25% of the size of the original inputs on average, compared to only 6~10% by existing works. Finally, NeuralSanitizer is also 1~25 times faster than existing works.","1556-6021","","10.1109/TIFS.2024.3390599","NSFC(grant numbers:92270204,62302498); Youth Innovation Promotion Association CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10504286","AI security;deep learning;backdoor attack;backdoor detection","Training;Data models;Artificial neural networks;Computational modeling;Image reconstruction;Computer vision;Analytical models","","","","54","IEEE","17 Apr 2024","","","IEEE","IEEE Journals"
"TensorFlow 2.0 Computer Vision Cookbook: Implement machine learning solutions to overcome various computer vision challenges","J. Martínez",NA,TensorFlow 2.0 Computer Vision Cookbook: Implement machine learning solutions to overcome various computer vision challenges,"","2021","","","","","Get well versed with state-of-the-art techniques to tailor training processes and boost the performance of computer vision models using machine learning and deep learning techniquesKey FeaturesDevelop, train, and use deep learning algorithms for computer vision tasks using TensorFlow 2.xDiscover practical recipes to overcome various challenges faced while building computer vision modelsEnable machines to gain a human level understanding to recognize and analyze digital images and videosBook DescriptionComputer vision is a scientific field that enables machines to identify and process digital images and videos. This book focuses on independent recipes to help you perform various computer vision tasks using TensorFlow. The book begins by taking you through the basics of deep learning for computer vision, along with covering TensorFlow 2.x’s key features, such as the Keras and tf.data.Dataset APIs. You’ll then learn about the ins and outs of common computer vision tasks, such as image classification, transfer learning, image enhancing and styling, and object detection. The book also covers autoencoders in domains such as inverse image search indexes and image denoising, while offering insights into various architectures used in the recipes, such as convolutional neural networks (CNNs), region-based CNNs (R-CNNs), VGGNet, and You Only Look Once (YOLO). Moving on, you’ll discover tips and tricks to solve any problems faced while building various computer vision applications. Finally, you’ll delve into more advanced topics such as Generative Adversarial Networks (GANs), video processing, and AutoML, concluding with a section focused on techniques to help you boost the performance of your networks. By the end of this TensorFlow book, you’ll be able to confidently tackle a wide range of computer vision problems using TensorFlow 2.x.What you will learnUnderstand how to detect objects using state-of-the-art models such as YOLOv3Use AutoML to predict gender and age from imagesSegment images using different approaches such as FCNs and generative modelsLearn how to improve your network’s performance using rank-N accuracy, label smoothing, and test time augmentationEnable machines to recognize people’s emotions in videos and real-time streamsAccess and reuse advanced TensorFlow Hub models to perform image classification and object detectionGenerate captions for images using CNNs and RNNsWho this book is forThis book is for computer vision developers and engineers, as well as deep learning practitioners looking for go-to solutions to various problems that commonly arise in computer vision. You will discover how to employ modern machine learning (ML) techniques and deep learning architectures to perform a plethora of computer vision tasks. Basic knowledge of Python programming and computer vision is required.","","9781838820688","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163226.pdf&bkn=10163225&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Building AI Intensive Python Applications: Create intelligent apps with LLMs and vector databases","R. Palmer; B. Perlmutter; A. Gangadhar; N. Larew; S. Narváez; T. Rueckstiess; H. Weller; R. Alake; S. Ranjan",NA; NA; NA; NA; NA; NA; NA; NA; NA,Building AI Intensive Python Applications: Create intelligent apps with LLMs and vector databases,"","2024","","","","","Master retrieval-augmented generation architecture and fine-tune your AI stack, along with discovering real-world use cases and best practices to create powerful AI appsKey FeaturesGet to grips with the fundamentals of LLMs, vector databases, and Python frameworksImplement effective retrieval-augmented generation strategies with MongoDB AtlasOptimize AI models for performance and accuracy with model compression and deployment optimizationPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe era of generative AI is upon us, and this book serves as a roadmap to harness its full potential. With its help, you’ll learn the core components of the AI stack: large language models (LLMs), vector databases, and Python frameworks, and see how these technologies work together to create intelligent applications. The chapters will help you discover best practices for data preparation, model selection, and fine-tuning, and teach you advanced techniques such as retrieval-augmented generation (RAG) to overcome common challenges, such as hallucinations and data leakage. You’ll get a solid understanding of vector databases, implement effective vector search strategies, refine models for accuracy, and optimize performance to achieve impactful results. You’ll also identify and address AI failures to ensure your applications deliver reliable and valuable results. By evaluating and improving the output of LLMs, you’ll be able to enhance their performance and relevance. By the end of this book, you’ll be well-equipped to build sophisticated AI applications that deliver real-world value.What you will learnUnderstand the architecture and components of the generative AI stackExplore the role of vector databases in enhancing AI applicationsMaster Python frameworks for AI developmentImplement Vector Search in AI applicationsFind out how to effectively evaluate LLM outputOvercome common failures and challenges in AI developmentWho this book is forThis book is for software engineers and developers looking to build intelligent applications using generative AI. While the book is suitable for beginners, a basic understanding of Python programming is required to make the most of it.","","9781836207245","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769332.pdf&bkn=10769331&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"A Cognitive Deception Model for Generating Fake Documents to Curb Data Exfiltration in Networks During Cyber-Attacks","O. T. Taofeek; M. Alawida; A. Alabdulatif; A. E. Omolara; O. I. Abiodun","Department of Computer and Mathematics, Universiti Teknologi MARA (UiTM), Shah Alam, Malaysia; Department of Computer Science, Abu Dhabi University, Abu Dhabi, United Arab Emirates; Department of Computer Science, College of Computer, Qassim University, Buraydah, Saudi Arabia; Department of Computer Science, University of Abuja, Gwagwalada, Nigeria; Department of Computer Science, University of Abuja, Gwagwalada, Nigeria",IEEE Access,"25 Apr 2022","2022","10","","41457","41476","The exponential increase in the compromise of sensitive and intellectual properties alludes to the huge price the global community must pay for the digital revolution we are currently experiencing. This irrefutable reality is a major reason why cybersecurity defences continue to be a pressing and timely area of research. Traditional countermeasures of cyber defence using boundary controllers and filters such as intrusion detection, access controls, firewalls and so on, have proven ineffective. Such measures fail to account for the attacker’s inherent advantage of being increasingly techno-savvy, as well as their persistence in attempting to compromise the security of not only high-value targets, but also the vast pool of oblivious users of technology. The use of decoys and deception is one of the emerging solutions for cyber defence. Leveraging decoys and deception for security pre-date the advent of the digital revolution as centuries have witnessed the military using human decoys to deceive and successfully defeat their adversaries during wars. However, its benefits for reducing cyberattacks in these digital times have not been thoroughly investigated. One of its use requires that fake text documents are positioned in the repository of critical documents in order to mislead and catch hackers attempting to exfiltrate sensitive documents. Current methods of generating fake text documents involve using symbols, junk documents, randomly generated texts. Such approaches fail to capture the empirical and linguistic properties of language, resulting in messages that do not scale well, are not realistic, fail in the context of syntax and are semantically void. Consequently, failing to convince the attackers to believe they are the original messages. This paper presents a Cognitive Deception Model (CDM) based on a neural model which takes an input message and generates syntactically cohesive and semantically coherent independent looking but plausible and convincing decoy messages to cognitively burden and deceive the adversaries. The experimental results used to validate the models, as well as the comparison with state-of-the-art tools, show that it outperforms existing systems.","2169-3536","","10.1109/ACCESS.2022.3166628","Office of Research & Sponsored Programs of Abu Dhabi University, United Arab Emirates; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9755446","Artificial advanced persistent threats (APTs);cyber-attacks;cyber defence;deception;decoys","Electronic mail;Task analysis;Servers;Natural language processing;Symbols;Production;Passwords","","9","","50","CCBY","12 Apr 2022","","","IEEE","IEEE Journals"
"Power Platform and the AI Revolution: Explore modern AI services to develop apps, bots, and automation patterns to enhance customer experiences","A. Guilmette",NA,"Power Platform and the AI Revolution: Explore modern AI services to develop apps, bots, and automation patterns to enhance customer experiences","","2024","","","","","Unlock the untapped potential of ChatGPT, CoPilot, and Azure AI services by integrating them with the Microsoft Power PlatformKey FeaturesGain insights into the latest AI technologies and their business applicationsUse generative AI to build apps, workflows, and chatbotsLearn how to integrate AI services to automate work and deliver apps for specific business needsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionIn this AI era, employing leading machine learning and AI models such as ChatGPT for responding to customer feedback and prototyping applications is crucial to drive business success in the competitive market. This book is an indispensable guide to integrating cutting-edge technology into business operations and leveraging AI to analyze sentiment at scale, helping free up valuable time to enhance customer relationships. Immerse yourself in the future of AI-enabled application development by working with Power Automate, Power Apps, and the new Copilot Studio. With this book, you’ll learn foundational AI concepts as you explore the extensive capabilities of the low-code Power Platform. You’ll see how Microsoft's advanced machine learning technologies can streamline common business tasks such as extracting key data elements from customer documents, reviewing customer emails, and validating passports and drivers’ licenses. The book also guides you in harnessing the power of generative AI to expedite tasks like creating executive summaries, building presentations, and analyzing resumes. You’ll build apps using natural language prompting and see how ChatGPT can be used to power chatbots in your organization. By the end of this book, you’ll have charted your path to developing your own reusable AI automation patterns to propel your business operations into the future.What you will learnInteract with ChatGPT using connectors and HTTP callsTrain AI models to identify the key elements of documentsUse generative AI to answer questions about organizational contentLeverage AI image recognition services to describe picturesUse generative AI tools to help build workflows and appsBuild chatbots using the new Copilot StudioAnalyze customer feedback using AI sentiment analysis tools such as AI BuilderWho this book is forIf you’re interested in exploring the capabilities of modern AI technologies in the workplace, this book is for you. Specially tailored for IT professionals, developers, business leaders, human resources administrators, managers, and entrepreneurs–anyone aspiring to become a productivity rockstar will find this book helpful for extending their skill set through hands-on exercises. The content is beginner-friendly, assuming no knowledge of machine learning or artificial intelligence concepts, making it a perfect starting point for newcomers to the field.","","9781835089927","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10559422.pdf&bkn=10559421&pdfType=book","","","","","","","","17 Jun 2024","","","Packt Publishing","Packt Publishing eBooks"
"Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python","S. Raschka; Y. (. Liu; V. Mirjalili; D. Dzhulgakov",NA; NA; NA; NA,Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python,"","2022","","","","","This book of the bestselling and widely acclaimed Python Machine Learning series is a comprehensive guide to machine and deep learning using PyTorch's simple to code framework. Purchase of the print or Kindle book includes a free eBook in PDF format.Key FeaturesLearn applied machine learning with a solid foundation in theoryClear, intuitive explanations take you deep into the theory and practice of Python machine learningFully updated and expanded to cover PyTorch, transformers, XGBoost, graph neural networks, and best practicesBook DescriptionMachine Learning with PyTorch and Scikit-Learn is a comprehensive guide to machine learning and deep learning with PyTorch. It acts as both a step-by-step tutorial and a reference you'll keep coming back to as you build your machine learning systems. Packed with clear explanations, visualizations, and examples, the book covers all the essential machine learning techniques in depth. While some books teach you only to follow instructions, with this machine learning book, we teach the principles allowing you to build models and applications for yourself. Why PyTorch? PyTorch is the Pythonic way to learn machine learning, making it easier to learn and simpler to code with. This book explains the essential parts of PyTorch and how to create models using popular libraries, such as PyTorch Lightning and PyTorch Geometric. You will also learn about generative adversarial networks (GANs) for generating new data and training intelligent agents with reinforcement learning. Finally, this new edition is expanded to cover the latest trends in deep learning, including graph neural networks and large-scale transformers used for natural language processing (NLP). This PyTorch book is your companion to machine learning with Python, whether you're a Python developer new to machine learning or want to deepen your knowledge of the latest developments.What you will learnExplore frameworks, models, and techniques for machines to 'learn' from dataUse scikit-learn for machine learning and PyTorch for deep learningTrain machine learning classifiers on images, text, and moreBuild and train neural networks, transformers, and boosting algorithmsDiscover best practices for evaluating and tuning modelsPredict continuous target outcomes using regression analysisDig deeper into textual and social media data using sentiment analysisWho this book is forIf you have a good grasp of Python basics and want to start learning about machine learning and deep learning, then this is the book for you. This is an essential resource written for developers and data scientists who want to create practical machine learning and deep learning applications using scikit-learn and PyTorch. Before you get started with this book, you’ll need a good understanding of calculus, as well as linear algebra.","","9781801816380","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162165.pdf&bkn=10162164&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"The Regularization Cookbook: Explore practical recipes to improve the functionality of your ML models","V. Vandenbussche; A. O. Kazakci",NA; NA,The Regularization Cookbook: Explore practical recipes to improve the functionality of your ML models,"","2023","","","","","Methodologies and recipes to regularize any machine learning and deep learning model using cutting-edge technologies such as stable diffusion, Dall-E and GPT-3 Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn to diagnose the need for regularization in any machine learning modelRegularize different ML models using a variety of techniques and methodsEnhance the functionality of your models using state of the art computer vision and NLP techniquesBook DescriptionRegularization is an infallible way to produce accurate results with unseen data, however, applying regularization is challenging as it is available in multiple forms and applying the appropriate technique to every model is a must. The Regularization Cookbook provides you with the appropriate tools and methods to handle any case, with ready-to-use working codes as well as theoretical explanations. After an introduction to regularization and methods to diagnose when to use it, you’ll start implementing regularization techniques on linear models, such as linear and logistic regression, and tree-based models, such as random forest and gradient boosting. You’ll then be introduced to specific regularization methods based on data, high cardinality features, and imbalanced datasets. In the last five chapters, you’ll discover regularization for deep learning models. After reviewing general methods that apply to any type of neural network, you’ll dive into more NLP-specific methods for RNNs and transformers, as well as using BERT or GPT-3. By the end, you’ll explore regularization for computer vision, covering CNN specifics, along with the use of generative models such as stable diffusion and Dall-E. By the end of this book, you’ll be armed with different regularization techniques to apply to your ML and DL models.What you will learnDiagnose overfitting and the need for regularizationRegularize common linear models such as logistic regressionUnderstand regularizing tree-based models such as XGBoosUncover the secrets of structured data to regularize ML modelsExplore general techniques to regularize deep learning modelsDiscover specific regularization techniques for NLP problems using transformersUnderstand the regularization in computer vision models and CNN architecturesApply cutting-edge computer vision regularization with generative modelsWho this book is forThis book is for data scientists, machine learning engineers, and machine learning enthusiasts, looking to get hands-on knowledge to improve the performances of their models. Basic knowledge of Python is a prerequisite.","","9781837639724","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251384.pdf&bkn=10251383&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Vision-Semantics-Label: A New Two-step Paradigm for Action Recognition with Large Language Model","X. Chen; W. Xu; S. Kan; L. Zhang; Y. Jin; Y. Cen; Y. Li","State Key Laboratory of Advanced Rail Autonomous Operation, the School of Computer Science and Technology, and Visual Intellgence +X International Cooperation Joint Laboratory of MOE, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Advanced Rail Autonomous Operation, the School of Computer Science and Technology, and Visual Intellgence +X International Cooperation Joint Laboratory of MOE, Beijing Jiaotong University, Beijing, China; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; College of Mechanical Engineering, Guizhou University, Guiyang, Guizhou, China; State Key Laboratory of Advanced Rail Autonomous Operation, the School of Computer Science and Technology, and Visual Intellgence +X International Cooperation Joint Laboratory of MOE, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Advanced Rail Autonomous Operation, the School of Computer Science and Technology, and Visual Intellgence +X International Cooperation Joint Laboratory of MOE, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Advanced Rail Autonomous Operation, the School of Computer Science and Technology, and Visual Intellgence +X International Cooperation Joint Laboratory of MOE, Beijing Jiaotong University, Beijing, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","In recent years, the rapid advancement of multi-modal large language models has propelled the development of video-based conversation models. Due to their exceptional video understanding capabilities, there is often an expectation that these models can handle all video-related tasks, including action recognition. However, because action recognition datasets typically lack semantic information, limiting the performance of dialogue models. Additionally, as these dialogue models are designed for video understanding, they frequently overlook critical information required for action recognition—continuous motion—in their model architecture and training dataset configurations. To address these challenges, we first propose a novel two-step mapping framework based on large language models, termed “Vision-Semantics-Label” mapping, to better adapt video-based large language models for action recognition. In the first step, we proposed a visual-skeletal collaborative learning large language model (VS-LLM), which utilizes human keypoints to compensate for the missing motion details without increasing the input token length of the large language model. In the second step, we designed two mapping methods: verb noun match (VN-Match) and all text match (ALL-Match), which can effectively extract relevant action descriptions from the text. Finally, we construct semantic action recognition datasets to ensure that the training data inherently contains action details, enabling the model to better achieve action recognition. We evaluate our approach on five benchmark datasets, demonstrating the state-of-the-art performance of large language models in action recognition. The source code and dataset are publicly available at https://github.com/xiaoyu92568/VS-LLM.","1558-2205","","10.1109/TCSVT.2025.3548845","National Natural Science Foundation of China(grant numbers:62202499,62463002,62473033); Beijing Municipal Natural Science Foundation(grant numbers:4242028,L231012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10915651","Action recognition;video-based large language model;semantic mapping;semantic action recognition datasets","Large language models;Visualization;Feature extraction;Semantics;Oral communication;Streaming media;Sports;Load modeling;Adaptation models;Electronic mail","","","","","IEEE","6 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Natural Language Interfaces to Data","A. Quamar; V. Efthymiou; C. Lei; F. Özcan",NA; NA; NA; NA,Natural Language Interfaces to Data,"","2022","","","","","Natural language interfaces provide an easy way to query and interact with data and enable non-technical users to investigate data sets without the need to know a query language. Recent advances in natural language understanding and processing have resulted in a renewed interest in natural language interfaces to data. The main challenges in natural language querying are identifying the entities involved in the user utterance, connecting the different entities in a meaningful way over the underlying data source to interpret user intents, and generating a structured query. There are two main approaches in the literature for interpreting a user’s natural language query. The first are rule-based systems that make use of semantic indices, ontologies, and knowledge graphs to identify the entities in the query, understand the intended relationships between those entities, and utilize grammars to generate the target queries. Second are hybrid approaches that utilize both rule-based techniques as well as deep learning models. Conversational interfaces are the next natural step to one-shot natural language querying by exploiting query context between multiple turns of conversation for disambiguation. In this monograph, the authors review the rule-based and hybrid technologies that are used in natural language interfaces and survey the different approaches to natural language querying. They also describe conversational interfaces for data analytics and discuss several benchmarks used for natural language querying research and evaluation. The monograph concludes with discussion on challenges that need to be addressed before these systems can be widely adopted.","","9781638280293","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9785670.pdf&bkn=9785669&pdfType=book","","","","","","","","31 May 2022","","","now","Now Foundations and Trends Books"
"Machine Learning for Imbalanced Data: Tackle imbalanced datasets using machine learning and deep learning techniques","K. Abhishek; D. M. Abdelaziz",NA; NA,Machine Learning for Imbalanced Data: Tackle imbalanced datasets using machine learning and deep learning techniques,"","2023","","","","","Take your machine learning expertise to the next level with this essential guide, utilizing libraries like imbalanced-learn, PyTorch, scikit-learn, pandas, and NumPy to maximize model performance and tackle imbalanced dataKey FeaturesUnderstand how to use modern machine learning frameworks with detailed explanations, illustrations, and code samplesLearn cutting-edge deep learning techniques to overcome data imbalanceExplore different methods for dealing with skewed data in ML and DL applicationsPurchase of the print or Kindle book includes a free eBook in the PDF formatBook DescriptionAs machine learning practitioners, we often encounter imbalanced datasets in which one class has considerably fewer instances than the other. Many machine learning algorithms assume an equilibrium between majority and minority classes, leading to suboptimal performance on imbalanced data. This comprehensive guide helps you address this class imbalance to significantly improve model performance. Machine Learning for Imbalanced Data begins by introducing you to the challenges posed by imbalanced datasets and the importance of addressing these issues. It then guides you through techniques that enhance the performance of classical machine learning models when using imbalanced data, including various sampling and cost-sensitive learning methods. As you progress, you’ll delve into similar and more advanced techniques for deep learning models, employing PyTorch as the primary framework. Throughout the book, hands-on examples will provide working and reproducible code that’ll demonstrate the practical implementation of each technique. By the end of this book, you’ll be adept at identifying and addressing class imbalances and confidently applying various techniques, including sampling, cost-sensitive techniques, and threshold adjustment, while using traditional machine learning or deep learning models.What you will learnUse imbalanced data in your machine learning models effectivelyExplore the metrics used when classes are imbalancedUnderstand how and when to apply various sampling methods such as over-sampling and under-samplingApply data-based, algorithm-based, and hybrid approaches to deal with class imbalanceCombine and choose from various options for data balancing while avoiding common pitfallsUnderstand the concepts of model calibration and threshold adjustment in the context of dealing with imbalanced datasetsWho this book is forThis book is for machine learning practitioners who want to effectively address the challenges of imbalanced datasets in their projects. Data scientists, machine learning engineers/scientists, research scientists/engineers, and data scientists/engineers will find this book helpful. Though complete beginners are welcome to read this book, some familiarity with core machine learning concepts will help readers maximize the benefits and insights gained from this comprehensive resource.","","9781801070881","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10489934.pdf&bkn=10489933&pdfType=book","","","","","","","","3 Apr 2024","","","Packt Publishing","Packt Publishing eBooks"
"Deceptive Deepfakes: Is the Law Coping with AI-Altered Representations of Ourselves?","C. Jasserand","Transboundary Legal Department, STeP Faculty of Law, University of Groningen, Groningen, the Netherlands",2024 International Conference of the Biometrics Special Interest Group (BIOSIG),"11 Dec 2024","2024","","","1","4","Deceptive deepfakes are AI-generated hyper-realistic content that harms individuals (e.g. pornographic deepfake content) or society (as disinformation and impersonation tools). The technologies are becoming increasingly sophisticated, making their detection extremely difficult. Deepfakes can impair national and international security, undermine elections, threaten democracy, and challenge justice, but also cause mental distress and reputational damage. What are the regulatory answers to these deceptive deepfakes? As this paper will show, there is no single approach and answer to the topic. Currently, the problem is tackled by existing, albeit non-specific regulations, (e.g. privacy, intellectual property, image rights), and by newly adopted regulations focusing on a specific aspect (deepfakes aimed at manipulating elections or non-consensual pornographic materials). To present an overview of these regulations, the paper compares the approaches of the USA and the EU, including examples of national legislation. Summarizing the findings, the paper shows the laws need to regulate the ecosystem of actors involved in these deceptive contents (from the creators to the distributors and possibly the viewers in case of deepfake child pornography). Beyond the ecosystem, solutions should also focus on individuals who are victims of these deepfakes created without their consent and knowledge. A different approach might be well needed to ensure they can defend their digital self and prevent malicious AI-altered reproductions and use of themselves.","1617-5468","979-8-3503-7371-4","10.1109/BIOSIG61931.2024.10786729","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10786729","Deepfakes;Regulation;harms;Europe;digital self;societal issue","Deepfakes;Privacy;Voting;Face recognition;Ecosystems;Legislation;Pressing;Speech recognition;Regulation;Security","","","","29","IEEE","11 Dec 2024","","","IEEE","IEEE Conferences"
"Biomedical Image Reconstruction: From the Foundations to Deep Neural Networks","M. T. McCann; M. Unser",NA; NA,Biomedical Image Reconstruction: From the Foundations to Deep Neural Networks,"","2019","","","","","Biomedical imaging is a vast and diverse field. There are a plethora of imaging devices using light, X-rays, sound waves, magnetic fields, electrons, or protons, to measure structures ranging from nano to macroscale. In many cases, computer software is needed to turn the signals collected by the hardware into a meaningful image. These computer algorithms are similarly diverse and numerous. This survey presents a wide swath of biomedical image reconstruction algorithms under a single framework. It is a coherent, yet brief survey of some six decades of research. The underpinning theory of the techniques are described and practical considerations for designing reconstruction algorithms for use in biomedical systems form the central theme of each chapter. The unifying framework deployed throughout the monograph models imaging modalities as combinations of a small set of building blocks, which identify connections between modalities Thus, the user can quickly port ideas and computer code from one to the next. Furthermore, reconstruction algorithms can treat the imaging model as a black. box, meaning that one algorithm can work for many modalities. This provides a pragmatic approach to designing effective reconstruction algorithms. This monograph is written in a tutorial style that concisely introduces students, researchers and practitioners to the development and design of effective biomedical image reconstruction algorithms.","","9781680836516","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8922856.pdf&bkn=8922855&pdfType=book","","","","","","","","5 Dec 2019","","","now","Now Foundations and Trends Books"
"Apache Spark for Machine Learning: Build and deploy high-performance big data AI solutions for large-scale clusters","D. Gowda",NA,Apache Spark for Machine Learning: Build and deploy high-performance big data AI solutions for large-scale clusters,"","2024","","","","","Develop your data science skills with Apache Spark to solve real-world problems for Fortune 500 companies using scalable algorithms on large cloud computing clustersKey FeaturesApply techniques to analyze big data and uncover valuable insights for machine learningLearn to use cloud computing clusters for training machine learning models on large datasetsDiscover practical strategies to overcome challenges in model training, deployment, and optimizationPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionIn the world of big data, efficiently processing and analyzing massive datasets for machine learning can be a daunting task. Written by Deepak Gowda, a data scientist with over a decade of experience and 30+ patents, this book provides a hands-on guide to mastering Spark’s capabilities for efficient data processing, model building, and optimization. With Deepak’s expertise across industries such as supply chain, cybersecurity, and data center infrastructure, he makes complex concepts easy to follow through detailed recipes. This book takes you through core machine learning concepts, highlighting the advantages of Spark for big data analytics. It covers practical data preprocessing techniques, including feature extraction and transformation, supervised learning methods with detailed chapters on regression and classification, and unsupervised learning through clustering and recommendation systems. You’ll also learn to identify frequent patterns in data and discover effective strategies to deploy and optimize your machine learning models. Each chapter features practical coding examples and real-world applications to equip you with the knowledge and skills needed to tackle complex machine learning tasks. By the end of this book, you’ll be ready to handle big data and create advanced machine learning models with Apache Spark.What you will learnMaster Apache Spark for efficient, large-scale data processing and analysisUnderstand core machine learning concepts and their applications with SparkImplement data preprocessing techniques for feature extraction and transformationExplore supervised learning methods – regression and classification algorithmsApply unsupervised learning for clustering tasks and recommendation systemsDiscover frequent pattern mining techniques to uncover data trendsWho this book is forThis book is ideal for data scientists, ML engineers, data engineers, students, and researchers who want to deepen their knowledge of Apache Spark’s tools and algorithms. It’s a must-have for those struggling to scale models for real-world problems and a valuable resource for preparing for interviews at Fortune 500 companies, focusing on large dataset analysis, model training, and deployment.","","9781835460016","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10763461.pdf&bkn=10763460&pdfType=book","","","","","","","","22 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Software Architecture for Web Developers: An introductory guide for developers striving to take the first steps toward software architecture or just looking to grow as professionals","M. R. Ghidersa",NA,Software Architecture for Web Developers: An introductory guide for developers striving to take the first steps toward software architecture or just looking to grow as professionals,"","2022","","","","","Discover an accessible pathway to advancing your career and becoming a web architect by building a solid technical ground in software architectureKey FeaturesFollow your desired career path that leads to a lucrative job as a web architectDevelop a solid technical background in software architecture using real-world practices and patternsLearn proven techniques and design considerations from an industry expertBook DescriptionLarge-scale web applications require you to write code efficiently following business and architectural considerations. They require web developers to understand the impact of their work on the system and how they can evolve the product. With this handbook, every developer will find something to take away. This book will help web developers looking to change projects or work on a new project in understanding the context of the application, along with how some design decisions or patterns fit better in their application’s architecture. It acts as a guide, taking you through different levels of professional growth with a focus on best practices, coding guidelines, business considerations, and soft skills that will help you gain the knowledge to craft a career in web development. Finally, you’ll work with examples and ways of applying the discussed concepts in practical situations. By the end of this book, you’ll have gained valuable insights into what it means to be a web architect, as well as the impact architecture has on a web application.What you will learnUnderstand the context of software architecture, from shaping the product to delivery and beyondBecome well versed in what a web architect’s role meansExplore go-to key concepts for every time you try your hand at app developmentAnalyze the importance of relationships with stakeholdersGet acquainted with the benefits of well-designed architectureDig into and solve myths web developers have come across or created along the wayWho this book is forThis book is for web developers who want to become web architects. Beginner-level web developers will be able to develop a strong technical background, and experienced web developers will learn techniques to become better professionals by understanding the web architect's role and the impact of efficient architecture on their projects.","","9781803231617","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163019.pdf&bkn=10163018&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Mastering Python Design Patterns: Craft essential Python patterns by following core design principles","K. Ayeva; S. Kasampalis",NA; NA,Mastering Python Design Patterns: Craft essential Python patterns by following core design principles,"","2024","","","","","Explore Python design patterns such as observer, proxy, throttling, dependency injection, and anti-patterns to develop efficient and scalable applications Key FeaturesMaster essential design principles to build robust software architecture with the latest features in Python 3.10Apply proven design patterns to solve complex problems efficientlyUnderstand anti-patterns to avoid common pitfalls in Python programmingPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionAs software systems become increasingly complex, maintaining code quality, scalability, and efficiency can be a daunting challenge. Mastering Python Design Patterns is an essential resource that equips you with the tools you need to overcome these hurdles and create robust, scalable applications. The book delves into design principles and patterns in Python, covering both classic and modern patterns, and showing you how to apply them to solve daily challenges as a Python developer or architect. This new edition covers creational, structural, behavioral, and architectural patterns, including concurrency, asynchronous, and performance patterns. You'll explore how these patterns are relevant to various domains, such as event handling, concurrency, distributed systems, and testing. Whether you're working on user interfaces (UIs), web apps, APIs, data pipelines, or AI models, this book equips you with the knowledge to build robust and maintainable software. The book also presents Python anti-patterns, helping you avoid common pitfalls and ensuring your code remains clean and efficient. By the end of this book, you'll be able to confidently apply classic and modern Python design patterns to build robust, scalable applications.What you will learnMaster fundamental design principles and SOLID conceptsBecome familiar with Gang of Four (GoF) patterns and apply them effectively in PythonExplore architectural design patterns to architect robust systemsDelve into concurrency and performance patterns for optimized codeDiscover distributed systems patterns for scalable applicationsGet up to speed with testing patterns to ensure code reliability and maintainabilityDevelop modular, decoupled systems and manage dependencies efficientlyWho this book is forWith a focus on intermediate and advanced Python programmers, this book offers valuable insights into the best practices for software design, backed by real-world examples and decades of experience. The book is also an excellent resource for software architects and team leaders who want to improve code quality and maintainability across their projects. Prior Python proficiency, including syntax, data structures, and OOP will help you get the most out of this book. ","","9781837637652","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10559420.pdf&bkn=10559419&pdfType=book","","","","","","","","17 Jun 2024","","","Packt Publishing","Packt Publishing eBooks"
"Building AI Applications with OpenAI APIs: Leverage ChatGPT, Whisper, and DALL-E APIs to build 10 innovative AI projects","M. Yanev",NA,"Building AI Applications with OpenAI APIs: Leverage ChatGPT, Whisper, and DALL-E APIs to build 10 innovative AI projects","","2024","","","","","Improve your app development skills by building a ChatGPT clone, code bug fixer, quiz generator, translation app, email auto-reply, PowerPoint generator, and moreKey FeaturesTransition into an expert AI developer by mastering ChatGPT concepts, including fine-tuning and integrationsGain hands-on experience through real-world projects covering a wide range of AI applicationsImplement payment systems in your applications by integrating the ChatGPT API with StripePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionUnlock the power of AI in your applications with ChatGPT with this practical guide that shows you how to seamlessly integrate OpenAI APIs into your projects, enabling you to navigate complex APIs and ensure seamless functionality with ease. This new edition is updated with key topics such as OpenAI Embeddings, which’ll help you understand the semantic relationships between words and phrases. You’ll find out how to use ChatGPT, Whisper, and DALL-E APIs through 10 AI projects using the latest OpenAI models, GPT-3.5, and GPT-4, with Visual Studio Code as the IDE. Within these projects, you’ll integrate ChatGPT with frameworks and tools such as Flask, Django, Microsoft Office APIs, and PyQt. You’ll get to grips with NLP tasks, build a ChatGPT clone, and create an AI code bug-fixing SaaS app. The chapters will also take you through speech recognition, text-to-speech capabilities, language translation, generating email replies, creating PowerPoint presentations, and fine-tuning ChatGPT, along with adding payment methods by integrating the ChatGPT API with Stripe. By the end of this book, you’ll be able to develop, deploy, and monetize your own groundbreaking applications by harnessing the full potential of ChatGPT APIs.What you will learnDevelop a solid foundation in using the OpenAI API for NLP tasksBuild, deploy, and integrate payments into various desktop and SaaS AI applicationsIntegrate ChatGPT with frameworks such as Flask, Django, and Microsoft Office APIsUnleash your creativity by integrating DALL-E APIs to generate stunning AI art within your desktop appsExperience the power of Whisper API's speech recognition and text-to-speech featuresFind out how to fine-tune ChatGPT models for your specific use caseMaster AI embeddings to measure the relatedness of text stringsWho this book is forThis book is for a diverse range of professionals, including programmers, entrepreneurs, and software enthusiasts. Beginner programmers, Python developers exploring AI applications with ChatGPT, software developers integrating AI technology, and web developers creating AI-powered web applications with ChatGPT will find this book beneficial. Scholars and researchers working on AI projects with ChatGPT will also find it valuable. Basic knowledge of Python and familiarity with APIs is needed to understand the topics covered in this book.","","9781835884010","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10740987.pdf&bkn=10740986&pdfType=book","","","","","","","","1 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Tutorial on Amortized Optimization","B. Amos",NA,Tutorial on Amortized Optimization,"","2023","","","","","Optimization is a ubiquitous modeling tool and is often deployed in settings which repeatedly solve similar instances of the same problem. Amortized optimization methods use learning to predict the solutions to problems in these settings, exploiting the shared structure between similar problem instances. These methods have been crucial in variational inference and reinforcement learning and are capable of solving optimization problems many orders of magnitudes times faster than traditional optimization methods that do not use amortization. In this tutorial, the author presents an introduction to the amortized optimization foundations behind these advancements and overviews their applications in variational inference, sparse coding, gradient-based meta-learning, control, reinforcement learning, convex optimization, optimal transport, and deep equilibrium networks. Of practical use for the reader, is the source code accompanying the Implementation and Software Examples chapter. This tutorial provides the reader with a complete source for understanding the theory behind and implementing amortized optimization in many machine learning applications. It will be of interest to students and practitioners alike.","","9781638282099","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10168058.pdf&bkn=10168057&pdfType=book","","","","","","","","29 Jun 2023","","","now","Now Foundations and Trends Books"
"Applying Math with Python: Over 70 practical recipes for solving real-world computational math problems","S. Morley",NA,Applying Math with Python: Over 70 practical recipes for solving real-world computational math problems,"","2022","","","","","Discover easy-to-follow solutions and techniques to help you to implement applied mathematical concepts such as probability, calculus, and equations using Python's numeric and scientific librariesKey FeaturesCompute complex mathematical problems using programming logic with the help of step-by-step recipesLearn how to use Python libraries for computation, mathematical modeling, and statisticsDiscover simple yet effective techniques for solving mathematical equations and apply them in real-world statisticsBook DescriptionThe updated edition of Applying Math with Python will help you solve complex problems in a wide variety of mathematical fields in simple and efficient ways. Old recipes have been revised for new libraries and several recipes have been added to demonstrate new tools such as JAX. You'll start by refreshing your knowledge of several core mathematical fields and learn about packages covered in Python's scientific stack, including NumPy, SciPy, and Matplotlib. As you progress, you'll gradually get to grips with more advanced topics of calculus, probability, and networks (graph theory). Once you’ve developed a solid base in these topics, you’ll have the confidence to set out on math adventures with Python as you explore Python's applications in data science and statistics, forecasting, geometry, and optimization. The final chapters will take you through a collection of miscellaneous problems, including working with specific data formats and accelerating code. By the end of this book, you'll have an arsenal of practical coding solutions that can be used and modified to solve a wide range of practical problems in computational mathematics and data science.What you will learnBecome familiar with basic Python packages, tools, and libraries for solving mathematical problemsExplore real-world applications of mathematics to reduce a problem in optimizationUnderstand the core concepts of applied mathematics and their application in computer scienceFind out how to choose the most suitable package, tool, or technique to solve a problemImplement basic mathematical plotting, change plot styles, and add labels to plots using MatplotlibGet to grips with probability theory with the Bayesian inference and Markov Chain Monte Carlo (MCMC) methodsWho this book is forWhether you are a professional programmer or a student looking to solve mathematical problems computationally using Python, this is the book for you. Advanced mathematics proficiency is not a prerequisite, but basic knowledge of mathematics will help you to get the most out of this Python math book. Familiarity with the concepts of data structures in Python is assumed.","","9781804616802","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162905.pdf&bkn=10162904&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Ablating Concepts in Text-to-Image Diffusion Models","N. Kumari; B. Zhang; S. -Y. Wang; E. Shechtman; R. Zhang; J. -Y. Zhu",Carnegie Mellon University; Tsinghua University; Carnegie Mellon University; Adobe Research; Adobe Research; Carnegie Mellon University,2023 IEEE/CVF International Conference on Computer Vision (ICCV),"15 Jan 2024","2023","","","22634","22645","Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated concept while preserving closely related concepts in the model.","2380-7504","979-8-3503-0718-4","10.1109/ICCV51070.2023.02074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10377865","","Training;Computer vision;Computational modeling;Data models;Internet","","34","","82","IEEE","15 Jan 2024","","","IEEE","IEEE Conferences"
"Generating Creative Images With DALL-E 3: Create accurate images with effective prompting for real-world applications","H. Picano",NA,Generating Creative Images With DALL-E 3: Create accurate images with effective prompting for real-world applications,"","2024","","","","","Learn to craft fine art prints, NFTs, and captivating covers for books and magazines with Dall-E 3 and ChatGPTKey FeaturesExplore Dall-E 3's diverse practical applications across art, design, education, and beyondMaster AI-generated art creation through step-by-step tutorials, ranging from basic to advanced projectsEnhance your prompt crafting skills with the exclusive prompt cheat sheetPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionUnveil the extraordinary capabilities of the groundbreaking AI model, DALL-E 3, as it transforms text prompts into accurate images. This book addresses the challenge of creating meaningful images by writing prompts, guiding you step by step through creating stunning visual art regardless of your skill level. Prepare to delve deep into the inner workings of DALL-E 3's architecture and training process. With clear explanations, practical tutorials, and real-world examples that can be easily applied, you’ll unlock secrets to creating awe-inspiring AI-generated art, from fine art prints to digital designs. This book provides comprehensive insights into various lens options, camera angles, lighting techniques, and art movements, helping you integrate AI capabilities with your artistic skills. You’ll also learn to create NFTs that can be monetized and gain invaluable insights into designing compelling covers, all within the ethical boundaries of AI-generated art. And with the invaluable prompt cheat sheet by your side, you’ll hone your skills in formulating captivating prompts for diverse purposes. By the end of this book, you’ll have learned how to produce generative AI art at a rapid pace and relatively low cost and push the boundaries of imagination with DALL-E 3.What you will learnMaster DALL-E 3's architecture and training methodsCreate fine prints and other AI-generated art with precisionSeamlessly blend AI with traditional artistryAddress ethical dilemmas in AI artExplore the future of digital creativityImplement practical optimization techniques for your artistic endeavorsWho this book is forWhether you’re an artist looking to integrate AI into your work, a designer seeking new creative horizons, a tech enthusiast intrigued by the intersection of art and artificial intelligence, an educator in the fields of art and technology, or a curious individual venturing into AI-generated art, this book is for you. For anyone interested in the innovative fusion of creativity and technology, the DALL-E 3 Guide to AI Artistry offers invaluable insights and practical skills that you can apply right away.","","9781835089903","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10522539.pdf&bkn=10522538&pdfType=book","","","","","","","","8 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"A Permutation-based Reversible Data Hiding Method with Zero Visual Distortion","W. Zhu; K. Wong; M. Kuribayashi","Monash University Malaysia, Malaysia; Monash University Malaysia, Malaysia; Tohoku University, Japan",2024 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),"27 Jan 2025","2024","","","1","6","Recently, several approaches have been designed to hide data in portable document format (PDF) files. These approaches have demonstrated their advantages in different application scenarios, including copyright verification, covert communication/steganography, and content forensics. However, they often suffer from visual distortion or lack universal applicability. In this work, we propose a reversible and transparent method that exploits the coding properties of text objects (i.e., substrings) in a PDF-compliant document to embed data. In particular, the position information of the substrings is adjusted to hide data, where each unique permutation of the substrings encodes a bit sequence. Subsequently, the distance of each substring from the left margin is corrected so that the processed PDF has the exact layout or appearance of the original PDF, hence completely preserving the quality of the original PDF file. In the best-case scenario, to hide one bit of data, 5.88 bits of the PDF file are required, i.e., 1 : 5.88. In addition, this method can be deployed in tandem with conventional data hiding methods to hide more data and to hide data in different ways.","2640-0103","979-8-3503-6733-1","10.1109/APSIPAASC63619.2025.10849141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10849141","","Visualization;Forensics;Layout;Passwords;Information processing;Portable document format;Distortion;Encryption;Multimedia communication;Payloads","","1","","19","IEEE","27 Jan 2025","","","IEEE","IEEE Conferences"
"Revisiting Black-box Ownership Verification for Graph Neural Networks","R. Zhou; K. Yang; X. Wang; W. H. Wang; J. Xu",University of Utah; University of Utah; Stevens Insititute of Technology; Stevens Insititute of Technology; University of Utah,2024 IEEE Symposium on Security and Privacy (SP),"5 Sep 2024","2024","","","2478","2496","Graph Neural Networks (GNNs) have emerged as powerful tools for processing graph-structured data, enabling applications in various domains. Yet, GNNs are vulnerable to model extraction attacks, imposing risks to intellectual property. To mitigate model extraction attacks, model ownership verification is considered an effective method. However, throughout a series of empirical studies, we found that the existing GNN ownership verification methods either mandate unrealistic conditions or present unsatisfactory accuracy under the most practical settings—the black-box setting where the verifier only requires access to the final output (e.g., posterior probability) of the target model and the suspect model.Inspired by the studies, we propose a new, black-box GNN ownership verification method that involves local independent models and shadow surrogate models to train a classifier for performing ownership verification. Our method boosts the verification accuracy by exploiting two insights: (1) We consider the overall behaviors of the target model for decision-making, better utilizing its holistic fingerprinting; (2) We enrich the fingerprinting of the target model by masking a subset of features of its training data, injecting extra information to facilitate ownership verification.To assess the effectiveness of our proposed method, we perform an intensive series of evaluations with 5 popular datasets, 5 mainstream GNN architectures, and 16 different settings. Our method achieves nearly perfect accuracy with a marginal impact on the target model in all cases, significantly outperforming the existing methods and enlarging their practicality. We also demonstrate that our method maintains robustness against adversarial attempts to evade the verification.","2375-1207","979-8-3503-3130-1","10.1109/SP54263.2024.00232","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646808","","Privacy;Accuracy;Decision making;Closed box;Training data;Fingerprint recognition;Graph neural networks","","","","67","IEEE","5 Sep 2024","","","IEEE","IEEE Conferences"
"Watermarking Deep Neural Networks in Image Processing","Y. Quan; H. Teng; Y. Chen; H. Ji","School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Department of Mathematics, National University of Singapore, Singapore",IEEE Transactions on Neural Networks and Learning Systems,"3 May 2021","2021","32","5","1852","1865","Publishing/sharing pretrained deep neural network (DNN) models is a common practice in the community of computer vision. The increasing popularity of pretrained models has made it a serious concern: how to protect the intellectual properties of model owners and avert illegal usages by malicious attackers. This article aims at developing a framework for watermarking DNNs, with a particular focus on low-level image processing tasks that map images to images. Using image denoising and superresolution as case studies, we develop a black-box watermarking method for pretrained models, which exploits the overparameterization of the DNNs in image processing. In addition, an auxiliary module for visualizing the watermark information is proposed for further verification. Extensive experiments show that the proposed watermarking framework has no noticeable impact on model performance and enjoys the robustness against the often-seen attacks.","2162-2388","","10.1109/TNNLS.2020.2991378","National Natural Science Foundation of China(grant numbers:61872151,U1611461,61602184); Natural Science Foundation of Guangdong Province(grant numbers:2017A030313376); Fundamental Research Funds for Central Universities of China(grant numbers:x2js-D2181690); Singapore MOE AcRF(grant numbers:R146000229114,MOE2017-T2-2-156); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093125","Black box;deep learning;neural network;watermark","Watermarking;Image processing;Task analysis;Computational modeling;Training;Data models;Manifolds","","45","","52","IEEE","13 May 2020","","","IEEE","IEEE Journals"
"IID-Net: Image Inpainting Detection Network via Neural Architecture Search and Attention","H. Wu; J. Zhou","State Key Laboratory of Internet of Things for Smart City, University of Macau, Taipa, Macau; State Key Laboratory of Internet of Things for Smart City, University of Macau, Taipa, Macau",IEEE Transactions on Circuits and Systems for Video Technology,"8 Mar 2022","2022","32","3","1172","1185","Deep learning (DL) has demonstrated its powerful capabilities in the field of image inpainting, which could produce visually plausible results. Meanwhile, the malicious use of advanced image inpainting tools (e.g. removing key objects to report fake news, erasing visible copyright watermarks, etc.) has led to increasing threats to the reliability of image data. To fight against the inpainting forgeries (not only DL-based but also traditional ones), in this work, we propose a novel end-to-end Image Inpainting Detection Network (IID-Net), to detect the inpainted regions at pixel accuracy. The proposed IID-Net consists of three sub-blocks: the enhancement block, the extraction block and the decision block. Specifically, the enhancement block aims to enhance the inpainting traces by using hierarchically combined special layers. The extraction block, automatically designed by Neural Architecture Search (NAS) algorithm, is targeted to extract features for the actual inpainting detection tasks. To further optimize the extracted latent features, we integrate global and local attention modules in the decision block, where the global attention reduces the intra-class differences by measuring the similarity of global features, while the local attention strengthens the consistency of local features. Furthermore, we thoroughly study the generalizability of our IID-Net, and find that different training data could result in vastly different generalization capability. By carefully examining 10 popular inpainting methods, we identify that the IID-Net trained on only one specific deep inpainting method exhibits desirable generalizability; namely, the obtained IID-Net can accurately detect and localize inpainting manipulations for various unseen inpainting methods as well. Extensive experimental results are presented to validate the superiority of the proposed IID-Net, compared with the state-of-the-art competitors. Our results would suggest that common artifacts are shared across diverse image inpainting methods. Finally, we build a public inpainting dataset of 10K image pairs for future research in this area.","1558-2205","","10.1109/TCSVT.2021.3075039","Macau Science and Technology Development Fund(grant numbers:SKL-IOTSC-2021-2023,077/2018/A2,0060/2019/A1); Research Committee at the University of Macau(grant numbers:MYRG2018-00029-FST,MYRG2019-00023-FST); Natural Science Foundation of China(grant numbers:61971476); Alibaba Group through Alibaba Innovative Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9410590","Inpainting forensics;generalizability;deep neural networks","Feature extraction;Forensics;Forgery;Training;Task analysis;Semantics;Computer architecture","","64","","73","IEEE","22 Apr 2021","","","IEEE","IEEE Journals"
"A Survey of Text Classification With Transformers: How Wide? How Large? How Long? How Accurate? How Expensive? How Safe?","J. Fields; K. Chovanec; P. Madiraju","Business Analytics, Concordia University Wisconsin-Ann Arbor, Mequon, WI, USA; Department of Computer Science, Marquette University, Milwaukee, WI, USA; Department of Computer Science, Marquette University, Milwaukee, WI, USA",IEEE Access,"12 Jan 2024","2024","12","","6518","6531","Text classification in natural language processing (NLP) is evolving rapidly, particularly with the surge in transformer-based models, including large language models (LLM). This paper presents an in-depth survey of text classification techniques across diverse benchmarks, addressing applications from sentiment analysis to chatbot-driven question-answering. Methodologically, it utilizes NLP-facilitated approaches such as co-citation and bibliographic coupling alongside traditional research techniques. Because new use cases continue to emerge in this dynamic field, the study proposes an expanded taxonomy of text classification applications, extending the focus beyond unimodal (text-only) inputs to explore the emerging field of multimodal classification. While offering a comprehensive review of text classification with LLMs, this review highlights novel questions that arise when approaching the task with transformers: It evaluates the use of multimodal data, including text, numeric, and columnar data, and discusses the evolution of text input lengths (tokens) for long text classification; it covers the historical development of transformer-based models, emphasizing recent advancements in LLMs; it evaluates model accuracy on 358 datasets across 20 applications, with results challenging the assumption that LLMs are universally superior, revealing unexpected findings related to accuracy, cost, and safety; and it explores issues related to cost and access as models become increasingly expensive. Finally, the survey discusses new social and ethical implications raised when using LLMs for text classification, including bias and copyright. Throughout, the review emphasizes the importance of a nuanced understanding of model performance and a holistic approach to deploying transformer-based models in real-world applications.","2169-3536","","10.1109/ACCESS.2024.3349952","Northwestern Mutual Data Science Institute and NSF(grant numbers:1950826); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10380590","NLP;text classification;transformers;survey","Text categorization;Transformers;Surveys;Task analysis;Taxonomy;Data models;Chatbots","","18","","175","CCBYNCND","4 Jan 2024","","","IEEE","IEEE Journals"
"ChatGPT for Conversational AI and Chatbots: Learn how to automate conversations with the latest large language model technologies","A. Thompson",NA,ChatGPT for Conversational AI and Chatbots: Learn how to automate conversations with the latest large language model technologies,"","2024","","","","","Explore ChatGPT technologies to create state-of-the-art chatbots and voice assistants, and prepare to lead the AI revolutionKey FeaturesLearn how to leverage ChatGPT to create innovative conversational AI solutions for your organizationHarness LangChain and delve into step-by-step LLM application development for conversational AIGain insights into security, privacy, and the future landscape of large language models and conversational AIPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionChatGPT for Conversational AI and Chatbots is a definitive resource for exploring conversational AI, ChatGPT, and large language models. This book introduces the fundamentals of ChatGPT and conversational AI automation. You’ll explore the application of ChatGPT in conversation design, the use of ChatGPT as a tool to create conversational experiences, and a range of other practical applications. As you progress, you’ll delve into LangChain, a dynamic framework for LLMs, covering topics such as prompt engineering, chatbot memory, using vector stores, and validating responses. Additionally, you’ll learn about creating and using LLM-enabling tools, monitoring and fine tuning, LangChain UI tools such as LangFlow, and the LangChain ecosystem. You’ll also cover popular use cases, such as using ChatGPT in conjunction with your own data. Later, the book focuses on creating a ChatGPT-powered chatbot that can comprehend and respond to queries directly from your unique data sources. The book then guides you through building chatbot UIs with ChatGPT API and some of the tools and best practices available. By the end of this book, you’ll be able to confidently leverage ChatGPT technologies to build conversational AI solutions.What you will learnGain a solid understanding of ChatGPT and its capabilities and limitationsUnderstand how to use ChatGPT for conversation designDiscover how to use advanced LangChain techniques, such as prompting, memory, agents, chains, vector stores, and toolsCreate a ChatGPT chatbot that can answer questions about your own dataDevelop a chatbot powered by ChatGPT APIExplore the future of conversational AI, LLMs, and ChatGPT alternativesWho this book is forThis book is for tech-savvy readers, conversational AI practitioners, engineers, product owners, business analysts, and entrepreneurs wanting to integrate ChatGPT into conversational experiences and explore the possibilities of this game-changing technology. Anyone curious about using internal data with ChatGPT and looking to stay up to date with the developments in large language models will also find this book helpful. Some expertise in coding and standard web design concepts would be useful, along with familiarity with conversational AI terminology, though not essential.","","9781805122357","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769210.pdf&bkn=10769209&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Semantic sensing for data innovation","G. Cherry; N. Kazantsev; T. Rai; S. Williams; A. Wright; T. Street; K. Wells; A. J. Cook; T. Kanellos","vHive, School of Veterinary Medicine, University of Surrey, Guildford, UK; Institute for Manufacturing (IfM), University of Cambridge, Cambridge, UK; vHive, School of Veterinary Medicine, University of Surrey, Guildford, UK; Limitless Research Ltd, Smart7 Business Hub, 19 Park Lane Business Centre, Nottingham, UK; Outcomes Research, Zoetis, Loughlinstown, County Dublin, Ireland; vHive, School of Veterinary Medicine, University of Surrey, Guildford, UK; vHive, School of Veterinary Medicine, University of Surrey, Guildford, UK; vHive, School of Veterinary Medicine, University of Surrey, Guildford, UK; Outcomes Research, Zoetis, Loughlinstown, County Dublin, Ireland",International Conference on AI and the Digital Economy (CADE 2023),"21 Nov 2023","2023","2023","","182","189","Industrial regulation to protect privacy, intellectual property and proprietary information often restricts data sharing - an important prerequisite for developing services in the digital economy. Social media data is publicly available for data mining but requires intensive cleaning and harmonisation before analysis. This paper reveals the process of semantic sensing to convert social network tweets into meaningful insights. Our research question is: how to realise semantic sensing for data innovation? We use design science research to develop an artefact-ontology that collects tweets by pet owners talking about their itchy pet into knowledge graphs, including symptoms, location, breed, timestamp and potential cause and converts them into a thematic map of the regional occurrence of symptoms and potential treatment needs, providing vital information for data innovation. The semantic engine can predict potential causes of itching from the tweet, so a Chatbot may contact the pet owner, inviting them to a veterinary screening. Animal health and pharma companies can use this information to position their services. Our theoretical contribution is a process of semantic sensing, which is a vital part of dynamic capability. Although limited to animal health, the results could be transferred to other contexts.","","978-1-83953-959-6","10.1049/icp.2023.2612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10324612","","","","","","","","21 Nov 2023","","","IET","IET Conferences"
"Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization","R. C. Barron; V. Grantcharov; S. Wanna; M. E. Eren; M. Bhattarai; N. Solovyev; G. Tompkins; C. Nicholas; K. Ø. Rasmussen; C. Matuszek; B. S. Alexandrov","Analytics, Intelligence and Technology Division, Los Alamos National Laboratory, Los Alamos, New Mexico, USA; University of New Mexico; University of Texas at Austin; University of Maryland Baltimore County; Theoretical Division, Los Alamos National Laboratory, New Mexico, USA; Theoretical Division, Los Alamos National Laboratory, New Mexico, USA; Analytics, Intelligence and Technology Division, Los Alamos National Laboratory, Los Alamos, New Mexico, USA; University of Maryland Baltimore County; Theoretical Division, Los Alamos National Laboratory, New Mexico, USA; University of Maryland Baltimore County; Theoretical Division, Los Alamos National Laboratory, New Mexico, USA",2024 International Conference on Machine Learning and Applications (ICMLA),"4 Mar 2025","2024","","","1669","1676","Large Language Models (LLMs) are pre-trained on large-scale corpora and excel in numerous general natural language processing (NLP) tasks, such as question answering (QA). Despite their advanced language capabilities, when it comes to domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations, knowledge cut-offs, and lack of knowledge attributions. Additionally, fine tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and time consuming process. The retrieval-augmented generation (RAG) process has recently emerged as a method capable of optimization of LLM responses, by referencing them to a predetermined ontology. It was shown that using a Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into account relevant sub-graphs that preserve the information in a structured manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM framework, that integrates RAG with KG and a vector store (VS) that store factual domain specific information. Importantly, to avoid hallucinations in the KG, we build these highly domain-specific KGs and VSs without the use of LLMs, but via NLP, data mining, and nonnegative tensor factorization with automatic model selection. Pairing our RAG with a domain-specific: (i) KG (containing structured information), and (ii) VS (containing unstructured information) enables the development of domain-specific chat-bots that attribute the source of information, mitigate hallucinations, lessen the need for fine-tuning, and excel in highly domain-specific question answering tasks. We pair SMART-SLIC with chain-of-thought prompting agents. The framework is designed to be generalizable to adapt to any specific or specialized domain. In this paper, we demonstrate the question answering capabilities of our framework on a corpus of scientific publications on malware analysis and anomaly detection.","1946-0759","979-8-3503-7488-9","10.1109/ICMLA61862.2024.00258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903241","Artificial Intelligence;Retrieval Augmented Generation;Knowledge Graph;Natural Language Processing;Non-Negative Tensor Factorization;Topic Modeling;Agents","Tensors;Accuracy;Retrieval augmented generation;Knowledge graphs;Ontologies;Question answering (information retrieval);Vectors;Malware;Reliability;Tuning","","","","53","IEEE","4 Mar 2025","","","IEEE","IEEE Conferences"
"Filtering Resistant Large Language Model Watermarking via Style Injection","Z. Guo; G. Li; J. Huang; X. Zhang; Z. Qian; S. Li","Fudan University, Shanghai, China; Fudan University, Shanghai, China; Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; Fudan University, Shanghai, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","The exorbitant cost of training Large Language Models (LLMs) makes it essential to protect the models from illegal copying and unauthorized usage. Recent attempts at LLM protection utilize black-box watermarking schemes, which embed distinctive input-output mapping (i.e., trigger set) directly into the models. However, most of them construct trigger inputs by injecting abnormal characters into normal text, which can easily be filtered out by unauthorized users, leading to a failure in watermark verification. In this paper, we propose a novel filtering-resistant LLM watermarking scheme, which takes advantage of imperceptible text styles to trigger the watermark. To achieve this, we adopt a trigger generation network to transform normal text into stylized sentences, which are assigned a specific watermarking label to build the trigger set. We then fine-tune the LLMs on both the trigger sets and clean samples for watermark embedding and performance stabilization. To boost watermark accuracy, we further propose a feature separation loss term to distinguish between normal and trigger inputs. Experimental results indicate the effectiveness of our proposed scheme for resisting the filtering attack.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10888201","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10888201","Large language model;black-box watermarking;filtering attack","Training;Accuracy;Filtering;Large language models;Watermarking;Transforms;Resists;Signal processing;Robustness;Speech processing","","","","27","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral Pedestrian Detection","T. Kim; S. Chung; D. Yeom; Y. Yu; H. G. Kim; Y. M. Ro","Integrated Vision and Language Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 291, Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea; Integrated Vision and Language Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 291, Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea; Integrated Vision and Language Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 291, Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea; Integrated Vision and Language Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 291, Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea; Department of Image Science and Arts, GSAIM, Chung-Ang University, Seoul, Republic of Korea; Integrated Vision and Language Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 291, Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea",IEEE Transactions on Circuits and Systems for Video Technology,"","2024","PP","99","1","1","Multispectral pedestrian detection is attractive for around-the-clock applications due to the complementary information between RGB and thermal modalities. However, current models often fail to detect pedestrians in certain cases (e.g., thermal-obscured pedestrians), particularly due to the modality bias learned from statistically biased datasets. In this paper, we investigate how to mitigate modality bias in multispectral pedestrian detection using a Large Language Model (LLM). Accordingly, we design a Multispectral Chain-of-Thought (MSCoT) prompting strategy, which prompts the LLM to perform multispectral pedestrian detection. Moreover, we propose a novel Multispectral Chain-of-Thought Detection (MSCoTDet) framework that integrates MSCoT prompting into multispectral pedestrian detection. To this end, we design a Language-driven Multi-modal Fusion (LMF) strategy that enables fusing the outputs of MSCoT prompting with the detection results of vision-based multispectral pedestrian detection models. Extensive experiments validate that MSCoTDet effectively mitigates modality biases and improves multispectral pedestrian detection.","1558-2205","","10.1109/TCSVT.2024.3524645","Center for Applied Research in Artificial Intelligence (CARAI) grant funded by DAPA and ADD(grant numbers:UD230017TD); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10819422","Multispectral Chain-of-Thought Detection;Language-driven Multi-modal Fusion;Multispectral Pedestrian Detection;Large Language Models","Pedestrians;Detectors;Circuits and systems;Training;Feature extraction;Visualization;Large language models;Accuracy;Training data;Reliability","","","","","IEEE","31 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Layer Sequence Extraction of Optimized DNNs Using Side-Channel Information Leaks","Y. Sun; G. Jiang; X. Liu; P. He; S. -K. Lam","Cyber Security Research Centre @ NTU (CYSREN) and the School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; School of Computer Science and Technology, Ocean University of China, Qingdao, China; School of Computer, National University of Defense Technology, Changsha, China; School of Computer Science and Technology, Ocean University of China, Qingdao, China; Cyber Security Research Centre @ NTU (CYSREN) and the School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,"19 Sep 2024","2024","43","10","3102","3115","Deep neural network (DNN) intellectual property (IP) models must be kept undisclosed to avoid revealing trade secrets. Recent works have devised machine learning techniques that leverage on side-channel information leakage of the target platform to reverse engineer DNN architectures. However, these works fail to perform successful attacks on DNNs that have undergone performance optimizations (i.e., operator fusion) using DNN compilers, e.g., Apache tensor virtual machine (TVM). We propose a two-phase attack framework to infer the layer sequences of optimized DNNs through side-channel information leakage. In the first phase, we use a recurrent network with multihead attention components to learn the intra and interlayer fusion patterns from GPU traces of TVM-optimized DNNs, in order to accurately predict the operation distribution. The second phase uses a model to learn the run-time temporal correlations between operations and layers, which enables the prediction of layer sequence. An encoding strategy is proposed to overcome the convergence issues faced by existing learning-based methods when inferring the layer sequences of optimized DNNs. Extensive experiments show that our learning-based framework outperforms state-of-the-art DNN model extraction techniques. Our framework is also the first to effectively reverse engineer both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) using side-channel leakage.","1937-4151","","10.1109/TCAD.2024.3389554","NTU-DESAY SV Research Program(grant numbers:2018-0980); Ministry of Education, Singapore, under its Academic Research Fund Tier 2(grant numbers:MOE-T2EP20121-0008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500842","Deep neural network (DNN);layer inference;model stealing;side-channel attack (SCA);tensor virtual machine (TVM)","Graphics processing units;Kernel;Computational modeling;Timing;Training;Optimization;Topology","","","","35","IEEE","16 Apr 2024","","","IEEE","IEEE Journals"
"Self-adaptive Prompt-tuning for Event Extraction in Ancient Chinese Literature","J. Zhang; Y. Wei; Y. Zhu; B. Wu",Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications,2023 International Joint Conference on Neural Networks (IJCNN),"2 Aug 2023","2023","","","1","8","Extracting different types of war events from ancient Chinese literature is significant, as war is an important factor in driving the development of Chinese history. The existing trend of event extraction models utilizes template-based generative approaches, which do not take into account the brevity and obscurity of ancient Chinese, as well as the diversity of templates for similar event types. In this paper, we propose a novel Knowledge Graph-based generative event extraction framework with a self-Adaptive Prompt (KGAP) for ancient Chinese war. Specifically, we construct a self-adaptive prompt, which considers its unique trigger words for different types of wars and is designed to solve the problem of the similarity in events. Moreover, we construct a semantic knowledge graph of ancient literature, assisting the pre-trained language model to better understand the ancient Chinese text. Since there is no public dataset for the ancient Chinese event extraction task, we provide an event extraction dataset and conduct experiments on it. Experimental results show that our model is more state-of-the-art than both the classification-based and generative-based methods for event extraction in ancient Chinese literature.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10191495","National Natural Science Foundation of China(grant numbers:61972047); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191495","Ancient Chinese;Event Extraction;Self-adaptive Prompt;Knowledge Graph","Semantics;Neural networks;Knowledge graphs;Market research;History;Task analysis","","","","44","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Adapting Large Language Models for Smart Contract Defects Detection in The Open Network Blockchain","H. Ge; Z. Wang; R. Liu; Z. Qiu; J. Xia; T. Chen; H. Zhu","Jiangsu University of Science and Technology, Zhenjiang, China; Jiangsu University of Science and Technology, Zhenjiang, China; Jiangsu University of Science and Technology, Zhenjiang, China; Jiangsu University of Science and Technology, Zhenjiang, China; Jiangsu University of Science and Technology, Zhenjiang, China; University of Electronic Science and Technology of China, China; Shanghai Jiao Tong University, China",IEEE Internet of Things Journal,"","2025","PP","99","1","1","Smart contracts on The Open Network (TON) have become vital in IoT applications due to their low latency and high scalability. However, the unique architectural features of TON introduce specialized vulnerabilities that existing tools fail to address comprehensively. In this paper, we propose a novel defect detection framework that combines Large Language Models (LLMs) for automated defect discovery with a locatable call graph for precise and efficient code analysis. Our method identifies four new types of TON-specific defects: Ignore Errors Mode Usage, Premature Acceptance, Pseudo Deletion, and Improper Jetton Refund. Evaluated on 1640 real-world smart contracts written in FunC and Tact, the framework uncovers 669 defects, with an average of one defect every 2.45 code segments. The detection achieves an average F1 score of 99.75% for FunC and 100% for Tact contracts. Additionally, our approach demonstrates lightweight computational overhead, consuming only 12.6MB of memory and achieving a mean response time of 0.05s. These results highlight the accuracy, efficiency, and practicality of our framework for securing TON-based smart contracts in IoT ecosystems.","2327-4662","","10.1109/JIOT.2025.3553917","Zhenjiang City Key Research and Development Program(grant numbers:SH2022013); Jiangsu Provincial Key Research and Development Program(grant numbers:BE2022783); National Natural Science Foundation of China(grant numbers:62476113); Ministry of Science and Technology Xiong'an New Area Science and Technology Innovation Special Course(grant numbers:2022XAGG0126); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10937768","TON;FunC;static analysis","","","","","","IEEE","25 Mar 2025","","","IEEE","IEEE Early Access Articles"
"ChatGPT and AI for Accountants: A practitioner's guide to harnessing the power of GenAI to revolutionize your accounting practice","D. S. Dell; D. M. Akpan",NA; NA,ChatGPT and AI for Accountants: A practitioner's guide to harnessing the power of GenAI to revolutionize your accounting practice,"","2024","","","","","Elevate your accounting skills by applying ChatGPT across audit, tax, consulting, and beyondKey FeaturesLeverage the impact of AI on modern accounting, from audits to corporate governanceUse ChatGPT to streamline your accounting tasks with practical hands-on techniquesUnderstand the impact of AI in accounting through in-depth chapters covering various domains, including ethical considerations and data analyticsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionIn the fast-paced AI world, accounting professionals are increasingly challenged by the complexities of AI. Many struggle to integrate these advanced tools into their workflows, leading to a sense of overwhelm. ChatGPT for Accounting bridges this gap by not only simplifying AI concepts but also offering practical insights for its application in various accounting domains. This book takes you from the foundational principles of Generative Artificial Intelligence (GAI) to its practical applications in audits, tax planning, practice management, fraud examination, financial analysis, and beyond. Each chapter equips you with essential skills, showing you how AI can revolutionize internal control systems, enhance recruitment processes, streamline marketing plans, optimize tax strategies, and boost efficiency in audits. You’ll then advance to exploring the role of AI in forensic accounting, financial analysis, managerial accounting, and corporate governance, while also addressing ethical and security implications. Concluding with a reflective outlook on the promises and challenges of AI, you’ll gain a holistic view of the future of accounting. By the end of this book, you’ll be equipped with the knowledge to harness the power of AI effectively and ethically, transforming your accounting practice and staying ahead in the ever-evolving landscape.What you will learnUnderstand the fundamentals of AI and its impact on the accounting sectorGrasp how AI streamlines and enhances the auditing process for high accuracyUncover the potential of AI in simplifying tax processes and ensuring complianceGet to grips with using AI to identify discrepancies and prevent financial fraudMaster the art of AI-powered data analytics for informed decision-makingGain insights into seamlessly integrating AI tools within existing accounting systemsStay ahead in the evolving landscape of AI-led accounting tools and practicesWho this book is forWhether you're a seasoned accounting professional, a C-suite executive, a business owner, an accounting educator, a student of accounting, or a technology enthusiast, this book provides the knowledge and insights you need to navigate the changing landscape in applying GAI technology to make a difference in all you do. An appreciation and understanding of the accounting process and concepts will be beneficial.","","9781835462256","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769389.pdf&bkn=10769388&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Digital Steganography and Watermarking for Digital Images: A Review of Current Research Directions","O. Evsutin; A. Melman; R. Meshcheryakov","Department of Cyber-Physical Systems Information Security, Higher School of Economics, National Research University, Moscow, Russia; Department of Information Systems Security, Tomsk State University of Control Systems and Radioelectronics, Tomsk, Russia; Laboratory of Cyber-Physical Systems, V. A. Trapeznikov Institute of Control Sciences of Russian Academy of Sciences, Moscow, Russia",IEEE Access,"21 Sep 2020","2020","8","","166589","166611","The development of information technology has led to a significant increase in the share of multimedia traffic in data networks. This has necessitated to solve the following information security tasks in relation to multimedia data: protection against leakage of confidential information, as well as identifying the source of the leak; ensuring the impossibility of unauthorized changes; copyright protection for digital objects. To solve such kind of problems, methods of steganography and watermarking are designed that implement embedding in digital objects hidden information sequences for various purposes. In this paper, an overview of promising research in the specified area is provided. First of all, we provide basic information about this field of research and consider the main applications of its methods. Next, we review works demonstrating current trends in the development of methods and algorithms for data hiding in digital images. This review is not exhaustive; it focuses on contemporary works illustrating current research directions in the field of information embedding in digital images. This is the main feature of review, which distinguishes it from previously published reviews. The paper concludes with an analysis of identified problems in the field of digital steganography and digital watermarking.","2169-3536","","10.1109/ACCESS.2020.3022779","Russian Science Foundation(grant numbers:19-71-00106); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187785","Information security;digital image steganography;digital watermarking;data hiding","Watermarking;Digital images;Distortion;Robustness;Data mining;Receivers","","56","","105","CCBY","8 Sep 2020","","","IEEE","IEEE Journals"
"Student Placement Probabilistic Assessment Using Emotional Quotient With Machine Learning: A Conceptual Case Study","N. Kathirisetty; R. Jadeja; H. K. Thakkar; D. Garg; C. -C. Chang; R. Mahadeva; S. P. Patole","Department of Computer Engineering, Marwadi University, Rajkot, Gujarat, India; Department of Electrical Engineering, Marwadi University, Rajkot, Gujarat, India; Department of Computer Science and Engineering, School of Technology, Pandit Deendayal Energy University, Gandhinagar, Gujarat, India; Distinguished Professor, School of Computer Science and Artificial Intelligence, SR University, Warangal, Telangana, India; Institute of Education and Center of Teacher Education, Taiwan Marine Education Center, National Taiwan Ocean University, Keelung, Taiwan; Department of Physics, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates; Department of Physics, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates",IEEE Access,"14 Nov 2023","2023","11","","125716","125737","The primary goal of the proposed study is to measure a student’s Emotional Quotient (EQ) for job placement and to correlate the EQ with the ability of the student to survive in the industry. EQ is expected to be influenced by several demographic factors such as age, gender, academic performance, location, parental education, parental income, and family structure. However, the previous studies did not consider these factors. To validate the correlation of demographic factors with EQ, developed a data set considering the above-mentioned factors followed by designing several Machine Learning (ML) based ensemble techniques. Ratings for each parameter ranged from 1 to 10. Based on that, evaluating the results to choose the best approach. The primary goal of this inquiry was to identify the factors other than academic performance that prompt a student to get hired by a company more quickly. The final grade for all students is determined by ascertaining a student’s emotional and intellectual ability. The fundamental contribution of this study is the establishment of a student’s emotional calculation, along with an explanation of how to evaluate it, the advantages of such a concept, its psychometric validity, and its difficulties. The background and variety of validation studies will show how measurements can accurately and rigorously evaluate the behavioral level of EQ.","2169-3536","","10.1109/ACCESS.2023.3330320","Ministry of Science and Technology, Taiwan(grant numbers:NSTC 111-2410-H-019-002-MY3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308794","Emotional intelligence (EI);machine learning (ML);data mining (DM);student placements;student assessment;intelligence quotient (IQ);emotional quotient (EQ)","Computational modeling;Psychology;Machine learning;Data models;Correlation;Education;Data mining;Emotion recognition;Behavioral sciences;Performance evaluation;Engineering students","","2","","43","CCBYNCND","6 Nov 2023","","","IEEE","IEEE Journals"
"The AI Value Playbook: How to make AI work in the real world","L. Weaver-Lambert",NA,The AI Value Playbook: How to make AI work in the real world,"","2024","","","","","Learn from real-world examples how leveraging AI, including machine learning and generative AI, is imperative for businesses to navigate risk, drive value, and gain a competitive advantageKey FeaturesUnderstand machine learning and generative AI terminology, concepts, and the AI technology stack.Learn from diverse real-world case studies narrated by business leaders in their own voice.Apply a value-driven approach to AI applications across multiple business sectors.Book DescriptionBusiness leaders are challenged by the speed of AI innovation and how to navigate disruption and uncertainty. This book is a crucial resource for those who want to understand how to leverage AI to drive business value, drawn from the firsthand experience of those who have been implementing this technology successfully. The AI Value Playbook focuses on questions frequently posed by leaders and boards. How can businesses adapt to these emerging technologies? How can they start building and deploying AI as a strategic asset to drive efficiency? What risks or threats need to be considered? How quickly can value be created? This book is a response to those demands. In a series of in-depth and wide-ranging conversations with practitioners, from CEOs leading new generative AI-based companies to Data Scientists and CFOs working in more traditional companies. Our experts share their hard-earned wisdom, talking candidly about their successes and failures, and what excites them about the future. These interviews offer unique insights for business leaders to apply to their own organizations. The book distils a value-driven playbook for how AI can be put to work today.What you will learnFundamentals of AI concepts and the tech stackHow AI works with real-world practical applicationsHow to integrate into your company's overall strategyHow to incorporate generative AI in your processesHow to drive value with sector-wide examplesHow to organize an AI-driven operating modelHow to use AI for competitive advantageThe dos and don'ts of AI applicationWho this book is forThe AI Value Playbook is aimed at supporting non-technical executives and board members to quickly formulate a perspective on how to integrate AI. This book addresses the gap in data and AI knowledge in leadership teams that have an appetite for nuanced, targeted and practical solutions. It includes which levers and processes to consider to future-proof their business. It speaks to an audience interested in understanding how AI can drive value for their organisations.","","9781835467596","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769334.pdf&bkn=10769333&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Hardware Trojan Detection in Open-Source Hardware Designs Using Machine Learning","V. T. Hayashi; W. Vicente Ruggiero","Department of Computer Engineering and Digital Systems (PCS), Escola Politécnica, Laboratory of Computer Architecture and Networks (LARC), Universidade de São Paulo, São Paulo, Brazil; Department of Computer Engineering and Digital Systems (PCS), Escola Politécnica, Laboratory of Computer Architecture and Networks (LARC), Universidade de São Paulo, São Paulo, Brazil",IEEE Access,"4 Mar 2025","2025","13","","37771","37788","The globalization of the hardware supply chain reduces costs but increases security challenges with the potential insertion of hardware trojans by third parties. Traditional detection methods face scalability limitations by relying solely on simple examples (e.g., AES). Although open-source hardware promotes transparency, it does not guarantee security. In this research, Natural Language Processing (NLP) and Machine Learning (ML) techniques were applied to identify hardware trojans in complex open hardware designs (e.g., RISC-V, MIPS). Using data from existing benchmarks (ISCAS85-89, TrustHub) and synthetic data generated with Large Language Models (LLM), a dataset of 3,808 instances was used in this research. The approach using TF-IDF and Decision Tree (DT) achieved 97.26%, surpassing the state of the art. The use of LLMs with prompt optimization achieved a recall of 99%, minimizing false negatives. A novel framework integrating NLP, ML, and LLMs was developed to enhance the security of open-source hardware.","2169-3536","","10.1109/ACCESS.2025.3546156","Graduate Program in Electrical Engineering (PPGEE) from the Escola Politécnica, Universidade de São Paulo; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10904479","Hardware security;hardware trojan;machine learning;natural language processing;large language models;open hardware;open source","Hardware;Trojan horses;Machine learning;Hardware design languages;Open source hardware;Benchmark testing;Static analysis;Integrated circuit modeling;Hardware security;Computer architecture","","","","85","CCBY","26 Feb 2025","","","IEEE","IEEE Journals"
"Adversarial AI Attacks, Mitigations, and Defense Strategies: A cybersecurity professional's guide to AI attacks, threat modeling, and securing AI with MLSecOps","J. Sotiropoulos",NA,"Adversarial AI Attacks, Mitigations, and Defense Strategies: A cybersecurity professional's guide to AI attacks, threat modeling, and securing AI with MLSecOps","","2024","","","","","Understand how adversarial attacks work against predictive and generative AI, and learn how to safeguard AI and LLM projects with practical examples leveraging OWASP, MITRE, and NISTKey FeaturesUnderstand the connection between AI and security by learning about adversarial AI attacksDiscover the latest security challenges in adversarial AI by examining GenAI, deepfakes, and LLMsImplement secure-by-design methods and threat modeling, using standards and MLSecOps to safeguard AI systemsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionAdversarial attacks trick AI systems with malicious data, creating new security risks by exploiting how AI learns. This challenges cybersecurity as it forces us to defend against a whole new kind of threat. This book demystifies adversarial attacks and equips cybersecurity professionals with the skills to secure AI technologies, moving beyond research hype or business-as-usual strategies. The strategy-based book is a comprehensive guide to AI security, presenting a structured approach with practical examples to identify and counter adversarial attacks. This book goes beyond a random selection of threats and consolidates recent research and industry standards, incorporating taxonomies from MITRE, NIST, and OWASP. Next, a dedicated section introduces a secure-by-design AI strategy with threat modeling to demonstrate risk-based defenses and strategies, focusing on integrating MLSecOps and LLMOps into security systems. To gain deeper insights, you’ll cover examples of incorporating CI, MLOps, and security controls, including open-access LLMs and ML SBOMs. Based on the classic NIST pillars, the book provides a blueprint for maturing enterprise AI security, discussing the role of AI security in safety and ethics as part of Trustworthy AI. By the end of this book, you’ll be able to develop, deploy, and secure AI systems effectively.What you will learnUnderstand poisoning, evasion, and privacy attacks and how to mitigate themDiscover how GANs can be used for attacks and deepfakesExplore how LLMs change security, prompt injections, and data exposureMaster techniques to poison LLMs with RAG, embeddings, and fine-tuningExplore supply-chain threats and the challenges of open-access LLMsImplement MLSecOps with CIs, MLOps, and SBOMsWho this book is forThis book tackles AI security from both angles - offense and defense. AI builders (developers and engineers) will learn how to create secure systems, while cybersecurity professionals, such as security architects, analysts, engineers, ethical hackers, penetration testers, and incident responders will discover methods to combat threats and mitigate risks posed by attackers. The book also provides a secure-by-design approach for leaders to build AI with security in mind. To get the most out of this book, you’ll need a basic understanding of security, ML concepts, and Python. ","","9781835088678","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769347.pdf&bkn=10769346&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"AI Product Manager's Handbook: Build, integrate, scale, and optimize products to grow as an AI product manager","I. Bratsis",NA,"AI Product Manager's Handbook: Build, integrate, scale, and optimize products to grow as an AI product manager","","2024","","","","","Whether you're a seasoned professional or a newcomer to the world of AI product management, this is your definitive guide. Embark on a transformative journey into the future of intelligent product management.Key FeaturesChart a successful career path in the AI product management field Packed with real-world examples, practical insights, and actionable strategiesNavigate the complexities of AI product development and evolve your existing products Book DescriptionThis book will provide you with a detailed roadmap for successfully building, maintaining, and evolving artificial intelligence (AI)-driven products, serving as an indispensable companion on your journey to becoming an effective AI PM. We'll explore the AI landscape, demystify complex terms, and walk you through infrastructure, algorithms, and deployment strategies. You’ll master essential skills to understand the optimal flow of AI processes, learn about the product development life cycle from ideation to deployment, and familiarize yourself with commonly used model development techniques. We'll discuss the intricacies of building products natively with AI, as well as evolving traditional software product to AI products. Regardless of your use case, we’ll show you how you can craft compelling stories to captivate your audience. We'll help you find the right balance between foundational product design elements and the unique aspects of managing AI products, so you can prioritize wisely. We’ll also explore career considerations for AI PMs. By the end of this book, you will understand the importance of AI integration and be able to explore emerging AI/ML models like Generative AI and LLMs. You’ll discover open-source capabilities and best practices for ideating, building, and deploying AI products across verticals.What you will learnPlan your AI PM roadmap and navigate your career with clarity and confidenceGain a foundational understanding of AI/ML capabilitiesAlign your product strategy, nurture your team, and navigate the ongoing challenges of cost, tech, compliance, and risk managementIdentify pitfalls and green flags for optimal commercializationSeparate hype from reality and identify quick wins for AI enablement and GenAIUnderstand how to develop and manage both native and evolving AI productsBenchmark product success from a holistic perspectiveWho this book is forThis book is for aspiring and experienced product managers, as well as other professionals interested in incorporating AI into their products. Foundational knowledge of AI is expected and reinforced. If you are looking to better understand machine learning principles and data science methodologies, you will benefit from this book, particularly if you’re in a role where the application of AI/ML directly influences marketing outcomes and business strategies.","","9781835882856","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769304.pdf&bkn=10769303&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Information Bottleneck based Self-distillation: Boosting Lightweight Network for Real-world Super-Resolution","H. Zhu; Z. Chen; S. Liu","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Tencent, Palo Alto, CA, USA",IEEE Transactions on Circuits and Systems for Video Technology,"","2024","PP","99","1","1","Most existing single-image super-resolution (SISR) methods focus on addressing predefined uniform degradations, such as bicubic. However, these methods often perform poorly in real-world scenarios due to complicated and varying realistic degradations. In this paper, we propose a novel information bottle-neck-based self-distillation method (IBSD) to boost lightweight networks for real-world image super-resolution. The proposed IBSD leverages the principle of information bottleneck to guide SR networks to learn invariant correlations from low-resolution (LR) to high-resolution (HR) across various degradations, thereby improving their generalization capacity. Specifically, the target super-resolution network (i.e., student) is interpreted as a Markov chain, and the distillation process is carried out through two modules. Mutual information (MI) estimation networks are used to quantify the mutual information between adjacent nodes within the Markov chain. To enhance robustness against blur and noise in real-world scenarios, an auxiliary loss with a progressive soft target is employed to better identify what is effective for reconstruction in the high-frequency domain. Minimizing the mutual information while preserving task-relevant features can help remove information that reflects spurious correlations between specific degradations and reconstructed targets. Experiments conducted on real-world image super-resolution datasets demonstrate that our proposed method can significantly improve the performance of recent lightweight SR models without adding any extra inference complexity, and it outperforms existing self-distillation approaches. Code is publicly available at https://github.com/hanzhu1121/IBSD.","1558-2205","","10.1109/TCSVT.2024.3519136","National Natural Science Foundation of China(grant numbers:62036005); Tencent; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10804836","Knowledge distillation;Real-world super-resolution;Information bottleneck principle;Lightweight network;Mutual information estimation","Degradation;Superresolution;Mutual information;Image reconstruction;Feature extraction;Correlation;Kernel;Robustness;Overfitting;Transformers","","","","","IEEE","17 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Platform and Model Design for Responsible AI: Design and build resilient, private, fair, and transparent machine learning models","A. Kapoor; S. Chatterjee",NA; NA,"Platform and Model Design for Responsible AI: Design and build resilient, private, fair, and transparent machine learning models","","2023","","","","","Craft ethical AI projects with privacy, fairness, and risk assessment features for scalable and distributed systems while maintaining explainability and sustainability Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn risk assessment for machine learning frameworks in a global landscapeDiscover patterns for next-generation AI ecosystems for successful product designMake explainable predictions for privacy and fairness-enabled ML trainingBook DescriptionAI algorithms are ubiquitous and used for tasks, from recruiting to deciding who will get a loan. With such widespread use of AI in the decision-making process, it’s necessary to build an explainable, responsible, transparent, and trustworthy AI-enabled system. With Platform and Model Design for Responsible AI, you’ll be able to make existing black box models transparent. You’ll be able to identify and eliminate bias in your models, deal with uncertainty arising from both data and model limitations, and provide a responsible AI solution. You’ll start by designing ethical models for traditional and deep learning ML models, as well as deploying them in a sustainable production setup. After that, you’ll learn how to set up data pipelines, validate datasets, and set up component microservices in a secure and private way in any cloud-agnostic framework. You’ll then build a fair and private ML model with proper constraints, tune the hyperparameters, and evaluate the model metrics. By the end of this book, you’ll know the best practices to comply with data privacy and ethics laws, in addition to the techniques needed for data anonymization. You’ll be able to develop models with explainability, store them in feature stores, and handle uncertainty in model predictions.What you will learnUnderstand the threats and risks involved in ML modelsDiscover varying levels of risk mitigation strategies and risk tiering toolsApply traditional and deep learning optimization techniques efficientlyBuild auditable and interpretable ML models and feature storesUnderstand the concept of uncertainty and explore model explainability toolsDevelop models for different clouds including AWS, Azure, and GCPExplore ML orchestration tools such as Kubeflow and Vertex AIIncorporate privacy and fairness in ML models from design to deploymentWho this book is forThis book is for experienced machine learning professionals looking to understand the risks and leakages of ML models and frameworks, and learn to develop and use reusable components to reduce effort and cost in setting up and maintaining the AI ecosystem.","","9781803249773","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251240.pdf&bkn=10251239&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Digital Modernity","K. O’Hara",NA,Digital Modernity,"","2022","","","","","Modernity is a social, cultural, or historical descriptor for a certain type of society or set of social arrangements. It is a contentious and disputed term, often understood implicitly. It is a way of describing and classifying highly complex, dynamic, and emergent aggregate social phenomena, and so dramatically simplifies such contexts. However, the language of modernity remains attractive to commentators, academics, and policymakers. In this monograph, the author reviews the literature that characterises what is called digital modernity. Digital modernity narratives focus on the possibilities of the data gathered by an ambient data infrastructure, enabled by ubiquitous devices such as the smartphone, and activities such as social networking and e-commerce. It is characterised by (1) a subjunctive outlook where people’s choices can be anticipated and improved upon, (2) the valorisation of disruptive innovation on demand, and (3) control provided by data analysis within a virtual realm that can be extended and applied to the physical world. The author explored the synergies and tensions between these three aspects as well as the opportunities for and dilemmas posed by misinformation. The author identifies five principles that emerge from the study of relevant texts and business models and concludes by contrasting digital modernity with other theories of the 21st century information society. Narratives of digital modernity are useful because they help explain the development of technology. It matters because many influential people accept, and often generate, the digital modernity narrative. Given digital modernity’s strong association with the Web, it is a central topic for Web Science as the interdisciplinary study of the World Wide Web from the technological, social, and individual points of view.","","9781638281054","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9955459.pdf&bkn=9955458&pdfType=book","","","","","","","","21 Nov 2022","","","now","Now Foundations and Trends Books"
"Mastering Cloud Security Posture Management (CSPM): Secure multi-cloud infrastructure across AWS, Azure, and Google Cloud using proven techniques","Q. Nomani; J. Davila; R. Khan",NA; NA; NA,"Mastering Cloud Security Posture Management (CSPM): Secure multi-cloud infrastructure across AWS, Azure, and Google Cloud using proven techniques","","2024","","","","","Strengthen your security posture in all aspects of CSPM technology, from security infrastructure design to implementation strategies, automation, and remedial actions using operational best practices across your cloud environmentKey FeaturesChoose the right CSPM tool to rectify cloud security misconfigurations based on organizational requirementsOptimize your security posture with expert techniques for in-depth cloud security insightsImprove your security compliance score by adopting a secure-by-design approach and implementing security automationPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThis book will help you secure your cloud infrastructure confidently with cloud security posture management (CSPM) through expert guidance that’ll enable you to implement CSPM effectively, ensuring an optimal security posture across multi-cloud infrastructures. The book begins by unraveling the fundamentals of cloud security, debunking myths about the shared responsibility model, and introducing key concepts such as defense-in-depth, the Zero Trust model, and compliance. Next, you’ll explore CSPM's core components, tools, selection criteria, deployment strategies, and environment settings, which will be followed by chapters on onboarding cloud accounts, dashboard customization, cloud assets inventory, configuration risks, and cyber threat hunting. As you progress, you’ll get to grips with operational practices, vulnerability and patch management, compliance benchmarks, and security alerts. You’ll also gain insights into cloud workload protection platforms (CWPPs). The concluding chapters focus on Infrastructure as Code (IaC) scanning, DevSecOps, and workflow automation, providing a thorough understanding of securing multi-cloud environments. By the end of this book, you’ll have honed the skills to make informed decisions and contribute effectively at every level, from strategic planning to day-to-day operations.What you will learnFind out how to deploy and onboard cloud accounts using CSPM toolsUnderstand security posture aspects such as the dashboard, asset inventory, and risksExplore the Kusto Query Language (KQL) and write threat hunting queriesExplore security recommendations and operational best practicesGet to grips with vulnerability, patch, and compliance management, and governanceFamiliarize yourself with security alerts, monitoring, and workload protection best practicesManage IaC scan policies and learn how to handle exceptionsWho this book is forIf you’re a cloud security administrator, security engineer, or DevSecOps engineer, you’ll find this book useful every step of the way—from proof of concept to the secured, automated implementation of CSPM with proper auto-remediation configuration. This book will also help cybersecurity managers, security leads, and cloud security architects looking to explore the decision matrix and key requirements for choosing the right product. Cloud security enthusiasts who want to enhance their knowledge to bolster the security posture of multi-cloud infrastructure will also benefit from this book.","","9781837630707","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769298.pdf&bkn=10769297&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Artificial Intelligence for Cybersecurity: Develop AI approaches to solve cybersecurity problems in your organization","B. Kolosnjaji; H. Xiao; P. Xu; A. Zarras",NA; NA; NA; NA,Artificial Intelligence for Cybersecurity: Develop AI approaches to solve cybersecurity problems in your organization,"","2024","","","","","Gain well-rounded knowledge of AI methods in cybersecurity and obtain hands-on experience in implementing them to bring value to your organizationKey FeaturesFamiliarize yourself with AI methods and approaches and see how they fit into cybersecurityLearn how to design solutions in cybersecurity that include AI as a key featureAcquire practical AI skills using step-by-step exercises and code examplesPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionArtificial intelligence offers data analytics methods that enable us to efficiently recognize patterns in large-scale data. These methods can be applied to various cybersecurity problems, from authentication and the detection of various types of cyberattacks in computer networks to the analysis of malicious executables. Written by a machine learning expert, this book introduces you to the data analytics environment in cybersecurity and shows you where AI methods will fit in your cybersecurity projects. The chapters share an in-depth explanation of the AI methods along with tools that can be used to apply these methods, as well as design and implement AI solutions. You’ll also examine various cybersecurity scenarios where AI methods are applicable, including exercises and code examples that’ll help you effectively apply AI to work on cybersecurity challenges. The book also discusses common pitfalls from real-world applications of AI in cybersecurity issues and teaches you how to tackle them. By the end of this book, you’ll be able to not only recognize where AI methods can be applied, but also design and execute efficient solutions using AI methods.What you will learnRecognize AI as a powerful tool for intelligence analysis of cybersecurity dataExplore all the components and workflow of an AI solutionFind out how to design an AI-based solution for cybersecurityDiscover how to test various AI-based cybersecurity solutionsEvaluate your AI solution and describe its advantages to your organizationAvoid common pitfalls and difficulties when implementing AI solutionsWho this book is forThis book is for machine learning practitioners looking to apply their skills to overcome cybersecurity challenges. Cybersecurity workers who want to leverage machine learning methods will also find this book helpful. Fundamental concepts of machine learning and beginner-level knowledge of Python programming are needed to understand the concepts present in this book. Whether you’re a student or an experienced professional, this book offers a unique and valuable learning experience that will enable you to protect your network and data against the ever-evolving threat landscape.","","9781805123552","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769101.pdf&bkn=10769100&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Causal Inference and Discovery in Python: Unlock the secrets of modern causal machine learning with DoWhy, EconML, PyTorch and more","A. Molak; A. Jaokar",NA; NA,"Causal Inference and Discovery in Python: Unlock the secrets of modern causal machine learning with DoWhy, EconML, PyTorch and more","","2023","","","","","Demystify causal inference and casual discovery by uncovering causal principles and merging them with powerful machine learning algorithms for observational and experimental data Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesExamine Pearlian causal concepts such as structural causal models, interventions, counterfactuals, and moreDiscover modern causal inference techniques for average and heterogenous treatment effect estimationExplore and leverage traditional and modern causal discovery methodsBook DescriptionCausal methods present unique challenges compared to traditional machine learning and statistics. Learning causality can be challenging, but it offers distinct advantages that elude a purely statistical mindset. Causal Inference and Discovery in Python helps you unlock the potential of causality. You’ll start with basic motivations behind causal thinking and a comprehensive introduction to Pearlian causal concepts, such as structural causal models, interventions, counterfactuals, and more. Each concept is accompanied by a theoretical explanation and a set of practical exercises with Python code. Next, you’ll dive into the world of causal effect estimation, consistently progressing towards modern machine learning methods. Step-by-step, you’ll discover Python causal ecosystem and harness the power of cutting-edge algorithms. You’ll further explore the mechanics of how “causes leave traces” and compare the main families of causal discovery algorithms. The final chapter gives you a broad outlook into the future of causal AI where we examine challenges and opportunities and provide you with a comprehensive list of resources to learn more.What you will learnMaster the fundamental concepts of causal inferenceDecipher the mysteries of structural causal modelsUnleash the power of the 4-step causal inference process in PythonExplore advanced uplift modeling techniquesUnlock the secrets of modern causal discovery using PythonUse causal inference for social impact and community benefitWho this book is forThis book is for machine learning engineers, data scientists, and machine learning researchers looking to extend their data science toolkit and explore causal machine learning. It will also help developers familiar with causality who have worked in another technology and want to switch to Python, and data scientists with a history of working with traditional causality who want to learn causal machine learning. It’s also a must-read for tech-savvy entrepreneurs looking to build a competitive edge for their products and go beyond the limitations of traditional machine learning.","","9781804611739","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251332.pdf&bkn=10251331&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Cracking the Data Science Interview: Unlock insider tips from industry experts to master the data science field","L. R. Gonzalez; A. Baltes; A. Stubberfield",NA; NA; NA,Cracking the Data Science Interview: Unlock insider tips from industry experts to master the data science field,"","2024","","","","","Rise above the competition and excel in your next interview with this one-stop guide to Python, SQL, version control, statistics, machine learning, and much moreKey FeaturesAcquire highly sought-after skills of the trade, including Python, SQL, statistics, and machine learningGain the confidence to explain complex statistical, machine learning, and deep learning theoryExtend your expertise beyond model development with version control, shell scripting, and model deployment fundamentalsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe data science job market is saturated with professionals of all backgrounds, including academics, researchers, bootcampers, and Massive Open Online Course (MOOC) graduates. This poses a challenge for companies seeking the best person to fill their roles. At the heart of this selection process is the data science interview, a crucial juncture that determines the best fit for both the candidate and the company. Cracking the Data Science Interview provides expert guidance on approaching the interview process with full preparation and confidence. Starting with an introduction to the modern data science landscape, you’ll find tips on job hunting, resume writing, and creating a top-notch portfolio. You’ll then advance to topics such as Python, SQL databases, Git, and productivity with shell scripting and Bash. Building on this foundation, you'll delve into the fundamentals of statistics, laying the groundwork for pre-modeling concepts, machine learning, deep learning, and generative AI. The book concludes by offering insights into how best to prepare for the intensive data science interview. By the end of this interview guide, you’ll have gained the confidence, business acumen, and technical skills required to distinguish yourself within this competitive landscape and land your next data science job.What you will learnExplore data science trends, job demands, and potential career pathsSecure interviews with industry-standard resume and portfolio tipsPractice data manipulation with Python and SQLLearn about supervised and unsupervised machine learning modelsMaster deep learning components such as backpropagation and activation functionsEnhance your productivity by implementing code versioning through GitStreamline workflows using shell scripting for increased efficiencyWho this book is forWhether you're a seasoned professional who needs to brush up on technical skills or a beginner looking to enter the dynamic data science industry, this book is for you. To get the most out of this book, basic knowledge of Python, SQL, and statistics is necessary. However, anyone familiar with other analytical languages, such as R, will also find value in this resource as it helps you revisit critical data science concepts like SQL, Git, statistics, and deep learning, guiding you to crack through data science interviews.","","9781805120193","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769233.pdf&bkn=10769232&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"The Stable Signature: Rooting Watermarks in Latent Diffusion Models","P. Fernandez; G. Couairon; H. Jégou; M. Douze; T. Furon",Meta AI; Meta AI; Meta AI; Meta AI; Centre Inria de l’Université de Rennes,2023 IEEE/CVF International Conference on Computer Vision (ICCV),"15 Jan 2024","2023","","","22409","22420","Generative image modeling enables a wide range of applications but raises ethical concerns about responsible deployment. We introduce an active content tracing method combining image watermarking and Latent Diffusion Models. The goal is for all generated images to conceal an invisible watermark allowing for future detection and/or identification. The method quickly fine-tunes the latent decoder of the image generator, conditioned on a binary signature. A pre-trained watermark extractor recovers the hidden signature from any generated image and a statistical test then determines whether it comes from the generative model. We evaluate the invisibility and robustness of the watermarks on a variety of generation tasks, showing that the Stable Signature is robust to image modifications. For instance, it detects the origin of an image generated from a text prompt, then cropped to keep 10% of the content, with 90+% accuracy at a false positive rate below 10−6.","2380-7504","979-8-3503-0718-4","10.1109/ICCV51070.2023.02053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10377226","","Ethics;Computer vision;Computational modeling;Watermarking;Robustness;Generators;Decoding","","39","","108","IEEE","15 Jan 2024","","","IEEE","IEEE Conferences"
"Static Automated Program Repair for Heap Properties","R. van Tonder; C. Le Goues",NA; NA,2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),"2 Sep 2018","2018","","","151","162","Static analysis tools have demonstrated effectiveness at finding bugs in real world code. Such tools are increasingly widely adopted to improve software quality in practice. Automated Program Repair (APR) has the potential to further cut down on the cost of improving software quality. However, there is a disconnect between these effective bug-finding tools and APR. Recent advances in APR rely on test cases, making them inapplicable to newly discovered bugs or bugs difficult to test for deterministically (like memory leaks). Additionally, the quality of patches generated to satisfy a test suite is a key challenge. We address these challenges by adapting advances in practical static analysis and verification techniques to enable a new technique that finds and then accurately fixes real bugs without test cases. We present a new automated program repair technique using Separation Logic. At a high-level, our technique reasons over semantic effects of existing program fragments to fix faults related to general pointer safety properties: resource leaks, memory leaks, and null dereferences. The procedure automatically translates identified fragments into source-level patches, and verifies patch correctness with respect to reported faults. In this work we conduct the largest study of automatically fixing undiscovered bugs in real-world code to date. We demonstrate our approach by correctly fixing 55 bugs, including 11 previously undiscovered bugs, in 11 real-world projects.","1558-1225","978-1-4503-5638-1","10.1145/3180155.3180250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453073","Automated Program Repair;Separation Logic","Computer bugs;Maintenance engineering;Tools;Static analysis;Semantics;Software;Safety","","26","1","","","2 Sep 2018","","","IEEE","IEEE Conferences"
"A Unified Open Adapter for Open-World Noisy Label Learning: Data-Centric and Learning-Based Insights","C. -C. Zong; P. -H. Yang; M. -K. Xie; S. -J. Huang","Collaborative Innovation Center of Novel Software Technology and Industrialization, College of Computer Science and Technology, MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; RIKEN Center for Advanced Intelligence Project (AIP), Tokyo, Japan; Collaborative Innovation Center of Novel Software Technology and Industrialization, College of Computer Science and Technology, MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Nanjing University of Aeronautics and Astronautics, Nanjing, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Noisy label learning (NLL) in open-world scenarios poses a novel challenge due to the presence of noisy data from both known and unknown classes. Most existing methods operate under the closed-set assumption, rendering them vulnerable to open-set noise, which significantly degrades their performance. While some approaches attempt to mitigate the impact of open-set examples, they struggle to learn effective discriminative representations for them, leading to unsatisfactory recognition performance. To address these issues, we propose a unified Open Adapter (OpenAda) that identifies open-set noise from both data-centric and learning-based perspectives, and can be easily integrated into mainstream NLL methods to improve their performance and robustness. Specifically, the data-centric part leverages label clusterability to sequentially identify basic clean and basic open-set examples both with high neighbor agreement. The learning-based part integrates one-vs-all classifiers with a progressive open disambiguation strategy to learn a reliable “inlier vs. outlier” boundary for each class. This enables the model to detect challenging open-set examples that partially overlap in the representation space with closed-set ones. Extensive experiments on synthetic and real-world datasets validate the superiority of our approach. Notably, with minor modifications, DivideMix with OpenAda achieves performance improvements of 9.31% and 18.26% on the open-world CIFAR-80 dataset under 80% symmetric noise and 40% asymmetric noise. The code is available at https://github.com/chenchenzong/OpenAda.","1558-2205","","10.1109/TCSVT.2025.3550899","National Natural Science Foundation of China(grant numbers:62222605); National Science and Technology Major Project(grant numbers:2020AAA0107000); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20211517,BK20222012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10925362","Noisy label learning;open-world noise;open adapter;label clusterability","Noise;Noise measurement;Training;US Department of Defense;Robustness;Performance gain;Feature extraction;Circuits and systems;Symmetric matrices;Predictive models","","","","","IEEE","13 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Distilling Multi-level Semantic Cues across Multi-modalities for Face Forgery Detection","L. Yu; T. Xie; C. Liu; G. Jin; Z. Ding; H. Xie","School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; AHU-IAI AI Joint Laboratory, Anhui University, Hefei, Anhui, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; State Key Laboratory of Communication Content Cognition, People’s Daily Online, Beijing, China; Third Research Institute of the Ministry of Public Security, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2024","PP","99","1","1","Existing face forgery detection methods attempt to identify low-level forgery artifacts (e.g., blending boundary, flickering) in spatial-temporal domains or high-level semantic inconsistencies (e.g., abnormal lip movements) between visual-auditory modalities for generalized face forgery detection. However, they still suffer from significant performance degradation when dealing with out-of-domain artifacts, as they only consider single semantic mode inconsistencies, but ignore the complementarity of forgery traces at different levels and different modalities. In this paper, we propose a novel Multi-modal Multi-level Semantic Cues Distillation Detection framework that adopts the teacher-student protocol to focus on both spatial-temporal artifacts and visual-auditory incoherence to capture multi-level semantic cues. Specifically, our framework primarily comprises the Spatial-Temporal Pattern Learning module and the Visual-Auditory Consistency Modeling module. The Spatial-Temporal Pattern Learning module employs a mask-reconstruction strategy, in which the student network learns diverse spatial-temporal patterns from a pixel-wise teacher network to capture low-level forgery artifacts. The Visual-Auditory Consistency Modeling module is designed to enhance the student network’s ability to identify high-level semantic irregularities, with a visual-auditory consistency modeling expert serving as a guide. Furthermore, a novel Real-Similarity loss is proposed to enhance the proximity of real faces in feature space without explicitly penalizing the distance from manipulated faces, which prevents the overfitting in particular manipulation methods and improves the generalization capability. Extensive experiments show that our method substantially improves the generalization and robustness performance. Particularly, our approach outperforms the SOTA detector by 1.4% in generalization performance on DFDC with large domain gaps, and by 2.0% in the robustness evaluation on the FF++ dataset under various extreme settings. Our code is available at https://github.com/TianXie834/M2SD.","1558-2205","","10.1109/TCSVT.2024.3524602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10819430","face forgery detection;spatial-temporal artifacts;transformer;knowledge distillation;metric learning","Forgery;Semantics;Faces;Training;Overfitting;Feature extraction;Detectors;Robustness;Lips;Visualization","","","","","IEEE","31 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Platform Engineering for Architects: Crafting modern platforms as a product","M. Körbächer; A. Grabner; H. Lipsig",NA; NA; NA,Platform Engineering for Architects: Crafting modern platforms as a product,"","2024","","","","","Design and build Internal Developer Platforms (IDPs) with future-oriented design strategies, using the Platform as a Product mindsetKey FeaturesLearn how to design platforms that create value and drive user adoptionBenefit from expert techniques for shifting to a product-centric mindset as an architect and platform teamImplement best practices to understand platform complexity, manage technical debt, and ensure its evolutionPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe rapid pace of technological advancements, the shortage of IT talent, and the complexity of modern systems highlight the need for structured guidance in building resilient, user-centric platforms for cloud-native environments. This book empowers platform engineers and architects to implement value-driven internal development platforms. You’ll learn how to identify end users, understand their challenges, and define the purpose of a platform, with a focus on self-service solutions for modern cloud-native software development, delivery, and operations. The book incorporates real-world examples of building platforms within and for the cloud, leveraging the power of Kubernetes. You’ll learn how adopting a product mindset for architecting and building platforms helps foster successful platform engineering teams. This emphasizes early end-user involvement and provides a framework that gives you the flexibility to easily adapt and extend for future use cases. The book also offers insights into building a sustainable platform without accumulating technical debt. By the end of this book, you’ll be able to drive the design, definition, and implementation of platform capabilities as a product that aligns with your organizational requirements and strategy.What you will learnMake informed decisions based on your organization's platform needsIdentify missing platform capabilities and technical debtDevelop a critical user journey through your platform capabilitiesDefine the purpose, principles, and key performance indicators (KPIs) for your platformUtilize relevant data points for making data-driven product decisionsImplement your own platform reference and target architecturesWho this book is forThis book is for platform architects and solutions architects seeking to enhance their skills in designing and building a platform as a product. It also offers valuable insights for decision-makers, platform engineers, and DevOps professionals. While familiarity with cloud-native concepts, CI/CD, and Kubernetes is beneficial, the book builds on these topics to address self-service, cost management, and technical debt. It’s particularly suited to experts tackling the challenge of integrating diverse domains to create effective internal developer platforms with top-notch operational readiness.","","9781836203582","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769395.pdf&bkn=10769394&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Extracting, Mining and Predicting Users’ Interests from Social Media","F. Zarrinkalam; S. Faralli; G. Piao; E. Bagheri",NA; NA; NA; NA,"Extracting, Mining and Predicting Users’ Interests from Social Media","","2020","","","","","Mining user interests from user behavioral data is critical for many applications. Based on user interests, service providers like advertisers can significantly reduce service delivery costs by offering the most relevant products to their customers. The challenge of accurately and efficiently identifying user interests has been the subject of increasing attention for several years. With the emergence and growing popularity of social media, many users are extensively engaged in social media applications to express their feelings and views about a wide variety of social events/topics as they happen in real time. The abundance of user generated content on social media provides the opportunity to build models that are able to accurately and effectively extract, mine, and predict users’ interests with the hopes of enabling more effective user engagement, better quality delivery of appropriate services, and higher user satisfaction. While traditional methods for building user profiles relied on AI-based preference elicitation techniques that could have been considered intrusive and undesirable by the users, more recent advances are focused on a non-intrusive yet accurate way of determining users’ interests and preferences. In this monograph, the authors cover five important subjects related to the mining of user interests from social media: (1) the foundations of social user interest modeling, (2) techniques that have been adopted or proposed for mining user interests, (3) different evaluation methodologies and benchmark datasets, (4) different applications that have been taking advantage of user interest mining from social media platforms, and (5) existing challenges, open research questions, and opportunities for further work. The monograph is a valuable resource for those who have familiarity with social media mining and the basics of information retrieval (IR) techniques.","","9781680837391","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9250709.pdf&bkn=9250708&pdfType=book","","","","","","","","9 Nov 2020","","","now","Now Foundations and Trends Books"
"Mastering NLP from Foundations to LLMs: Apply advanced rule-based techniques to LLMs and solve real-world business problems using Python","L. Gazit; M. Ghaffari; A. Saxena",NA; NA; NA,Mastering NLP from Foundations to LLMs: Apply advanced rule-based techniques to LLMs and solve real-world business problems using Python,"","2024","","","","","Enhance your NLP proficiency with modern frameworks like LangChain, explore mathematical foundations and code samples, and gain expert insights into current and future trends Key FeaturesLearn how to build Python-driven solutions with a focus on NLP, LLMs, RAGs, and GPTMaster embedding techniques and machine learning principles for real-world applicationsUnderstand the mathematical foundations of NLP and deep learning designsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionDo you want to master Natural Language Processing (NLP) but don’t know where to begin? This book will give you the right head start. Written by leaders in machine learning and NLP, Mastering NLP from Foundations to LLMs provides an in-depth introduction to techniques. Starting with the mathematical foundations of machine learning (ML), you’ll gradually progress to advanced NLP applications such as large language models (LLMs) and AI applications. You’ll get to grips with linear algebra, optimization, probability, and statistics, which are essential for understanding and implementing machine learning and NLP algorithms. You’ll also explore general machine learning techniques and find out how they relate to NLP. Next, you’ll learn how to preprocess text data, explore methods for cleaning and preparing text for analysis, and understand how to do text classification. You’ll get all of this and more along with complete Python code samples. By the end of the book, the advanced topics of LLMs’ theory, design, and applications will be discussed along with the future trends in NLP, which will feature expert opinions. You’ll also get to strengthen your practical skills by working on sample real-world NLP business problems and solutions.What you will learnMaster the mathematical foundations of machine learning and NLP Implement advanced techniques for preprocessing text data and analysis Design ML-NLP systems in PythonModel and classify text using traditional machine learning and deep learning methodsUnderstand the theory and design of LLMs and their implementation for various applications in AIExplore NLP insights, trends, and expert opinions on its future direction and potentialWho this book is forThis book is for deep learning and machine learning researchers, NLP practitioners, ML/NLP educators, and STEM students. Professionals working with text data as part of their projects will also find plenty of useful information in this book. Beginner-level familiarity with machine learning and a basic working knowledge of Python will help you get the best out of this book. ","","9781804616383","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10522559.pdf&bkn=10522558&pdfType=book","","","","","","","","8 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"BenchING: A Benchmark for Evaluating Large Language Models in Following Structured Output Format Instruction in Text-Based Narrative Game Tasks","P. Taveekitworachai; M. F. Dewantoro; Y. Xia; P. Suntichaikul; R. Thawonmas","Graduate School of Information Science and Engineering, Ritsumeikan University, Osaka, Japan; Graduate School of Information Science and Engineering, Ritsumeikan University, Osaka, Japan; Graduate School of Information Science and Engineering, Ritsumeikan University, Osaka, Japan; Graduate School of Information Science and Engineering, Ritsumeikan University, Osaka, Japan; College of Information Science and Engineering, Ritsumeikan University, Osaka, Japan",IEEE Transactions on Games,"","2025","PP","99","1","11","This paper presents BenchING, a new benchmark for evaluating large language models (LLMs) on their ability to follow structured output format instructions in text-based procedural content generation (PCG) tasks. The ability to condition LLMs to output in specified formats proves useful, as downstream components in LLM-integrated games often require structured outputs for exchanging information. However, there is a gap in evaluating this aspect of LLMs, especially in narrative PCG tasks, making it difficult to select LLMs and design games or applications integrating these LLMs. To demonstrate the potential of our benchmark, we evaluate nine LLMs for their ability to generate parseable formatted outputs using five selected text-based PCG tasks. We report on the performance of these LLMs on these tasks. Additionally, we categorize more detailed error types and propose solutions by utilizing LLMs to fix these errors. We also conduct a scaling study, investigating an emergent point of LLMs for their ability to fix malformed formatted content using eight quantized LLMs with varying original sizes from 0.62B to 72.3B. Furthermore, we perform a qualitative study to assess the quality of the generated content. We make our source code and raw data available for future research.","2475-1510","","10.1109/TG.2025.3529117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10840256","Benchmark;JSON;LLMs;XML;YAML","Games;Benchmark testing;XML;Complexity theory;Syntactics;Standards;Large language models;Birds;User interfaces;Servers","","","","","IEEE","14 Jan 2025","","","IEEE","IEEE Early Access Articles"
"Fairness in Search Systems","Y. Fang; A. Singh; Z. Tao",NA; NA; NA,Fairness in Search Systems,"","2024","","","","","Search engines play a crucial role in organizing and delivering information to billions of users worldwide. However, these systems often reflect and amplify existing societal biases and stereotypes through their search results and rankings. This concern has prompted researchers to investigate methods for measuring and reducing algorithmic bias, with the goal of developing more equitable search systems. This monograph presents a comprehensive taxonomy of fairness in search systems and surveys the current research landscape. This work systematically examines how bias manifests across key search components, including query interpretation and processing, document representation and indexing, result ranking algorithms, and system evaluation metrics. By critically analyzing the existing literature, persistent challenges and promising research directions in the pursuit of fairer search systems are identified. The aim is to provide a foundation for future work in this rapidly evolving field while highlighting opportunities to create more inclusive and equitable information retrieval technologies.","","9781638284994","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10816377.pdf&bkn=10816376&pdfType=book","","","","","","","","26 Dec 2024","","","now","Now Foundations and Trends Books"
"AI and 6G Into the Metaverse: Fundamentals, Challenges and Future Research Trends","M. Zawish; F. A. Dharejo; S. A. Khowaja; S. Raza; S. Davy; K. Dev; P. Bellavista","Walton Institute for Information and Communication Systems Science, South East Technological University, Waterford, Ireland; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, UAE; Faculty of Engineering and Technology, University of Sindh, Jamshoro, Pakistan; Department of Electronic Engineering, Quaid-e-Awam University of Engineering, Science and Technology, Larkana, Pakistan; Centre for Sustainable Digital Technologies, Technological University Dublin, Dublin, Ireland; Department of Computer Science, Munster Technological University, Cork, Ireland; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy",IEEE Open Journal of the Communications Society,"29 Jan 2024","2024","5","","730","778","Since Facebook was renamed Meta, a lot of attention, debate, and exploration have intensified about what the Metaverse is, how it works, and the possible ways to exploit it. It is anticipated that Metaverse will be a continuum of rapidly emerging technologies, usecases, capabilities, and experiences that will make it up for the next evolution of the Internet. Several researchers have already surveyed the literature on artificial intelligence (AI) and wireless communications in realizing the Metaverse. However, due to the rapid emergence and continuous evolution of technologies, there is a need for a comprehensive and in-depth survey of the role of AI, 6G, and the nexus of both in realizing the immersive experiences of Metaverse. Therefore, in this survey, we first introduce the background and ongoing progress in augmented reality (AR), virtual reality (VR), mixed reality (MR) and spatial computing, followed by the technical aspects of AI and 6G. Then, we survey the role of AI in the Metaverse by reviewing the state-of-the-art in deep learning, computer vision, and Edge AI to extract the requirements of 6G in Metaverse. Next, we investigate the promising services of B5G/6G towards Metaverse, followed by identifying the role of AI in 6G networks and 6G networks for AI in support of Metaverse applications, and the need for sustainability in Metaverse. Finally, we enlist the existing and potential applications, usecases, and projects to highlight the importance of progress in the Metaverse. Moreover, in order to provide potential research directions to researchers, we underline the challenges, research gaps, and lessons learned identified from the literature review of the aforementioned technologies.","2644-125X","","10.1109/OJCOMS.2024.3349465","Science Foundation Ireland(grant numbers:21/FFP-A/9174); Science Foundation Ireland and the Department of Agriculture, Food and Marine on behalf of the Government of Ireland VistaMilk Research Centre(grant numbers:16/RC/3835); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415393","Metaverse;5G;6G;AI;cloud and edge computing;AR/VR/XR;spatial computing","6G mobile communication;Surveys;Wireless communication;Computer vision;Telepresence;Metaverse;Artificial intelligence","","33","","307","CCBYNCND","29 Jan 2024","","","IEEE","IEEE Journals"
"AI-Powered Commerce: Building the products and services of the future with Commerce.AI","A. Pandharikar; F. Bussler",NA; NA,AI-Powered Commerce: Building the products and services of the future with Commerce.AI,"","2022","","","","","Learn how to use artificial intelligence for product and service innovation, including the diverse use cases of Commerce.AIKey FeaturesLearn how to integrate data and AI in your innovation workflowsUnlock insights into how various industries are using AI for innovationApply your knowledge to real innovation use cases like product strategy and market intelligenceBook DescriptionCommerce.AI is a suite of artificial intelligence (AI) tools, trained on over a trillion data points, to help businesses build next-gen products and services. If you want to be the best business on the block, using AI is a must. Developers and analysts working with AI will be able to put their knowledge to work with this practical guide. You'll begin by learning the core themes of new product and service innovation, including how to identify market opportunities, come up with ideas, and predict trends. With plenty of use cases as reference, you'll learn how to apply AI for innovation, both programmatically and with Commerce.AI. You'll also find out how to analyze product and service data with tools such as GPT-J, Python pandas, Prophet, and TextBlob. As you progress, you'll explore the evolution of commerce in AI, including how top businesses today are using AI. You'll learn how Commerce.AI merges machine learning, product expertise, and big data to help businesses make more accurate decisions. Finally, you'll use the Commerce.AI suite for product ideation and analyzing market trends. By the end of this artificial intelligence book, you'll be able to strategize new product opportunities by using AI, and also have an understanding of how to use Commerce.AI for product ideation, trend analysis, and predictions.What you will learnFind out how machine learning can help you identify new market opportunitiesUnderstand how to use consumer data to create new products and servicesUse state-of-the-art AI frameworks and tools for data analysisLaunch, track, and improve products and services with AIRise above the competition with unparalleled insights from AITurn customer touchpoints into business winsGenerate high-conversion product and service copyWho this book is forThis AI book is for AI developers, data scientists, data product managers, analysts, and consumer insights professionals. The book will guide you through the process of product and service innovation, no matter your pre-existing skillset.","","9781803234076","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162694.pdf&bkn=10162693&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"API Analytics for Product Managers: Understand key API metrics that can help you grow your business","D. Goyal; K. Lane",NA; NA,API Analytics for Product Managers: Understand key API metrics that can help you grow your business,"","2023","","","","","Research, strategize, market, and continuously measure the effectiveness of APIs to meet your SaaS business goals with this practical handbookKey FeaturesTransform your APIs into revenue-generating entities by turning them into productsMeet your business needs by improving the way you research, strategize, market, and measure resultsCreate and implement a variety of metrics to promote growthBook DescriptionAPIs are crucial in the modern market as they allow faster innovation. But have you ever considered your APIs as products for revenue generation? API Analytics for Product Managers takes you through the benefits of efficient researching, strategizing, marketing, and continuously measuring the effectiveness of your APIs to help grow both B2B and B2C SaaS companies. Once you've been introduced to the concept of an API as a product, this fast-paced guide will show you how to establish metrics for activation, retention, engagement, and usage of your API products, as well as metrics to measure the reach and effectiveness of documentation—an often-overlooked aspect of development. Of course, it's not all about the product—as any good product manager knows; you need to understand your customers’ needs, expectations, and satisfaction too. Once you've gathered your data, you’ll need to be able to derive actionable insights from it. This is where the book covers the advanced concepts of leading and lagging metrics, removing bias from the metric-setting process, and bringing metrics together to establish long- and short-term goals. By the end of this book, you'll be perfectly placed to apply product management methodologies to the building and scaling of revenue-generating APIs.What you will learnBuild a long-term strategy for an APIExplore the concepts of the API life cycle and API maturityUnderstand APIs from a product management perspectiveCreate support models for your APIs that scale with the productApply user research principles to APIsExplore the metrics of activation, retention, engagement, and churnCluster metrics together to provide contextExamine the consequences of gameable and vanity metricsWho this book is forIf you’re a product manager, engineer, or product executive charged with making the most of APIs for your SaaS business, then this book is for you. Basic knowledge of how APIs work and what they do is essential before you get started with this book, since the book covers the analytical side of measuring their performance to help your business grow.","","9781803241968","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162233.pdf&bkn=10162232&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Data Augmentation with Python: Enhance deep learning accuracy with data augmentation methods for image, text, audio, and tabular data","D. Haba",NA,"Data Augmentation with Python: Enhance deep learning accuracy with data augmentation methods for image, text, audio, and tabular data","","2023","","","","","Boost your AI and generative AI accuracy using real-world datasets with over 150 functional object-oriented methods and open source libraries Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesExplore beautiful, customized charts and infographics in full colorWork with fully functional OO code using open source libraries in the Python Notebook for each chapterUnleash the potential of real-world datasets with practical data augmentation techniquesBook DescriptionData is paramount in AI projects, especially for deep learning and generative AI, as forecasting accuracy relies on input datasets being robust. Acquiring additional data through traditional methods can be challenging, expensive, and impractical, and data augmentation offers an economical option to extend the dataset. The book teaches you over 20 geometric, photometric, and random erasing augmentation methods using seven real-world datasets for image classification and segmentation. You’ll also review eight image augmentation open source libraries, write object-oriented programming (OOP) wrapper functions in Python Notebooks, view color image augmentation effects, analyze safe levels and biases, as well as explore fun facts and take on fun challenges. As you advance, you’ll discover over 20 character and word techniques for text augmentation using two real-world datasets and excerpts from four classic books. The chapter on advanced text augmentation uses machine learning to extend the text dataset, such as Transformer, Word2vec, BERT, GPT-2, and others. While chapters on audio and tabular data have real-world data, open source libraries, amazing custom plots, and Python Notebook, along with fun facts and challenges. By the end of this book, you will be proficient in image, text, audio, and tabular data augmentation techniques.What you will learnWrite OOP Python code for image, text, audio, and tabular dataAccess over 150,000 real-world datasets from the Kaggle websiteAnalyze biases and safe parameters for each augmentation methodVisualize data using standard and exotic plots in colorDiscover 32 advanced open source augmentation librariesExplore machine learning models, such as BERT and TransformerMeet Pluto, an imaginary digital coding companionExtend your learning with fun facts and fun challengesWho this book is forThis book is for data scientists and students interested in the AI discipline. Advanced AI or deep learning skills are not required; however, knowledge of Python programming and familiarity with Jupyter Notebooks are essential to understanding the topics covered in this book.","","9781803235912","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251194.pdf&bkn=10251193&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Deep Learning with PyTorch Lightning: Swiftly build high-performance Artificial Intelligence (AI) models using Python","K. Sawarkar",NA,Deep Learning with PyTorch Lightning: Swiftly build high-performance Artificial Intelligence (AI) models using Python,"","2022","","","","","Build, train, and deploy deep learning models quickly and accurately to improve your productivity using PyTorch Lightning WrapperKey FeaturesBecome well-versed with PyTorch Lightning and learn how to implement it in various applicationsSpeed up your research using PyTorch Lightning by creating new loss functions, and architecturesTrain and build new DL applications for images, audio, video, structured and unstructured dataBook DescriptionBuilding and implementing deep learning (DL) is becoming a key skill for those who want to be at the forefront of progress.But with so much information and complex study materials out there, getting started with DL can feel quite overwhelming. Written by an AI thought leader, Deep Learning with PyTorch Lightning helps researchers build their first DL models quickly and easily without getting stuck on the complexities. With its help, you’ll be able to maximize productivity for DL projects while ensuring full flexibility – from model formulation to implementation. Throughout this book, you’ll learn how to configure PyTorch Lightning on a cloud platform, understand the architectural components, and explore how they are configured to build various industry solutions. You’ll build a neural network architecture, deploy an application from scratch, and see how you can expand it based on your specific needs, beyond what the framework can provide. In the later chapters, you’ll also learn how to implement capabilities to build and train various models like Convolutional Neural Nets (CNN), Natural Language Processing (NLP), Time Series, Self-Supervised Learning, Semi-Supervised Learning, Generative Adversarial Network (GAN) using PyTorch Lightning. By the end of this book, you’ll be able to build and deploy DL models with confidence.What you will learnCustomize models that are built for different datasets, model architecturesUnderstand a variety of DL models from image recognition, NLP to time seriesCreate advanced DL models to write poems (Semi-Supervised) or create fake images (GAN)Learn to train on unlabelled images using Self-Supervised Contrastive LearningLearn to use pre-trained models using transfer learning to save computeMake use of out-of-the-box SOTA model architectures using Lightning FlashExplore techniques for model deployment & scoring using ONNX formatRun and tune DL models in a multi-GPU environment using mixed-mode precisionsWho this book is forIf you’re a data scientist curious about deep learning but don't know where to start or feel intimidated by the complexities of large neural networks, then this book is for you. Expert data scientists making the transition from other DL frameworks to PyTorch will also find plenty of useful information in this book, as will researchers interested in using PyTorch Lightning as a reference guide. To get started, you’ll need a solid grasp on Python; the book will teach you the rest","","9781800569270","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163174.pdf&bkn=10163173&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"An Introduction to Quantum Machine Learning for Engineers","O. Simeone",NA,An Introduction to Quantum Machine Learning for Engineers,"","2022","","","","","This monograph is motivated by a number of recent developments that appear to define a possible new role for researchers with an engineering profile. First, there are now several software libraries – such as IBM’s Qiskit, Google’s Cirq, and Xanadu’s PennyLane – that make programming quantum algorithms more accessible, while also providing cloud-based access to actual quantum computers. Second, a new framework is emerging for programming quantum algorithms to be run on current quantum hardware: quantum machine learning. In the current noisy intermediate-scale quantum (NISQ) era, quantum machine learning is emerging as a dominant paradigm to program gate-based quantum computers. In quantum machine learning, the gates of a quantum circuit are parametrized, and the parameters are tuned via classical optimization based on data and on measurements of the outputs of the circuit. Parametrized quantum circuits (PQCs) can efficiently address combinatorial optimization problems, implement probabilistic generative models, and carry out inference (classification and regression). This monograph provides a self-contained introduction to quantum machine learning for an audience of engineers with a background in probability and linear algebra. It first describes the background, concepts, and tools necessary to describe quantum operations and measurements. Then, it covers parametrized quantum circuits, the variational quantum eigensolver, as well as unsupervised and supervised quantum machine learning formulations.","","9781638280590","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9844668.pdf&bkn=9844667&pdfType=book","","","","","","","","29 Jul 2022","","","now","Now Foundations and Trends Books"
"Deep Learning for Image/Video Restoration and Super-resolution","A. M. Tekalp",NA,Deep Learning for Image/Video Restoration and Super-resolution,"","2022","","","","","In this monograph, an overview of recent developments and the state-of-the-art in image/video restoration and super-resolution (SR) using deep learning is presented. Deep learning has made a significant impact, not only on computer vision and natural language processing but also on classical signal processing problems such as image/video restoration/SR and compression. Recent advances in neural architectures led to significant improvements in the performance of learned image/video restoration and SR. An important benefit of data-driven deep learning approaches is that neural models can be optimized for any differentiable loss function, including visual perceptual loss functions, leading to perceptual video restoration and SR, which cannot be easily handled by traditional model-based approaches. The publication starts with a problem statement and a short discussion on traditional vs. data-driven solutions. Thereafter, recent advances in neural architectures are considered, and the loss functions and evaluation criteria for image/video restoration and SR are discussed. Also considered are the learned image restoration and SR, as learning either a mapping from the space of degraded images to ideal images based on the universal approximation theorem, or a generative model that captures the probability distribution of ideal images. Practical problems in applying supervised training to real-life restoration and SR are also included, as well as the solution models. In the section on learned video SR, approaches to exploit temporal correlations in learned video processing are covered, and then the perceptual optimization of the network parameters to obtain natural texture and motion is discussed. A comparative discussion of various approaches concludes the publication.","","9781680839739","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9770482.pdf&bkn=9770481&pdfType=book","","","","","","","","10 May 2022","","","now","Now Foundations and Trends Books"
"Mastering PyTorch: Create and deploy deep learning models from CNNs to multimodal models, LLMs, and beyond","A. R. Jha",NA,"Mastering PyTorch: Create and deploy deep learning models from CNNs to multimodal models, LLMs, and beyond","","2024","","","","","Master advanced techniques and algorithms for machine learning with PyTorch using real-world examples Updated for PyTorch 2.x, including integration with Hugging Face, mobile deployment, diffusion models, and graph neural networks Purchase of the print or Kindle book includes a free eBook in PDF formatKey FeaturesUnderstand how to use PyTorch to build advanced neural network modelsGet the best from PyTorch by working with Hugging Face, fastai, PyTorch Lightning, PyTorch Geometric, Flask, and DockerUnlock faster training with multiple GPUs and optimize model deployment using efficient inference frameworksBook DescriptionPyTorch is making it easier than ever before for anyone to build deep learning applications. This PyTorch deep learning book will help you uncover expert techniques to get the most out of your data and build complex neural network models. You’ll build convolutional neural networks for image classification and recurrent neural networks and transformers for sentiment analysis. As you advance, you'll apply deep learning across different domains, such as music, text, and image generation, using generative models, including diffusion models. You'll not only build and train your own deep reinforcement learning models in PyTorch but also learn to optimize model training using multiple CPUs, GPUs, and mixed-precision training. You’ll deploy PyTorch models to production, including mobile devices. Finally, you’ll discover the PyTorch ecosystem and its rich set of libraries. These libraries will add another set of tools to your deep learning toolbelt, teaching you how to use fastai to prototype models and PyTorch Lightning to train models. You’ll discover libraries for AutoML and explainable AI (XAI), create recommendation systems, and build language and vision transformers with Hugging Face. By the end of this book, you'll be able to perform complex deep learning tasks using PyTorch to build smart artificial intelligence models.What you will learnImplement text, vision, and music generation models using PyTorchBuild a deep Q-network (DQN) model in PyTorchDeploy PyTorch models on mobile devices (Android and iOS)Become well versed in rapid prototyping using PyTorch with fastaiPerform neural architecture search effectively using AutoMLEasily interpret machine learning models using CaptumDesign ResNets, LSTMs, and graph neural networks (GNNs)Create language and vision transformer models using Hugging FaceWho this book is forThis deep learning with PyTorch book is for data scientists, machine learning engineers, machine learning researchers, and deep learning practitioners looking to implement advanced deep learning models using PyTorch. This book is ideal for those looking to switch from TensorFlow to PyTorch. Working knowledge of deep learning with Python is required.","","9781801079969","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10559430.pdf&bkn=10559429&pdfType=book","","","","","","","","17 Jun 2024","","","Packt Publishing","Packt Publishing eBooks"
"UX for Enterprise ChatGPT Solutions: A practical guide to designing enterprise-grade LLMs","R. H. Miller; J. Johnson",NA; NA,UX for Enterprise ChatGPT Solutions: A practical guide to designing enterprise-grade LLMs,"","2024","","","","","Create engaging AI experiences by mastering ChatGPT for business and leveraging user interface design practices, research methods, prompt engineering, the feeding lifecycle, and moreKey FeaturesLearn in-demand design thinking and user research techniques applicable to all conversational AI platformsMeasure the quality and evaluate ChatGPT from a customer’s perspective for optimal user experienceSet up and use your secure private data, documents, and materials to enhance your ChatGPT modelsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionMany enterprises grapple with new technology, often hopping on the bandwagon only to abandon it when challenges emerge. This book is your guide to seamlessly integrating ChatGPT into enterprise solutions with a UX-centered approach. UX for Enterprise ChatGPT Solutions empowers you to master effective use case design and adapt UX guidelines through an engaging learning experience. Discover how to prepare your content for success by tailoring interactions to match your audience’s voice, style, and tone using prompt-engineering and fine-tuning. For UX professionals, this book is the key to anchoring your expertise in this evolving field. Writers, researchers, product managers, and linguists will learn to make insightful design decisions. You’ll explore use cases like ChatGPT-powered chat and recommendation engines, while uncovering the AI magic behind the scenes. The book introduces a and feeding model, enabling you to leverage feedback and monitoring to iterate and refine any Large Language Model solution. Packed with hundreds of tips and tricks, this guide will help you build a continuous improvement cycle suited for AI solutions. By the end, you’ll know how to craft powerful, accurate, responsive, and brand-consistent generative AI experiences, revolutionizing your organization’s use of ChatGPT.What you will learnAlign with user needs by applying design thinking to tailor ChatGPT to meet customer expectationsHarness user research to enhance chatbots and recommendation enginesTrack quality metrics and learn methods to evaluate and monitor ChatGPT's quality and usabilityEstablish and maintain a uniform style and tone with prompt engineering and fine-tuningApply proven heuristics by monitoring and assessing the UX for conversational experiences with trusted methodsRefine continuously by implementing an ongoing process for chatbot and feedingWho this book is forThis book is for user experience designers, product managers, and product owners of business and enterprise ChatGPT solutions who are interested in learning how to design and implement ChatGPT-4 solutions for enterprise needs. You should have a basic-to-intermediate level of understanding in UI/UX design concepts and fundamental knowledge of ChatGPT-4 and its capabilities.","","9781835463802","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769349.pdf&bkn=10769348&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Dynamical Variational Autoencoders: A Comprehensive Review","L. Girin; S. Leglaive; X. Bie; J. Diard; T. Hueber; X. Alameda-Pineda",NA; NA; NA; NA; NA; NA,Dynamical Variational Autoencoders: A Comprehensive Review,"","2021","","","","","Variational autoencoders (VAEs) are powerful deep generative models widely used to represent high-dimensional complex data through a low-dimensional latent space learned in an unsupervised manner. In this monograph the authors introduce and discuss a general class of models, called dynamical variational autoencoders (DVAEs), which extend VAEs to model temporal vector sequences. In doing so the authors provide: • a formal definition of the general class of DVAEs • a detailed and complete technical description of seven DVAE models • a rapid overview of other DVAE models presented in the recent literature • discussion of the recent developments in DVAEs in relation to the history and technical background of the classical models DVAEs are built on • a quantitative benchmark of the selected DVAE models • a discussion to put the DVAE class of models into perspective This monograph is a comprehensive review of the current state-of-the-art in DVAEs. It gives the reader an accessible summary of the technical aspects of the different DVAE models, their connections with classical models, their cross-connections, and their unification in the DVAE class in a concise, easy-to-read book. The authors have put considerable effort into unifying the terminology and notation used across the various models which all students, researchers and practitioners working in machine learning will find an invaluable resource.","","9781680839135","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9638605.pdf&bkn=9638604&pdfType=book","","","","","","","","7 Dec 2021","","","now","Now Foundations and Trends Books"
"Energy-Based Models with Applications to Speech and Language Processing","Z. Ou",NA,Energy-Based Models with Applications to Speech and Language Processing,"","2024","","","","","Energy-Based Models (EBMs) are an important class of probabilistic models, also known as random fields and undirected graphical models. EBMs are un-normalized and thus radically different from other popular self-normalized probabilistic models such as hidden Markov models (HMMs), autoregressive models, generative adversarial nets (GANs) and variational auto-encoders (VAEs). Over the past years, EBMs have attracted increasing interest not only from the core machine learning community, but also from application domains such as speech, vision, natural language processing (NLP) and so on, due to significant theoretical and algorithmic progress. The sequential nature of speech and language also presents special challenges and needs a different treatment from processing fix-dimensional data (e.g., images). Therefore, the purpose of this monograph is to present a systematic introduction to energy-based models, including both algorithmic progress and applications in speech and language processing. First, the basics of EBMs are introduced, including classic models, recent models parameterized by neural networks, sampling methods, and various learning methods from the classic learning algorithms to the most advanced ones. Then, the application of EBMs in three different scenarios is presented, i.e., for modeling marginal, conditional and joint distributions, respectively. 1) EBMs for sequential data with applications in language modeling, where the main focus is on the marginal distribution of a sequence itself; 2) EBMs for modeling conditional distributions of target sequences given observation sequences, with applications in speech recognition, sequence labeling and text generation; 3) EBMs for modeling joint distributions of both sequences of observations and targets, and their applications in semi-supervised learning and calibrated natural language understanding. Lastly, several open-source toolkits are introduced to help readers get familiar with the techniques for developing and applying energy-based models.","","9781638283072","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10473695.pdf&bkn=10473694&pdfType=book","","","","","","","","18 Mar 2024","","","now","Now Foundations and Trends Books"
"Deep Learning with TensorFlow and Keras: Build and deploy supervised, unsupervised, deep, and reinforcement learning models","A. Kapoor; A. Gulli; S. Pal; F. Chollet",NA; NA; NA; NA,"Deep Learning with TensorFlow and Keras: Build and deploy supervised, unsupervised, deep, and reinforcement learning models","","2022","","","","","Build cutting edge machine and deep learning systems for the lab, production, and mobile devices. Purchase of the print or Kindle book includes a free eBook in PDF format.Key FeaturesUnderstand the fundamentals of deep learning and machine learning through clear explanations and extensive code samplesImplement graph neural networks, transformers using Hugging Face and TensorFlow Hub, and joint and contrastive learningLearn cutting-edge machine and deep learning techniquesBook DescriptionDeep Learning with TensorFlow and Keras teaches you neural networks and deep learning techniques using TensorFlow (TF) and Keras. You'll learn how to write deep learning applications in the most powerful, popular, and scalable machine learning stack available. TensorFlow 2.x focuses on simplicity and ease of use, with updates like eager execution, intuitive higher-level APIs based on Keras, and flexible model building on any platform. This book uses the latest TF 2.0 features and libraries to present an overview of supervised and unsupervised machine learning models and provides a comprehensive analysis of deep learning and reinforcement learning models using practical examples for the cloud, mobile, and large production environments. This book also shows you how to create neural networks with TensorFlow, runs through popular algorithms (regression, convolutional neural networks (CNNs), transformers, generative adversarial networks (GANs), recurrent neural networks (RNNs), natural language processing (NLP), and graph neural networks (GNNs)), covers working example apps, and then dives into TF in production, TF mobile, and TensorFlow with AutoML.What you will learnLearn how to use the popular GNNs with TensorFlow to carry out graph mining tasksDiscover the world of transformers, from pretraining to fine-tuning to evaluating themApply self-supervised learning to natural language processing, computer vision, and audio signal processingCombine probabilistic and deep learning models using TensorFlow ProbabilityTrain your models on the cloud and put TF to work in real environmentsBuild machine learning and deep learning systems with TensorFlow 2.x and the Keras APIWho this book is forThis hands-on machine learning book is for Python developers and data scientists who want to build machine learning and deep learning systems with TensorFlow. This book gives you the theory and practice required to use Keras, TensorFlow, and AutoML to build machine learning systems. Some machine learning knowledge would be useful. We don’t assume TF knowledge.","","9781803245713","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162596.pdf&bkn=10162595&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"PLCs for Beginners: An introductory guide to building robust PLC programs with Structured Text","M. T. White",NA,PLCs for Beginners: An introductory guide to building robust PLC programs with Structured Text,"","2024","","","","","Unleash the power of PLCs by understanding and applying Structured Text, programming logic, and technologies like ChatGPT and much moreKey FeaturesBuild a solid foundation of Structured Text by understanding its syntax, features, and applicationsLearn how to apply programming logic and design by taking a design-first approach to PLC programmingIntegrate advanced concepts and technologies such as cybersecurity and generative AI with PLCsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionWith the rise of smart factories and advanced technology, the demand for PLC programmers with expertise beyond ladder logic is surging. Written by M.T. White, a seasoned DevOps engineer and adjunct CIS instructor, this guide offers insights from the author’s extensive experience in PLC and HMI programming across industries. This book introduces a fresh approach to PLC programming, preparing you for future automation challenges through computer science and text-based programming. Starting with the basic components of PLCs and their integration with other modules, this book gives you a clear understanding of system functionality and helps you master PLC program execution by learning about flow and essential components for effective programming. You'll understand program design with pseudocode and flowcharts, vital for planning programs, and cover Boolean logic intricacies, harnessing logical functions and truth tables for precise control statements. The book gives you a comprehensive grasp of Structured Text, its syntax and features crucial for efficient programming. The book also focuses on advanced topics like cybersecurity in PLC systems and leveraging generative AI (GenAI), such as ChatGPT, to enhance productivity. By the end of this book, you’ll be able to design real-world projects using pseudocode and flowcharts, and implement those designs in Structured Text.What you will learnImplement PLC programs in Structured textExperiment with common functions in Structured TextControl the flow of a PLC program with loop and conditional statementsDesign a PLC program with pseudocode and flowchartsImplement common sorting algorithms such as bubble sort and insertion sort, and understand concepts such as Big OUnderstand the basics of cybersecurity to protect PLC-based systemsLeverage ChatGPT for PLC programmingGet to grips with troubleshooting hardware and fixing common problemsWho this book is forThis book is for automation engineering students and individuals who are aspiring to be software, electrical, mechanical, or automation engineers with an interest in reshaping the automation industry.","","9781801814348","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769379.pdf&bkn=10769378&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"An Introduction to Variational Autoencoders","D. P. Kingma; M. Welling",NA; NA,An Introduction to Variational Autoencoders,"","2019","","","","","In this monograph, the authors present an introduction to the framework of variational autoencoders (VAEs) that provides a principled method for jointly learning deep latent-variable models and corresponding inference models using stochastic gradient descent. The framework has a wide array of applications from generative modeling, semi-supervised learning to representation learning. The authors expand earlier work and provide the reader with the fine detail on the important topics giving deep insight into the subject for the expert and student alike. Written in a survey-like nature the text serves as a review for those wishing to quickly deepen their knowledge of the topic. An Introduction to Variational Autoencoders provides a quick summary for the reader of a topic that has become an important tool in modern-day deep learning techniques.","","9781680836233","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9051781.pdf&bkn=9051780&pdfType=book","","","","","","","","2 Apr 2020","","","now","Now Foundations and Trends Books"
"Front Matter","D. Minoli; B. Occhiogrosso","Stevens Institute of Technology, AT&T, Red Bank, NJ; NA",AI Applications to Communications and Information Technologies: The Role of Ultra Deep Neural Networks,"","2024","","","i","xiv","The prelims comprise: <list style=""bulleted""> <listItem>Half‐Title Page</listItem> <listItem>IEEE Press Editorial Board</listItem> <listItem>Title Page</listItem> <listItem>Copyright Page</listItem> <listItem>Table of Contents</listItem> <listItem>About the Authors</listItem> <listItem>Preface</listItem> </list>","","9781394190027","10.1002/9781394190034.fmatter","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10320223.pdf&bkn=10320140&pdfType=chapter","","","","","","","","16 Nov 2023","","","IEEE","Wiley-IEEE Press eBook Chapters"
"Terahertz Data Extraction and Analysis Based on Deep Learning Techniques for Emerging Applications","M. Gezimati; G. Singh","Department of Electrical and Electronic Engineering Science, Centre for Smart Information and Communication Systems, University of Johannesburg, Auckland Park Kingsway Campus, Johannesburg, South Africa; Department of Electrical and Electronic Engineering Science, Centre for Smart Information and Communication Systems, University of Johannesburg, Auckland Park Kingsway Campus, Johannesburg, South Africa",IEEE Access,"12 Feb 2024","2024","12","","21174","21198","Following the recent progress in the development of Terahertz (THz) generation and detection, THz technology is being widely used to characterize test sample properties in various applications including nondestructive testing, security inspection and medical applications. In this paper, we have presented a broad review of the recent usage of artificial intelligence (AI) particularly, deep learning techniques in various THz sensing, imaging, and spectroscopic applications with emphasis on their implementation for medical imaging of cancerous cells. Initially, the fundamentals principles and techniques for THz generation and detection, imaging and spectroscopy are introduced. Subsequently, a brief overview of AI – machine learning and deep learning techniques is summarized, and their performance is compared. Further, the usage of deep learning algorithms in various THz applications is reported, with focus on metamaterials design and classification, detection, reconstruction, segmentation, parameter extraction and denoising tasks. Moreover, we also report the metrics used to evaluate the performance of deep learning models and finally, the existing research challenges in the application of deep learning in THz cancer imaging applications are identified and possible solutions are suggested through emerging trends. With the continuous increase of acquired THz data – sensing, spectral and imaging, artificial intelligence has emerged as a dominant paradigm for embedded data extraction, understanding, perception, decision making and analysis. Towards this end, the integration of state-of-the-art machine learning techniques such as deep learning with THz applications enable detailed computational and theoretical analysis for better validation and verification than modelling techniques that precede the era of machine learning. The study will facilitate the large-scale clinical applications of deep learning enabled THz imaging systems for the development of smart and connected next generation healthcare systems as well as provide a roadmap for future research direction.","2169-3536","","10.1109/ACCESS.2024.3360930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418122","Artificial intelligence;deep learning;terahertz technology","Deep learning;Imaging;Machine learning;Sensors;Artificial intelligence;Task analysis;Support vector machines;Terahertz materials","","4","","214","CCBYNCND","31 Jan 2024","","","IEEE","IEEE Journals"
"Coverless Image Steganography based on Semantic-Controlled Text-to-Image Generation","X. Li; L. Chen; T. Fu; Z. Fu; Y. Gao","School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China; Ministry of Education, Engineering Research Center of Digital Forensics, Nanjing University of Information Science and Technology, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Artificial Intelligence Generated Content (AIGC) has created a fertile ground for image steganography. Existing Coverless Image Steganography (CIS) methods rely on image semantics to encode secrets, transmitting stego images without embedding, inherently resisting steganalysis. However, constructing CIS Datasets (CISDs) for these methods demands excessive resources, making them impractical for communication. Moreover, achieving low cost and high security is unattainable under these conditions. Therefore, we propose a CIS method based on semantic-controlled text-to-image generation. Our method disguises users as typical AIGC community members utilizing mainstream black-box text-to-image generation with Stable Diffusion (SD). During pre-processing, plain prompts, derived from dialogues with a large language model, are divided into coded and uncoded prompts through our encryption process, where a secret key determines coded prompts. In communication, confusion prompts are selected from uncoded and coded prompts, excluding those determined by secrets. Subsequently, our stego shuffling process combines topic, secret, and confusion prompts to produce stego prompt sets. Diverse stego images maintaining visual topic consistency are generated from these sets using SD with generation seeds indicating transmission order. By introducing confusion prompts, our method is secure from recognition when revealing stego prompts. Experimental results demonstrate our method achieves low communication costs and enhances communication security.","1558-2205","","10.1109/TCSVT.2025.3545067","National Natural Science Foundation of China(grant numbers:U22B2026); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10902504","Coverless Image Steganography (CIS);Image Steganography;Text-to-Image Generation;Artificial Intelligence Generated Content (AIGC)","Semantics;Text to image;Steganography;Security;Costs;Artificial intelligence;Visualization;Receivers;Image recognition;Electronic mail","","","","","IEEE","25 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Advanced Persistent Threats Based on Supply Chain Vulnerabilities: Challenges, Solutions, and Future Directions","Z. Tan; S. P. Parambath; C. Anagnostopoulos; J. Singer; A. K. Marnerides","School of Computing Science, University of Glasgow, Glasgow, Scotland; School of Computing Science, University of Glasgow, Glasgow, Scotland; School of Computing Science, University of Glasgow, Glasgow, Scotland; School of Computing Science, University of Glasgow, Glasgow, Scotland; Department of Electrical and Computer Engineering, KIOS Centre of Excellence, University of Cyprus, Nicosia, Cyprus",IEEE Internet of Things Journal,"13 Mar 2025","2025","12","6","6371","6395","Due to the ever increasing interdependency across a variety of diverse software and hardware components in information and communications technology (ICT) provisioning, supply chain vulnerabilities (SCVs) targeting such dependencies have evolved as a primary choice for malicious actors to stealthy and complex cyber-attacks. The current modus operandi in the cyber threat spectrum is solely correlated with advanced persistent threats (APTs) that have shown to be prevalent across diversified attacks underpinning cyberwarfare and cybercrime. Hence, defense against such threats is undoubtedly considered as a high priority on a global scale. Nonetheless, the reliance on third-party supply chain software and device across diverse ICT ecosystems, combined with the current defense mechanisms’ inability to identify specific compromised entry points, results in an increased risk of APTs. This survey explores the state-of-the-art to stratify and showcase the properties of supply chain-based APTs, elaborate on reported risks from such APTs, and expand on existing defense methods. This study connects academic research with industry practices to highlight a new and growing problem. It examines supply chain compromises, offers unique insight into how these exploitations occur, and equips cybersecurity practitioners with the knowledge required to design next-generation APT defense mechanisms.","2327-4662","","10.1109/JIOT.2025.3528744","EU Horizon(grant numbers:COCOON 101120221); Jumpsec Ltd. Ph.D. Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10838587","Advanced persistent threats (APTs);classification;defense methods;supply chain attack (SCA)","Supply chains;Security;Malware;Surveys;Reconnaissance;Payloads;Systematic literature review;Internet of Things;Weapons;Vectors","","","","162","IEEE","13 Jan 2025","","","IEEE","IEEE Journals"
"5 Soma Design Theory","K. Höök; K. Friedman; E. Stolterman",Royal Institute of Technology (KTH); NA; NA,Designing with the Body: Somaesthetic Interaction Design,"","2018","","","117","142","As you look at the title of this chapter, you might wonder why I waited until now—halfway through the book—to offer a coherent theory of soma design. The answer is quite simple: Because soma design is based so heavily on experiences and somatic feelings, I felt it was necessary to give you, the reader, a sense of what such designs actually feel like—how they work, what makes them unique, and how they came about. Articulating a theory of design in words requires a certain distanced, intellectual approach, and yet I am arguing for designs that recognize the soma as the unity of mind and body, intellect and experience. I hope that by now, you have an intuitive sense of what soma designs might look and feel like, which will make my discussion here of soma design theory more meaningful.","","9780262348324","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8555414.pdf&bkn=8555408&pdfType=chapter","","Art;Buildings;Shape;Navigation;Probes;Architecture","","","","","","13 Dec 2018","","","MIT Press","MIT Press eBook Chapters"
"Deepfake Audio Detection via MFCC Features Using Machine Learning","A. Hamza; A. R. R. Javed; F. Iqbal; N. Kryvinska; A. S. Almadhor; Z. Jalil; R. Borghol","Faculty of Computing and AI, Air University, Islamabad, Pakistan; Department of Cyber Security, Air University, Islamabad, Pakistan; College of Technological Innovation, Zayed University, Abu Dhabi, United Arab Emirates; Department of Information Systems, Faculty of Management, Comenius University in Bratislava, Bratislava, Slovakia; College of Computer and Information Sciences, Jouf University, Sakaka, Saudi Arabia; Department of Cyber Security, Air University, Islamabad, Pakistan; Rochester Institute of Technology of Dubai, Dubai, United Arab Emirates",IEEE Access,"30 Dec 2022","2022","10","","134018","134028","Deepfake content is created or altered synthetically using artificial intelligence (AI) approaches to appear real. It can include synthesizing audio, video, images, and text. Deepfakes may now produce natural-looking content, making them harder to identify. Much progress has been achieved in identifying video deepfakes in recent years; nevertheless, most investigations in detecting audio deepfakes have employed the ASVSpoof or AVSpoof dataset and various machine learning, deep learning, and deep learning algorithms. This research uses machine and deep learning-based approaches to identify deepfake audio. Mel-frequency cepstral coefficients (MFCCs) technique is used to acquire the most useful information from the audio. We choose the Fake-or-Real dataset, which is the most recent benchmark dataset. The dataset was created with a text-to-speech model and is divided into four sub-datasets: for-rece, for-2-sec, for-norm and for-original. These datasets are classified into sub-datasets mentioned above according to audio length and bit rate. The experimental results show that the support vector machine (SVM) outperformed the other machine learning (ML) models in terms of accuracy on for-rece and for-2-sec datasets, while the gradient boosting model performed very well using for-norm dataset. The VGG-16 model produced highly encouraging results when applied to the for-original dataset. The VGG-16 model outperforms other state-of-the-art approaches.","2169-3536","","10.1109/ACCESS.2022.3231480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9996362","Deepfakes;deepfake audio;synthetic audio;machine learning;acoustic data","Deepfakes;Deep learning;Speech synthesis;Training data;Feature extraction;Machine learning algorithms;Data models;Acoustics","","71","","36","CCBY","21 Dec 2022","","","IEEE","IEEE Journals"
"A Systematic Literature Review on Latest Keystroke Dynamics Based Models","S. Roy; J. Pradhan; A. Kumar; D. R. D. Adhikary; U. Roy; D. Sinha; R. K. Pal","Department of Computer Science and Engineering, University of Calcutta, Acharya Prafulla Chandra Roy Siksha Prangan, Saltlake, Kolkata, India; Department of Computer Science and Engineering, Siksha ‘O’ Anusandhan Deemed to be University, Bhubaneswar, Odisha, India; Department of Computer Science and Engineering, Siksha ‘O’ Anusandhan Deemed to be University, Bhubaneswar, Odisha, India; Department of Computer Science and Engineering, Siksha ‘O’ Anusandhan Deemed to be University, Bhubaneswar, Odisha, India; Department of Computer System Sciences, Visva-Bharati, Santiniketan, Bolpur, India; Department of Computer Science and Engineering, University of Calcutta, Acharya Prafulla Chandra Roy Siksha Prangan, Saltlake, Kolkata, India; Department of Computer Science and Engineering, University of Calcutta, Acharya Prafulla Chandra Roy Siksha Prangan, Saltlake, Kolkata, India",IEEE Access,"9 Sep 2022","2022","10","","92192","92236","The purpose of this study is to conduct a comprehensive evaluation and analysis of the most recent studies on the implications of keystroke dynamics (KD) patterns in user authentication, identification, and the determination of useful information. Another aim is to provide an extensive and up-to-date survey of the recent literature and potential research directions to understand the present state-of-the-art methodologies in this particular domain that are expected to be beneficial for the KD research community. From January 1st, 2017 to March 13th, 2022, the popular six electronic databases have been searched using a search criterion (“keystroke dynamics” OR “typing pattern”) AND (“authentication” OR “verification” OR “identification”). With this criterion, a total of nine thousand three hundred forty-eight results, including duplicates, were produced. However, one thousand five hundred forty-seven articles have been chosen after removing duplicates and preliminary screening. Due to insufficient information, only one hundred twenty-seven high-quality quantitative research articles have been included in the article selection process. We compared and summarised several factors with multiple tables to comprehend the various methodologies, experimental settings, and findings. In this study, we have identified six unique KD-based designs and presented the status of findings toward an effective solution in authentication, identification, and prediction. We have also discovered considerable heterogeneity across studies in each KD-based design for desktops and smartphones separately. Finally, this paper found a few open research challenges and provided some indications for a deeper understanding of the issues and further study.","2169-3536","","10.1109/ACCESS.2022.3197756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9853506","Behavioural biometrics;computer security;keystroke dynamics;trait prediction;typing patterns;user authentication;user identification","Authentication;Biometrics (access control);Passwords;Behavioral sciences;Security;Knowledge based systems;Keystroke dynamics;Authentication;Identification of persons ","","9","","396","CCBYNCND","10 Aug 2022","","","IEEE","IEEE Journals"
"Artificial Intelligence for Climate Change: A Patent Analysis in the Manufacturing Sector","M. Podrecca; G. Culot; S. Tavassoli; G. Orzes","Department of Management, Information and Production Engineering, University of Bergamo, Bergamo, Italy; Polytechnic Department of Engineering and Architecture, University of Udine, Udine, Italy; Deakin Business School, Deakin University, Melbourne, Australia; Faculty of Engineering, Free University of Bozen-Bolzano, Bolzano, Italy",IEEE Transactions on Engineering Management,"23 Oct 2024","2024","71","","15005","15024","This study analyzes the current state of artificial intelligence (AI) technologies for addressing and mitigating climate change in the manufacturing sector and provides an outlook on future developments. The research is grounded in the concept of general-purpose technologies, motivated by a still limited understanding of innovation patterns for this application context. To this end, we focus on global patenting activity between 2011 and 2023 (5919 granted patents classified for “mitigation or adaptation against climate change” in the “production or processing of goods”). We examined time trends, applicant characteristics, and underlying technologies. A topic modeling analysis was performed to identify emerging themes from the unstructured textual data of the patent abstracts. This allowed the identification of six AI application domains. For each of them, we built a network analysis and ran growth trends and forecasting models. Our results show that patenting activities are mostly oriented toward improving the efficiency and reliability of manufacturing processes in five out of six identified domains (“predictive analytics,” “material sorting,” “defect detection,” “advanced robotics,” and “scheduling”). Instead, AI within the “resource optimization” domain relates to energy management, showing an interplay with other climate-related technologies. Our results also highlight interdependent innovations peculiar to each domain around core AI technologies. Forecasts show that the more specific technologies are within domains, the longer it will take for them to mature. From a practical standpoint, the study sheds light on the role of AI within the broader cleantech innovation landscape and urges policymakers to consider synergies. Managers can find information to define technology portfolios and alliances considering technological coevolution.","1558-0040","","10.1109/TEM.2024.3469370","European Union's Horizon MSCA 2021 programme; Marie Skłodowska-Curie(grant numbers:10108648); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10703137","Artificial intelligence (AI);climate change;patent analysis;sustainability;technology foresight","Artificial intelligence;Climate change;Manufacturing;Patents;Market research;Meteorology;Analytical models;Sustainable development;Research and development","","","","184","CCBYNCND","2 Oct 2024","","","IEEE","IEEE Journals"
"Metaverse Communications, Networking, Security, and Applications: Research Issues, State-of-the-Art, and Future Directions","M. Ali; F. Naeem; G. Kaddoum; E. Hossain","Electrical Engineering Department, ETS, University of Quebec, Montreal, QC, Canada; Electrical Engineering Department, ETS, University of Quebec, Montreal, QC, Canada; Electrical Engineering Department, ETS, University of Quebec, Montreal, QC, Canada; Department of Computer and Electrical Engineering, University of Manitoba, Winnipeg, MB, Canada",IEEE Communications Surveys & Tutorials,"22 May 2024","2024","26","2","1238","1278","Metaverse is an evolving orchestrator of the next-generation Internet architecture that produces an immersive and self-adapting virtual world in which humans perform activities similar to those in the real world, such as playing sports, doing work, and socializing. It is becoming a reality and is driven by ever-evolving advanced technologies such as extended reality, artificial intelligence, and blockchain. In this context, Metaverse will play an essential role in developing smart cities, which becomes more evident in the post-COVID-19-pandemic metropolitan setting. However, the new paradigm imposes new challenges, such as developing novel privacy and security threats that can emerge in the digital Metaverse ecosystem. Moreover, it requires the convergence of several media types with the capability to quickly process massive amounts of data to keep the residents safe and well-informed, which can raise issues related to scalability and interoperability. In light of this, this research study aims to review the literature on the state of the art of integrating the Metaverse architecture concepts in smart cities. First, this paper presents the theoretical architecture of Metaverse and discusses international companies’ interest in this emerging technology. It also examines the notion of Metaverse relevant to virtual reality, identifies the prevalent threats, and determines the importance of communication infrastructure in information gathering for efficient Metaverse operation. Next, the notion of blockchain technologies is discussed regarding privacy preservation and how it can provide tamper-proof content sharing among Metaverse users. Finally, the application of distributed Metaverse for social good is highlighted. Most importantly, the paper explores the reflections of this cutting-edge technology on the smart city, talks about the role and impact of the Metaverse in the production of urban policies, and eventually identifies the research gaps and the future research directions in this domain.","1553-877X","","10.1109/COMST.2023.3347172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10373900","Metaverse;smart city;networking;security;blockchain","Metaverse;Smart cities;Tutorials;Surveys;Avatars;Artificial intelligence;X reality","","18","","257","IEEE","25 Dec 2023","","","IEEE","IEEE Journals"
"Offensive Security Using Python: A hands-on guide to offensive tactics and threat mitigation using practical strategies","R. Rehim; M. Mohan; G. Ongers",NA; NA; NA,Offensive Security Using Python: A hands-on guide to offensive tactics and threat mitigation using practical strategies,"","2024","","","","","Unlock Python's hacking potential and discover the art of exploiting vulnerabilities in the world of offensive cybersecurityKey FeaturesGet in-depth knowledge of Python's role in offensive security, from fundamentals through to advanced techniquesDiscover the realm of cybersecurity with Python and exploit vulnerabilities effectivelyAutomate complex security tasks with Python, using third-party tools and custom solutionsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionOffensive Security Using Python is your go-to manual for mastering the quick-paced field of offensive security. This book is packed with valuable insights, real-world examples, and hands-on activities to help you leverage Python to navigate the complicated world of web security, exploit vulnerabilities, and automate challenging security tasks. From detecting vulnerabilities to exploiting them with cutting-edge Python techniques, you’ll gain practical insights into web security, along with guidance on how to use automation to improve the accuracy and effectiveness of your security activities. You’ll also learn how to design personalized security automation tools. While offensive security is a great way to stay ahead of emerging threats, defensive security plays an equal role in protecting organizations from cyberattacks. In this book, you’ll get to grips with Python secure coding techniques to improve your ability to recognize dangers quickly and take appropriate action. As you progress, you’ll be well on your way to handling the contemporary challenges in the field of cybersecurity using Python, as well as protecting your digital environment from growing attacks. By the end of this book, you’ll have a solid understanding of sophisticated offensive security methods and be able to stay ahead in the constantly evolving cybersecurity space.What you will learnFamiliarize yourself with advanced Python techniques tailored to security professionals' needsUnderstand how to exploit web vulnerabilities using PythonEnhance cloud infrastructure security by utilizing Python to fortify infrastructure as code (IaC) practicesBuild automated security pipelines using Python and third-party toolsDevelop custom security automation tools to streamline your workflowImplement secure coding practices with Python to boost your applicationsDiscover Python-based threat detection and incident response techniquesWho this book is forThis book is for a diverse audience interested in cybersecurity and offensive security. Whether you're an experienced Python developer looking to enhance offensive security skills, an ethical hacker, a penetration tester eager to learn advanced Python techniques, or a cybersecurity enthusiast exploring Python's potential in vulnerability analysis, you'll find valuable insights. If you have a solid foundation in Python programming language and are eager to understand cybersecurity intricacies, this book will help you get started on the right foot.","","9781835460634","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769419.pdf&bkn=10769418&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Trustworthy Federated Learning: A Comprehensive Review, Architecture, Key Challenges, and Future Research Prospects","A. Tariq; M. A. Serhani; F. M. Sallabi; E. S. Barka; T. Qayyum; H. M. Khater; K. A. Shuaib","College of Information Technology, UAE University, Abu Dhabi, UAE; College of Computing and Informatics, University of Sharjah, Sharjah, UAE; College of Information Technology, UAE University, Abu Dhabi, UAE; College of Information Technology, UAE University, Abu Dhabi, UAE; College of Information Technology, UAE University, Abu Dhabi, UAE; College of Information Technology, UAE University, Abu Dhabi, UAE; College of Information Technology, UAE University, Abu Dhabi, UAE",IEEE Open Journal of the Communications Society,"22 Aug 2024","2024","5","","4920","4998","Federated Learning (FL) emerged as a significant advancement in the field of Artificial Intelligence (AI), enabling collaborative model training across distributed devices while maintaining data privacy. As the importance of FL and its application in various areas increased, addressing trustworthiness issues in its various aspects became crucial. In this survey, we provided a comprehensive overview of the state-of-the-art research on Trustworthy FL, exploring existing solutions and key foundations relevant to Trustworthiness in FL. There has been significant growth in the literature on trustworthy centralized Machine Learning (ML) and Deep Learning (DL). However, there is still a need for more focused efforts toward identifying trustworthiness pillars and evaluation metrics in FL. In this paper, we proposed a taxonomy encompassing five main classifications for Trustworthy FL, including Interpretability and Explainability, Transparency, Privacy and Robustness, Fairness, and Accountability. Each category represents a dimension of trust and is further broken down into different sub-categories. Moreover, we addressed trustworthiness in a Decentralized FL (DFL) setting. Communication efficiency is essential for ensuring Trustworthy FL. This paper also highlights the significance of communication efficiency within various Trustworthy FL pillars and investigates existing research on communication-efficient techniques across these pillars. Our survey comprehensively addresses trustworthiness challenges across all aspects within the Trustworthy FL settings. We also proposed a comprehensive architecture for Trustworthy FL, detailing the fundamental principles underlying the concept, and provided an in-depth analysis of trust assessment mechanisms. In conclusion, we identified key research challenges related to every aspect of Trustworthy FL and suggested future research directions. This comprehensive survey served as a valuable resource for researchers and practitioners working on the development and implementation of Trustworthy FL systems, contributing to a more secure and reliable AI landscape.","2644-125X","","10.1109/OJCOMS.2024.3438264","Zayed Center for Health Sciences(grant numbers:12R005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10623386","Federated learning;AI;trust;privacy;fairness;transparency;interpretability;accountability;explainability;communication efficiency;intelligent communications and networks","Artificial intelligence;Data models;Training;Privacy;Surveys;Data privacy;Security","","","","426","CCBY","5 Aug 2024","","","IEEE","IEEE Journals"
"Privacy-Preserving Tools and Technologies: Government Adoption and Challenges","S. Prabowo; A. G. Putrada; I. D. Oktaviani; M. Abdurohman; M. Janssen; H. H. Nuha; S. Sutikno","School of Computing, Telkom University, Bandung, Indonesia; School of Computing, Telkom University, Bandung, Indonesia; School of Computing, Telkom University, Bandung, Indonesia; School of Computing, Telkom University, Bandung, Indonesia; Delft University of Technology, Delft, The Netherlands; School of Computing, Telkom University, Bandung, Indonesia; School of Informatics Engineering, Institut Teknologi Sumatera, Lampung, Indonesia",IEEE Access,"25 Feb 2025","2025","13","","33904","33934","Understanding the landscape of privacy protection in governmental systems is crucial for ensuring the trustworthiness of public services and safeguarding citizens’ sensitive data from breaches or misuse. Systematic mapping and synthesis of previous research can help identify existing privacy-preserving techniques, assess their effectiveness, and highlight areas for improvement, offering valuable insights for policymakers and practitioners. We aim to conduct a systematic literature review (SLR) of privacy-preserving tools and technologies, focusing on their adoption and governments’ challenges. This study also uncovers emerging trends and future research directions, contributing to developing more robust privacy strategies tailored to government needs.Given its extensive reach and government-centric methodology, this evaluation distinguishes itself from previous research. Our work methodically synthesizes privacy-preserving tools and technologies from the distinct perspective of government roles, in contrast to previous assessments that concentrate narrowly on certain technologies or areas. Our findings offer a synthesis of the government’s diverse roles in privacy preservation—regulator, enforcer, user, and service provider—and address existing concerns and key privacy-related technologies. Finally, we identify significant research opportunities, such as improving privacy-preserving mechanisms to strengthen the integrity of public services and mitigate the risks of data breaches and misuse.","2169-3536","","10.1109/ACCESS.2025.3540878","Telkom University Research and Community Services (PPM) through International Collaboration Scheme(grant numbers:514/PNLT3/PPM/2023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10879480","Privacy;regulation;privacy-preserving mechanisms;government;systematic literature study","Privacy;Government;Data privacy;Surveys;Systematic literature review;Protection;Blockchains;Taxonomy;Systematics;Usability","","","","253","CCBY","11 Feb 2025","","","IEEE","IEEE Journals"
"DTITD: An Intelligent Insider Threat Detection Framework Based on Digital Twin and Self-Attention Based Deep Learning Models","Z. Q. Wang; A. El Saddik","Multimedia Communications Research Laboratory (MCRLab), School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada; Multimedia Communications Research Laboratory (MCRLab), School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada",IEEE Access,"19 Oct 2023","2023","11","","114013","114030","Recent statistics and studies show that the loss generated by insider threats is much higher than that generated by external attacks. More and more organizations are investing in or purchasing insider threat detection systems to prevent insider risks. However, the accurate and timely detection of insider threats faces significant challenges. In this study, we proposed an intelligent insider threat detection framework based on Digital Twins and self-attentions based deep learning models. First, this paper introduces insider threats and the challenges in detecting them. Then this paper presents recent related works on solving insider threat detection problems and their limitations. Next, we propose our solutions to address these challenges: building an innovative intelligent insider threat detection framework based on Digital Twin (DT) and self-attention based deep learning models, performing insight analysis of users’ behavior and entities, adopting contextual word embedding techniques using Bidirectional Encoder Representations from Transformers (BERT) model and sentence embedding technique using Generative Pre-trained Transformer 2 (GPT-2) model to perform data augmentation to overcome significant data imbalance, and adopting temporal semantic representation of users’ behaviors to build user behavior time sequences. Subsequently, this study built self-attention-based deep learning models to quickly detect insider threats. This study proposes a simplified transformer model named DistilledTrans and applies the original transformer model, DistilledTrans, BERT + final layer, Robustly Optimized BERT Approach (RoBERTa) + final layer, and a hybrid method combining pre-trained (BERT, RoBERTa) with a Convolutional Neural Network (CNN) or Long Short-term Memory (LSTM) network model to detect insider threats. Finally, this paper presents experimental results on a dense dataset CERT r4.2 and augmented sporadic dataset CERT r6.2, evaluates their performance, and performs a comparison analysis with state-of-the-art models. Promising experimental results show that 1) contextual word embedding insert and substitution predicted by the BERT model, and context embedding sentences predicted by the GPT-2 model are effective data augmentation approaches to address high data imbalance; 2) DistilledTrans trained with sporadic dataset CERT r6.2 augmented by the contextual embedding sentence method predicted by GPT-2, outperforms the state-of-the-art models in terms of all evaluation metrics, including accuracy, precision, recall, F1-score, and Area Under the ROC Curve (AUC). Additionally, its structure is much simpler, and thus training time and computing cost are much less than those of recent models; 3) when trained with the dense dataset CERT r4.2, pre-trained models BERT plus a final layer or RoBERTa plus a final layer can achieve significantly higher performance than the current models with a very little sacrifice of precision. However, complex hybrid methods may not be required.","2169-3536","","10.1109/ACCESS.2023.3324371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10285086","Digital twin;cybersecurity;insider threat;deep learning;transformer;BERT;RoBERTa;GPT-2;data augmentation;artificial intelligence;machine learning;UEBA","Context modeling;Computational modeling;Data models;Behavioral sciences;Transformers;Organizations;Threat assessment;Deep learning;Digital twins;Deep learning;Data augmentation","","7","","51","CCBYNCND","13 Oct 2023","","","IEEE","IEEE Journals"
"Hands-On Data Preprocessing in Python: Learn how to effectively prepare data for successful data analytics","R. Jafari",NA,Hands-On Data Preprocessing in Python: Learn how to effectively prepare data for successful data analytics,"","2022","","","","","Get your raw data cleaned up and ready for processing to design better data analytic solutionsKey FeaturesDevelop the skills to perform data cleaning, data integration, data reduction, and data transformationMake the most of your raw data with powerful data transformation and massaging techniquesPerform thorough data cleaning, including dealing with missing values and outliersBook DescriptionHands-On Data Preprocessing is a primer on the best data cleaning and preprocessing techniques, written by an expert who’s developed college-level courses on data preprocessing and related subjects. With this book, you’ll be equipped with the optimum data preprocessing techniques from multiple perspectives, ensuring that you get the best possible insights from your data. You'll learn about different technical and analytical aspects of data preprocessing – data collection, data cleaning, data integration, data reduction, and data transformation – and get to grips with implementing them using the open source Python programming environment. The hands-on examples and easy-to-follow chapters will help you gain a comprehensive articulation of data preprocessing, its whys and hows, and identify opportunities where data analytics could lead to more effective decision making. As you progress through the chapters, you’ll also understand the role of data management systems and technologies for effective analytics and how to use APIs to pull data. By the end of this Python data preprocessing book, you'll be able to use Python to read, manipulate, and analyze data; perform data cleaning, integration, reduction, and transformation techniques, and handle outliers or missing values to effectively prepare data for analytic tools.What you will learnUse Python to perform analytics functions on your dataUnderstand the role of databases and how to effectively pull data from databasesPerform data preprocessing steps defined by your analytics goalsRecognize and resolve data integration challengesIdentify the need for data reduction and execute itDetect opportunities to improve analytics with data transformationWho this book is forThis book is for junior and senior data analysts, business intelligence professionals, engineering undergraduates, and data enthusiasts looking to perform preprocessing and data cleaning on large amounts of data. You don’t need any prior experience with data preprocessing to get started with this book. However, basic programming skills, such as working with variables, conditionals, and loops, along with beginner-level knowledge of Python and simple analytics experience, are a prerequisite.","","9781801079952","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162797.pdf&bkn=10162796&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Image Inpainting And Digital Camouflage: Methods, Applications, And Perspectives For Remote Sensing","K. Karwowska; D. Wierzbicki; M. Kedzierski",NA; NA; NA,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"","2025","PP","99","1","35","Image inpainting refers to the process of restoring missing or damaged areas in an image. This research field has been very active in recent years, driven by various applications such as reconstructing lost fragments, concealing data loss in corrupted image transmissions, removing objects in image editing, and interpolating image content for reconstruction in image-based rendering (IBR) from various fields of view. This paper presents existing methods of image inpainting, covering classical approaches, CNN-based methods, and GAN-based methods. Additionally, it explores techniques related to steganography, adversarial image synthesis, and false image generation. Examples of applications are provided for each category of image modification methods. Although image inpainting and digital camouflage are not yet widely studied in the remote sensing community, there has been a growing interest in these topics in recent years. To broaden the understanding of these methods, this study also reviews techniques developed in the field of computer science, which have the potential to be adapted for remote sensing applications. The main contribution of this paper is the presentation of various forms of digital masking, extending beyond traditional inpainting. We also provide a curated list of publicly available datasets that can support the development of new solutions, along with a selection of qualitative metrics for the robust evaluation of image inpainting algorithms. The code are available at https://github.com/KK-MUT/DigitalCamouflage","2151-1535","","10.1109/JSTARS.2025.3547917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10909411","computer vision;image processing;image inpainting;image fusion;deep learning;convolutional neural network","Reviews;Digital images;Classification algorithms;Databases;Steganography;Satellite images;Image reconstruction;Satellites;Remote sensing;Visualization","","","","","CCBY","3 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Computational Optimal Transport: With Applications to Data Science","G. Peyré; M. Cuturi",NA; NA,Computational Optimal Transport: With Applications to Data Science,"","2019","","","","","The goal of Optimal Transport (OT) is to define geometric tools that are useful to compare probability distributions. Their use dates back to 1781. Recent years have witnessed a new revolution in the spread of OT, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This monograph reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications. Computational Optimal Transport presents an overview of the main theoretical insights that support the practical effectiveness of OT before explaining how to turn these insights into fast computational schemes. Written for readers at all levels, the authors provide descriptions of foundational theory at two-levels. Generally accessible to all readers, more advanced readers can read the specially identified more general mathematical expositions of optimal transport tailored for discrete measures. Furthermore, several chapters deal with the interplay between continuous and discrete measures, and are thus targeting a more mathematically-inclined audience. This monograph will be a valuable reference for researchers and students wishing to get a thorough understanding of Computational Optimal Transport, a mathematical gem at the interface of probability, analysis and optimization.","","9781680835519","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8641477.pdf&bkn=8641476&pdfType=book","","","","","","","","14 Feb 2019","","","now","Now Foundations and Trends Books"
"Understanding and Mitigating Gender Bias in Information Retrieval Systems","S. Seyedsalehi; A. Bigdeli; N. Arabzadeh; B. AlMousawi; Z. Marshall; M. Zihayat; E. Bagheri",NA; NA; NA; NA; NA; NA; NA,Understanding and Mitigating Gender Bias in Information Retrieval Systems,"","2025","","","","","Gender bias is a pervasive issue that continues to influence various aspects of society, including the outcomes of information retrieval (IR) systems. As these systems become increasingly integral to accessing and navigating the vast amounts of information available today, the need to understand and mitigate gender bias within them is paramount. This monograph provides a comprehensive examination of the origins, manifestations, and consequences of gender bias in IR systems, as well as the current methodologies employed to address these biases. Theoretical frameworks surrounding gender and its representation in artificial intelligence (AI) systems are explored, particularly focusing on how traditional gender binaries are perpetuated and reinforced through data and algorithmic processes. Metrics and methodologies used to identify and measure gender bias within IR systems are then analyzed, offering a detailed evaluation of existing approaches and their limitations. Subsequent sections address the sources of gender bias, including biased input queries, retrieval methods, and gold standard datasets. Various data-driven and method-level debiasing strategies are presented, including techniques for debiasing neural embeddings and algorithmic approaches aimed at reducing bias in IR system outputs. The monograph concludes with a discussion of the challenges and limitations faced by current debiasing efforts and provides insights into future research directions that could lead to more equitable and inclusive IR systems. This monograph serves as a valuable resource for researchers, practitioners, and students in the fields of information retrieval, artificial intelligence, and data science, providing the knowledge and tools needed to address gender bias and contribute to the development of fair and unbiased information systems.","","9781638285199","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10882926.pdf&bkn=10882925&pdfType=book","","","","","","","","12 Feb 2025","","","now","Now Foundations and Trends Books"
"Efficient and Effective Tree-based and Neural Learning to Rank","S. Bruch; C. Lucchese; F. M. Nardini",NA; NA; NA,Efficient and Effective Tree-based and Neural Learning to Rank,"","2023","","","","","Information retrieval researchers develop algorithmic solutions to hard problems and insist on a proper, multifaceted evaluation of ideas. As we move towards even more complex deep learning models in a wide range of applications, questions on efficiency once again resurface with renewed urgency. Efficiency is no longer limited to time and space but has found new, challenging dimensions that stretch to resource-, sample- and energy-efficiency with ramifications for researchers, users, and the environment. This monograph takes a step towards promoting the study of efficiency in the era of neural information retrieval by offering a comprehensive survey of the literature on efficiency and effectiveness in ranking and retrieval. It is inspired by the parallels that exist between the challenges in neural network-based ranking solutions and their predecessors, decision forest-based learning-to-rank models, and the connections between the solutions the literature to date has to offer. By understanding the fundamentals underpinning these algorithmic and data structure solutions one can better identify future directions and more efficiently determine the merits of ideas.","","9781638281993","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10130508.pdf&bkn=10130507&pdfType=book","","","","","","","","22 May 2023","","","now","Now Foundations and Trends Books"
"Microsoft Azure AI Fundamentals AI-900 Exam Guide: Gain proficiency in Azure AI and machine learning concepts and services to excel in the AI-900 exam","A. Guilmette; S. Miles; P. D. Tender",NA; NA; NA,Microsoft Azure AI Fundamentals AI-900 Exam Guide: Gain proficiency in Azure AI and machine learning concepts and services to excel in the AI-900 exam,"","2024","","","","","Get ready to pass the certification exam on your first attempt by gaining actionable insights into AI concepts, ML techniques, and Azure AI services covered in the latest AI-900 exam syllabus from two industry experts Key FeaturesDiscover Azure AI services, including computer vision, Auto ML, NLP, and OpenAIExplore AI use cases, such as image identification, chatbots, and moreWork through 145 practice questions under chapter-end self-assessments and mock examsPurchase of this book unlocks access to web-based exam prep resources, including mock exams, flashcards, and exam tipsBook DescriptionThe AI-900 exam helps you take your first step into an AI-shaped future. Regardless of your technical background, this book will help you test your understanding of the key AI-related topics and tools used to develop AI solutions in Azure cloud. This exam guide focuses on AI workloads, including natural language processing (NLP) and large language models (LLMs). You’ll explore Microsoft’s responsible AI principles like safety and accountability. Then, you’ll cover the basics of machine learning (ML), including classification and deep learning, and learn how to use training and validation datasets with Azure ML. Using Azure AI Vision, face detection, and Video Indexer services, you’ll get up to speed with computer vision-related topics like image classification, object detection, and facial detection. Later chapters cover NLP features such as key phrase extraction, sentiment analysis, and speech processing using Azure AI Language, speech, and translator services. The book also guides you through identifying GenAI models and leveraging Azure OpenAI Service for content generation. At the end of each chapter, you’ll find chapter review questions with answers, provided as an online resource. By the end of this exam guide, you’ll be able to work with AI solutions in Azure and pass the AI-900 exam using the online exam prep resources.What you will learnDiscover various types of artificial intelligence (AI)workloads and services in AzureCover Microsoft's guiding principles for responsible AI development and useUnderstand the fundamental principles of how AI and machine learning workExplore how AI models can recognize content in images and documentsGain insights into the features and use cases for natural language processingExplore the capabilities of generative AI servicesWho this book is forWhether you're a cloud engineer, software developer, an aspiring data scientist, or simply interested in learning AI/ML concepts and capabilities on Azure, this book is for you. The book also serves as a foundation for those looking to attempt more advanced AI and data science-related certification exams (e.g. Microsoft Certified: Azure AI Engineer Associate). Although no experience in data science and software engineering is required, basic knowledge of cloud concepts and client-server applications is assumed. ","","9781835885673","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769247.pdf&bkn=10769246&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Fairness in Information Access Systems","M. D. Ekstrand; A. Das; R. Burke; F. Diaz",NA; NA; NA; NA,Fairness in Information Access Systems,"","2022","","","","","Recommendation, information retrieval, and other information access systems pose unique challenges for investigating and applying the fairness and non-discrimination concepts that have been developed for studying other machine learning systems. While fair information access shares many commonalities with fair classification, there are important differences such as the multistakeholder nature of information access applications, the rank-based problem setting, the centrality of personalization in many cases, and the role of user response. These all complicate the problem of identifying precisely what types and operationalizations of fairness may be relevant. In this monograph, the authors present a taxonomy of the various dimensions of fair information access and survey the literature to date on this new and rapidly-growing topic. They preface this with brief introductions to information access and algorithmic fairness to facilitate the use of this work by scholars who wish to study their intersection. The authors conclude with several open problems in fair information access and present suggestions for how to approach research in this space.","","9781638280415","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9828001.pdf&bkn=9828000&pdfType=book","","","","","","","","13 Jul 2022","","","now","Now Foundations and Trends Books"
"Data Analytics on Graphs Part III: Machine Learning on Graphs, from Graph Topology to Applications","L. Stanković; D. P. Mandic; M. Daković; M. Brajović; B. Scalzo; S. Li; A. G. Constantinides",NA; NA; NA; NA; NA; NA; NA,"Data Analytics on Graphs Part III: Machine Learning on Graphs, from Graph Topology to Applications","","2020","","","","","Modern data analytics applications on graphs often operate on domains where graph topology is not known a priori, and hence its determination becomes part of the problem definition, rather than serving as prior knowledge which aids the problem solution. Part III of this monograph starts by a comprehensive account of ways to learn the pertinent graph topology, ranging from the simplest case where the physics of the problem already suggest a possible graph structure, through to general cases where the graph structure is to be learned from the data observed on a graph. A particular emphasis is placed on the use of standard “relationship measures” in this context, including the correlation and precision matrices, together with the ways to combine these with the available prior knowledge and structural conditions, such as the smoothness of the graph signals or sparsity of graph connections. Next, for learning sparse graphs (that is, graphs with a small number of edges), the utility of the least absolute shrinkage and selection operator, known as LASSO is addressed, along with its graph specific variant, the graphical LASSO. For completeness, both variants of LASSO are derived in an intuitive way, starting from basic principles. An in-depth elaboration of the graph topology learning paradigm is provided through examples on physically well defined graphs, such as electric circuits, linear heat transfer, social and computer networks, and springmass systems. We also review main trends in graph neural networks (GNN) and graph convolutional networks (GCN) from the perspective of graph signal filtering. Particular insight is given to the role of diffusion processes over graphs, to show that GCNs can be understood from the graph diffusion perspective. Given the largely heuristic nature of the existing GCNs, their treatment through graph diffusion processes may also serve as a basis for new designs of GCNs. Tensor representation of lattice-structured graphs is next considered, and it is shown that tensors (multidimensional data arrays) can be treated a special class of graph signals, whereby the graph vertices reside on a high-dimensional regular lattice structure. The concept of graph tensor networks then provides a unifying framework for learning on irregular domains. This part of monograph concludes with an in-dept account of emerging applications in financial data processing and underground transportation network modeling. By means of portfolio cuts of an asset graph, we show how domain knowledge can be meaningfully incorporated into investment analysis. In the underground transportation example, we demonstrate how graph theory can be used to identify those stations in the London underground network which have the greatest influence on the functionality of the traffic, and proceed, in an innovative way, to assess the impact of a station closure on service levels across the city.","","9781680839807","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9419665.pdf&bkn=9419664&pdfType=book","","","","","","","","29 Apr 2021","","","now","Now Foundations and Trends Books"
"Artificial Intelligence for Robotics: Build intelligent robots using ROS 2, Python, OpenCV, and AI/ML techniques for real-world tasks","F. X. G. III; D. K. Namuduri",NA; NA,"Artificial Intelligence for Robotics: Build intelligent robots using ROS 2, Python, OpenCV, and AI/ML techniques for real-world tasks","","2024","","","","","Let an AI and robotics expert help you apply AI, systems engineering, and ML concepts to create smart robots capable of interacting with their environment and users, making decisions, and navigating autonomouslyKey FeaturesGain a holistic understanding of robot design, systems engineering, and task analysisImplement AI/ML techniques to detect and manipulate objects and navigate robots using landmarksIntegrate voice and natural language interactions to create a digital assistant and artificial personality for your robotPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionUnlock the potential of your robots by enhancing their perception with cutting-edge artificial intelligence and machine learning techniques. From neural networks to computer vision, this second edition of the book equips you with the latest tools, new and expanded topics such as object recognition and creating artificial personality, and practical use cases to create truly smart robots. Starting with robotics basics, robot architecture, control systems, and decision-making theory, this book presents systems-engineering methods to design problem-solving robots with single-board computers. You'll explore object recognition using YOLO and genetic algorithms to teach your robot to identify and pick up objects, leverage natural language processing to give your robot a voice, and master neural networks to classify and separate objects and navigate autonomously, before advancing to guiding your robot arms using reinforcement learning and genetic algorithms. The book also covers path planning and goal-oriented programming to prioritize your robot's tasks, showing you how to connect all software using Python and ROS 2 for a seamless experience. By the end of this book, you'll have learned how to transform your robot into a helpful assistant with NLP and give it an artificial personality, ready to tackle real-world tasks and even crack jokes.What you will learnGet started with robotics and AI essentialsUnderstand path planning, decision trees, and search algorithms to enhance your robotExplore object recognition using neural networks and supervised learning techniquesEmploy genetic algorithms to enable your robot arm to manipulate objectsTeach your robot to listen using Natural Language Processing through an expert systemProgram your robot in how to avoid obstacles and retrieve objects with machine learning and computer visionApply simulation techniques to give your robot an artificial personalityWho this book is forThis book is for practicing robotics engineers and enthusiasts aiming to advance their skills by applying AI and ML techniques. Students and researchers looking for practical guidance for solving specific problems or approaching a difficult robot design will find this book insightful. Proficiency in Python programming, familiarity with electronics and wiring, single board computers, Linux-based command-line interface (CLI), and knowledge of AI/ML concepts are required to get started with this book.","","9781805124399","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769253.pdf&bkn=10769252&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Automated Deep Learning: Neural Architecture Search Is Not the End","X. Dong; D. J. Kedziora; K. Musial; B. Gabrys",NA; NA; NA; NA,Automated Deep Learning: Neural Architecture Search Is Not the End,"","2024","","","","","Deep learning (DL) has proven to be a highly effective approach for developing models in diverse contexts, including visual perception, speech recognition, and machine translation. Automated deep learning (AutoDL) endeavors to minimize the need for human involvement and is best known for its achievements in neural architecture search (NAS). In this monograph, the authors examine research efforts into automation across the entirety of an archetypal DL workflow. In so doing, they propose a comprehensive set of ten criteria by which to assess existing work in both individual publications and broader research areas, namely novelty, solution quality, efficiency, stability, interpretability, reproducibility, engineering quality, scalability, generalizability, and eco-friendliness. Aimed at students and researchers, this monograph provides an evaluative overview of AutoDL in the early 2020s, identifying where future opportunities for progress may exist.","","9781638283195","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10453492.pdf&bkn=10453491&pdfType=book","","","","","","","","29 Feb 2024","","","now","Now Foundations and Trends Books"
"Knowledge Graphs: An Information Retrieval Perspective","R. Reinanda; E. Meij; M. de Rijke",NA; NA; NA,Knowledge Graphs: An Information Retrieval Perspective,"","2020","","","","","The aim of this survey is to bridge two important components of modern information access: information retrieval (IR) and knowledge graphs (KGs). Modern IR systems can benefit from information available in KGs in multiple ways, independent of whether the KGs are publicly available or proprietary ones. The authors provide an overview of the literature on KGs in the context of IR and the components required when building IR systems that leverage KGs. As an understanding of the intersection of IR and KGs is beneficial to many researchers and practitioners, they consider prior work from two complementary angles: leveraging KGs for information retrieval and enriching KGs using IR techniques. They summarize research work, group related approaches, and discuss challenges shared across tasks at the interface of IR and KGs. In Knowledge Graphs: An Information Retrieval Perspective, the authors present an extensive overview of tasks related to KGs from an IR perspective, provide a thorough review for each task, and present discussions on common issues that are shared among the tasks. They discuss common issues that appear across the tasks that consider and identify future directions for addressing them. They also provide pointers to datasets and other resources that should be useful for both newcomers and experienced researchers in the area.","","9781680837292","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9228849.pdf&bkn=9228848&pdfType=book","","","","","","","","19 Oct 2020","","","now","Now Foundations and Trends Books"
"Misinformation Detection: A Survey of AI Techniques and Research Opportunities","G. Taylor; W. Jiang; X. Qin; A. Gupta",NA; NA; NA; NA,Misinformation Detection: A Survey of AI Techniques and Research Opportunities,"","2024","","","","","Misinformation Detection: A Survey of AI Techniques and Research Opportunities highlights the evolution of techniques within misinformation detection. This monograph provides an overview of the types of misinformation, classification strategies, strengths and weaknesses, and potential solutions for open issues. This comprehensive survey is helpful for future research as a resource for successful solutions and possible ideas. The authors first present background information including search strategies in Sections 1 and 2 and identify the types of misinformation in Section 3. The monograph then elaborates on the methods and compares the existing successful techniques from various perspectives in Section 4. Section 5 provides tools and datasets for misinformation detection. Section 6 reviews the challenges and open issues of misinformation and makes suggestions on how one can address those challenges. The monograph concludes in Section 7 with final thoughts on the areas of misinformation detection and an overall analysis of this field.","","9781638284178","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10706755.pdf&bkn=10706754&pdfType=book","","","","","","","","7 Oct 2024","","","now","Now Foundations and Trends Books"
"A Design Space of Sports Interaction Technology","D. B. W. Postma; R. W. van Delden; J. H. Koekoek; W. W. Walinga; I. M. van Hilvoorde; B. J. F. van Beijnum; F. A. Salim; D. Reidsma",NA; NA; NA; NA; NA; NA; NA; NA,A Design Space of Sports Interaction Technology,"","2022","","","","","This monograph introduces a new, systematic taxonomy of Sports Interaction Technology (Sports ITech) that defines a design space of existing and future work in this domain. The authors put the taxonomy in a context of sport science and practice, target outcomes of sports, the underlying factors that influence them, and the role that sports technology plays in supporting sports science and practice. They build on the existing taxonomies and a vast body of literature from multiple domains of HCI, technology, sports science, and related work in Sports ITech, and complement it with identified gaps in the literature. This taxonomy is meant to be used by designers of Sports ITech. It will help better highlight and position existing work as well as provide input and inspiration for the design and deployment of such technology. It offers a description of a design space suitable to support designers, technologists, and sports people with a mindset to design, deploy, and adapt Sports ITech. The authors present this work as a call to action to bring HCI and the sports sciences closer together in the new field of Sports Interaction Technology, and aim to set a shared agenda for future developments. A Design Space of Sports Interaction Technology is a complete guide to navigate the literature from the many underlying disciplines of Sports ITech.","","9781638280651","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9868318.pdf&bkn=9868317&pdfType=book","","","","","","","","29 Aug 2022","","","now","Now Foundations and Trends Books"
"Network Science with Python: Explore the networks around us using network science, social network analysis, and machine learning","D. Knickerbocker",NA,"Network Science with Python: Explore the networks around us using network science, social network analysis, and machine learning","","2023","","","","","Discover the use of graph networks to develop a new approach to data science using theoretical and practical methods with this expert guide using Python, printed in colorKey FeaturesCreate networks using data points and informationLearn to visualize and analyze networks to better understand communitiesExplore the use of network data in both - supervised and unsupervised machine learning projectsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionNetwork analysis is often taught with tiny or toy data sets, leaving you with a limited scope of learning and practical usage. Network Science with Python helps you extract relevant data, draw conclusions and build networks using industry-standard – practical data sets. You’ll begin by learning the basics of natural language processing, network science, and social network analysis, then move on to programmatically building and analyzing networks. You’ll get a hands-on understanding of the data source, data extraction, interaction with it, and drawing insights from it. This is a hands-on book with theory grounding, specific technical, and mathematical details for future reference. As you progress, you’ll learn to construct and clean networks, conduct network analysis, egocentric network analysis, community detection, and use network data with machine learning. You’ll also explore network analysis concepts, from basics to an advanced level. By the end of the book, you’ll be able to identify network data and use it to extract unconventional insights to comprehend the complex world around you.What you will learnExplore NLP, network science, and social network analysisApply the tech stack used for NLP, network science, and analysisExtract insights from NLP and network dataGenerate personalized NLP and network projectsAuthenticate and scrape tweets, connections, the web, and data streamsDiscover the use of network data in machine learning projectsWho this book is forNetwork Science with Python demonstrates how programming and social science can be combined to find new insights. Data scientists, NLP engineers, software engineers, social scientists, and data science students will find this book useful. An intermediate level of Python programming is a prerequisite. Readers from both – social science and programming backgrounds will find a new perspective and add a feather to their hat.","","9781801075213","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162494.pdf&bkn=10162493&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Graph Neural Networks for Natural Language Processing: A Survey","L. Wu; Y. Chen; K. Shen; X. Guo; H. Gao; S. Li; J. Pei; B. Long",NA; NA; NA; NA; NA; NA; NA; NA,Graph Neural Networks for Natural Language Processing: A Survey,"","2023","","","","","Deep learning has become the dominant approach in addressing various tasks in Natural Language Processing (NLP). Although text inputs are typically represented as a sequence of tokens, there is a rich variety of NLP problems that can be best expressed with a graph structure. As a result, there is a surge of interest in developing new deep learning techniques on graphs for a large number of NLP tasks. In this monograph, the authors present a comprehensive overview on Graph Neural Networks (GNNs) for Natural Language Processing. They propose a new taxonomy of GNNs for NLP, which systematically organizes existing research of GNNs for NLP along three axes: graph construction, graph representation learning, and graph based encoder-decoder models. They further introduce a large number of NLP applications that exploits the power of GNNs and summarize the corresponding benchmark datasets, evaluation metrics, and open-source codes. Finally, they discuss various outstanding challenges for making the full use of GNNs for NLP as well as future research directions. This is the first comprehensive overview of Graph Neural Networks for Natural Language Processing. It provides students and researchers with a concise and accessible resource to quickly get up to speed with an important area of machine learning research.","","9781638281436","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10026677.pdf&bkn=10026676&pdfType=book","","","","","","","","27 Jan 2023","","","now","Now Foundations and Trends Books"
"Flask Framework Cookbook: Enhance your Flask skills with advanced techniques and build dynamic, responsive web applications","S. Aggarwal",NA,"Flask Framework Cookbook: Enhance your Flask skills with advanced techniques and build dynamic, responsive web applications","","2023","","","","","Design and deploy robust state-of-the-art web applications using Flask 2.x and Python 3 frameworks and libraries for streamlined development and optimal performance Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesA practical and rich companion guide for web developers, offering real-world situations and use cases to learn FlaskGet the most out of the powerful Flask framework while preserving the flexibility of your design choicesWrite cleaner, testable, and maintainable code with the help of sample appsBook DescriptionDiscover what makes Flask, the lightweight Python web framework, popular, as you delve into its modular design that enables the development of scalable web apps. With this practical guide, you'll explore modern solutions, recommended design patterns, and best practices for Flask web development. Updated to the latest version of Flask and Python, this third edition of the Flask Framework Cookbook moves away from the outdated libraries, updates content to incorporate new coding patterns, and introduces recipes for the latest tools. You'll explore different ways to integrate with GPT to build AI-ready Flask applications. The book starts with an exploration of Flask application configurations and then guides you through working with templates and understanding the ORM and view layers. You’ll also be able to write an admin interface and get to grips with testing using the factory pattern, debugging, and logging errors. Then you’ll discover different ways of using Flask to create, deploy, and manage microservices using AWS, GCP, and Kubernetes. Finally, you’ll gain insights into various deployment and post-deployment techniques for platforms such as Apache, Tornado, and Datadog. By the end of this book, you'll have acquired the knowledge necessary to write Flask applications that cater to a wide range of use cases in the best possible way and scale them using standard industry practices.What you will learnExplore advanced templating and data modeling techniquesDiscover effective debugging, logging, and error-handling techniques in FlaskWork with different types of databases, including RDBMS and NoSQLIntegrate Flask with different technologies such as Redis, Sentry, and DatadogDeploy and package Flask applications with Docker and KubernetesIntegrate GPT with your Flask application to build future-ready platformsImplement continuous integration and continuous deployment (CI/CD) to ensure efficient and consistent updates to your Flask web applicationsWho this book is forIf you are a web developer seeking to expand your knowledge of developing scalable and production-ready applications in Flask, this is the book for you. It is also highly valuable if you are already aware of Flask's major extensions and want to leverage them for better application development. This book will come handy as a quick reference for specific topic on Flask, its popular extensions, or specific use cases. It assumes basic Python programming experience, as well as familiarity with web development and related terminology.","","9781804610008","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251290.pdf&bkn=10251289&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Mastering React Test-Driven Development: Build simple and maintainable web apps with React, Redux, and GraphQL","D. Irvine; J. Searls",NA; NA,"Mastering React Test-Driven Development: Build simple and maintainable web apps with React, Redux, and GraphQL","","2022","","","","","Learn test-driven and behavior-driven development techniques that will give you greater confidence when building React applicationsKey FeaturesExplore the TDD process, how it works, and why it will help you write maintainable React appsDevelop a component testing framework from scratch, which will help you understand the mechanics of good unit testingReduce complexity by using unit tests and end-to-end acceptance tests to drive the design of your appsBook DescriptionTest-driven development (TDD) is a programming workflow that helps you build your apps by specifying behavior as automated tests. The TDD workflow future-proofs apps so that they can be modified without fear of breaking existing functionality. Another benefit of TDD is that it helps software development teams communicate their intentions more clearly, by way of test specifications. This book teaches you how to apply TDD when building React apps. You’ll create a sample app using the same React libraries and tools that professional React developers use, such as Jest, React Router, Redux, Relay (GraphQL), Cucumber, and Puppeteer. The TDD workflow is supported by various testing techniques and patterns, which are useful even if you’re not following the TDD process. This book covers these techniques by walking you through the creation of a component test framework. You’ll learn automated testing theory which will help you work with any of the test libraries that are in standard usage today, such as React Testing Library. This second edition has been revised with a stronger focus on concise code examples and has been fully updated for React 18. By the end of this TDD book, you’ll be able to use React, Redux, and GraphQL to develop robust web apps.What you will learnBuild test-driven applications using React 18 and JestUnderstand techniques and patterns for writing great automated testsUse test doubles and mocks effectivelyTest-drive browser APIs, including the Fetch API and the WebSocket APIIntegrate with libraries such as React Router, Redux, and Relay (GraphQL)Use Cucumber.js and Puppeteer to build Behaviour- Driven Development (BDD) style tests for your applicationsBuild and test async Redux code using redux-saga and expect-reduxWho this book is forThis book is for frontend developers who are looking to improve their testing practices and increase the quality and maintainability of their applications. To make the most of this book, you’ll need knowledge of the JavaScript programming language.","","9781803230559","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162147.pdf&bkn=10162146&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Interpretable Machine Learning with Python: Learn to build interpretable high-performance models with hands-on real-world examples","S. Masís",NA,Interpretable Machine Learning with Python: Learn to build interpretable high-performance models with hands-on real-world examples,"","2021","","","","","A deep and detailed dive into the key aspects and challenges of machine learning interpretability, complete with the know-how on how to overcome and leverage them to build fairer, safer, and more reliable modelsKey FeaturesLearn how to extract easy-to-understand insights from any machine learning modelBecome well-versed with interpretability techniques to build fairer, safer, and more reliable modelsMitigate risks in AI systems before they have broader implications by learning how to debug black-box modelsBook DescriptionDo you want to gain a deeper understanding of your models and better mitigate poor prediction risks associated with machine learning interpretation? If so, then Interpretable Machine Learning with Python deserves a place on your bookshelf. We’ll be starting off with the fundamentals of interpretability, its relevance in business, and exploring its key aspects and challenges. As you progress through the chapters, you'll then focus on how white-box models work, compare them to black-box and glass-box models, and examine their trade-off. You’ll also get you up to speed with a vast array of interpretation methods, also known as Explainable AI (XAI) methods, and how to apply them to different use cases, be it for classification or regression, for tabular, time-series, image or text. In addition to the step-by-step code, this book will also help you interpret model outcomes using examples. You’ll get hands-on with tuning models and training data for interpretability by reducing complexity, mitigating bias, placing guardrails, and enhancing reliability. The methods you’ll explore here range from state-of-the-art feature selection and dataset debiasing methods to monotonic constraints and adversarial retraining. By the end of this book, you'll be able to understand ML models better and enhance them through interpretability tuning. What you will learnRecognize the importance of interpretability in businessStudy models that are intrinsically interpretable such as linear models, decision trees, and Naïve BayesBecome well-versed in interpreting models with model-agnostic methodsVisualize how an image classifier works and what it learnsUnderstand how to mitigate the influence of bias in datasetsDiscover how to make models more reliable with adversarial robustnessUse monotonic constraints to make fairer and safer modelsWho this book is forThis book is primarily written for data scientists, machine learning developers, and data stewards who find themselves under increasing pressures to explain the workings of AI systems, their impacts on decision making, and how they identify and manage bias. It’s also a useful resource for self-taught ML enthusiasts and beginners who want to go deeper into the subject matter, though a solid grasp on the Python programming language and ML fundamentals is needed to follow along.","","9781800206571","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162157.pdf&bkn=10162156&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Quantum Computing with Silq Programming: Get up and running with quantum computing with the simplicity of this new high-level programming language","S. Ganguly; T. Cambier",NA; NA,Quantum Computing with Silq Programming: Get up and running with quantum computing with the simplicity of this new high-level programming language,"","2021","","","","","Learn the mathematics behind quantum computing and explore the high-level quantum language Silq to take your quantum programming skills to the next levelKey FeaturesHarness the potential of quantum computers more effectively using SilqLearn how to solve core problems that you may face while writing quantum programsExplore useful quantum applications such as cryptography and quantum machine learningBook DescriptionQuantum computing is a growing field, with many research projects focusing on programming quantum computers in the most efficient way possible. One of the biggest challenges faced with existing languages is that they work on low-level circuit model details and are not able to represent quantum programs accurately. Developed by researchers at ETH Zurich after analyzing languages including Q# and Qiskit, Silq is a high-level programming language that can be viewed as the C++ of quantum computers! Quantum Computing with Silq Programming helps you explore Silq and its intuitive and simple syntax to enable you to describe complex tasks with less code. This book will help you get to grips with the constructs of the Silq and show you how to write quantum programs with it. You’ll learn how to use Silq to program quantum algorithms to solve existing and complex tasks. Using quantum algorithms, you’ll also gain practical experience in useful applications such as quantum error correction, cryptography, and quantum machine learning. Finally, you’ll discover how to optimize the programming of quantum computers with the simple Silq. By the end of this Silq book, you’ll have mastered the features of Silq and be able to build efficient quantum applications independently.What you will learnIdentify the challenges that researchers face in quantum programmingUnderstand quantum computing concepts and learn how to make quantum circuitsExplore Silq programming constructs and use them to create quantum programsUse Silq to code quantum algorithms such as Grover's and Simon’sDiscover the practicalities of quantum error correction with SilqExplore useful applications such as quantum machine learning in a practical wayWho this book is forThis Silq quantum computing book is for students, researchers, and scientists looking to learn quantum computing techniques and software development. Quantum computing enthusiasts who want to explore this futuristic technology will also find this book useful. Beginner-level knowledge of any programming language as well as mathematical topics such as linear algebra, probability, complex numbers, and statistics is required.","","9781800561212","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163611.pdf&bkn=10163610&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Deep Learning for Time Series Cookbook: Use PyTorch and Python recipes for forecasting, classification, and anomaly detection","V. Cerqueira; L. Roque",NA; NA,"Deep Learning for Time Series Cookbook: Use PyTorch and Python recipes for forecasting, classification, and anomaly detection","","2024","","","","","Learn how to deal with time series data and how to model it using deep learning and take your skills to the next level by mastering PyTorch using different Python recipesKey FeaturesLearn the fundamentals of time series analysis and how to model time series data using deep learningExplore the world of deep learning with PyTorch and build advanced deep neural networksGain expertise in tackling time series problems, from forecasting future trends to classifying patterns and anomaly detectionPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionMost organizations exhibit a time-dependent structure in their processes, including fields such as finance. By leveraging time series analysis and forecasting, these organizations can make informed decisions and optimize their performance. Accurate forecasts help reduce uncertainty and enable better planning of operations. Unlike traditional approaches to forecasting, deep learning can process large amounts of data and help derive complex patterns. Despite its increasing relevance, getting the most out of deep learning requires significant technical expertise. This book guides you through applying deep learning to time series data with the help of easy-to-follow code recipes. You’ll cover time series problems, such as forecasting, anomaly detection, and classification. This deep learning book will also show you how to solve these problems using different deep neural network architectures, including convolutional neural networks (CNNs) or transformers. As you progress, you’ll use PyTorch, a popular deep learning framework based on Python to build production-ready prediction solutions. By the end of this book, you'll have learned how to solve different time series tasks with deep learning using the PyTorch ecosystem.What you will learnGrasp the core of time series analysis and unleash its power using PythonUnderstand PyTorch and how to use it to build deep learning modelsDiscover how to transform a time series for training transformersUnderstand how to deal with various time series characteristicsTackle forecasting problems, involving univariate or multivariate dataMaster time series classification with residual and convolutional neural networksGet up to speed with solving time series anomaly detection problems using autoencoders and generative adversarial networks (GANs)Who this book is forIf you’re a machine learning enthusiast or someone who wants to learn more about building forecasting applications using deep learning, this book is for you. Basic knowledge of Python programming and machine learning is required to get the most out of this book.","","9781805122739","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10522535.pdf&bkn=10522534&pdfType=book","","","","","","","","8 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"15 Math Concepts Every Data Scientist Should Know: Understand and learn how to apply the math behind data science algorithms","D. Hoyle",NA,15 Math Concepts Every Data Scientist Should Know: Understand and learn how to apply the math behind data science algorithms,"","2024","","","","","Create more effective and powerful data science solutions by learning when, where, and how to apply key math principles that drive most data science algorithmsKey FeaturesUnderstand key data science algorithms with Python-based examplesIncrease the impact of your data science solutions by learning how to apply existing algorithmsTake your data science solutions to the next level by learning how to create new algorithmsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionData science combines the power of data with the rigor of scientific methodology, with mathematics providing the tools and frameworks for analysis, algorithm development, and deriving insights. As machine learning algorithms become increasingly complex, a solid grounding in math is crucial for data scientists. David Hoyle, with over 30 years of experience in statistical and mathematical modeling, brings unparalleled industrial expertise to this book, drawing from his work in building predictive models for the world's largest retailers. Encompassing 15 crucial concepts, this book covers a spectrum of mathematical techniques to help you understand a vast range of data science algorithms and applications. Starting with essential foundational concepts, such as random variables and probability distributions, you’ll learn why data varies, and explore matrices and linear algebra to transform that data. Building upon this foundation, the book spans general intermediate concepts, such as model complexity and network analysis, as well as advanced concepts such as kernel-based learning and information theory. Each concept is illustrated with Python code snippets demonstrating their practical application to solve problems. By the end of the book, you’ll have the confidence to apply key mathematical concepts to your data science challenges.What you will learnMaster foundational concepts that underpin all data science applicationsUse advanced techniques to elevate your data science proficiencyApply data science concepts to solve real-world data science challengesImplement the NumPy, SciPy, and scikit-learn concepts in PythonBuild predictive machine learning models with mathematical conceptsGain expertise in Bayesian non-parametric methods for advanced probabilistic modelingAcquire mathematical skills tailored for time-series and network data typesWho this book is forThis book is for data scientists, machine learning engineers, and data analysts who already use data science tools and libraries but want to learn more about the underlying math. Whether you’re looking to build upon the math you already know, or need insights into when and how to adopt tools and libraries to your data science problem, this book is for you. Organized into essential, general, and selected concepts, this book is for both practitioners just starting out on their data science journey and experienced data scientists.","","9781837631940","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769351.pdf&bkn=10769350&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Hands-On Graph Neural Networks Using Python: Practical techniques and architectures for building powerful graph and deep learning apps with PyTorch","M. Labonne",NA,Hands-On Graph Neural Networks Using Python: Practical techniques and architectures for building powerful graph and deep learning apps with PyTorch,"","2023","","","","","Design robust graph neural networks with PyTorch Geometric by combining graph theory and neural networks with the latest developments and apps Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesImplement state-of-the-art graph neural network architectures in PythonCreate your own graph datasets from tabular dataBuild powerful traffic forecasting, recommender systems, and anomaly detection applicationsBook DescriptionGraph neural networks are a highly effective tool for analyzing data that can be represented as a graph, such as social networks, chemical compounds, or transportation networks. The past few years have seen an explosion in the use of graph neural networks, with their application ranging from natural language processing and computer vision to recommendation systems and drug discovery. Hands-On Graph Neural Networks Using Python begins with the fundamentals of graph theory and shows you how to create graph datasets from tabular data. As you advance, you’ll explore major graph neural network architectures and learn essential concepts such as graph convolution, self-attention, link prediction, and heterogeneous graphs. Finally, the book proposes applications to solve real-life problems, enabling you to build a professional portfolio. The code is readily available online and can be easily adapted to other datasets and apps. By the end of this book, you’ll have learned to create graph datasets, implement graph neural networks using Python and PyTorch Geometric, and apply them to solve real-world problems, along with building and training graph neural network models for node and graph classification, link prediction, and much more.What you will learnUnderstand the fundamental concepts of graph neural networksImplement graph neural networks using Python and PyTorch GeometricClassify nodes, graphs, and edges using millions of samplesPredict and generate realistic graph topologiesCombine heterogeneous sources to improve performanceForecast future events using topological informationApply graph neural networks to solve real-world problemsWho this book is forThis book is for machine learning practitioners and data scientists interested in learning about graph neural networks and their applications, as well as students looking for a comprehensive reference on this rapidly growing field. Whether you’re new to graph neural networks or looking to take your knowledge to the next level, this book has something for you. Basic knowledge of machine learning and Python programming will help you get the most out of this book.","","9781804610701","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251350.pdf&bkn=10251349&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"​Application Lifecycle Management on Microsoft Power Platform: A comprehensive guide to managing the deployment of your solutions","B. Bergmann; S. Durow",NA; NA,​Application Lifecycle Management on Microsoft Power Platform: A comprehensive guide to managing the deployment of your solutions,"","2024","","","","","Implement modern DevOps techniques in the Power Platform to boost business and maker productivityKey FeaturesDemystify ALM concepts and how they apply to Microsoft Power Platform Application Lifecycle Management on Microsoft Power PlatformDefine the best strategy for possible solutions, source code, and environmentsAutomate build and deployment tasks using Azure DevOps and GitHubPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionManaging Power Platform solutions manually can be challenging and time-consuming, as is application lifecycle management (ALM), which encompasses governance, development, and maintenance. This book provides comprehensive coverage of ALM, addressing planning, development, testing, deployment, and maintenance. Drawing on his extensive experience as a Power Platform consultant and Microsoft MVP, Benedikt Bergmann simplifies complex topics, making them accessible and easy to grasp. From planning and designing applications to deploying and maintaining them, this book provides step-by-step instructions, best practices, and real-world examples to effectively manage the entire application lifecycle. You’ll gain insights into optimizing Power Platform's toolbox, including Power Apps, Power Automate, Power Pages, and Power Virtual Agents, for seamless collaboration, agile development, and rapid application delivery. You’ll also implement best practices for version control, code management, and collaboration using the Microsoft Power Platform. By the end of this book, you’ll be equipped with the knowledge and skills to effectively manage the entire application lifecycle, accelerate development cycles, and deliver exceptional solutions with the Microsoft Power Platform.What you will learnUnderstand the importance of ALM in the context of Microsoft Power PlatformLeverage the Power Platform CLI to streamline ALM practicesDevelop a comprehensive strategy for managing Power Platform environmentsExplore techniques for defining robust Dataverse solutions for scalability and performanceApply ALM concepts to Microsoft Power PlatformUse Managed Pipelines in managed Power Platform environmentsImplement a source-code-centric approach with Azure DevOps Pipelines and GitHub ActionsWho this book is forIf you are involved in managing the deployment of Microsoft Power Platform solutions, whether as a solution architect, developer, functional consultant, or DevOps specialist, this book is for you. Familiarity with Power Platform is recommended.","","9781835460894","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10803995.pdf&bkn=10803994&pdfType=book","","","","","","","","16 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"Python for Finance Cookbook: Over 80 powerful recipes for effective financial data analysis","E. Lewinson",NA,Python for Finance Cookbook: Over 80 powerful recipes for effective financial data analysis,"","2022","","","","","Use modern Python libraries such as pandas, NumPy, and scikit-learn and popular machine learning and deep learning methods to solve financial modeling problems Purchase of the print or Kindle book includes a free eBook in the PDF formatKey FeaturesExplore unique recipes for financial data processing and analysis with PythonApply classical and machine learning approaches to financial time series analysisCalculate various technical analysis indicators and backtest trading strategiesBook DescriptionPython is one of the most popular programming languages in the financial industry, with a huge collection of accompanying libraries. In this new edition of the Python for Finance Cookbook, you will explore classical quantitative finance approaches to data modeling, such as GARCH, CAPM, factor models, as well as modern machine learning and deep learning solutions. You will use popular Python libraries that, in a few lines of code, provide the means to quickly process, analyze, and draw conclusions from financial data. In this new edition, more emphasis was put on exploratory data analysis to help you visualize and better understand financial data. While doing so, you will also learn how to use Streamlit to create elegant, interactive web applications to present the results of technical analyses. Using the recipes in this book, you will become proficient in financial data analysis, be it for personal or professional projects. You will also understand which potential issues to expect with such analyses and, more importantly, how to overcome them.What you will learnPreprocess, analyze, and visualize financial dataExplore time series modeling with statistical (exponential smoothing, ARIMA) and machine learning modelsUncover advanced time series forecasting algorithms such as Meta’s ProphetUse Monte Carlo simulations for derivatives valuation and risk assessmentExplore volatility modeling using univariate and multivariate GARCH modelsInvestigate various approaches to asset allocationLearn how to approach ML-projects using an example of default predictionExplore modern deep learning models such as Google’s TabNet, Amazon’s DeepAR and NeuralProphetWho this book is forThis book is intended for financial analysts, data analysts and scientists, and Python developers with a familiarity with financial concepts. You’ll learn how to correctly use advanced approaches for analysis, avoid potential pitfalls and common mistakes, and reach correct conclusions for a broad range of finance problems. Working knowledge of the Python programming language (particularly libraries such as pandas and NumPy) is necessary.","","9781803238838","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162460.pdf&bkn=10162459&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Advanced Natural Language Processing with TensorFlow 2: Build effective real-world NLP applications using NER, RNNs, seq2seq models, Transformers, and more","A. Bansal",NA,"Advanced Natural Language Processing with TensorFlow 2: Build effective real-world NLP applications using NER, RNNs, seq2seq models, Transformers, and more","","2021","","","","","One-stop solution for NLP practitioners, ML developers, and data scientists to build effective NLP systems that can perform real-world complicated tasksKey FeaturesApply deep learning algorithms and techniques such as BiLSTMS, CRFs, BPE and more using TensorFlow 2Explore applications like text generation, summarization, weakly supervised labelling and moreRead cutting edge material with seminal papers provided in the GitHub repository with full working codeBook DescriptionRecently, there have been tremendous advances in NLP, and we are now moving from research labs into practical applications. This book comes with a perfect blend of both the theoretical and practical aspects of trending and complex NLP techniques. The book is focused on innovative applications in the field of NLP, language generation, and dialogue systems. It helps you apply the concepts of pre-processing text using techniques such as tokenization, parts of speech tagging, and lemmatization using popular libraries such as Stanford NLP and SpaCy. You will build Named Entity Recognition (NER) from scratch using Conditional Random Fields and Viterbi Decoding on top of RNNs. The book covers key emerging areas such as generating text for use in sentence completion and text summarization, bridging images and text by generating captions for images, and managing dialogue aspects of chatbots. You will learn how to apply transfer learning and fine-tuning using TensorFlow 2. Further, it covers practical techniques that can simplify the labelling of textual data. The book also has a working code that is adaptable to your use cases for each tech piece. By the end of the book, you will have an advanced knowledge of the tools, techniques and deep learning architecture used to solve complex NLP problems. What you will learnGrasp important pre-steps in building NLP applications like POS taggingUse transfer and weakly supervised learning using libraries like SnorkelDo sentiment analysis using BERTApply encoder-decoder NN architectures and beam search for summarizing textsUse Transformer models with attention to bring images and text togetherBuild apps that generate captions and answer questions about images using custom TransformersUse advanced TensorFlow techniques like learning rate annealing, custom layers, and custom loss functions to build the latest DeepNLP modelsWho this book is forThis is not an introductory book and assumes the reader is familiar with basics of NLP and has fundamental Python skills, as well as basic knowledge of machine learning and undergraduate-level calculus and linear algebra. The readers who can benefit the most from this book include intermediate ML developers who are familiar with the basics of supervised learning and deep learning techniques and professionals who already use TensorFlow/Python for purposes such as data science, ML, research, analysis, etc. ","","9781800201057","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163517.pdf&bkn=10163516&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Computer Vision on AWS: Build and deploy real-world CV solutions with Amazon Rekognition, Lookout for Vision, and SageMaker","L. Mullennex; N. Bachmeier; J. Rao",NA; NA; NA,"Computer Vision on AWS: Build and deploy real-world CV solutions with Amazon Rekognition, Lookout for Vision, and SageMaker","","2023","","","","","Develop scalable computer vision solutions for real-world business problems and discover scaling, cost reduction, security, and bias mitigation best practices with AWS AI/ML services Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn how to quickly deploy and automate end-to-end CV pipelines on AWSImplement design principles to mitigate bias and scale production of CV workloadsWork with code examples to master CV concepts using AWS AI/ML servicesBook DescriptionComputer vision (CV) is a field of artificial intelligence that helps transform visual data into actionable insights to solve a wide range of business challenges. This book provides prescriptive guidance to anyone looking to learn how to approach CV problems for quickly building and deploying production-ready models. You’ll begin by exploring the applications of CV and the features of Amazon Rekognition and Amazon Lookout for Vision. The book will then walk you through real-world use cases such as identity verification, real-time video analysis, content moderation, and detecting manufacturing defects that’ll enable you to understand how to implement AWS AI/ML services. As you make progress, you'll also use Amazon SageMaker for data annotation, training, and deploying CV models. In the concluding chapters, you'll work with practical code examples, and discover best practices and design principles for scaling, reducing cost, improving the security posture, and mitigating bias of CV workloads. By the end of this AWS book, you'll be able to accelerate your business outcomes by building and implementing CV into your production environments with the help of AWS AI/ML services.What you will learnApply CV across industries, including e-commerce, logistics, and mediaBuild custom image classifiers with Amazon Rekognition Custom LabelsCreate automated end-to-end CV workflows on AWSDetect product defects on edge devices using Amazon Lookout for VisionBuild, deploy, and monitor CV models using Amazon SageMakerDiscover best practices for designing and evaluating CV workloadsDevelop an AI governance strategy across the entire machine learning life cycleWho this book is forIf you are a machine learning engineer or data scientist looking to discover best practices and learn how to build comprehensive CV solutions on AWS, this book is for you. Knowledge of AWS basics is required to grasp the concepts covered in this book more effectively. A solid understanding of machine learning concepts and the Python programming language will also be beneficial.","","9781803248202","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163521.pdf&bkn=10163520&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Internet of Things Programming Projects: Build exciting IoT projects using Raspberry Pi 5, Raspberry Pi Pico, and Python","C. Dow",NA,"Internet of Things Programming Projects: Build exciting IoT projects using Raspberry Pi 5, Raspberry Pi Pico, and Python","","2024","","","","","Unleash the potential of IoT by creating weather indicators, information displays, alarm systems, and a vision recognition-enabled robot carKey FeaturesGet to grips with the Raspberry Pi ecosystem and its role in IoT developmentIntegrate cutting-edge technologies such as MQTT, LoRa, and ROS for advanced IoT applicationsAchieve superior control in your robot car with vision recognition and the power of ROSPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionRenowned for its versatility, affordability, and active community support, Raspberry Pi is at the forefront of IoT development. Unlock the vast potential of Raspberry Pi and Raspberry Pi Pico by learning how to develop practical projects with this updated edition of Internet of Things Programming Projects. Written by an expert programmer who’s worked for some of Canada’s largest companies, this book starts with foundational concepts and practical exercises such as building a basic weather indicator, and gradually progressed toward more complex projects. You’ll get to grips with coding nuances and web service integrations that will help you create a sophisticated IoT robot car equipped with motor control, wireless communication, and sensor amalgamation. The book also explores LoRa technology, a game-changer for long-range, low-power communication in your projects, and delves into robot car development by implementing the Robot Operating System (ROS) for advanced control and coordination. Through clear, step-by-step instructions and insightful explanations, you’ll gain the skills and confidence to develop innovative IoT solutions for real-world applications. By the end of the book, you’ll have mastered the intricacies of IoT programming, from harnessing Raspberry Pi's capabilities to seamlessly integrating external components.What you will learnIntegrate web services into projects for real-time data display and analysisIntegrate sensors, motors, and displays to build smart IoT devicesBuild a weather indicator using servo motors and LEDsCreate an autonomous IoT robot car capable of performing tasksDevelop a home security system with real-time alerts and SMS notificationsExplore LoRa and LoRaWAN for remote environmental monitoringWho this book is forThis book is for beginners as well as experienced programmers, IoT developers, and Raspberry Pi enthusiasts. With just basic knowledge of IoT, you can dive right in and explore the projects with ease.","","9781835088685","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769326.pdf&bkn=10769325&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Mastering Kubernetes: Dive into Kubernetes and learn how to create and operate world-class cloud-native systems","G. Sayfan; B. Ibryam",NA; NA,Mastering Kubernetes: Dive into Kubernetes and learn how to create and operate world-class cloud-native systems,"","2023","","","","","Go beyond the basics of Kubernetes and explore more advanced concepts, including Kubernetes in production, governance, serverless computing, and service meshes. Purchase of the print or Kindle book includes a free eBook in PDF format.Key FeaturesMaster Kubernetes architecture and design to build, deploy, and secure large-scale distributed systemsLearn advanced concepts like autoscaling, multi-cluster management, serverless computing, service meshes and policy enginesExplore Kubernetes 1.25 and its rich ecosystem of tools like Kubectl, Krew, K9s, Lens, and HelmBook DescriptionThe fourth edition of the bestseller Mastering Kubernetes includes the most recent tools and code to enable you to learn the latest features of Kubernetes 1.25. This book contains a thorough exploration of complex concepts and best practices to help you master the skills of designing and deploying large-scale distributed systems on Kubernetes clusters. You’ll learn how to run complex stateless and stateful microservices on Kubernetes, including advanced features such as horizontal pod autoscaling, rolling updates, resource quotas, and persistent storage backends. In addition, you’ll understand how to utilize serverless computing and service meshes. Further, two new chapters have been added. “Governing Kubernetes” covers the problem of policy management, how admission control addresses it, and how policy engines provide a powerful governance solution. “Running Kubernetes in Production” shows you what it takes to run Kubernetes at scale across multiple cloud providers, multiple geographical regions, and multiple clusters, and it also explains how to handle topics such as upgrades, capacity planning, dealing with cloud provider limits/quotas, and cost management. By the end of this Kubernetes book, you’ll have a strong understanding of, and hands-on experience with, a wide range of Kubernetes capabilities.What you will learnLearn how to govern Kubernetes using policy enginesLearn what it takes to run Kubernetes in production and at scaleBuild and run stateful applications and complex microservicesMaster Kubernetes networking with services, Ingress objects, load balancers, and service meshesAchieve high availability for your Kubernetes clustersImprove Kubernetes observability with tools such as Prometheus, Grafana, and JaegerExtend Kubernetes with the Kubernetes API, plugins, and webhooksWho this book is forIf you're a system administrator or cloud developer who wants to become comfortable with Kubernetes and would like to master its advanced features, then this book is for you. Sofware and DevOps engineers with a working knowledge of Kubernetes, as well as technical managers of Kubernetes-based systems, will also find this book useful. Those deciding on whether to migrate to Kubernetes and are curious about its inner workings will find plenty of answers here as well. Basic familiarity with networking concepts will prove beneficial.","","9781804614754","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251264.pdf&bkn=10251263&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Using Stable Diffusion with Python: Leverage Python to control and automate high-quality AI image generation using Stable Diffusion","A. Zhu (Shudong Zhu); M. Fisher",NA; NA,Using Stable Diffusion with Python: Leverage Python to control and automate high-quality AI image generation using Stable Diffusion,"","2024","","","","","Master AI image generation by leveraging GenAI tools and techniques such as diffusers, LoRA, textual inversion, ControlNet, and prompt design in this hands-on guide, with key images printed in colorKey FeaturesMaster the art of generating stunning AI artwork with the help of expert guidance and ready-to-run Python codeGet instant access to emerging extensions and open-source modelsLeverage the power of community-shared models and LoRA to produce high-quality images that captivate audiencesPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionStable Diffusion is a game-changing AI tool that enables you to create stunning images with code. The author, a seasoned Microsoft applied data scientist and contributor to the Hugging Face Diffusers library, leverages his 15+ years of experience to help you master Stable Diffusion by understanding the underlying concepts and techniques. You’ll be introduced to Stable Diffusion, grasp the theory behind diffusion models, set up your environment, and generate your first image using diffusers. You'll optimize performance, leverage custom models, and integrate community-shared resources like LoRAs, textual inversion, and ControlNet to enhance your creations. Covering techniques such as face restoration, image upscaling, and image restoration, you’ll focus on unlocking prompt limitations, scheduled prompt parsing, and weighted prompts to create a fully customized and industry-level Stable Diffusion app. This book also looks into real-world applications in medical imaging, remote sensing, and photo enhancement. Finally, you'll gain insights into extracting generation data, ensuring data persistence, and leveraging AI models like BLIP for image description extraction. By the end of this book, you'll be able to use Python to generate and edit images and leverage solutions to build Stable Diffusion apps for your business and users.What you will learnExplore core concepts and applications of Stable Diffusion and set up your environment for successRefine performance, manage VRAM usage, and leverage community-driven resources like LoRAs and textual inversionHarness the power of ControlNet, IP-Adapter, and other methodologies to generate images with unprecedented control and qualityExplore developments in Stable Diffusion such as video generation using AnimateDiffWrite effective prompts and leverage LLMs to automate the processDiscover how to train a Stable Diffusion LoRA from scratchWho this book is forIf you're looking to gain control over AI image generation, particularly through the diffusion model, this book is for you. Moreover, data scientists, ML engineers, researchers, and Python application developers seeking to create AI image generation applications based on the Stable Diffusion framework can benefit from the insights provided in the book. ","","9781835084311","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769371.pdf&bkn=10769370&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Automated Machine Learning on AWS: Fast-track the development of your production-ready machine learning applications the AWS way","T. Potgieter; J. Dahlberg",NA; NA,Automated Machine Learning on AWS: Fast-track the development of your production-ready machine learning applications the AWS way,"","2022","","","","","Automate the process of building, training, and deploying machine learning applications to production with AWS solutions such as SageMaker Autopilot, AutoGluon, Step Functions, Amazon Managed Workflows for Apache Airflow, and moreKey FeaturesExplore the various AWS services that make automated machine learning easierRecognize the role of DevOps and MLOps methodologies in pipeline automationGet acquainted with additional AWS services such as Step Functions, MWAA, and more to overcome automation challengesBook DescriptionAWS provides a wide range of solutions to help automate a machine learning workflow with just a few lines of code. With this practical book, you'll learn how to automate a machine learning pipeline using the various AWS services. Automated Machine Learning on AWS begins with a quick overview of what the machine learning pipeline/process looks like and highlights the typical challenges that you may face when building a pipeline. Throughout the book, you'll become well versed with various AWS solutions such as Amazon SageMaker Autopilot, AutoGluon, and AWS Step Functions to automate an end-to-end ML process with the help of hands-on examples. The book will show you how to build, monitor, and execute a CI/CD pipeline for the ML process and how the various CI/CD services within AWS can be applied to a use case with the Cloud Development Kit (CDK). You'll understand what a data-centric ML process is by working with the Amazon Managed Services for Apache Airflow and then build a managed Airflow environment. You'll also cover the key success criteria for an MLSDLC implementation and the process of creating a self-mutating CI/CD pipeline using AWS CDK from the perspective of the platform engineering team. By the end of this AWS book, you'll be able to effectively automate a complete machine learning pipeline and deploy it to production.What you will learnEmploy SageMaker Autopilot and Amazon SageMaker SDK to automate the machine learning processUnderstand how to use AutoGluon to automate complicated model building tasksUse the AWS CDK to codify the machine learning processCreate, deploy, and rebuild a CI/CD pipeline on AWSBuild an ML workflow using AWS Step Functions and the Data Science SDKLeverage the Amazon SageMaker Feature Store to automate the machine learning software development life cycle (MLSDLC)Discover how to use Amazon MWAA for a data-centric ML processWho this book is forThis book is for the novice as well as experienced machine learning practitioners looking to automate the process of building, training, and deploying machine learning-based solutions into production, using both purpose-built and other AWS services. A basic understanding of the end-to-end machine learning process and concepts, Python programming, and AWS is necessary to make the most out of this book.","","9781801814522","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162977.pdf&bkn=10162976&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Causal Inference in R: Decipher complex relationships with advanced R techniques for data-driven decision-making","S. Das",NA,Causal Inference in R: Decipher complex relationships with advanced R techniques for data-driven decision-making,"","2024","","","","","Master the fundamentals to advanced techniques of causal inference through a practical, hands-on approach with extensive R code examples and real-world applicationsKey FeaturesExplore causal analysis with hands-on R tutorials and real-world examplesGrasp complex statistical methods by taking a detailed, easy-to-follow approachEquip yourself with actionable insights and strategies for making data-driven decisionsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionDetermining causality in data is difficult due to confounding factors. Written by an applied scientist specializing in causal inference with over a decade of experience, Causal Inference in R provides the tools and methods you need to accurately establish causal relationships, improving data-driven decision-making. This book helps you get to grips with foundational concepts, offering a clear understanding of causal models and their relevance in data analysis. You’ll progress through chapters that blend theory with hands-on examples, illustrating how to apply advanced statistical methods to real-world scenarios. You’ll discover techniques for establishing causality, from classic approaches to contemporary methods, such as propensity score matching and instrumental variables. Each chapter is enriched with detailed case studies and R code snippets, enabling you to implement concepts immediately. Beyond technical skills, this book also emphasizes critical thinking in data analysis to empower you to make informed, data-driven decisions. The chapters enable you to harness the power of causal inference in R to uncover deeper insights from data. By the end of this book, you’ll be able to confidently establish causal relationships and make data-driven decisions with precision.What you will learnGet a solid understanding of the fundamental concepts and applications of causal inferenceUtilize R to construct and interpret causal modelsApply techniques for robust causal analysis in real-world dataImplement advanced causal inference methods, such as instrumental variables and propensity score matchingDevelop the ability to apply graphical models for causal analysisIdentify and address common challenges and pitfalls in controlled experiments for effective causal analysisBecome proficient in the practical application of doubly robust estimation using RWho this book is forThis book is for data practitioners, statisticians, and researchers keen on enhancing their skills in causal inference using R, as well as individuals who aspire to make data-driven decisions in complex scenarios. It serves as a valuable resource for both beginners and experienced professionals in data analysis, public policy, economics, and social sciences. Academics and students looking to deepen their understanding of causal models and their practical implementation will also find it highly beneficial.","","9781803238166","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10803941.pdf&bkn=10803940&pdfType=book","","","","","","","","16 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"Natural Language Processing with AWS AI Services: Derive strategic insights from unstructured data with Amazon Textract and Amazon Comprehend","M. M; P. Rangarajan; J. Simon",NA; NA; NA,Natural Language Processing with AWS AI Services: Derive strategic insights from unstructured data with Amazon Textract and Amazon Comprehend,"","2021","","","","","Work through interesting real-life business use cases to uncover valuable insights from unstructured text using AWS AI servicesKey FeaturesGet to grips with AWS AI services for NLP and find out how to use them to gain strategic insightsRun Python code to use Amazon Textract and Amazon Comprehend to accelerate business outcomesUnderstand how you can integrate human-in-the-loop for custom NLP use cases with Amazon A2IBook DescriptionNatural language processing (NLP) uses machine learning to extract information from unstructured data. This book will help you to move quickly from business questions to high-performance models in production. To start with, you'll understand the importance of NLP in today’s business applications and learn the features of Amazon Comprehend and Amazon Textract to build NLP models using Python and Jupyter Notebooks. The book then shows you how to integrate AI in applications for accelerating business outcomes with just a few lines of code. Throughout the book, you'll cover use cases such as smart text search, setting up compliance and controls when processing confidential documents, real-time text analytics, and much more to understand various NLP scenarios. You'll deploy and monitor scalable NLP models in production for real-time and batch requirements. As you advance, you'll explore strategies for including humans in the loop for different purposes in a document processing workflow. Moreover, you'll learn best practices for auto-scaling your NLP inference for enterprise traffic. Whether you're new to ML or an experienced practitioner, by the end of this NLP book, you'll have the confidence to use AWS AI services to build powerful NLP applications.What you will learnAutomate various NLP workflows on AWS to accelerate business outcomesUse Amazon Textract for text, tables, and handwriting recognition from images and PDF filesGain insights from unstructured text in the form of sentiment analysis, topic modeling, and more using Amazon ComprehendSet up end-to-end document processing pipelines to understand the role of humans in the loopDevelop NLP-based intelligent search solutions with just a few lines of codeCreate both real-time and batch document processing pipelines using PythonWho this book is forIf you're an NLP developer or data scientist looking to get started with AWS AI services to implement various NLP scenarios quickly, this book is for you. It will show you how easy it is to integrate AI in applications with just a few lines of code. A basic understanding of machine learning (ML) concepts is necessary to understand the concepts covered. Experience with Jupyter notebooks and Python will be helpful.","","9781801815482","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162588.pdf&bkn=10162587&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Transformers for Natural Language Processing: Build, train, and fine-tune deep neural network architectures for NLP with Python, Hugging Face, and OpenAI's GPT-3, ChatGPT, and GPT-4","D. Rothman; A. Gulli",NA; NA,"Transformers for Natural Language Processing: Build, train, and fine-tune deep neural network architectures for NLP with Python, Hugging Face, and OpenAI's GPT-3, ChatGPT, and GPT-4","","2022","","","","","OpenAI’s GPT-3, ChatGPT, GPT-4 and Hugging Face transformers for language tasks in one book. Get a taste of the future of transformers, including computer vision tasks and code writing and assistance. Purchase of the print or Kindle book includes a free eBook in PDF formatKey FeaturesImprove your productivity with OpenAI’s ChatGPT and GPT-4 from prompt engineering to creating and analyzing machine learning modelsPretrain a BERT-based model from scratch using Hugging FaceFine-tune powerful transformer models, including OpenAI's GPT-3, to learn the logic of your dataBook DescriptionTransformers are...well...transforming the world of AI. There are many platforms and models out there, but which ones best suit your needs? Transformers for Natural Language Processing, 2nd Edition, guides you through the world of transformers, highlighting the strengths of different models and platforms, while teaching you the problem-solving skills you need to tackle model weaknesses. You'll use Hugging Face to pretrain a RoBERTa model from scratch, from building the dataset to defining the data collator to training the model. If you're looking to fine-tune a pretrained model, including GPT-3, then Transformers for Natural Language Processing, 2nd Edition, shows you how with step-by-step guides. The book investigates machine translations, speech-to-text, text-to-speech, question-answering, and many more NLP tasks. It provides techniques to solve hard language problems and may even help with fake news anxiety (read chapter 13 for more details). You'll see how cutting-edge platforms, such as OpenAI, have taken transformers beyond language into computer vision tasks and code creation using DALL-E 2, ChatGPT, and GPT-4. By the end of this book, you'll know how transformers work and how to implement them and resolve issues like an AI detective.What you will learnDiscover new techniques to investigate complex language problemsCompare and contrast the results of GPT-3 against T5, GPT-2, and BERT-based transformersCarry out sentiment analysis, text summarization, casual speech analysis, machine translations, and more using TensorFlow, PyTorch, and GPT-3Find out how ViT and CLIP label images (including blurry ones!) and create images from a sentence using DALL-ELearn the mechanics of advanced prompt engineering for ChatGPT and GPT-4Who this book is forIf you want to learn about and apply transformers to your natural language (and image) data, this book is for you. You'll need a good understanding of Python and deep learning and a basic understanding of NLP to benefit most from this book. Many platforms covered in this book provide interactive user interfaces, which allow readers with a general interest in NLP and AI to follow several chapters. And don't worry if you get stuck or have questions; this book gives you direct access to our AI/ML community to help guide you on your transformers journey!","","9781803243481","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162341.pdf&bkn=10162340&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Learn Amazon SageMaker: A guide to building, training, and deploying machine learning models for developers and data scientists","J. Simon",NA,"Learn Amazon SageMaker: A guide to building, training, and deploying machine learning models for developers and data scientists","","2021","","","","","Swiftly build and deploy machine learning models without managing infrastructure and boost productivity using the latest Amazon SageMaker capabilities such as Studio, Autopilot, Data Wrangler, Pipelines, and Feature StoreKey FeaturesBuild, train, and deploy machine learning models quickly using Amazon SageMakerOptimize the accuracy, cost, and fairness of your modelsCreate and automate end-to-end machine learning workflows on Amazon Web Services (AWS)Book DescriptionAmazon SageMaker enables you to quickly build, train, and deploy machine learning models at scale without managing any infrastructure. It helps you focus on the machine learning problem at hand and deploy high-quality models by eliminating the heavy lifting typically involved in each step of the ML process. This second edition will help data scientists and ML developers to explore new features such as SageMaker Data Wrangler, Pipelines, Clarify, Feature Store, and much more. You'll start by learning how to use various capabilities of SageMaker as a single toolset to solve ML challenges and progress to cover features such as AutoML, built-in algorithms and frameworks, and writing your own code and algorithms to build ML models. The book will then show you how to integrate Amazon SageMaker with popular deep learning libraries, such as TensorFlow and PyTorch, to extend the capabilities of existing models. You'll also see how automating your workflows can help you get to production faster with minimum effort and at a lower cost. Finally, you'll explore SageMaker Debugger and SageMaker Model Monitor to detect quality issues in training and production. By the end of this Amazon book, you'll be able to use Amazon SageMaker on the full spectrum of ML workflows, from experimentation, training, and monitoring to scaling, deployment, and automation.What you will learnBecome well-versed with data annotation and preparation techniquesUse AutoML features to build and train machine learning models with AutoPilotCreate models using built-in algorithms and frameworks and your own codeTrain computer vision and natural language processing (NLP) models using real-world examplesCover training techniques for scaling, model optimization, model debugging, and cost optimizationAutomate deployment tasks in a variety of configurations using SDK and several automation toolsWho this book is forThis book is for software engineers, machine learning developers, data scientists, and AWS users who are new to using Amazon SageMaker and want to build high-quality machine learning models without worrying about infrastructure. Knowledge of AWS basics is required to grasp the concepts covered in this book more effectively. A solid understanding of machine learning concepts and the Python programming language will also be beneficial.","","9781801814157","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162805.pdf&bkn=10162804&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Data Engineering with AWS Cookbook: A recipe-based approach to help you tackle data engineering problems with AWS services","T. N. Phạm; G. H. González; V. Khan; H. Nofal",NA; NA; NA; NA,Data Engineering with AWS Cookbook: A recipe-based approach to help you tackle data engineering problems with AWS services,"","2024","","","","","Master AWS data engineering services and techniques for orchestrating pipelines, building layers, and managing migrationsKey FeaturesGet up to speed with the different AWS technologies for data engineeringLearn the different aspects and considerations of building data lakes, such as security, storage, and operationsGet hands on with key AWS services such as Glue, EMR, Redshift, QuickSight, and Athena for practical learningPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionPerforming data engineering with Amazon Web Services (AWS) combines AWS's scalable infrastructure with robust data processing tools, enabling efficient data pipelines and analytics workflows. This comprehensive guide to AWS data engineering will teach you all you need to know about data lake management, pipeline orchestration, and serving layer construction. Through clear explanations and hands-on exercises, you’ll master essential AWS services such as Glue, EMR, Redshift, QuickSight, and Athena. Additionally, you’ll explore various data platform topics such as data governance, data quality, DevOps, CI/CD, planning and performing data migration, and creating Infrastructure as Code. As you progress, you will gain insights into how to enrich your platform and use various AWS cloud services such as AWS EventBridge, AWS DataZone, and AWS SCT and DMS to solve data platform challenges. Each recipe in this book is tailored to a daily challenge that a data engineer team faces while building a cloud platform. By the end of this book, you will be well-versed in AWS data engineering and have gained proficiency in key AWS services and data processing techniques. You will develop the necessary skills to tackle large-scale data challenges with confidence.What you will learnDefine your centralized data lake solution, and secure and operate it at scaleIdentify the most suitable AWS solution for your specific needsBuild data pipelines using multiple ETL technologiesDiscover how to handle data orchestration and governanceExplore how to build a high-performing data serving layerDelve into DevOps and data quality best practicesMigrate your data from on-premises to AWSWho this book is forIf you're involved in designing, building, or overseeing data solutions on AWS, this book provides proven strategies for addressing challenges in large-scale data environments. Data engineers as well as big data professionals looking to enhance their understanding of AWS features for optimizing their workflow, even if they're new to the platform, will find value. Basic familiarity with AWS security (users and roles) and command shell is recommended. ","","9781805126850","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10818434.pdf&bkn=10818433&pdfType=book","","","","","","","","30 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"High Performance with Java: Discover strategies and best practices to develop high performance Java applications","D. E. L. Jr.",NA,High Performance with Java: Discover strategies and best practices to develop high performance Java applications,"","2024","","","","","Take your Java programming skills to the next level and learn to overcome real-world challenges to optimize application performanceKey FeaturesLeverage key features of the Java Virtual Machine to maximize runtime performanceUnlock optimization strategies to effectively manage objects and memory useApply your knowledge to utilize frameworks and libraries ripe for increasing program performancePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionBuilding high performance into your applications is key to creating an optimal user experience, although it is not the only consideration for the performant nature of your apps. High performance applications can lead to cost-effective resource utilization, especially when scalability and cloud computing are involved. They can also provide highly reliable systems that are easier to maintain. High Performance with Java begins by helping you explore the Java Virtual Machine (JVM) and understand how to push it to its limits to further optimize your programs. You’ll take a hands-on approach to go through memory optimization strategies, input/output operations, concurrency, networking, as well as frameworks and libraries focused on performance. You’ll also learn key strategies and best practices by using industry-relevant examples to architect scalable and resource-efficient applications. The concluding chapters provide valuable insights on optimizing your Java code when interacting with databases and show you how to leverage artificial intelligence (AI) for high performance Java applications. By the end of this book, you’ll grasp the importance of developing high performance Java applications and gain practical experience in implementing key strategies to help ensure your Java applications perform optimally.What you will learnExplore optimization strategies for garbage collection and the JIT compilerDiscover best practices when using data structures for high performanceTest and compare various approaches to using loopsUnderstand how and when to use object poolingDiscern the difference between low-performance and high-performance algorithmsUncover strategies for object creation and immutability to improve performanceGain hands-on experience in avoiding memory leaksWho this book is forThis book is for developers, software engineers, and software architects looking to take their Java skills to the next level. Aimed at individuals with a string grasp of Java fundamentals, this book is a practical guide to helping you write high-performing applications.","","9781835462553","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769381.pdf&bkn=10769380&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"VLSI Architecture Design for Compact Shortcut Denoising Autoencoder Neural Network of ECG Signal","S. -C. Lai; S. -T. Wang; S. M. S. Morsalin; J. -H. Lin; S. -C. Hsia; C. -Y. Chang; M. -H. Sheu","Department of Automation Engineering and the Smart Machinery and Intelligent Manufacturing Research Center, National Formosa University, Yunlin County, Huwei, Taiwan; Program in Smart Industry Technology Research and Development, National Formosa University, Huwei, Taiwan; Department of Electronic Engineering, National Yunlin University of Science and Technology, Yunlin County, Douliu, Taiwan; Department of Electronic Engineering, National Yunlin University of Science and Technology, Yunlin County, Douliu, Taiwan; Department of Electronic Engineering, National Yunlin University of Science and Technology, Yunlin County, Douliu, Taiwan; Department of Electronic Engineering, National Yunlin University of Science and Technology, Yunlin County, Douliu, Taiwan; Department of Electronic Engineering, National Yunlin University of Science and Technology, Yunlin County, Douliu, Taiwan",IEEE Transactions on Circuits and Systems I: Regular Papers,"","2025","PP","99","1","13","The Electrocardiogram (ECG) test detects and records cardiac-related electrical activity of the heart. The ECG test identifies and documents cardiac-related electrical activity in the heart. The use of ECG signals for cardiovascular disease nursing as a crucial component of preoperative evaluation is increasing. ECG signals need to denoise and display in a clear waveform due to the numerous noises. We have introduced Compact Shortcut Denoising Auto-encoder (CS-DAE) neural network, which reduces the noise from ECG signals. The Compact Shortcut approach compresses the features passed through the shortcut layers, which lowers the operation’s memory needs and improves the noise reduction impact. In addition, the encoder and decoder process the Pixel-Unshuffled and Pixel-Shuffled, which effectively mitigates the feature loss caused by down-sampling and up-sampling operations. As a result, the CS-DAE algorithm decreases the computation and required memory size while maintaining higher accuracy. We have used MITDB and NSTDB datasets for training and testing the proposed CS-DAE model, resulting in the average Percentage of Root Mean Square Difference (PRD) being 46.30% and the improvement of Signal-to-Noise Ratio (SNRimp) being 10.50. In addition, we have designed VLSI architect ure for the proposed CS-DAE neural network to accelerate low hardware cost and less computation. The TUL PYNQTM-Z2 development platform runs the Verilog code, which is used for VLSI architecture and has the lowest power consumption of 1.65W.","1558-0806","","10.1109/TCSI.2025.3533544","National Science and Technology Council, Taiwan(grant numbers:NSTC 113-2221-E-150-002); National Formosa University, Yunlin, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10857698","Electrocardiogram;compact shortcut;denoising autoencoder;neural network;ECG signals;shortcut layers;pixel-unshuffled and pixel-shuffled;VLSI architecture;hardware design","Electrocardiography;Noise reduction;Decoding;Noise;Neural networks;Convolution;Very large scale integration;Heart;Hardware;Signal to noise ratio","","","","","IEEE","29 Jan 2025","","","IEEE","IEEE Early Access Articles"
"Python Scripting in Blender: Extend the power of Blender using Python to create objects, animations, and effective add-ons","P. Acampora",NA,"Python Scripting in Blender: Extend the power of Blender using Python to create objects, animations, and effective add-ons","","2023","","","","","Learn how to use Python scripts in Blender 3.3 to automate tasks, optimize your workflow, think like a 3D programmer, and start creating your tools quickly Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesDiscover ready-to-go scripts that provide a clear solution to your problemsFind out how to automate repetitive tasks in an efficient wayExtend Blender’s actions and user interface with your codeBook DescriptionBlender, a powerful open source 3D software, can be extended and powered up using the Python programming language. This book teaches you how to automate laborious operations using scripts, and expand the set of available commands, graphic interfaces, tools, and event responses, which will enable you to add custom features to meet your needs and bring your creative ideas to life. The book begins by covering essential Python concepts and showing you how to create a basic add-on. You’ll then gain a solid understanding of the entities that affect the look of Blender’s objects such as modifiers, constraints, and materials. As you advance, you’ll get to grips with the animation system in Blender and learn how to set up its behavior using Python. The examples, tools, patterns, and best practices present throughout the book will familiarize you with the Python API and build your knowledge base, along with enabling you to produce valuable code that empowers the users and is ready for publishing or production. By the end of this book, you’ll be able to successfully design add-ons that integrate seamlessly with the software and its ecosystem.What you will learnUnderstand the principles of 3D and programming, and learn how they operate in BlenderBuild engaging and navigation-friendly user interfaces that integrate with the native look and feelRespect coding guidelines and deliver readable and compliant code without the loss of originalityPackage your extensions into a complete add-on, ready for installation and distributionCreate interactive tools with a direct response to the user’s actionCode comfortably and safely using version controlWho this book is forThis book is for Blender users who want to expand their skills and learn scripting, technical directors looking to automate laborious tasks, and professionals and hobbyists who want to learn more about the Python architecture underlying the Blender interface. Prior experience with Blender is a prerequisite, along with a basic understanding of the Python syntax—however, the book does provide quick explanations to bridge potential gaps in your background knowledge.","","9781803243276","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251170.pdf&bkn=10251169&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Decoding Musical Neural Activity in Patients With Disorders of Consciousness Through Self-Supervised Contrastive Domain Generalization","H. Cai; J. Pan; Q. Xiao; J. Jin; Y. Li; Q. Xie","School of Software, South China Normal University, Foshan, China; School of Software, South China Normal University, Foshan, China; Zhujiang Hospital, Southern Medical University, Guangzhou, China; School of Software, South China Normal University, Foshan, China; Center for Brain-Computer Interfaces and Brain Information Processing, South China University of Technology, Guangzhou, China; Zhujiang Hospital, Southern Medical University, Guangzhou, China",IEEE Transactions on Affective Computing,"","2024","PP","99","1","18","Identifying the brain responses of patients with disorders of consciousness (DOCs), which include comas, vegetative states (VSs, also called unresponsive wakefulness syndrome (UWS)) and minimally conscious states (MCSs), based on electroencephalography (EEG) has important clinical diagnosis implications. However, due to impaired motor and cognitive abilities, patients with DOCs may not be able to express their feelings and their brain responses to different stimuli, making it difficult to correctly label data. EEG classification algorithms trained with these data cannot make reliable classifications and predictions for clinical diagnosis purposes. To identify the brain responses produced for different types of stimuli in patients with DOCs, we proposed a self-supervised contrastive domain generalization framework (SSCDG) for cross-subject EEG classification. The model was first trained with healthy-subject EEG data induced by different stimuli to learn their corresponding unsupervised representations. Then, we used these representations to train a classifier to predict the emotional states of patients with DOCs under the corresponding stimuli. SSCDG was first evaluated on the SEED dataset, and it achieved an accuracy of 87.6%, which was 1.1% higher than that of the state-of-the-art (SOTA) approaches. Moreover, the SSCDG method was utilized to categorize EEG data acquired from seventeen DOC patients, including eleven in a UWS state and six in an MCS state, with seven patients demonstrating notable accuracy in three-class EEG classification tasks. The SSCDG results indicated that the seven patients with DOCs may have shown classifiable EEG responses to the presented stimuli.","1949-3045","","10.1109/TAFFC.2024.3462603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10681490","Cross-subject EEG classification;disorder of consciousness;domain generalization;self-supervised contrastive learning","Electroencephalography;Brain modeling;Data models;Motors;Transfer learning;Training;Music","","1","","","IEEE","17 Sep 2024","","","IEEE","IEEE Early Access Articles"
"Stable Audio Open","Z. Evans; J. D. Parker; C. Carr; Z. Zukowski; J. Taylor; J. Pons",Stability AI; Stability AI; Stability AI; Stability AI; Stability AI; Stability AI,"ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Open generative models are vitally important for the community, allowing for fine-tunes and serving as baselines when presenting new models. However, most current text-to-audio models are private and not accessible for artists and researchers to build upon. Here we describe the architecture and training process of a new open-weights text-to-audio model trained with Creative Commons data. Our evaluation shows that the model’s performance is competitive with the state-of-the-art across various metrics. Notably, the reported FDopenl3 results (measuring the realism of the generations) showcase its potential for high-quality stereo sound synthesis at 44.1kHz.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10888461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10888461","Stable Audio Open;Latent Diffusion;Audio","Training;Measurement;Autoencoders;Data transparency;Signal processing;Data models;Acoustics;Speech processing","","","","33","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"The rapid rise of AI art: Has humanity unwittingly entered a radical new era of art and artistic expression? That's the suggestion being circulated in creative communities and online forums as a new breed of powerful artificial intelligence emerges from the shadows","S. Cousins",NA,Engineering & Technology,"2 Apr 2024","2023","18","2","20","25","GENERATIVE AI art has exploded onto the scene over the past few months through advanced online platforms like DALL-E2, Midjourney and Stable Diffusion, which enable anyone with access to a smartphone or PC to create highly polished art by typing in simple text instructions.","1750-9637","","10.1049/et.2023.0208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489870","","","","","","","","2 Apr 2024","","","IET","IET Magazines"
"Navigating the Web of Disinformation and Misinformation: Large Language Models as Double-Edged Swords","S. B. Shah; S. Thapa; A. Acharya; K. Rauniyar; S. Poudel; S. Jain; A. Masood; U. Naseem","Department of Computer Science and Engineering, Delhi Technological University, New Delhi, India; Virginia Tech, Blacksburg, VA, USA; Herald College Kathmandu, Sanogaucharan, Naxal, Bhagwatibahal, Nepal; Department of Computer Science and Engineering, Delhi Technological University, New Delhi, India; Kathmandu Engineering College, Tribhuvan University, Kathmandu, Nepal; Virginia Tech, Blacksburg, VA, USA; Department of Circulation and Medical Imaging, Norwegian University of Science and Technology, Trondheim, Norway; School of Computing, Macquarie University, Sydney, NSW, Australia",IEEE Access,"","2024","PP","99","1","1","This paper explores the dual role of Large Language Models (LLMs) in the context of online misinformation and disinformation. In today’s digital landscape, where the internet and social media facilitate the rapid dissemination of information, discerning between accurate content and falsified information presents a formidable challenge. Misinformation, often arising unintentionally, and disinformation, crafted deliberately, are at the forefront of this challenge. LLMs such as OpenAI’s GPT-4, equipped with advanced language generation abilities, present a double-edged sword in this scenario. While they hold promise in combating misinformation by fact-checking and detecting LLM-generated text, their ability to generate realistic, contextually relevant text also poses risks for creating and propagating misinformation. Further, LLMs are plagued with many problems such as biases, knowledge cutoffs, and hallucinations, which may further perpetuate misinformation and disinformation. The paper outlines historical developments in misinformation detection and how it affects social media consumption, especially among youth, and introduces LLMs and their applications in various domains. It then critically analyzes the potential of LLMs to generate and counter misinformation and disinformation in sensitive topics such as healthcare, COVID-19, and political agendas. Further, it discusses mitigation strategies, ethical considerations, and regulatory measures, summarizing previous methods and proposing future research direction toward leveraging the benefits of LLMs while minimizing misuse risks. The paper concludes by acknowledging LLMs as powerful tools with significant implications in both spreading and combating misinformation in the digital age.","2169-3536","","10.1109/ACCESS.2024.3406644","Norges Teknisk-Naturvitenskapelige Universitet; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10540581","Large Language Models;Disinformation;Computational Social Sciences;ChatGPT;Hallucinations in LLMs","Fake news;Information integrity;Social networking (online);Navigation;Market research;Feature extraction;Neural networks;Large language models;Social sciences","","4","","","CCBY","29 May 2024","","","IEEE","IEEE Early Access Articles"
"Eleven Years of Gender Data Visualization: A Step Towards More Inclusive Gender Representation","F. Cabric; M. V. Bjarnadóttir; M. Ling; G. L. Rafnsdóttir; P. Isenberg","Université Paris-Saclay, CNRS, Inria, LISN, France; Robert H. Smith School of Business, University of Maryland, College Park, USA; Ohio State University, USA; Faculty of Social and Human Science, University of Iceland, Reykjavik, Iceland; Université Paris-Saclay, CNRS, Inria, LISN, France",IEEE Transactions on Visualization and Computer Graphics,"25 Dec 2023","2024","30","1","316","326","We present an analysis of the representation of gender as a data dimension in data visualizations and propose a set of considerations around visual variables and annotations for gender-related data. Gender is a common demographic dimension of data collected from study or survey participants, passengers, or customers, as well as across academic studies, especially in certain disciplines like sociology. Our work contributes to multiple ongoing discussions on the ethical implications of data visualizations. By choosing specific data, visual variables, and text labels, visualization designers may, inadvertently or not, perpetuate stereotypes and biases. Here, our goal is to start an evolving discussion on how to represent data on gender in data visualizations and raise awareness of the subtleties of choosing visual variables and words in gender visualizations. In order to ground this discussion, we collected and coded gender visualizations and their captions from five different scientific communities (Biology, Politics, Social Studies, Visualisation, and Human-Computer Interaction), in addition to images from Tableau Public and the Information Is Beautiful awards showcase. Overall we found that representation types are community-specific, color hue is the dominant visual channel for gender data, and nonconforming gender is under-represented. We end our paper with a discussion of considerations for gender visualization derived from our coding and the literature and recommendations for large data collection bodies. A free copy of this paper and all supplemental materials are available at https://osf.io/v9ams/.","1941-0506","","10.1109/TVCG.2023.3327369","Inria; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304299","Visualization;gender;visual gender representation;ethics","Data visualization;Visualization;Surveys;Image color analysis;Behavioral sciences;Ethics;Biology","Humans;Data Visualization;Computer Graphics;Surveys and Questionnaires","1","","84","IEEE","1 Nov 2023","","","IEEE","IEEE Journals"
"Concealed Object Detection","D. -P. Fan; G. -P. Ji; M. -M. Cheng; L. Shao","College of Computer Science, Nankai University, Tianjin, China; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE; Department of Computer Science, Nankai University, Tianjin, China; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE",IEEE Transactions on Pattern Analysis and Machine Intelligence,"14 Sep 2022","2022","44","10","6024","6042","We present the first systematic study on concealed object detection (COD), which aims to identify objects that are visually embedded in their background. The high intrinsic similarities between the concealed objects and their background make COD far more challenging than traditional object detection/segmentation. To better understand this task, we collect a large-scale dataset, called COD10K, which consists of 10,000 images covering concealed objects in diverse real-world scenarios from 78 object categories. Further, we provide rich annotations including object categories, object boundaries, challenging attributes, object-level labels, and instance-level annotations. Our COD10K is the largest COD dataset to date, with the richest annotations, which enables comprehensive concealed object understanding and can even be used to help progress several other vision tasks, such as detection, segmentation, classification etc. Motivated by how animals hunt in the wild, we also design a simple but strong baseline for COD, termed the Search Identification Network (SINet). Without any bells and whistles, SINet outperforms twelve cutting-edge baselines on all datasets tested, making them robust, general architectures that could serve as catalysts for future research in COD. Finally, we provide some interesting findings, and highlight several potential applications and future directions. To spark research in this new field, our code, dataset, and online demo are available at our project page: http://mmcheng.net/cod.","1939-3539","","10.1109/TPAMI.2021.3085766","National Key Research and Development Program of China(grant numbers:2018AAA0100400); NSFC(grant numbers:61922046); Chinese Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444794","Concealed object detection;camouflaged object detection;COD;dataset;benchmark","Object detection;Annotations;Task analysis;Image segmentation;Benchmark testing;Animals;Art","Algorithms;Animals;Image Interpretation, Computer-Assisted","292","","110","IEEE","1 Jun 2021","","","IEEE","IEEE Journals"
"Toward Explainable Affective Computing: A Review","K. Cortiñas-Lorenzo; G. Lacey","School of Computer Science and Statistics, Trinity College Dublin, Dublin, Ireland; Department of Electronic Engineering, Maynooth University, Kildare, Ireland",IEEE Transactions on Neural Networks and Learning Systems,"7 Oct 2024","2024","35","10","13101","13121","Affective computing has an unprecedented potential to change the way humans interact with technology. While the last decades have witnessed vast progress in the field, multimodal affective computing systems are generally black box by design. As affective systems start to be deployed in real-world scenarios, such as education or healthcare, a shift of focus toward improved transparency and interpretability is needed. In this context, how do we explain the output of affective computing models? and how to do so without limiting predictive performance? In this article, we review affective computing work from an explainable AI (XAI) perspective, collecting and synthesizing relevant papers into three major XAI approaches: premodel (applied before training), in-model (applied during training), and postmodel (applied after training). We present and discuss the most fundamental challenges in the field, namely, how to relate explanations back to multimodal and time-dependent data, how to integrate context and inductive biases into explanations using mechanisms such as attention, generative modeling, or graph-based methods, and how to capture intramodal and cross-modal interactions in post hoc explanations. While explainable affective computing is still nascent, existing methods are promising, contributing not only toward improved transparency but, in many cases, surpassing state-of-the-art results. Based on these findings, we explore directions for future research and discuss the importance of data-driven XAI and explanation goals, and explainee needs definition, as well as causability or the extent to which a given method leads to human understanding.","2162-2388","","10.1109/TNNLS.2023.3270027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10130818","Affective computing;explainable AI (XAI);multimodal machine learning;review","Affective computing;Training;Data models;Computational modeling;Task analysis;Terminology;Predictive models","","7","","195","CCBY","23 May 2023","","","IEEE","IEEE Journals"
"Toward Large-Scale Test for Certifying Autonomous Driving Software in Collaborative Virtual Environment","B. Kim; E. Kang","Department of Electrical Engineering and Computer Science, DGIST, Daegu, South Korea; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA",IEEE Access,"21 Jul 2023","2023","11","","72641","72654","Virtual simulation environments are widely used to test autonomous driving software by creating highly complex driving scenarios that are non-trivial to set up in a physical environment. However, the current practice of using the virtual test still does not fully utilize its potential to build a much larger scale test. We propose a perspective and research vision to build a large-scale test architecture in which participants collaboratively construct, execute and analyze complex test scenarios at scale in the virtual world. In particular, the architectural concept is built on the existing concept of the Collaborative Virtual Environment (CVE) that has been successfully applied in other domains, such as entertainment or military training applications. The proposed domain-specific architectural requirements extend the CVE to include the following necessary properties - selective sharing and collaboration - to test autonomous driving software. In addition, the test architectural concept is explained as to how a large number of participants interact with each other collaboratively to build and execute diverse test scenarios at scale. Finally, we explain the new research directions to make this test architectural concept realized for testing autonomous driving software.","2169-3536","","10.1109/ACCESS.2023.3295500","National Research Foundation of Korea (NRF)(grant numbers:2022R1C1C1003123); Institute of Information and Communications Technology Planning and Evaluation (IITP) funded by the Korea Government Ministry of Science and ICT (MSIT)(grant numbers:2022-0-01053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10184011","Certification;collaborative virtual environment;driving scenario selection;software safety;testing autonomous driving software;test automation;test scalability","Virtual environments;Autonomous vehicles;Testing;Collaboration;Software systems;Road traffic;Virtual environments;Vehicle safety;Autonomous driving;Software testing;Automation","","2","","62","CCBYNCND","14 Jul 2023","","","IEEE","IEEE Journals"
"MetaCloak: Preventing Unauthorized Subject-Driven Text-to-Image Diffusion-Based Synthesis via Meta-Learning","Y. Liu; C. Fan; Y. Dai; X. Chen; P. Zhou; L. Sun",Lehigh University; Huazhong University of Science and Technology; Lehigh University; Samsung Research America; Huazhong University of Science and Technology; Lehigh University,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","24219","24228","Text-to-image diffusion models allow seamless generation of personalized images from scant reference photos. Yet, these tools, in the wrong hands, can fabricate misleading or harmful content, endangering individuals. To address this problem, existing poisoning-based approaches perturb user images in an imperceptible way to render them “unlearnable” from malicious uses. We identify two limitations of these defending approaches: i) sub-optimal due to the hand-crafted heuristics for solving the intractable bilevel optimization and ii) lack of robustness against simple data transformations like Gaussian filtering. To solve these challenges, we propose MetaCloak, which solves the bi-level poisoning problem with a meta-learning framework with an additional transformation sampling process to craft transferable and robust perturbation. Specifically, we employ a pool of surrogate diffusion models to craft transferable and model-agnostic perturbation. Furthermore, by incorporating an additional transformation process, we design a simple denoising-error maximization loss that is sufficient for causing transformation-robust semantic distortion and degradation in a personalized generation. Extensive experiments on the VGGFace2 and CelebA-HQ datasets show that MetaCloak outperforms existing approaches. Notably, MetaCloak can successfully fool online training services like Replicate, in a black-box manner, demonstrating the effectiveness of Meta Cloak in real-world scenarios. Our code is available at https://github.com/liuyixin-louis/MetaCloak.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.02286","National Science Foundation(grant numbers:CRII-2246067); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10658223","DeepFake Defense;Unlearnable Example;Adversarial ML;Unauthorized Exploitation;Imperceptible Perturbation","Metalearning;Training;Filtering;Perturbation methods;Semantics;Text to image;Diffusion models","","1","","59","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"AdaFlow: Learning and Utilizing Workflows for Enhanced Service Recommendation in Dynamic Environments","M. Liu; G. Wu; H. Xu; J. Wang; X. Xu; Z. Wang","Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; School of Economics and Management, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China",IEEE Transactions on Services Computing,"","2025","PP","99","1","14","Service provisioning represents a nuanced form of recommendation, offering a bundle of services (APIs) tailored to the specifics needs of an application (mashup) as defined by the developer, significantly easing development efforts. Unlike standard product recommendations, service recommendations face unique challenges, including cold-start, long-tail phenomena, constraints, dynamic environments, and workflows. While the first four issues have seen some resolution in the literature, the workflow mining and integration among services remains underexplored. In this paper, we focus on this gap by introducing AdaFlow, a model designed to understand and leverage service workflows within mashups, identifying viable service patterns for recommendations. AdaFlow employs a Graph Neural Network (GNN)-based framework, AdaptiveNN, to capture and learn service interactions. This learned workflow knowledge feeds into a dynamic GNN, enhancing service evolution representations that inform our recommendation process. Moreover, AdaFlow exhibits superior performance in managing dynamic and imbalanced scenarios. Our code is publicized on GitHub: https://github.com/HIT-ICES/AdaFlow","1939-1374","","10.1109/TSC.2025.3547219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912769","Mashup creation;Service Recommendation;Graph Neural Network;Workflow","Mashups;Computational modeling;Graph neural networks;Training;Semantics;Noise;Heavily-tailed distribution;Feature extraction;Data mining;Collaborative filtering","","","","","IEEE","5 Mar 2025","","","IEEE","IEEE Early Access Articles"
"A Novel Approach for Tweet Similarity in a Context-Aware Fake News Detection Model","J. F. R. Bezerra; A. Kozierkiewicz; M. Pietranik","Faculty of Information and Communication Technology, Wrocław University of Science and Technology, Wybrzeze Stanislawa Wyspianskiego 27, Wrocław, Poland; Faculty of Information and Communication Technology, Wrocław University of Science and Technology, Wybrzeze Stanislawa Wyspianskiego 27, Wrocław, Poland; Faculty of Information and Communication Technology, Wrocław University of Science and Technology, Wybrzeze Stanislawa Wyspianskiego 27, Wrocław, Poland",IEEE Access,"","2025","PP","99","1","1","In today’s information-driven world, identifying fake news is more critical than ever. The spread of false information threatens societal stability, eroding trust in institutions and fostering community polarization. While numerous algorithms and methods have been developed to detect fake news, their effectiveness varies depending on the context and data used. Recent advancements, particularly those leveraging transformer-based models like BERT, have significantly improved detection performance, especially in multimodal environments. Social media platforms are the primary focus of fake news research, as they serve as fertile grounds for the unchecked spread of misinformation. Their rapid dissemination capabilities and global reach make them ideal channels for propagating disinformation. Addressing these challenges requires innovative approaches and robust solutions. In response to these essential issues, this study presents a formal framework for detecting fake news using a multilayered approach. The proposed three-tiered model includes the topic, social, and context layers, providing a comprehensive approach to identifying fake news. One of the critical tasks within the topic layer involves assessing the similarity between two messages. To achieve this, we developed a novel method for evaluating the similarity of two tweets by extending a state-of-the-art model. Our approach leverages FastText vectors, cosine similarity, and word positions within sentences to improve accuracy. For verification, we utilized the STSBenchmark dataset and employed correlation values to measure our model’s quality. We conducted a rigorous 50-fold evaluation of the validation data. Ultimately, our model achieved a superior correlation value (median 0.673528) compared to another leading model, which had previously delivered the best results in the field. This paper presents one of the important elements of a comprehensive tool for detecting fake news, with some components already published in other works by the authors ([4], [5]) or planned for future development (e.g. multimodal content processing or multi-lingual similarity).","2169-3536","","10.1109/ACCESS.2025.3554540","Politechnika Wroc?awska(grant numbers:50SD/0046/24); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10938537","context;fake news;natural language processing;tweet similarity","","","","","","CCBY","25 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Improving Local fidelity and Interpretability of LIME by Replacing Only the Sampling Process with CVAE","D. Yasui; H. Sato","Department of Computer Science, National Defense Academy, 1-10-20 Hashirimizu, Yokosuka, Kanagawa, Japan; Department of Computer Science, National Defense Academy, 1-10-20 Hashirimizu, Yokosuka, Kanagawa, Japan",IEEE Access,"","2025","PP","99","1","1","Knowing the basis of decisions is essential for using Machine Learning (ML) in various regions, such as medical diagnosis, finance, and organizational decision-making. The Local Interpretable Model agnostic Explanation (LIME) algorithm is an Explainable AI (XAI) method that can be applied to many black-box models. However, there is a problem in that the local fidelity of the interpretable model decreases. Therefore, local fidelity must be increased, and the more interpretability of the local surrogate model, the better. Moreover, the fewer the changes, the easier it is to apply them to other studies and the more significant the contribution to the XAI area. Therefore, there are studies that improve fidelity by changing only the sampling. However, no studies have been identified that improve local fidelity and interpretability by modifying only the sampling. To this end, we use a conditional variational autoencoder (CVAE) to modify the sampling of LIME in the classification task to improve local fidelity and interpretability. We conducted a study to improve local fidelity and interpretability by modifying only the sampling part and clarifying the mechanism behind this improvement. Results show that the proposed method improves local fidelity while increasing interpretability compared to existing methods and the factors contributing to this improvement. We also conducted experiments on stability, stability against noise, and execution time, and confirmed that the proposed method performs well, although not as well as methods specific to these issues. Our code is publicly released on GitHub (https://github.com/YasuiDaisuke/CVAE-LIME.git) under the Apache 2.0 License.","2169-3536","","10.1109/ACCESS.2025.3553505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10937093","LIME;Local fidelity;Interpretability;Conditional variational autoencoder","Closed box;Autoencoders;Training data;Predictive models;Linear regression;Hands;Adaptation models;Training;Stability criteria;Neural networks","","","","","CCBYNCND","21 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Audio-Language Datasets of Scenes and Events: A Survey","G. Wijngaard; E. Formisano; M. Esposito; M. Dumontier","Department of Advanced Computing Sciences, Faculty of Science and Engineering, Maastricht University, Maastricht, The Netherlands; Department of Cognitive Neuroscience, Faculty of Psychology and Neuroscience, Maastricht University, Maastricht, The Netherlands; Department of Cognitive Neuroscience, Faculty of Psychology and Neuroscience, Maastricht University, Maastricht, The Netherlands; Department of Advanced Computing Sciences, Faculty of Science and Engineering, Maastricht University, Maastricht, The Netherlands",IEEE Access,"31 Jan 2025","2025","13","","20328","20360","Audio-language models (ALMs) generate linguistic descriptions of sound-producing events and scenes. Advances in dataset creation and computational power have led to significant progress in this domain. This paper surveys 69 datasets used to train ALMs, covering research up to September 2024 (https://github.com/GLJS/audio-datasets). The survey provides a comprehensive analysis of dataset origins, audio and linguistic characteristics, and use cases. Key sources include YouTube-based datasets such as AudioSet, with over two million samples, and community platforms such as Freesound, with over one million samples. The survey evaluates acoustic and linguistic variability across datasets through principal component analysis of audio and text embeddings. The survey also analyzes data leakage through CLAP embeddings, and examines sound category distributions to identify imbalances. Finally, the survey identifies key challenges in developing large, diverse datasets to enhance ALM performance, including dataset overlap, biases, accessibility barriers, and the predominance of English-language content, while highlighting specific areas requiring attention: multilingual dataset development, specialized domain coverage and improved dataset accessibility.","2169-3536","","10.1109/ACCESS.2025.3534621","Dutch Research Council (NWO 406.20.GO.030 to Prof. Elia Formisano), the Dutch national e-infrastructure with the support of the SURF Cooperative(grant numbers:EINF-6190); Data Science Research Infrastructure (DSRI; Maastricht University) and the Dutch Province of Limburg; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10854210","Audio-to-language learning;language-to-audio learning;audio-language datasets;review","Surveys;Data models;Training;Gold;Decoding;Contrastive learning;Transformers;Source separation;Web sites;Video on demand","","","","268","CCBY","27 Jan 2025","","","IEEE","IEEE Journals"
"Hunt Camouflaged Objects via Revealing Mutation Regions","X. Zhang; J. Zhou; L. Yan; S. Zhong; X. Zou","National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; Wangxuan Institute of Computer Technology, Peking University, Beijing, China; National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China",IEEE Transactions on Information Forensics and Security,"13 Feb 2025","2025","20","","1836","1851","Due to the high similarity between hidden objects and the surrounding background, camouflaged object detection (COD) remains a challenge. While many recently proposed methods have shown remarkable performance, most of them begin object perception by indiscriminately considering every pixel of the image. However, these early-stage region-insensitive perception methods still struggle to resist background interference, potentially missing subtle pixel changes by not prioritizing potential camouflaged areas initially. Fortunately, we reveal that the availability of an accurate mutation map can significantly enhance camouflaged discrimination ability. To this end, we propose MRNet (Mutation Region Network). MRNet initially generates a mutation map that identifies potential mutation regions exhibiting subtle pixel changes. The generation method involves amplifying and differing pixel changes based on the position and corresponding values of pixels. Subsequently, the selective expansion search operation utilizes the mutation map to extract the mapped graph, effectively reducing interference from background pixels that are distant from the mutation regions. Finally, decoding the mapped graph generates precise masks. Furthermore, we have created the largest test dataset with known categories to advance community research. Extensive experiments conducted on three widely used datasets and our proposed dataset show that MRNet surpasses other methods with superior performance. Source code is publicly available at https://github.com/XinyueZhangHust/MRNet","1556-6021","","10.1109/TIFS.2025.3530703","National Natural Science Foundation of China (NSFC)(grant numbers:U24B20139,62176100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10843373","Camouflaged object detection;mutation map;selective expansion search operation","Feature extraction;Image edge detection;Transformers;Search problems;Interference;Image segmentation;Data mining;Convolution;Training;Object recognition","","","","55","IEEE","16 Jan 2025","","","IEEE","IEEE Journals"
"MultiTec: A Data-Driven Multimodal Short Video Detection Framework for Healthcare Misinformation on TikTok","L. Shang; Y. Zhang; Y. Deng; D. Wang","Department of Computer Science, Loyola Marymount University, Los Angeles, USA; School of Information Sciences, University of Illinois at Urbana-Champaign, Champaign, USA; School of Information Sciences, University of Illinois at Urbana-Champaign, Champaign, USA; School of Information Sciences, University of Illinois at Urbana-Champaign, Champaign, USA",IEEE Transactions on Big Data,"","2025","PP","99","1","18","With the prevalence of social media and short video sharing platforms (e.g., TikTok, YouTube Shorts), the proliferation of healthcare misinformation has become a widespread and concerning issue that threatens public health and undermines trust in mass media. This paper focuses on an important problem of detecting multimodal healthcare misinformation in short videos on TikTok. Our objective is to accurately identify misleading healthcare information that is jointly conveyed by the visual, audio, and textual content within the TikTok short videos. Three critical challenges exist in solving our problem: i) how to effectively extract information from distractive and manipulated visual content in short videos? ii) How to efficiently identify the interrelation of the heterogeneous visual and speech content in short videos? iii) How to accurately capture the complex dependency of the densely connected sequential content in short videos? To address the above challenges, we develop MultiTec, a multimodal detector that explicitly explores the audio and visual content in short videos to investigate both the sequential relation of video elements and their inter-modality dependencies to jointly detect misinformation in healthcare videos on TikTok. To the best of our knowledge, MultiTec is the first modality-aware dual-attentive short video detection model for multimodal healthcare misinformation on TikTok. We evaluate MultiTec on two real-world healthcare video datasets collected from TikTok. Evaluation results show that MultiTec achieves substantial performance gains compared to state-of-the-art baselines in accurately detecting misleading healthcare short videos.","2332-7790","","10.1109/TBDATA.2025.3533919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10854802","COVID-19;healthcare misinformation;misinformation detection;multimodal fusion;short video;TikTok","Fake news;Medical services;Web sites;Video on demand;Visualization;COVID-19;Vaccines;Semantics;Feature extraction;Social networking (online)","","","","","CCBY","27 Jan 2025","","","IEEE","IEEE Early Access Articles"
"Unsupervised Cross-Domain Deep-Fused Feature Descriptor for Efficient Image Retrieval","Q. He","School of Computer Science and Engineering, Guangxi Normal University, Guilin, China",IEEE Internet of Things Journal,"","2024","PP","99","1","1","Deep feature-based methods show advantages in image retrieval, yet their robustness and generalization warrant further investigation. Toward this end, we propose a robust unsupervised Cross-domain Deep-fused Feature Descriptor (CDFD) method for efficient image retrieval. It analyzes deep features from a frequency domain perspective, employing Discrete Wavelet Transform (DWT) and Discrete Cosine Transform (DCT) to enhance its robustness and distinguishability. Specifically, we begin with a distinct perspective and introduce DWT to identify low-frequency and high-frequency components in deep features. A multi-trait spatial fusion strategy is proposed to integrate various features and strengthen low-frequency information. It can highlight the target region in the image and suppress the background clutters by introducing a low-pass filter, thus improving the discriminating ability of the feature representation. To enhance the robustness of the features, a cross-domain convergence scheme is proposed, which enables the rational integration of spatial and frequency-domain information by utilizing DCT. In addition, a computationally simple decrement query expansion technique is introduced. Using it with CDFD can effectively improve retrieval performance. Extensive comparative experiments on several benchmark datasets demonstrate that our CDFD method is robust and effective in image retrieval and outperforms several unsupervised state-of-the-art methods. The Source code is published at https://github.com/sevenjava/CDFD.","2327-4662","","10.1109/JIOT.2024.3519175","the Innovation Project of Guangxi Graduate Education(grant numbers:YCBZ2024088); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10804638","Deep features;Discrete wavelet transform;Discrete cosine transform;Image retrieval;Cross-domain deep-fused feature descriptor","Image retrieval;Robustness;Discrete wavelet transforms;Computational modeling;Transforms;Feature extraction;Frequency-domain analysis;Vectors;Discrete cosine transforms;Computer vision","","","","","IEEE","17 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Open-Set Tattoo Semantic Segmentation","A. Brilhador; R. T. da Silva; C. R. Modinez-Junior; G. de Almeida Spadafora; H. S. Lopes; A. E. Lazzaretti","Bioinformatics and Computational Intelligence Laboratory - LABIC/CPGEI, Federal University of Technology—Paraná, Curitiba, Paraná, Brazil; Bioinformatics and Computational Intelligence Laboratory - LABIC/CPGEI, Federal University of Technology—Paraná, Curitiba, Paraná, Brazil; Bioinformatics and Computational Intelligence Laboratory - LABIC/CPGEI, Federal University of Technology—Paraná, Curitiba, Paraná, Brazil; Bioinformatics and Computational Intelligence Laboratory - LABIC/CPGEI, Federal University of Technology—Paraná, Curitiba, Paraná, Brazil; Bioinformatics and Computational Intelligence Laboratory - LABIC/CPGEI, Federal University of Technology—Paraná, Curitiba, Paraná, Brazil; Bioinformatics and Computational Intelligence Laboratory - LABIC/CPGEI, Federal University of Technology—Paraná, Curitiba, Paraná, Brazil",IEEE Access,"9 Aug 2024","2024","12","","107181","107200","Tattoos can serve as an essential source of biometric information for public security, aiding in identifying suspects and victims. In order to automate tattoo classification, tasks like classification require more detailed image content analysis, such as semantic segmentation. However, a dataset with appropriate semantic segmentation annotations is currently lacking. Also, there are countless ways to categorize tattoo classes, and many are not directly categorizable, either because they belong to a specific artistic trait or characterize an object with previously undefined semantics. An effective way to overcome these limitations is to build recognition systems based on open-set assumptions. Nevertheless, state-of-the-art open set approaches are not directly applicable in tattoo semantic segmentation, mainly due to the significant class imbalance (predominant background). To the best of our knowledge, this paper is the first to explore semantic segmentation in closed and open-set scenarios for tattoos. In this sense, this paper presents two key contributions: (i) a novel large-margin loss function and generalized open-set classifier approach and (ii) an open-set tattoo semantic segmentation dataset with a publicly accessible test set, enabling comparisons and future research in this area. The proposed approach outperforms other methods, achieving 0.8013 of AUROC, 0.6318 of Macro F1, 0.4900 of mIoU, and notably 0.2753 of IoU for the unknown class, demonstrating the feasibility of this approach for automatic tattoo analysis. The paper also highlights key limitations and open research areas in this challenging field. Dataset and codes are available at https://github.com/Brilhador/tssd2023.","2169-3536","","10.1109/ACCESS.2024.3438557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10623180","Open-world;open-set;semantic segmentation;large-margin learning;tattoo classification","Semantic segmentation;Semantics;Flowering plants;Biometrics (access control);Annotations;Task analysis;Complexity theory","","","","71","CCBYNCND","5 Aug 2024","","","IEEE","IEEE Journals"
"Robustness in Deep Neural Network: A Focus on Software Tampering and Counter Measures","R. Mahajan; P. Kaur","Uils, Chandigarh University, Mohali, India; Uils, Chandigarh University, Mohali, India","2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)","14 May 2024","2024","","","1","4","Over the past several years, there has been a proliferation of “Deep Neural Network” (DNN) models that have been constructed as well as implemented. These models require safeguarding against potential tampering by malevolent individuals. This paper aims to explore the significance of “recoverable, self-embedding fragile watermarking approach for deep neural network DNN” models in order to safeguard the integrity of the models. This system possesses the capacity to not only identify and locate the altered parameter blocks in the framework, but also to accurately recover the compromised values. The verified data and recovery data are derived through a comprehensive analysis of the specific attributes of the DNN model that requires safeguarding. These data are then embedded into the model using a reference sharing mechanism, without compromising its original functionality. This enables the recovery of the model parameters even when subjected to various levels of tampering.","2769-2884","979-8-3503-5035-7","10.1109/ICRITO61523.2024.10522431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522431","DNN;Watermarks;robustness;tampering","Measurement uncertainty;Artificial neural networks;Watermarking;Manuals;Market research;Software;Data models","","","","16","IEEE","14 May 2024","","","IEEE","IEEE Conferences"
"Threats","J. L. Bayuk","Stevens Institute of Technology, Hoboken, NJ",Stepping Through Cybersecurity Risk Management: A Systems Thinking Approach,"","2024","","","17","45","The mainstay declares that threats embolden adversaries to exploit vulnerabilities that expose assets that enable their objectives. That is the basic idea behind a cyber threat. The most important thing to know about cybersecurity threats is that the actors who enact them may be dangerous adversaries. The second most important thing to know is that there is an interaction between an adversary and its target whether or not the target chooses to actively participate. In the threat actor landscape, actors with diverse objectives often find kindred spirits in organized crime syndicates that are similar to legitimate technology businesses. Threat actors spend considerable time and energy in reconnaissance to identify vulnerabilities in their target's systems. In cyber security, potential attacks are the aggregated set of all publicly documented cyber attacks to date.","","9781394213962","10.1002/9781394213986.ch2","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10502009.pdf&bkn=10501957&pdfType=chapter","","Organizations;Computer security;Matched filters;Sun;Social implications of technology;Malware;NIST","","","","","","16 Apr 2024","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"Towards Synthetic Trace Generation of Modeling Operations using In-Context Learning Approach","V. Muttillo; C. Di Sipio; R. Rubei; L. Berardinelli; M. Dehghani","University of Teramo, Teramo, Italy; University of L’Aquila, L’Aquila, Italy; University of L’Aquila, L’Aquila, Italy; Johannes Kepler University, Linz, Austria; Johannes Kepler University, Linz, Austria",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","619","630","Producing accurate software models is crucial in model-driven software engineering (MDE). However, modeling complex systems is an error-prone task that requires deep application domain knowledge. In the past decade, several automated techniques have been proposed to support academic and industrial practitioners by providing relevant modeling operations. Nevertheless, those techniques require a huge amount of training data that cannot be available due to several factors, e.g., privacy issues. The advent of large language models (LLMs) can support the generation of synthetic data although state-of-the-art approaches are not yet supporting the generation of modeling operations. To fill the gap, we propose a conceptual framework that combines modeling event logs, intelligent modeling assistants, and the generation of modeling operations using LLMs. In particular, the architecture comprises modeling components that help the designer specify the system, record its operation within a graphical modeling environment, and automatically recommend relevant operations. In addition, we generate a completely new dataset of modeling events by telling on the most prominent LLMs currently available. As a proof of concept, we instantiate the proposed framework using a set of existing modeling tools employed in industrial use cases within different European projects. To assess the proposed methodology, we first evaluate the capability of the examined LLMs to generate realistic modeling operations by relying on well-founded distance metrics. Then, we evaluate the recommended operations by considering real-world industrial modeling artifacts. Our findings demonstrate that LLMs can generate modeling events even though the overall accuracy is higher when considering human-based operations. In this respect, we see generative AI tools as an alternative when the modeling operations are not available to train traditional IMAs specifically conceived to support industrial practitioners.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764862","","Data privacy;Accuracy;Large language models;Training data;Data models;Software;Regulation;Context modeling;Software engineering;Synthetic data","","","","75","","29 Nov 2024","","","IEEE","IEEE Conferences"
"IEEE Draft Standard for the Procurement of Artificial Intelligence and Automated Decision Systems","",,"IEEE P3119/D8, January 2025","4 Feb 2025","2025","","","1","188","The standard helps procurement teams reduce risks in artificial intelligence systems (AIS) by using tailored risk management practices when purchasing AIS. Specific process steps for AIS problem definition, solicitation preparation, vendor and solution evaluation, contract negotiation, and contract monitoring are described. Risk management methodologies that augment customary activities and tasks performed during the procurement life cycle are provided including identifying, analyzing, evaluating, prioritizing, mitigating, and controlling unique AIS risks that can detract from unique AIS benefits. Practical AIS procurement tools and metrics and how procurement teams can use and apply them are also provided. The standard focuses explicitly on AIS risks (when compared to traditional, non-AIS technology) and is designed to address the purchase of commercial AI products and services procured using a formal contract or contract framework. NOTE–AI and ADS are referred to as artificial intelligence systems (AIS) for simplicity.","","979-8-8557-1771-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10872881","artificial intelligence;automated decision systems;data;data governance;systems;responsible procurement;public interest;procurement;technology;solicitation;tender;impact assessment;due diligence;risks;harms;responsible AI;sociotechnical;high-risk;acquisition;human rights;commercial AI products and services","IEEE Standards;Artificial intelligence;Data governance;Risk management;Sociotechnical systems","","","","","","4 Feb 2025","","","IEEE","IEEE Standards"
"IEEE Draft Standard for the Procurement of Artificial Intelligence and Automated Decision Systems","",,"IEEE P3119/D7, December 2024","12 Dec 2024","2024","","","1","192","The standard helps procurement teams reduce risks in artificial intelligence systems (AIS) by using tailored risk management practices when purchasing AIS. Specific process steps for AIS problem definition, solicitation preparation, vendor and solution evaluation, contract negotiation, and contract monitoring are described. Risk management methodologies that augment customary activities and tasks performed during the procurement life cycle are provided including identifying, analyzing, evaluating, prioritizing, mitigating, and controlling unique AIS risks that can detract from unique AIS benefits. Practical AIS procurement tools and metrics and how procurement teams can use and apply them are also provided. The standard focuses explicitly on AIS risks (when compared to traditional, non-AIS technology) and is designed to address the purchase of commercial AI products and services procured using a formal contract or contract framework. NOTE–AI and ADS are referred to as artificial intelligence systems (AIS) for simplicity.","","979-8-8557-1603-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10797720","artificial intelligence;automated decision systems;data;data governance;systems;responsible procurement;public interest;procurement;technology;solicitation;tender;impact assessment;due diligence;risks;harms;responsible AI;sociotechnical;high-risk;acquisition;human rights;commercial AI products and services","IEEE Standards;Artificial intelligence;Risk management;Sociotechnical systems;Human factors;Decision making;Computer security","","","","","","12 Dec 2024","","","IEEE","IEEE Standards"
"The Evolution of Knowledge: Rethinking Science for the Anthropocene","J. Renn",NA,The Evolution of Knowledge: Rethinking Science for the Anthropocene,"","2020","","","","","A fundamentally new approach to the history of science and technologyThis book presents a new way of thinking about the history of science and technology, one that offers a grand narrative of human history in which knowledge serves as a critical factor of cultural evolution. Jürgen Renn examines the role of knowledge in global transformations going back to the dawn of civilization while providing vital perspectives on the complex challenges confronting us today in the Anthropocene—this new geological epoch shaped by humankind.Renn reframes the history of science and technology within a much broader history of knowledge, analyzing key episodes such as the evolution of writing, the emergence of science in the ancient world, the Scientific Revolution of early modernity, the globalization of knowledge, industrialization, and the profound transformations wrought by modern science. He investigates the evolution of knowledge using an array of disciplines and methods, from cognitive science and experimental psychology to earth science and evolutionary biology. The result is an entirely new framework for understanding structural changes in systems of knowledge—and a bold new approach to the history and philosophy of science.Written by one of today's preeminent historians of science, The Evolution of Knowledge features discussions of historiographical themes, a glossary of key terms, and practical insights on global issues ranging from climate change to digital capitalism. This incisive book also serves as an invaluable introduction to the history of knowledge.","","9780691185675","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9453356.pdf&bkn=9453355&pdfType=book","","","","","","","","13 Jul 2021","","","Princeton University Press","Princeton University Press eBooks"
"Distributed Energy Resource Management System (DERMS) Cybersecurity Scenarios, Trends, and Potential Technologies: A Review","N. Sugunaraj; S. R. A. Balaji; B. S. Chandar; P. Rajagopalan; U. Kose; D. C. Loper; T. Mahfuz; P. Chakraborty; S. Ahmad; T. Kim; G. Apruzzese; A. Dubey; L. Strezoski; B. Blakely; S. Ghosh; M. J. B. Reddy; H. V. Padullaparti; P. Ranganathan","Center for Cyber Security Research (C2SR), University of North Dakota, Grand Forks, ND, USA; Center for Cyber Security Research (C2SR), University of North Dakota, Grand Forks, ND, USA; Center for Cyber Security Research (C2SR), University of North Dakota, Grand Forks, ND, USA; Center for Cyber Security Research (C2SR), University of North Dakota, Grand Forks, ND, USA; Suleyman Demirel University, Isparta, Turkey; Center for Cyber Security Research (C2SR), University of North Dakota, Grand Forks, ND, USA; University of Maine, Orono, ME, USA; University of Maine, Orono, ME, USA; Texas A and M University-Kingsville, Kingsville, TX, USA; University of Missouri, MO, USA; Liechtenstein Business School, University of Liechtenstein, Vaduz, Liechtenstein; Washington State University, Pullman, WA, USA; University of Novi Sad, Faculty of Technical Sciences, Department for Power, Electronics, and Telecommunication Engineering, Serbia; Argonne National Laboratory, Lemont, IL, USA; National Institute of Technology, Raipur, India; National Institute of Technology, Tiruchirapalli, India; National Renewable Energy Laboratory (NREL), Golden, CO, USA; Center for Cyber Security Research (C2SR), University of North Dakota, Grand Forks, ND, USA",IEEE Communications Surveys & Tutorials,"","2025","PP","99","1","1","Critical infrastructures like the power grid are at risk from increasing cyber threats due to high penetration of interconnected distributed energy resources (DER). Compromised DER endpoints can cause events, data breaches, communication loss, intentional device failures, and even cascading outages. To address these challenges, this paper explores cybersecurity issues in DER management systems (DERMS), including state-of-the-art reviews on architectures, communication protocols, access control privileges, data breaches, identity management policies, attacks such as false data injection, denial of service, distributed denial of service, malware, threats affecting data integrity, and network vulnerabilities. Realistic threat scenarios are outlined, followed by discussions on futuristic solutions like the zero trust framework. The paper presents new architectural patterns for recently released multi-level hierarchical framework as per IEEE 1547.3 standard to handle DERMS data and assets. The paper also discusses potential threats compromising the Confidentiality, Integrity, Availability, and Accountability (CIAA) properties at each level of the IEEE 1547.3 framework. This review is unique and comprehensive, as it covers existing research on cybersecurity challenges in DER-related assets and outlines the necessary capabilities to equip Intrusion Diagnostic Units (IDUs) in future DERMS technologies, all while ensuring compliance with IEEE 1547.3 standard requirements.","1553-877X","","10.1109/COMST.2025.3534828","National Science Foundation(grant numbers:CNS-2219733); U.S. Department of Energy(grant numbers:DE-AC02-06CH1135); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10858368","advanced distribution management systems;cybersecurity;distributed energy resources;distributed energy resource management systems","Computer architecture;Distributed power generation;Protocols;Computer security;Tutorials;Surveys;Malware;Power system stability;Distributed databases;Data privacy","","","","","CCBY","30 Jan 2025","","","IEEE","IEEE Early Access Articles"
"E2BA: Environment Exploration and Backtracking Agent for Visual Language Object Navigation","Y. Shi; J. Liu; L. Sun; X. Zheng","National Engineering Research Center for Visual Information and Applications, National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, China; National Engineering Research Center for Visual Information and Applications, National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, China; National Engineering Research Center for Visual Information and Applications, National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, China; Intelligent Transportation Thrust, The Hong Kong University of Science and Technology, (Guangzhou), China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Robot navigation in an unknown environment is a challenging task, due to the lack of spatial awareness and semantic understanding of the environment. Previous works predominantly relied on prior scene knowledge and semantic information, lacking generalization and transferability. This paper proposes an environment exploration and backtracking agent (E2BA) for visual language object navigation, which leverages the rich semantic prior knowledge and commonsense reasoning of large language models (LLMs) to explore the environment and find the object. By fusing LLM scores and spatial geometric costs using particle filters, we select a redefined optimal frontier as sub-goal for environment exploration. To avoid redundant exploration and paths, we design a backtracking discriminator to evaluate the state of the agent and determine the timing of backtracking triggering through a double-level cascade mechanism. Additionally, we design a random instruction fuzzy semantic guessing task to verify the application diversity of this method. Comprehensive experiments on the Habitat-Matterport 3D dataset show that our method achieves a success rate of 0.704, which is higher than the existing baseline method. This study explores the potential application of LLMs in environment exploration without the need for additional training and semantic supplementation.","1558-2205","","10.1109/TCSVT.2025.3531410","National Natural Science Foundation of China(grant numbers:62373315); National Key Research and Development Program of China(grant numbers:2021ZD0110700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10845850","Vision language object navigation;environment exploration;backtracking;large language models","Navigation;Visualization;Semantics;Backtracking;Robots;Planning;Costs;Commonsense reasoning;Circuits and systems;Training","","","","","IEEE","20 Jan 2025","","","IEEE","IEEE Early Access Articles"
"Unambiguous and High-Fidelity Backdoor Watermarking for Deep Neural Networks","G. Hua; A. B. J. Teoh; Y. Xiang; H. Jiang","School of Electronic Information, Wuhan University, Wuhan, China; School of Electrical and Electronic Engineering, College of Engineering, Yonsei University, Seoul, South Korea; School of Information Technology, Deakin University, Burwood, VIC, Australia; School of Electronic Information, Wuhan University, Wuhan, China",IEEE Transactions on Neural Networks and Learning Systems,"5 Aug 2024","2024","35","8","11204","11217","The unprecedented success of deep learning could not be achieved without the synergy of big data, computing power, and human knowledge, among which none is free. This calls for the copyright protection of deep neural networks (DNNs), which has been tackled via DNN watermarking. Due to the special structure of DNNs, backdoor watermarks have been one of the popular solutions. In this article, we first present a big picture of DNN watermarking scenarios with rigorous definitions unifying the black- and white-box concepts across watermark embedding, attack, and verification phases. Then, from the perspective of data diversity, especially adversarial and open set examples overlooked in the existing works, we rigorously reveal the vulnerability of backdoor watermarks against black-box ambiguity attacks. To solve this problem, we propose an unambiguous backdoor watermarking scheme via the design of deterministically dependent trigger samples and labels, showing that the cost of ambiguity attacks will increase from the existing linear complexity to exponential complexity. Furthermore, noting that the existing definition of backdoor fidelity is solely concerned with classification accuracy, we propose to more rigorously evaluate fidelity via examining training data feature distributions and decision boundaries before and after backdoor embedding. Incorporating the proposed prototype guided regularizer (PGR) and fine-tune all layers (FTAL) strategy, we show that backdoor fidelity can be substantially improved. Experimental results using two versions of the basic ResNet18, advanced wide residual network (WRN28_10) and EfficientNet-B0, on MNIST, CIFAR-10, CIFAR-100, and FOOD-101 classification tasks, respectively, illustrate the advantages of the proposed method.","2162-2388","","10.1109/TNNLS.2023.3250210","International Scholar Exchange Fellowship (ISEF) Program through the Chey Institute for Advanced Studies, South Korea; National Natural Science Foundation of China (NSFC)(grant numbers:U19B2004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10059007","Backdoor watermarking;black-box ambiguity attack;high-fidelity deep neural network (DNN) watermarking;neural network watermarking;unambiguous verification","Watermarking;Training;Deep learning;Closed box;Training data;Neural networks;Computational modeling","","9","","48","IEEE","6 Mar 2023","","","IEEE","IEEE Journals"
"Research on the Application of Natural Language Processing Technology in Medicine Patent Retrieval","L. Tu; S. Lv; H. Shi","Department of Information Engineering, Shanwei Polytechnic, Shanwei, China; Department of Information Engineering, Shanwei Polytechnic, Shanwei, China; Department of Information Engineering, Shanwei Polytechnic, Shanwei, China","2024 5th International Conference on Information Science, Parallel and Distributed Systems (ISPDS)","12 Sep 2024","2024","","","176","181","In view of the high professionalism and complexity of medicine patent text, traditional patent retrieval mainly relies on manual operation and simple text Processing methods, this paper aims to use Natural Language Processing (NLP) technology to efficiently obtain valuable information in medicine patent retrieval. To transform unstructured text into meaningful information. Through in-depth research on the characteristics and structure of medicine patent text, a new structured feature extraction algorithm of patent text is proposed, and a patent retrieval system based on NLP is developed, which can carry out instant structured processing, and can automatically identify and translate text into corresponding patented medicines, and nearly 300 medicine patent abstracts are tested by experiment. The system performance is good. The system achieves the expected effect.","","979-8-3503-5271-9","10.1109/ISPDS62779.2024.10667520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10667520","medicine patent retrieval;Natural language processing technology;Patent text structuring;Feature extraction algorithm","Training;Patents;Adaptation models;Transfer learning;Transforms;Search problems;Natural language processing","","","","9","IEEE","12 Sep 2024","","","IEEE","IEEE Conferences"
"An Integrated Approach of Threat Analysis for Autonomous Vehicles Perception System","S. Ghosh; A. Zaboli; J. Hong; J. Kwon","Department of Electrical and Computer Engineering, University of Michigan--Dearborn, Dearborn, MI, USA; Department of Electrical and Computer Engineering, University of Michigan--Dearborn, Dearborn, MI, USA; Department of Electrical and Computer Engineering, University of Michigan--Dearborn, Dearborn, MI, USA; Department of Electrical and Computer Engineering, University of Michigan--Dearborn, Dearborn, MI, USA",IEEE Access,"16 Feb 2023","2023","11","","14752","14777","Automated vehicles are a revolutionary step in mobility, providing a safe and convenient riding experience while keeping the human-driving task minimal to none. Therefore, these intelligent vehicles are equipped with sophisticated perception sensors (e.g., cameras and radars), high-performance computers, artificial intelligence (AI)-driven algorithms, and connectivity with other internet-of-things (IoT) devices. This makes autonomous vehicles (AVs) a special kind of cyber-physical system (CPS) that is moving at speed in highly interactive and dynamic environments (e.g., public roads). Thus, AV is a potential target for cyber attackers to weaponize, compromising safety and mobility on the road. The first step in addressing this problem is to have a robust threat modeling framework that can address the evolving cyber-physical threats, especially to AV applications. In this regard, two areas are studied in this paper: the common practice of threat modeling in automotive and the ISO/SAE 21434 standard, and sensors and machine learning (ML) algorithms for AV perception systems and potential cyber-physical attacks. A comparative threat analysis for an AV perception system with the ISO/SAE 21434 standard and a system-theoretic process analysis for security (STPA-Sec) approach is also demonstrated in this paper. Based on the analysis, this paper proposes a robust threat analysis and risk assessment framework with mathematical modeling to identify cyber-physical threats to AV perception systems that are critical for the driving behaviors and complex interactions of AVs in their operational design domain.","2169-3536","","10.1109/ACCESS.2023.3243906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10041909","Threat modeling;cyber-physical system;autonomous and connected vehicles;perception;object classification;cyber-security","Threat modeling;Computer security;Sensors;Automotive engineering;Connected vehicles;Cyber-physical systems;Autonomous systems;Threat modeling","","17","","81","CCBYNCND","10 Feb 2023","","","IEEE","IEEE Journals"
"Secured and Privacy-Preserving GPU-Based Machine Learning Inference in Trusted Execution Environment: A Comprehensive Survey","A. Chaudhuri; S. Shukla; S. Bhattacharya; D. Mukhopadhyay","Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, Kharagpur, India; Centre for Computational and Data Sciences, Indian Institute of Technology, Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, Kharagpur, India",2025 17th International Conference on COMmunication Systems and NETworks (COMSNETS),"20 Feb 2025","2025","","","207","216","With the rapid advancement of machine learning (ML) models and their widespread application across various sectors such as intrusion detection, medical diagnosis, natural language processing, and autonomous driving, these technologies have achieved remarkable success. However, this progress has also raised significant concerns about ensuring the security of ML models and protecting both private training data and model outputs from getting exposed in a shared cloud environment. To address these challenges, researchers have proposed various methodologies to create privacy-preserving, secure, and trustworthy model execution environments to prevent adversarial attacks. This study provides a comprehensive review of Trusted Execution Environment (TEE) implementations across different hardware accelerators. It also offers an overview of modern techniques for preserving privacy and security in execution environments, while identifying critical research gaps that require attention. In essence, this survey is an important resource for researchers, providing insights into recent methodologies and guiding them to focus on pressing research challenges.","2155-2509","979-8-3315-3119-5","10.1109/COMSNETS63942.2025.10885734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10885734","security;privacy;trusted execution environment;GPU;deep learning","Surveys;Deep learning;Training;Privacy;Data privacy;Training data;Data models;Stability analysis;Security;Hardware acceleration","","","","59","IEEE","20 Feb 2025","","","IEEE","IEEE Conferences"
"Blockchain and NFTs for Trusted Ownership, Trading, and Access of AI Models","A. Battah; M. Madine; I. Yaqoob; K. Salah; H. R. Hasan; R. Jayaraman","Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Industrial and Systems Engineering, Khalifa University, Abu Dhabi, United Arab Emirates",IEEE Access,"28 Oct 2022","2022","10","","112230","112249","The demand for high-quality Artificial Intelligence (AI) models is ever-increasing in this digital era. However, most of the existing methods leveraged for managing the ownership, trading, and access of AI models fall short of providing traceability, transparency, audit, security, and trustful features. In this paper, we propose a solution based on blockchain and Non-fungible Tokens (NFTs) to manage ownership rights and exchange of AI models in a transparent, traceable, auditable, secure, and trustworthy manner. Smart contracts are employed to enforce ownership, ease of access, and exchange policies for the unique NFT linked to an AI model. We use decentralized storage of the InterPlanetary File System (IPFS) and proxy re-encryption oracles to securely fetch, store, and share data related to AI models. We present algorithms along with their implementation, testing, and validation details. The proposed solution is evaluated using cost and security analyses to show its affordability and resiliency against security threats and attacks. All smart contract codes are made publicly available on GitHub.","2169-3536","","10.1109/ACCESS.2022.3215660","Khalifa University of Science and Technology(grant numbers:RCII-2019-002–Center for Digital Supply Chain and Operations Management,CIRA-2019-001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9924181","Blockchain;decentralized storage;non-fungible tokens (NFTs);oracles;provenance;proxy re-encryption;smart contracts","Artificial intelligence;Data models;Collaboration;Security;Computational modeling;Smart contracts;Data privacy;Blockchains;Nonfungible tokens","","18","","54","CCBY","19 Oct 2022","","","IEEE","IEEE Journals"
"AI-Enhanced Robotic Process Automation: A Review of Intelligent Automation Innovations","S. Afrin; S. Roksana; R. Akram","Department of Computer Science, The University of Texas at San Antonio, San Antonio, TX, USA; Department of Computer Science, The University of Texas at San Antonio, San Antonio, TX, USA; Intel Corporation, Hillsboro, OR, USA",IEEE Access,"2 Jan 2025","2025","13","","173","197","The rapid technological growth in recent decades due to the integration of robust technologies and automation have led to the rise of digital services and the emergence of Industry 4.0. This paper explores the concept and potential of AI-powered intelligent automation based on the synergistic use of Robotic Process Automation (RPA) and Artificial Intelligence (AI) to enhance organizational and business processes across various sectors. RPA automates routine, rules-based tasks, thereby allowing human workers to engage in more innovative activities. When integrated with AI, RPA systems gain the capacity to analyze data, identify patterns, classify information and forecast which leads to significant improvement in accuracy and productivity. This literature review investigates the current state of RPA and AI integration while highlighting its applications in different sectors such as manufacturing, agriculture, healthcare, finance, and retail. Along with discussing the drawbacks and restrictions, such as technological issues and moral dilemmas, this paper also discusses the advantages of this integration, which include decreased costs, increased output, and simplified operations. By leveraging AI techniques such as classification, text mining of neural network, RPA technologies optimize business operations and advance Industry 4.0. This study also illustrates the challenges and limitations of this integration such as technical difficulties and ethical considerations. The aim of this review is to provide a comprehensive understanding of the synergistic potential of RPA and AI while offering insights into their contribution in shaping the future of intelligent automation.","2169-3536","","10.1109/ACCESS.2024.3513279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10781408","Artificial intelligence (AI);business process;intelligent process automation (IPA);robotic process automation (RPA)","Artificial intelligence;Automation;Business;Companies;Industries;Robot kinematics;Recording;Productivity;Ethics;Analytical models","","","","103","CCBY","9 Dec 2024","","","IEEE","IEEE Journals"
"ChatGPT security risks and solutions","S. Zaman","Department of Information Science, Kuwait University, Kuwait",7th IET Smart Cities Symposium (SCS 2023),"30 Apr 2024","2023","2023","","378","387","As artificial intelligence (AI) technologies become more popular in enterprise environments, special chatbot technologies such as ChatGPT are gaining popularity due to their abilities, services, and benefits. However, these chatbots suffer from significant security risks and vulnerabilities that need to be addressed, such as confidentiality weaknesses, authentication issues, chatbot breaches, data poisoning, and others. This article identifies the potential ChatGPT risks and threats and proposes solutions and remedies to address the security issues of the chatbots risks, such as calling for policy changes, proposing training, equipping the teams with AI technology and tools, implementing strong access controls, using secure servers and storage systems, limiting access to systems, supporting a well-defined incident response process, and calling for government oversight to ensure AI usage doesn't become detrimental to cybersecurity efforts. Ending with future directions and recommendations to protect the use of this technology and guide future research and practice in this field.","","978-1-83953-993-0","10.1049/icp.2024.0955","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10510680","","","","1","","","","30 Apr 2024","","","IET","IET Conferences"
"Combinatorial Optimization of Graphical User Interface Designs","A. Oulasvirta; N. R. Dayama; M. Shiripour; M. John; A. Karrenbauer","Department of Communications and Networking, School of Electrical Engineering, Aalto University, Espoo, Finland; Department of Communications and Networking, School of Electrical Engineering, Aalto University, Espoo, Finland; Department of Communications and Networking, School of Electrical Engineering, Aalto University, Espoo, Finland; Max Planck Institute for Informatics, Saarbrücken, Germany; Max Planck Institute for Informatics, Saarbrücken, Germany",Proceedings of the IEEE,"4 Mar 2020","2020","108","3","434","464","The graphical user interface (GUI) has become the prime means for interacting with computing systems. It leverages human perceptual and motor capabilities for elementary tasks such as command exploration and invocation, information search, and multitasking. For designing a GUI, numerous interconnected decisions must be made such that the outcome strikes a balance between human factors and technical objectives. Normally, design choices are specified manually and coded within the software by professional designers and developers. This article surveys combinatorial optimization as a flexible and powerful tool for computational generation and adaptation of GUIs. As recently as 15 years ago, applications were limited to keyboards and widget layouts. The obstacle has been the mathematical definition of design tasks, on the one hand, and the lack of objective functions that capture essential aspects of human behavior, on the other. This article presents definitions of layout design problems as integer programming tasks, a coherent formalism that permits identification of problem types, analysis of their complexity, and exploitation of known algorithmic solutions. It then surveys advances in formulating evaluative functions for common design-goal foci such as user performance and experience. The convergence of these two advances has expanded the range of solvable problems. Approaches to practical deployment are outlined with a wide spectrum of applications. This article concludes by discussing the position of this application area within optimization and human-computer interaction research and outlines challenges for future work.","1558-2256","","10.1109/JPROC.2020.2969687","H2020 European Research Council(grant numbers:637991); Academy of Finland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9000519","Combinatorial optimization;computational design;graphical user interfaces (GUIs);human–computer interaction (HCI);integer programming;interactive optimization;meta-heuristic optimization","Grahical user interfaces;Task analysis;Linear programming;Multitasking;Human factors;Adversarial machine learning;Learning systems;Human computer interaction;Human factors;Computer applications","","64","","170","CCBY","17 Feb 2020","","","IEEE","IEEE Journals"
"DDAP: Dual-Domain Anti-Personalization against Text-to-Image Diffusion Models","J. Yang; R. Xi; Y. Lai; X. Lin; Z. Yu",Northwestern Polytechnical University; Northwestern Polytechnical University; Great Bay University; Beihang University; Great Bay University,2024 IEEE International Joint Conference on Biometrics (IJCB),"11 Nov 2024","2024","","","1","10","Diffusion-based personalized visual content generation technologies have achieved significant breakthroughs, allowing for the creation of specific objects by just learning from a few reference photos. However, when misused to fabricate fake news or unsettling content targeting individuals, these technologies could cause considerable societal harm. To address this problem, current methods generate adversarial samples by adversarially maximizing the training loss, thereby disrupting the output of any personalized generation model trained with these samples. However, the existing methods fail to achieve effective defense and maintain stealthiness, as they overlook the intrinsic properties of diffusion models. In this paper, we introduce a novel Dual-Domain Anti-Personalization framework (DDAP). Specifically, we have developed Spatial Perturbation Learning (SPL) by exploiting the fixed and perturbation-sensitive nature of the image encoder in personalized generation. Subsequently, we have designed a Frequency Perturbation Learning (FPL) method that utilizes the characteristics of diffusion models in the frequency domain. The SPL disrupts the overall texture of the generated images, while the FPL focuses on image details. By alternating between these two methods, we construct the DDAP framework, effectively harnessing the strengths of both domains. To further enhance the visual quality of the adversarial samples, we design a localization module to accurately capture attentive areas while ensuring the effectiveness of the attack and avoiding unnecessary disturbances in the background. Extensive experiments on facial benchmarks have shown that the proposed DDAP enhances the disruption of personalized generation models while also maintaining high quality in adversarial samples, making it more effective in protecting privacy in practical applications.","2474-9699","979-8-3503-6413-2","10.1109/IJCB62174.2024.10744490","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10744490","","Location awareness;Training;Biometrics;Visualization;Privacy;Frequency-domain analysis;Perturbation methods;Text to image;Diffusion models;Fake news","","","","40","IEEE","11 Nov 2024","","","IEEE","IEEE Conferences"
"digitalSTS: A Field Guide for Science & Technology Studies","J. Vertesi; D. Ribes; L. Forlano; A. Camus; D. Vinck; D. Ribes; N. Calvillo; D. K. Rosner; C. Dunbar-Hester; X. (. Kerasidou; L. Stark; S. Couture; S. J. Jackson; A. S. Chan; C. A. Hawthorne; C. Ilten; P. -B. McInerney; D. Nemer; P. Chirumamilla; W. R. Poster; S. Sawyer; I. Erickson; M. H. Jarrahi; R. Singh; C. Hesselbein; J. Price; M. Lynch; E. Parmiggiani; E. Monteiro; D. Allhutter; B. R. Winthereik; J. Maguire; L. Watts; C. DiSalvo; J. Vertesi; G. Latzko-Toth; J. Söderberg; F. Millerand; S. Jones; N. Seaver; M. L. Cohn; Y. Loukissas; D. C. Llach; A. K. Munk; A. Meunier; T. Venturini; J. Salamanca; M. Jacomy",NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA,digitalSTS: A Field Guide for Science & Technology Studies,"","2019","","","","","New perspectives on digital scholarship that speak to today's computational realities Scholars across the humanities, social sciences, and information sciences are grappling with how best to study virtual environments, use computational tools in their research, and engage audiences with their results. Classic work in science and technology studies (STS) has played a central role in how these fields analyze digital technologies, but many of its key examples do not speak to today’s computational realities. This groundbreaking collection brings together a world-class group of contributors to refresh the canon for contemporary digital scholarship.In twenty-five pioneering and incisive essays, this unique digital field guide offers innovative new approaches to digital scholarship, the design of digital tools and objects, and the deployment of critically grounded technologies for analysis and discovery. Contributors cover a broad range of topics, including software development, hackathons, digitized objects, diversity in the tech sector, and distributed scientific collaborations. They discuss methodological considerations of social networks and data analysis, design projects that can translate STS concepts into durable scientific work, and much more.Featuring a concise introduction by Janet Vertesi and David Ribes and accompanied by an interactive microsite, this book provides new perspectives on digital scholarship that will shape the agenda for tomorrow’s generation of STS researchers and practitioners.","","9780691190600","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9453334.pdf&bkn=9453333&pdfType=book","","","","","","","","13 Jul 2021","","","Princeton University Press","Princeton University Press eBooks"
"Transforming the Insurance Landscape: How ChatGPT's Proficiency Empowers the Industry","R. S. Jha; P. R. Sahoo; B. Mahapatra; A. Mohapatra; Rajni","KIIT School of Management, KIIT University, Bhubaneswar, Odisha, India; KIIT School of Management, KIIT University, Bhubaneswar, Odisha, India; NA; KIIT School of Electrical Engineering, KIIT University, Bhubaneswar, Odisha, India; Bharati College, University of Delhi, Delhi, India",2023 6th International Conference on Contemporary Computing and Informatics (IC3I),"26 Jan 2024","2023","6","","1265","1270","The release of the ChatGPT by OpenAI in 2022 generated interest in academia and industry, with mixed reactions to its capabilities and ethical concerns. Numerous studies have explored its accuracy and adaptability in several industries, such as healthcare, education, finance, and atmospheric science. Still, its applicability in the insurance industry has yet to be comprehensively analyzed. The insurance industry protects financially against risks like property damage, accidents, and health issues. Insurance companies offer policies to individuals and businesses; policyholders pay premiums to cover potential losses. The industry relies on efficient communication, accurate information processing, and risk assessment to effectively underwrite policies and manage claims. In summary, the ChatGPT has the potential to become a game-changing tool in the Insurance industry, driving innovation in customer interactions, process automation, and risk management. As the technology continues to evolve, its' potential to reshape insurers' operations and customer service in the digital era is immense.","","979-8-3503-0448-0","10.1109/IC3I59117.2023.10397662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10397662","ChatGPT;Insurance;Innovation;Artificial Intelligence;Chatbot;Regulation;Data;OpenAI;Large Language Model","Industries;Data integrity;Insurance;Machine learning;Chatbots;Risk management;Faces","","","","32","IEEE","26 Jan 2024","","","IEEE","IEEE Conferences"
"Exploring Machine Learning for Semiconductor Process Optimization: A Systematic Review","Y. -L. Chen; S. Sacchi; B. Dey; V. Blanco; S. Halder; P. Leray; S. D. Gendt","Angstrom Patterning Department, Imec, Leuven, Belgium; Angstrom Patterning Department, Imec, Leuven, Belgium; Angstrom Patterning Department, Imec, Leuven, Belgium; Angstrom Patterning Department, Imec, Leuven, Belgium; Angstrom Patterning Department, Imec, Leuven, Belgium; Angstrom Patterning Department, Imec, Leuven, Belgium; Department of Materials Engineering and Department of Chemistry, KU Leuven, Leuven, Belgium",IEEE Transactions on Artificial Intelligence,"11 Dec 2024","2024","5","12","5969","5989","As machine learning (ML) continues to find applications, extensive research is currently underway across various domains. This study examines the current methodologies of ML being investigated to optimize semiconductor manufacturing processes. Our research involved searching the SPIE Digital Library, IEEE Xplore, and ArXiv databases, identifying 58 publications in the field of ML-based semiconductor process optimization. These investigations employ ML techniques such as feature extraction, feature selection, and neural network architecture are analyzed using different algorithms. These models find applications in advanced process control, virtual metrology, and quality control, critical aspects in semiconductor manufacturing for enhancing throughput and reducing production costs. We categorize the articles based on the methods and applications employed, summarizing the primary findings. Furthermore, we discuss the general conclusion of several studies. Overall, the reviewed literature suggests that ML-based semiconductor manufacturing is rapidly gaining popularity and advancing at a swift pace.","2691-4581","","10.1109/TAI.2024.3429479","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10601235","Advanced process control (APC);artificial intelligence (AI);chemical mechanical polishing (CMP);deep learning;etching;lithography;machine learning (ML);neural networks (NNs);predictive metrology (PM);root cause analysis (RCA);scatterometry;semiconductor manufacturing;semiconductor process optimization;thin film;virtual metrology (VM)","Machine learning;Process control;Metrology;Data models;Semiconductor device manufacture;Reviews;Semiconductor process modeling","","","","86","IEEE","17 Jul 2024","","","IEEE","IEEE Journals"
"CQS-Attention: Scaling Up the Standard Attention Computation for Infinitely Long Sequences","Y. Bian; A. K. Somani","Lewis-Sigler Institute for Integrative Genomics, Princeton University, Princeton, NJ, USA; Department of Electrical and Computer Engineering, Iowa State University, Ames, IA, USA",IEEE Access,"28 Feb 2025","2025","13","","35527","35538","Transformer models suffer from unaffordable high memory consumption when the sequence is long and standard self-attention is utilized. We developed a sequence parallelism scheme called CQS-Attention that can break the limit of sequence length. A long sequence is divided into multiple overlapping subsequences. The attention of each subsequence is independently computed and gathered as the final exact attention of the original long sequence. CQS-Attention is a fork-join parallel model comprising three components: Scheduler, Workers, and Tiler. The Scheduler equally partitions computation responsibility in a completely mutually exclusive manner and ensures the local subsequence length is minimum. Each worker independently computes the standard attention of the assigned subsequence and transfers local results to the Tiler, which produces the final attention. CQS-Attention makes attention computation embarrassingly parallel. Hence, it enjoys great performance regarding single-device memory and computation time consumption, mathematical stability and scalability. More importantly, it is fully compatible with all state-of-the-art attention optimizations. Our code and supplementary information (SI) are available at https://github.com/CQS-Attention/CQS_Attention.","2169-3536","","10.1109/ACCESS.2025.3544550","Philip and Virginia Sproul Professorship at Iowa State University; High Performance Computing at Iowa State University (HPC@ISU) equipment for computing through NSF(grant numbers:MRI 1726447,MRI 2018594); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10900388","Attention computation;cyclic quorum sets;parallel algorithm;transformer","Standards;Parallel processing;Memory management;Transformers;Indexes;Lower bound;Stability analysis;Scalability;Reproducibility of results;Proposals","","","","42","CCBY","24 Feb 2025","","","IEEE","IEEE Journals"
"Robust Watermarking Based on Multi-layer Watermark Feature Fusion","S. Wu; W. Lu; X. Luo","School of Computer Science and Engineering, Ministry of Education Key Laboratory of Information Technology, Guangdong Province Key Laboratory of Information Security Technology, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Ministry of Education Key Laboratory of Information Technology, Guangdong Province Key Laboratory of Information Security Technology, Sun Yat-sen University, Guangzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Henan, China",IEEE Transactions on Multimedia,"","2025","PP","99","1","14","The purpose of robust image watermarking is to embed a watermark into a carrier image in an invisible form and extract the watermark successfully even under noise interference conditions to achieve copyright confirmation and traceability. Although watermarking methods based on deep learning can improve the robustness by adding a noise simulation layer, few theoretical analyses of the codec structure have been conducted. Theoretical explainability is the theoretical basis for developing a network architecture, which plays a guiding role in network development. On the basis of the interpretability of convolutional networks, this paper analyzes the mathematical process of embedding and extracting watermarks in codecs and proposes a novel watermarking framework based on multi-layer watermark feature fusion. Specifically, the encoder can be a convolutional network structure of arbitrary depth, whereas the decoder needs only to adopt its corresponding deconvolution structure. To improve the quality and robustness of the generated watermarked image, the watermark is associated with an arbitrary layer feature space in the decoder. In the decoder, the network quickly converges to each original encoding feature space through the deconvolution structure, thus decoupling the watermark features. Finally, the watermark is extracted via the automatic fusion of multi-layer watermark features. The experimental results show that the proposed method is suitable for few-shot learning, and its invisibility, robustness and generalization performance on multiple datasets are significantly better than those of other advanced methods.","1941-0077","","10.1109/TMM.2025.3543079","National Natural Science Foundation of China(grant numbers:62261160653); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10891427","Robust watermarking;convolutional network;watermark feature fusion","Watermarking;Decoding;Robustness;Feature extraction;Noise;Convolutional neural networks;Training;Deep learning;Distortion;Convolution","","","","","IEEE","17 Feb 2025","","","IEEE","IEEE Early Access Articles"
"BjTT: A Large-Scale Multimodal Dataset for Traffic Prediction","C. Zhang; Y. Zhang; Q. Shao; J. Feng; B. Li; Y. Lv; X. Piao; B. Yin","Beijing Key Laboratory of Multimedia and Intelligent Software Technology, Beijing Institute of Artificial Intelligence, School of Information Science and Technology, Beijing University of Technology, Beijing, China; Beijing Key Laboratory of Multimedia and Intelligent Software Technology, Beijing Institute of Artificial Intelligence, School of Information Science and Technology, Beijing University of Technology, Beijing, China; Beijing Key Laboratory of Multimedia and Intelligent Software Technology, Beijing Institute of Artificial Intelligence, School of Information Science and Technology, Beijing University of Technology, Beijing, China; Beijing Key Laboratory of Multimedia and Intelligent Software Technology, Beijing Institute of Artificial Intelligence, School of Information Science and Technology, Beijing University of Technology, Beijing, China; Beijing Key Laboratory of Multimedia and Intelligent Software Technology, Beijing Institute of Artificial Intelligence, School of Information Science and Technology, Beijing University of Technology, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Beijing Key Laboratory of Multimedia and Intelligent Software Technology, Beijing Institute of Artificial Intelligence, School of Information Science and Technology, Beijing University of Technology, Beijing, China; Beijing Key Laboratory of Multimedia and Intelligent Software Technology, Beijing Institute of Artificial Intelligence, School of Information Science and Technology, Beijing University of Technology, Beijing, China",IEEE Transactions on Intelligent Transportation Systems,"5 Nov 2024","2024","25","11","18992","19003","Traffic prediction plays a significant role in Intelligent Transportation Systems (ITS). Although many datasets have been introduced to support the study of traffic prediction, most of them only provide time-series traffic data. However, urban transportation systems are always susceptible to various factors, including unusual weather and traffic accidents. Therefore, relying solely on historical data for traffic prediction greatly limits the accuracy of the prediction. In this paper, we introduce Beijing Text-Traffic (BjTT), a large-scale multimodal dataset for traffic prediction. BjTT comprises over 32,000 time-series traffic records, capturing velocity and congestion levels on more than 1,200 roads within the 5th ring area of Beijing. Meanwhile, each piece of traffic data is coupled with a text describing the traffic system (including time, location, and events). We detail the data collection and processing procedures and present a statistical analysis of the BjTT dataset. Furthermore, we conduct comprehensive experiments on the dataset with state-of-the-art traffic prediction methods and text-guided generative models, which reveal the unique characteristics of the BjTT. The dataset is available at https://github.com/ChyaZhang/BjTT.","1558-0016","","10.1109/TITS.2024.3440650","National Key Research and Development Program of China(grant numbers:2021ZD0111902); National Natural Science Foundation of China(grant numbers:62072015,U21B2038); Beijing Natural Science Foundation(grant numbers:4222021); Research and Development Program of Beijing Municipal Education Commission(grant numbers:KM202410005031,KM202410005031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10648646","Traffic prediction;large-scale;new dataset","Roads;Social networking (online);Data collection;Blogs;Meteorology;Traffic control;Predictive models;Large scale integration;Intelligent transportation systems","","2","","39","IEEE","26 Aug 2024","","","IEEE","IEEE Journals"
"Bridging M/EEG Source Imaging and Independent Component Analysis Frameworks Using Biologically Inspired Sparsity Priors","A. Ojeda; K. Kreutz-Delgado; J. Mishra","Neural Engineering and Translation Labs, Department of Psychiatry, and Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, CA 92093 U.S.A.; Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, CA 92093 U.S.A.; Neural Engineering and Translation Labs, Department of Psychiatry, University of California San Diego, CA 92093, U.S.A.",Neural Computation,"4 Dec 2023","2021","33","9","2408","2438","Electromagnetic source imaging (ESI) and independent component analysis (ICA) are two popular and apparently dissimilar frameworks for M/EEG analysis. This letter shows that the two frameworks can be linked by choosing biologically inspired source sparsity priors. We demonstrate that ESI carried out by the sparse Bayesian learning (SBL) algorithm yields source configurations composed of a few active regions that are also maximally independent from one another. In addition, we extend the standard SBL approach to source imaging in two important directions. First, we augment the generative model of M/EEG to include artifactual sources. Second, we modify SBL to allow for efficient model inversion with sequential data. We refer to this new algorithm as recursive SBL (RSBL), a source estimation filter with potential for online and offline imaging applications. We use simulated data to verify that RSBL can accurately estimate and demix cortical and artifactual sources under different noise conditions. Finally, we show that on real error-related EEG data, RSBL can yield single-trial source estimates in agreement with the experimental literature. Overall, by demonstrating that ESI can produce maximally independent sources while simultaneously localizing them in cortical space, we bridge the gap between the ESI and ICA frameworks for M/EEG analysis.","0899-7667","","10.1162/neco_a_01415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342565","","","","1","","","","4 Dec 2023","","","MIT Press","MIT Press Journals"
"Hierarchical vectorization for facial images","Q. Fu; L. Liu; F. Hou; Y. He","School of Computer Science and Engineering, Nanyang Technological University, 639798, Singapore; Data61, Commonwealth Scientific and Industrial Research Organisation, Sydney 2015, Australia; Interdisciplinary Graduate School, Nanyang Technological University and Alibaba Group, 639798, Singapore; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China; School of Computer Science and Engineering, Nanyang Technological University, 639798, Singapore",Computational Visual Media,"20 Feb 2025","2024","10","1","97","118","The explosive growth of social media means portrait editing and retouching are in high demand. While portraits are commonly captured and stored as raster images, editing raster images is non-trivial and requires the user to be highly skilled. Aiming at developing intuitive and easy-to-use portrait editing tools, we propose a novel vectorization method that can automatically convert raster images into a 3-tier hierarchical representation. The base layer consists of a set of sparse diffusion curves (DCs) which characterize salient geometric features and low-frequency colors, providing a means for semantic color transfer and facial expression editing. The middle level encodes specular highlights and shadows as large, editable Poisson regions (PRs) and allows the user to directly adjust illumination by tuning the strength and changing the shapes of PRs. The top level contains two types of pixel-sized PRs for high-frequency residuals and fine details such as pimples and pigmentation. We train a deep generative model that can produce high-frequency residuals automatically. Thanks to the inherent meaning in vector primitives, editing portraits becomes easy and intuitive. In particular, our method supports color transfer, facial expression editing, highlight and shadow editing, and automatic retouching. To quantitatively evaluate the results, we extend the commonly used FLIP metric (which measures color and feature differences between two images) to consider illumination. The new metric, illumination-sensitive FLIP, can effectively capture salient changes in color transfer results, and is more consistent with human perception than FLIP and other quality measures for portrait images. We evaluate our method on the FFHQR dataset and show it to be effective for common portrait editing tasks, such as retouching, light editing, color transfer, and expression editing.","2096-0662","","10.1007/s41095-022-0314-4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897676","face editing;vectorization;Poisson editing;color transfer;illumination editing;expression editing","Image color analysis;Vectors;Lighting;Laplace equations;Geometry;Faces;Videos;Shape;Image edge detection;Feature extraction","","","","","","20 Feb 2025","","","TUP","TUP Journals"
"Unleashing Competitive Intelligence: News Mining Analysis on Technology Trends and Digital Health Driving Healthcare Innovation","E. Cano-Marin; S. Sánchez-Alonso; M. Mora-Cantallops","Computer Science Department, University of Alcalá, Alcalá de Henares, Spain; Computer Science Department, University of Alcalá, Alcalá de Henares, Spain; Computer Science Department, University of Alcalá, Alcalá de Henares, Spain",IEEE Transactions on Engineering Management,"7 Aug 2024","2024","71","","12311","12325","In the rapidly evolving digital health landscape, technology plays a pivotal role in transforming the healthcare industry. With the exponential growth of data, uncovering valuable insights has become a daunting task. In today's data-driven world, healthcare businesses must leverage emerging technologies to stay informed about trends in their field. This research article presents a novel approach to deriving business insights in digital health enabled by technology, including artificial intelligence, and other cutting-edge advancements. We propose a methodology that utilizes news mining techniques and the global data on events, location, and tone database as the primary data source. By employing natural language processing, we developed a practical way of extracting relevant insights from vast amounts of public data. We implemented named-entity recognition (NER) enriched with the DBpedia knowledge base and relationship extraction. In addition, we leveraged graph analytics to identify and analyze the most significant concept relationships within the text corpus and their evolution in time. By integrating these advanced techniques, healthcare businesses can extract actionable insights from public datasets, empowering them to stay abreast of emerging trends and advancements in digital health, such as telehealth, precision medicine, or medical imaging.","1558-0040","","10.1109/TEM.2023.3326233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10314044","Digital health;emerging technologies;entity extraction;graph analytics;healthcare;innovation;natural language processing (NLP);news mining","Medical services;Data mining;Electronic healthcare;Market research;Telemedicine;Databases","","","","124","CCBY","9 Nov 2023","","","IEEE","IEEE Journals"
"Illustrating Classic Brazilian Books using a Text-To-Image Diffusion Model","F. Rodrigues Perche Mahlow; A. F. Zanella; W. A. Cruz Castañeda; R. Aparecida Sarzi-Ribeiro","Sao Paulo State University, Bauru, Sao Paulo, Brazil, BR; State University of Maringa, Maringa, Parana, Brazil, BR; Federal Technological University of Parana, Guarapuava, Parana, Brazil, MX; Sao Paulo State University, Bauru, Sao Paulo, Brazil, BR",IEEE Latin America Transactions,"11 Dec 2024","2024","22","12","1000","1008","In recent years, Generative Artificial Intelligence (GenAI) has undergone a profound transformation in addressing intricate tasks involving diverse modalities such as textual, auditory, visual, and pictorial generation. Within this spectrum, text-to-image (TTI) models have emerged as a formidable approach to generating varied and aesthetically appealing compositions, spanning applications from artistic creation to realistic facial synthesis, and demonstrating significant advancements in computer vision, image processing, and multimodal tasks. The advent of Latent Diffusion Models (LDMs) signifies a paradigm shift in the domain of AI capabilities. This article delves into the feasibility of employing the Stable Diffusion LDM to illustrate literary works. For this exploration, seven classic Brazilian books have been selected as case studies. The objective is to ascertain the practicality of this endeavor and to evaluate the potential of Stable Diffusion in producing illustrations that augment and enrich the reader's experience. We will outline the beneficial aspects, such as the capacity to generate distinctive and contextually pertinent images, as well as the drawbacks, including any shortcomings in faithfully capturing the essence of intricate literary depictions. Through this study, we aim to provide a comprehensive assessment of the viability and efficacy of utilizing AI-generated illustrations in literary contexts, elucidating both the prospects and challenges encountered in this pioneering application of technology.","1548-0992","","10.1109/TLA.2024.10789626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10789626","image generation;diffusion models;text-to-image;illustration","Artificial intelligence;Image synthesis;Diffusion models;Training;Visualization;Text to image;Noise reduction;Computational modeling;Refining;Ethics","","","","","IEEE","11 Dec 2024","","","IEEE","IEEE Journals"
"An Overview on Application of Machine Learning Techniques in Optical Networks","F. Musumeci; C. Rottondi; A. Nag; I. Macaluso; D. Zibar; M. Ruffini; M. Tornatore","Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy; Dalle Molle Institute for Artificial Intelligence, University of Lugano–University of Applied Science and Arts of Southern Switzerland, Lugano, Switzerland; School of Electrical and Electronic Engineering, University College Dublin, Dublin 4, Ireland; CONNECT, Electronic and Electrical Engineering, Trinity College Dublin, Dublin, Ireland; Fotonik, Technical University of Denmark, Lyngby, Denmark; CONNECT, School of Computer Science and Statistics, Trinity College Dublin, Dublin, Ireland; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy",IEEE Communications Surveys & Tutorials,"31 May 2019","2019","21","2","1383","1408","Today's telecommunication networks have become sources of enormous amounts of widely heterogeneous data. This information can be retrieved from network traffic traces, network alarms, signal quality indicators, users' behavioral data, etc. Advanced mathematical tools are required to extract meaningful information from these data and take decisions pertaining to the proper functioning of the networks from the network-generated data. Among these mathematical tools, machine learning (ML) is regarded as one of the most promising methodological approaches to perform network-data analysis and enable automated network self-configuration and fault management. The adoption of ML techniques in the field of optical communication networks is motivated by the unprecedented growth of network complexity faced by optical networks in the last few years. Such complexity increase is due to the introduction of a huge number of adjustable and interdependent system parameters (e.g., routing configurations, modulation format, symbol rate, coding schemes, etc.) that are enabled by the usage of coherent transmission/reception technologies, advanced digital signal processing, and compensation of nonlinear effects in optical fiber propagation. In this paper we provide an overview of the application of ML to optical communications and networking. We classify and survey relevant literature dealing with the topic, and we also provide an introductory tutorial on ML for researchers and practitioners interested in this field. Although a good number of research papers have recently appeared, the application of ML to optical networks is still in its infancy: to stimulate further work in this area, we conclude this paper proposing new possible research directions.","1553-877X","","10.1109/COMST.2018.2880039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8527529","Machine learning;data analytics;optical communications and networking;neural networks;bit error rate;optical signal-to-noise ratio;network monitoring","Optical fiber networks;Adaptive optics;Nonlinear optics;Artificial neural networks;Machine learning;Optical modulation","","437","","125","IEEE","8 Nov 2018","","","IEEE","IEEE Journals"
"Ethically Aligned Design - A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems","",,Ethically Aligned Design - A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems,"7 Apr 2021","2019","","","1","294","As the use and impact of autonomous and intelligent systems (A/IS) become pervasive, we need to establish societal and policy guidelines in order for such systems to remain human-centric, serving humanity’s values and ethical principles. These systems must be developed and should operate in a way that is beneficial to people and the environment, beyond simply reaching functional goals and addressing technical problems. This approach will foster the heightened level of trust between people and technology that is needed for its fruitful use in our daily lives.","","978-1-5044-7569-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9398613","autonomous and intelligent systems;A/IS;human-centric;intelligent technical systems;Ethics of Autonomous and Intelligent Systems;Ethically Aligned Design;EAD1e;Personal Data and Individual Agency","","","","","","","7 Apr 2021","","","IEEE","IEEE Standards"
"8 Speculative Futures","O. Bown",University of New South Wales,Beyond the Creative Species: Making Machines That Make Art and Music,"","2021","","","295","322","So fast moving is this area, and so muddied by various forms of hype and emotional responses, that speculating about where we might end up even in a couple of years, let alone a decade or so, is a risky activity. But speculation is not prediction, and certainly not prophecy. It serves to point to and examine different possible futures, to allow discussion of where we want to go, what might need to be done to get there, and what we might want to look out for. I draw on the topics and examples discussed so far to analyze such possibilities and to frame the most prominent themes appearing in the emerging future of computational creativity. Most of the trends imagined below are already apparent and have already been identified in the preceding chapters, and are merely being extrapolated given what we can see the technology capable of, framed from the perspective of distributed, networked creativity. I divide this concluding chapter into three areas: industry, covering the nature of creative production and the use of commercially developed computationally creative systems; society, covering the impact on cultural production and social behavior; and lastly, the most speculative and philosophical topic, the question of how machine creativity may cause us to reflect on our status as intelligent and creative beings.","","9780262361750","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9394882.pdf&bkn=9394486&pdfType=chapter","","","","","","","","5 Apr 2021","","","MIT Press","MIT Press eBook Chapters"
"1 Relevance of Cyber Security Innovations","S. Petrenko",NA,Cyber Security Innovation for the Digital Economy: A Case Study of the Russian Federation,"","2018","","","7","162","Cyber Security Innovation for the Digital Economy considers possible solutions to the relatively new scientific-technical problem of developing innovative solutions in the field of cyber security for the Digital Economy. The solutions proposed are based on the results of exploratory studies conducted by the author in the areas of Big Data acquisition, cognitive information technologies (cogno-technologies), new methods of analytical verification of digital ecosystems on the basis of similarity invariants and dimensions, and “computational cognitivism,” involving a number of existing models and methods. In practice, this successfully allowed the creation of new entities - the required safe and trusted digital ecosystems - on the basis of the development of digital and cyber security technologies, and the resulting changes in their behavioral preferences. Here, the ecosystem is understood as a certain system of organizations, created around a certain Technological Platform that use its services to make the best offers to customers and access to them to meet the ultimate needs of clients - legal entities and individuals. The basis of such ecosystems is a certain technological platform, created on advanced innovative developments, including the open interfaces and code, machine learning, cloud technologies, Big Data collection and processing, artificial intelligence technologies, etc. The mentioned Technological Platform allows creating the best offer for the client both from own goods and services and from the offers of external service providers in real time. This book contains four chapters devoted to the following subjects: • Relevance of the given scientific-technical problems in the cybersecurity of Digital Economy • Determination of the limiting capabilities • Possible scientific and technical solutions • Organization of perspective research studies in the area of Digital Economy cyber security in Russia.","","9788770220217","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9226791.pdf&bkn=9218856&pdfType=chapter","","","","","","","","20 Oct 2020","","","River Publishers","River eBook Chapters"
"A Survey of Privacy Risks and Mitigation Strategies in the Artificial Intelligence Life Cycle","S. Shahriar; S. Allana; S. M. Hazratifard; R. Dara","School of Computer Science, University of Guelph, Guelph, Canada; School of Computer Science, University of Guelph, Guelph, Canada; Department of Electrical and Computer Engineering, University of Victoria, Victoria, Canada; School of Computer Science, University of Guelph, Guelph, Canada",IEEE Access,"23 Jun 2023","2023","11","","61829","61854","Over the decades, Artificial Intelligence (AI) and machine learning has become a transformative solution in many sectors, services, and technology platforms in a wide range of applications, such as in smart healthcare, financial, political, and surveillance systems. In such applications, a large amount of data is generated about diverse aspects of our life. Although utilizing AI in real-world applications provides numerous opportunities for societies and industries, it raises concerns regarding data privacy. Data used in an AI system are cleaned, integrated, and processed throughout the AI life cycle. Each of these stages can introduce unique threats to individual’s privacy and have an impact on ethical processing and protection of data. In this paper, we examine privacy risks in different phases of the AI life cycle and review the existing privacy-enhancing solutions. We introduce four different categories of privacy risk, including (i) risk of identification, (ii) risk of making an inaccurate decision, (iii) risk of non-transparency in AI systems, and (iv) risk of non-compliance with privacy regulations and best practices. We then examined the potential privacy risks in each AI life cycle phase, evaluated concerns, and reviewed privacy-enhancing technologies, requirements, and process solutions to countermeasure these risks. We also reviewed some of the existing privacy protection policies and the need for compliance with available privacy regulations in AI-based systems. The main contribution of this survey is examining privacy challenges and solutions, including technology, process, and privacy legislation in the entire AI life cycle. In each phase of the AI life cycle, open challenges have been identified.","2169-3536","","10.1109/ACCESS.2023.3287195","Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155147","Artificial intelligence;machine learning;AI life cycle;privacy risk;privacy legislation;privacy enhancing solutions","Artificial intelligence;Privacy;Data privacy;Surveys;Regulation;Machine learning;Security","","20","","192","CCBYNCND","19 Jun 2023","","","IEEE","IEEE Journals"
"Effects of Microstate Dynamic Brain Network Disruption in Different Stages of Schizophrenia","T. Yan; G. Wang; T. Liu; G. Li; C. Wang; S. Funahashi; D. Suo; G. Pei","School of Life Science, Beijing Institute of Technology, Beijing, China; School of Life Science, Beijing Institute of Technology, Beijing, China; School of Life Science, Beijing Institute of Technology, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Neurosurgery, XuanWu Hospital, Capital Medical University, Beijing, China; Advanced Research Institute of Multidisciplinary Science, Beijing Institute of Technology, Beijing, China; School of Life Science, Beijing Institute of Technology, Beijing, China; School of Life Science, Beijing Institute of Technology, Beijing, China",IEEE Transactions on Neural Systems and Rehabilitation Engineering,"16 Jun 2023","2023","31","","2688","2697","Schizophrenia is a heterogeneous mental disorder with unknown etiology or pathological characteristics. Microstate analysis of the electroencephalogram (EEG) signal has shown significant potential value for clinical research. Importantly, significant changes in microstate-specific parameters have been extensively reported; however, these studies have ignored the information interactions within the microstate network in different stages of schizophrenia. Based on recent findings, since rich information about the functional organization of the brain can be revealed by functional connectivity dynamics, we use the first-order autoregressive model to construct the functional connectivity of intra- and intermicrostate networks to identify information interactions among microstate networks. We demonstrate that, beyond abnormal parameters, disrupted organization of the microstate networks plays a crucial role in different stages of the disease by 128-channel EEG data collected from individuals with first-episode schizophrenia, ultrahigh-risk, familial high-risk, and healthy controls. According to the characteristics of the microstates of patients at different stages, the parameters of microstate class A are reduced, those of class C are increased, and the transitions from intra- to intermicrostate functional connectivity are gradually disrupted. Furthermore, decreased integration of intermicrostate information might lead to cognitive deficits in individuals with schizophrenia and those in high-risk states. Taken together, these findings illustrate that the dynamic functional connectivity of intra- and intermicrostate networks captures more components of disease pathophysiology. Our work sheds new light on the characterization of dynamic functional brain networks based on EEG signals and provides a new interpretation of aberrant brain function in different stages of schizophrenia from the perspective of microstates.","1558-0210","","10.1109/TNSRE.2023.3283708","Scientific and Technological Innovation (STI) 2030–Major Projects(grant numbers:2022ZD0208500); National Natural Science Foundation of China(grant numbers:U20A20191,82071912,12104049,82202291); Fundamental Research Funds for the Central Universities(grant numbers:2023CX01024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10145423","Dynamic brain network;information interaction;microstates;resting-state EEG;schizophrenia stages","Electroencephalography;Mental disorders;Time series analysis;Diseases;Brain modeling;Electrodes;Pathology","Humans;Schizophrenia;Brain;Brain Mapping;Electroencephalography;Cognitive Dysfunction","8","","59","CCBYNCND","7 Jun 2023","","","IEEE","IEEE Journals"
"Open Datasets in Human Activity Recognition Research—Issues and Challenges: A Review","G. Alam; I. McChesney; P. Nicholl; J. Rafferty","School of Computing, Ulster University, Belfast, U.K.; School of Computing, Ulster University, Belfast, U.K.; School of Computing, Ulster University, Belfast, U.K.; School of Computing, Ulster University, Belfast, U.K.",IEEE Sensors Journal,"14 Nov 2023","2023","23","22","26952","26980","Huge amounts of data are generated with the emergence of new sensor technologies. Human activity recognition (HAR) datasets are generated from cameras, such as video or still images, capturing human behavior through sensors such as gyroscopes, Bluetooth, sound sensors, and accelerometers. These generated data sources are collected by the researchers and formed into open datasets. However, these datasets often show issues during dataset construction, sharing, and searching, which could produce further challenges for the reuse of the data by others. The main objective of this research is to explore the current issues and challenges faced by researchers in the HAR domain. A detail literature review was conducted to extract information from the published literature. Similarly, a questionnaire survey was sent to selected researchers having expertise in the HAR domain, who work with open datasets. The main issues and challenges were identified and classified into a hierarchical structure. This research will help HAR researchers to be aware of the current issues and challenges in the field of HAR open datasets. It will help to promote important attributes applicable to many open datasets, such as privacy, anonymity, platform maintenance, datasets’ descriptions, metadata, environmental conditions, resources, and training, while constructing and sharing new datasets.","1558-1748","","10.1109/JSEN.2023.3317645","Department for the Economy (DfE), Ulster University; School of Computing, Ulster University, Belfast Campus, U.K; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10272298","Artificial intelligence (AI);dataset quality;datasets’ issues and challenges;human activity recognition (HAR);open dataset lifecycle","Open data;Sensors;Human activity recognition;Surveys;Bibliographies;Medical services;Feature extraction","","2","","184","IEEE","4 Oct 2023","","","IEEE","IEEE Journals"
"A Survey of Deep Learning Techniques for Cybersecurity in Mobile Networks","E. Rodríguez; B. Otero; N. Gutiérrez; R. Canal","Department of Computer Architecture, Universitat Politecnica de Catalunya, Barcelona, Spain; Department of Computer Architecture, Universitat Politecnica de Catalunya, Barcelona, Spain; Department of Computer Architecture, Universitat Politecnica de Catalunya, Barcelona, Spain; Department of Computer Architecture, Universitat Politecnica de Catalunya, Barcelona, Spain",IEEE Communications Surveys & Tutorials,"20 Aug 2021","2021","23","3","1920","1955","The widespread use of mobile devices, as well as the increasing popularity of mobile services has raised serious cybersecurity challenges. In the last years, the number of cyberattacks has grown dramatically, as well as their complexity. Traditional cybersecurity systems have failed to detect complex attacks, unknown malware, and they do not guarantee the preservation of user privacy. Consequently, cybersecurity systems have embraced Deep Learning (DL) models as they provide efficient detection of novel attacks and better accuracy. This paper presents a comprehensive survey of recent cybersecurity works that use DL in mobile and wireless networks. It covers all cybersecurity aspects: infrastructure threads and attacks, software attacks and privacy preservation. First, we provide a detailed overview of DL techniques applied, or with potential applications, to cybersecurity. Then, we review cybersecurity works based on DL. For each cybersecurity threat or attack, we discuss the challenges for using DL methods. For each contribution, we review the implementation details and the performance of the solution. In a nutshell, this paper constitutes the first survey that provides a complete review of the DL methods for cybersecurity. Given the analysis performed, we identify the most effective DL methods for the different threats and attacks.","1553-877X","","10.1109/COMST.2021.3086296","Generalitat de Catalunya(grant numbers:2017SGR962); DRAC Project(grant numbers:001-P-001723); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9447833","Cyberattacks;deep learning;machine learning;mobile networking;privacy;security;wireless networking","Computer security;Malware;Security;Privacy;Computer crime;Software;Computer architecture","","35","","261","IEEE","7 Jun 2021","","","IEEE","IEEE Journals"
"WaveRecovery: Screen-shooting Watermarking based on Wavelet and Recovery","L. Fu; X. Liao; J. Guo; L. Dong; Z. Qin","College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; Faculty of System Engineering, National University of Defense Technology, Changsha, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2024","PP","99","1","1","The demand for resilient watermarking technology in the context of the screen-shooting scenario is steadily on the rise. The principal objective of this technique is to embed messages into the cover image, with the ability to effectively recover the message from the screen-captured image at the extraction end. However, current watermarking methods result in low visual quality watermarked images and are insufficiently robust in screen-shooting scenarios. This is mainly because they only utilize spatial domain information during embedding, and they do not consider the impact of noise that introduced during screen capturing. This paper introduces an innovative network framework, including the wavelet domain concatenation and recovery mechanism, to overcome the dual challenges encountered in robust watermarking, namely visual fidelity and robustness. For fidelity, we present a cascade network operating in the wavelet domain. This network excel at detecting watermark information in the wavelet domain. This capability makes it more sensitive to high and low-frequency details. Discrete wavelet transform can make CNN focus on different frequency characteristics, and the use of discrete inverse wavelet transform in upsampling can make the information bluehigh fidelity. As a result, it can more accurately identify and preserve critical visual details in this frequency domain, leading to an overall enhancement in visual quality. For robustness, a recovery network is specifically designed to mitigate the influence of noise introduced during screen-shooting on watermark information extraction. Experimental validation of our proposed method substantiates its effectiveness in significantly enhancing the visual quality and the accuracy of the watermarked images.","1558-2205","","10.1109/TCSVT.2024.3510355","National Natural Science Foundation of China(grant numbers:61972142,U22A2030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10772578","Robust watermarking;screen-shooting scenarios;wavelet domain;recovery network","Watermarking;Visualization;Noise;Wavelet domain;Robustness;Decoding;Distortion;Circuits and systems;Training;Frequency-domain analysis","","","","","IEEE","2 Dec 2024","","","IEEE","IEEE Early Access Articles"
"A Transdisciplinary Framework for Effective and Reliable Continuum of Care","",,A Transdisciplinary Framework for Effective and Reliable Continuum of Care,"7 Aug 2024","2024","","","1","37","There is a critical need to identify electronic health records without compromising the privacy and confidentiality of patient data. This white paper discusses important possible parameters associated with electronic health records that could be used for this identification. The core objective of this white paper is to set a stage for the development of IEEE standards associated with the identification of electronic health records, thereby discussing the feasibility and potential stakeholders for such standards.","","979-8-8557-1080-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10630697","confidentiality;electronic health record;Industry Connections;privacy standard development;white paper","","","","","","","7 Aug 2024","","","IEEE","IEEE Standards"
"Sourcebook in the Mathematics of Ancient Greece and the Eastern Mediterranean","",,Sourcebook in the Mathematics of Ancient Greece and the Eastern Mediterranean,"","2024","","","","","An invaluable reference book on the mathematics of Greek antiquityEuclid, Archimedes, and Apollonius are familiar names to many of us, and their contributions have shaped mathematical practice up to modern times. Yet the mathematical activity of Greek antiquity extended far beyond their achievements and was furthered by diverse individuals in different contexts. Sourcebook in the Mathematics of Ancient Greece and the Eastern Mediterranean brings together an extensive collection of primary source materials that document the extraordinary breadth of mathematical ideas developed in the Eastern Mediterranean from 500 BCE to 500 CE, a millennium in which Greek cultural influence spanned the ancient world.Weaving together ancient commentaries with the works themselves, Victor Katz and Clemency Montelle present a wealth of newly translated texts along with sources difficult to find elsewhere, from writings by the great mathematical thinkers of Greek antiquity to those by practitioners who used mathematics in everyday life. This comprehensive and wide-ranging sourcebook includes lesser-known authors who made critical contributions, sometimes in languages other than Greek, as well as accounts of technical instrumentation, papyri by anonymous authors designed for teaching purposes, and evidence of hand computations and numerical tables.An essential resource for anyone interested in the mathematical achievements of this remarkable intellectual culture, Sourcebook in the Mathematics of Ancient Greece and the Eastern Mediterranean encompasses disciplines that illustrate the important role of mathematics in ancient Greek society more broadly, from astronomy, music, and optics to philosophy, literature, and theater.","","9780691257686","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10614693.pdf&bkn=10614692&pdfType=book","Clemency Montelle;Greek mathematics;math;mathematical papyri;Proposition;Ptolemy;Sourcebook in Greek Mathematics;theoretical mathematics;Victor J. Katz;Euclid;Archimedes;Socrates;Greek;Apollonius;Pappus;Plato;Heron;Pythagorean;Glaucon;Eutocius;Archytas;Arabic;Diophantus;Conics;Theaetetus;Chord;Proclus;Aristarchus;Almagest;Eudoxus;Latin;Nicomachus;Antikythera;Mechanism;Egyptian;Eratosthenes;Boethius;Menelaus;Pythagorean Theorem;Hippocrates;Antikythera Mechanism;Strepsiades;Simplicius;Euclidean;Columella;Disciple;Meton;Western World;Great Books;Eudemus;Diocles;Tarentum;Theon;Conon;Peisthetaerus;Protarchus;Vitruvius;Geminus;Syene;Nicomedes;Sphaerica;Metrica;Museum;Venus;Thule;Mesopotamian;Sourcebook;Hypsicles;Theodorus;Polybius","","","","","","","30 Jul 2024","","","Princeton University Press","Princeton University Press eBooks"
"NoPeek-Infer: Preventing face reconstruction attacks in distributed inference after on-premise training","P. Vepakomma; A. Singh; E. Zhang; O. Gupta; R. Raskar",Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Lendbuzz; Massachusetts Institute of Technology,2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021),"12 Jan 2022","2021","","","1","8","For models trained on-premise but deployed in a distributed fashion across multiple entities, we demonstrate that minimizing distance correlation between sensitive data such as faces and intermediary representations enables prediction while preventing reconstruction attacks. Leakage (measured using distance correlation between input and intermediate representations) is the risk associated with the reconstruction of raw face data from intermediary representations that are communicated in a distributed setting. We demonstrate on face datasets that our method is resilient to reconstruction attacks during distributed inference while maintaining information required to sustain good classification accuracy. We share modular code for performing NoPeek-Infer at http://tiny.cc/nopeek along with corresponding trained models for benchmarking attack techniques.","","978-1-6654-3176-7","10.1109/FG52635.2021.9667085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9667085","","Training;Privacy;Correlation;Computational modeling;Distributed databases;Machine learning;Predictive models","","3","","50","IEEE","12 Jan 2022","","","IEEE","IEEE Conferences"
"Adversarial Examples for Image Cropping: Gradient-Based and Bayesian-Optimized Approaches for Effective Adversarial Attack","M. Yoshida; H. Namura; M. Okuda","Graduate School of Science and Engineering, Doshisha University, Kyoto, Japan; Graduate School of Science and Engineering, Doshisha University, Kyoto, Japan; Faculty of Science and Engineering, Doshisha University, Kyoto, Japan",IEEE Access,"25 Jun 2024","2024","12","","86541","86552","In this study, we propose novel approaches for generating adversarial examples targeting machine learning-based image cropping systems. Image cropping is crucial for meeting display space restrictions and highlighting content’s interest areas. However, existing image cropping systems often miss user-intended areas, have necessities to remove inherent biases in light of AI fairness, or might expose users to legal risks. To address these issues, our paper introduces approaches for effectively creating adversarial examples in both black-box and white-box settings. In the white-box approach, we utilize gradient-based perturbations focusing on the model’s blurring layer and targeting effective areas. For the black-box approach, even for models where gradient information is unavailable, we levered pixel attacks with Bayesian optimization and patch attacks to effectively narrow the search space. We also introduce a novel quantitative evaluation method for image cropping by measuring shifts in gaze saliency map peak values, reflecting a typical scenario with social network services. Our results suggest that our approaches not only outperform existing methods but also exhibit the potential to be an effective solution to the problems even with models on actual platforms.","2169-3536","","10.1109/ACCESS.2024.3415356","Japan Science and Technology Agency (JST) Support for Pioneering Research Initiated by the Next Generation (SPRING)(grant numbers:JPMJSP2129); Japan Society for the Promotion of Science (JSPS) KAKENHI(grant numbers:23K11174); Ministry of Education, Culture, Sports, Science and Technology (MEXT), Japan, Promotion of Distinctive Joint Research Center Program(grant numbers:JPMXP 0621467946); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10559590","Adversarial examples;image cropping;object detection;saliency map;Twitter","Glass box;Computational modeling;Perturbation methods;Predictive models;Closed box;Gaussian noise;Data models;Adversarial machine learning;Object detection;Social networking (online)","","1","","42","CCBYNCND","17 Jun 2024","","","IEEE","IEEE Journals"
"GaitC3I: Robust Cross-Covariate Gait Recognition via Causal Intervention","J. Wang; S. Hou; X. Guo; Y. Huang; Y. Huang; T. Zhang; L. Wang","Department of Automation, School of Information Science and Technology, University of Science and Technology of China, Hefei, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Computer Science, Wuhan University, China; New Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; Department of Automation, School of Information Science and Technology, University of Science and Technology of China, Hefei, China; New Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Cross-covariate gait recognition aims to analyze a pedestrian’s gait to extract an identity representation that is invariant across varying covariates. However, prevailing methods that have achieved good results on controlled in-the-lab datasets often perform poorly on realistic datasets. In this work, we find a significant cause is that the widely used pairwise metric learning paradigm cannot correctly handle the relationship between samples from different covariate conditions. Even worse, it may yield harmful signals that inadvertently mislead models to focus on covariate-related features, particularly when covariate distributions vary across subjects. To address this issue, we propose a Cross-Covariate Causal Intervention (GaitC3I) framework, a unified causality-inspired approach aimed at enhancing the robustness of gait recognition across diverse conditions. Specifically, our method consists of two parts: (i) an effective causal intervention metric learning paradigm based on backdoor adjustment, which strategically mitigates spurious correlations induced by covariates, thus ensuring a more invariant gait representation; and (ii) an annotation-free selection strategy that progressively matches each positive sample with negative samples from similar covariate conditions at various granularities. We demonstrate the effectiveness of our GaitC3I through extensive evaluation on six popular gait datasets–Gait3D, GREW, OUMVLP, CASIA-B, CCPG, and CCGR–achieving substantial improvements. Our method not only outperforms existing state-of-the-art models but also provides a systematic solution to remove the spurious correlations in gait recognition.","1558-2205","","10.1109/TCSVT.2025.3545210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10902484","Identification of persons;biometrics;gait recognition;debiasing;causal inference;backdoor adjustment","Gait recognition;Measurement;Correlation;Feature extraction;Visualization;Data mining;Clothing;Training;Annotations;Electronic mail","","","","","IEEE","25 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Decoding Large Language Models: An exhaustive guide to understanding, implementing, and optimizing LLMs for NLP applications","I. Cronin",NA,"Decoding Large Language Models: An exhaustive guide to understanding, implementing, and optimizing LLMs for NLP applications","","2024","","","","","Explore the architecture, development, and deployment strategies of large language models to unlock their full potentialKey FeaturesGain in-depth insight into LLMs, from architecture through to deploymentLearn through practical insights into real-world case studies and optimization techniquesGet a detailed overview of the AI landscape to tackle a wide variety of AI and NLP challengesPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionEver wondered how large language models (LLMs) work and how they're shaping the future of artificial intelligence? Written by a renowned author and AI, AR, and data expert, Decoding Large Language Models is a combination of deep technical insights and practical use cases that not only demystifies complex AI concepts, but also guides you through the implementation and optimization of LLMs for real-world applications. You’ll learn about the structure of LLMs, how they're developed, and how to utilize them in various ways. The chapters will help you explore strategies for improving these models and testing them to ensure effective deployment. Packed with real-life examples, this book covers ethical considerations, offering a balanced perspective on their societal impact. You’ll be able to leverage and fine-tune LLMs for optimal performance with the help of detailed explanations. You’ll also master techniques for training, deploying, and scaling models to be able to overcome complex data challenges with confidence and precision. This book will prepare you for future challenges in the ever-evolving fields of AI and NLP. By the end of this book, you’ll have gained a solid understanding of the architecture, development, applications, and ethical use of LLMs and be up to date with emerging trends, such as GPT-5.What you will learnExplore the architecture and components of contemporary LLMsExamine how LLMs reach decisions and navigate their decision-making processImplement and oversee LLMs effectively within your organizationMaster dataset preparation and the training process for LLMsHone your skills in fine-tuning LLMs for targeted NLP tasksFormulate strategies for the thorough testing and evaluation of LLMsDiscover the challenges associated with deploying LLMs in production environmentsDevelop effective strategies for integrating LLMs into existing systemsWho this book is forIf you’re a technical leader working in NLP, an AI researcher, or a software developer interested in building AI-powered applications, this book is for you. To get the most out of this book, you should have a foundational understanding of machine learning principles; proficiency in a programming language such as Python; knowledge of algebra and statistics; and familiarity with natural language processing basics.","","9781835081808","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10803969.pdf&bkn=10803968&pdfType=book","","","","","","","","16 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"Object removal from complex videos using a few annotations","T. T. Le; A. Almansa; Y. Gousseau; S. Masnou","LTCI, Télécom ParisTech, Université Paris-Saclay, 75013 Paris, France; MAP5, CNRS & Université Paris Descartes, 75006 Paris, France; LTCI, Télécom ParisTech, Université Paris-Saclay, 75013 Paris, France; Univ Lyon, Université Claude Bernard Lyon 1, CNRS UMR 5208, Institut Camille Jordan, 69622 Villeurbanne, France",Computational Visual Media,"20 Feb 2025","2019","5","3","267","291","We present a system for the removal of objects from videos. As input, the system only needs a user to draw a few strokes on the first frame, roughly delimiting the objects to be removed. To the best of our knowledge, this is the first system allowing the semi-automatic removal of objects from videos with complex backgrounds. The key steps of our system are the following: after initialization, segmentation masks are first refined and then automatically propagated through the video. Missing regions are then synthesized using video inpainting techniques. Our system can deal with multiple, possibly crossing objects, with complex motions, and with dynamic textures. This results in a computational tool that can alleviate tedious manual operations for editing high-quality videos.","2096-0662","","10.1007/s41095-019-0145-0","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897423","object removal;object segmentation;object tracking;video inpainting;video completion","Videos;Object segmentation;Annotations;Accuracy;Motion segmentation;Image reconstruction;Dynamics;Proposals;Adaptive optics;Semantic segmentation","","","","","","20 Feb 2025","","","TUP","TUP Journals"
"A City Is Not a Computer: Other Urban Intelligences","S. Mattern",NA,A City Is Not a Computer: Other Urban Intelligences,"","2021","","","","","A bold reassessment of ""smart cities"" that reveals what is lost when we conceive of our urban spaces as computersComputational models of urbanism—smart cities that use data-driven planning and algorithmic administration—promise to deliver new urban efficiencies and conveniences. Yet these models limit our understanding of what we can know about a city. A City Is Not a Computer reveals how cities encompass myriad forms of local and indigenous intelligences and knowledge institutions, arguing that these resources are a vital supplement and corrective to increasingly prevalent algorithmic models.Shannon Mattern begins by examining the ethical and ontological implications of urban technologies and computational models, discussing how they shape and in many cases profoundly limit our engagement with cities. She looks at the methods and underlying assumptions of data-driven urbanism, and demonstrates how the ""city-as-computer"" metaphor, which undergirds much of today's urban policy and design, reduces place-based knowledge to information processing. Mattern then imagines how we might sustain institutions and infrastructures that constitute more diverse, open, inclusive urban forms. She shows how the public library functions as a steward of urban intelligence, and describes the scales of upkeep needed to sustain a city's many moving parts, from spinning hard drives to bridge repairs.Incorporating insights from urban studies, data science, and media and information studies, A City Is Not a Computer offers a visionary new approach to urban planning and design.","","9780691226750","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9519686.pdf&bkn=9519685&pdfType=book","","","","","","","","20 Aug 2021","","","Princeton University Press","Princeton University Press eBooks"
"Multiple Agents based Disaster Prediction for Public Environments using Data Mining Techniques","U. K. Malviya; S. P. S. Chauhan","Department of CSE(IOT), Noida Institute of Engineering and Technology, Greater Noida, Uttar Pradesh, India; Department of Computer Science and Engineering, Galgotias University, Greater Noida, Uttar Pradesh, India",2023 2nd International Conference for Innovation in Technology (INOCON),"19 Apr 2023","2023","","","1","7","Real-time data on natural disasters are collected, explained, analysed, predicted, and shown in the disaster management system. The development of GIS-based informational understanding has been documented (GIS). Using GIS and geographic data mining, the disaster management approach can pinpoint the epicentre of an occurrence and direct relief workers along the safest possible paths to the scene. The precise geological state and geographical placement of many areas makes them vulnerable to a wide range of natural disasters, including earthquakes, floods, land debris, landslides, cloud bursts, and human casualties. An efficient real-time system for predicting natural occurrences and locations is necessary to minimise damages and suffering. This research presents a unique methodology for predicting the location of disasters using density-based spatiotemporal clustering and global positioning system data. Before implementing clustering and feature selection, the process of data cleansing removes redundant, irrelevant, and inconsistent information from the news databases based on natural events. Areas prone to natural disasters like earthquakes, floods, landslides, and so on will be culled using a spatiotemporal clustering technique. The clustered data is then sorted by terms associated with natural catastrophes, and features are selected accordingly. In order to aid event detectors and location estimators, extracted features are supplied to a decision tree, which then categorises the data into both positive and negative classes.","","979-8-3503-2092-3","10.1109/INOCON57975.2023.10101148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10101148","Geograpic data mining;Disasters;Detectors and Location estimators","Landslides;Earthquakes;Disaster management;Feature extraction;Real-time systems;Spatiotemporal phenomena;Data mining","","","","20","IEEE","19 Apr 2023","","","IEEE","IEEE Conferences"
"Educational Opportunities and Challenges in Augmented Reality: Featuring Implementations in Physics Education","J. W. Lai; K. H. Cheong","Science, Mathematics and Technology Cluster, Singapore University of Technology and Design (SUTD), Singapore; Science, Mathematics and Technology Cluster, Singapore University of Technology and Design (SUTD), Singapore",IEEE Access,"29 Apr 2022","2022","10","","43143","43158","This review paper provides the conceptualization and development of augmented reality (AR) environment for education by featuring implementations in physics education. The use of AR creates an environment designed to fully incorporate next-generation AR-aided notes, virtual laboratory and interactive problem-based learning with real-time automated generation of application-centric scenarios. This can be carried out via the fusion and technologizing of pre-existing teaching materials (such as books and notes) using AR and be mobile device friendly to fully leverage on learning beyond classrooms. Such a method is proposed to give students the access to resources anytime, anywhere without the spatial and temporal restrictions of synchronous-learning. This review discusses the advances of AR as an important tool in physics education, identify potential challenges and envisions the future by surveying recent trends and reviews. We provide perspective on practical AR implementation and evaluation for educators and school administrator, and potential academic advances through physics education research for researchers.","2169-3536","","10.1109/ACCESS.2022.3166478","Singapore Ministry of Education (MOE) under the Tertiary Education Research Fund(grant numbers:MOE2019-TRF-048); Education Research Funding Program; National Institute of Education (NIE)(grant numbers:DEV 03/21 CKH); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9755165","Augmented reality;immersive technology;education development;physics education research","Physics education;Research and development;Visualization;Educational courses;Mobile handsets;Augmented reality","","20","","101","CCBY","11 Apr 2022","","","IEEE","IEEE Journals"
"Credit Card Fraud Detection based on Ensemble Machine Learning Classifiers","K. J; A. Senthilselvi","Department of Computer Science and Engineering, SRM Institute of Science and Technology, Chennai, India; Department of Computer Science and Engineering, SRM Institute of Science and Technology, Chennai, India",2022 3rd International Conference on Electronics and Sustainable Communication Systems (ICESC),"19 Sep 2022","2022","","","1604","1610","Credit card is considered as one of the most popular paying methods for online and regular purchases, due to the advancement in communication and electronic commerce systems. Thus, the fraud associated with these transactions increased significantly. The great utilization of electronic payment is highly affected by this fraudulent transactions, which requires urgent detection to solve this issue. Therefore, effective and efficient approaches to detect fraud in credit card transactions are needed. To catch the fraudulent transaction, a good fitting model is needed, hence researchers recommends the use of various Machine Learning (ML) techniques, because of its beneficial characteristics. The main aim of the research work is to implement an ensemble based ML techniques for Credit Card Fraud Detection (CCFD). The strength of our model is a combination of the forces of the three subsystems; Recursive Feature Elimination (RFE), CCFD's using ensemble classifiers, and Synthetic Minority Oversampling (S MOTE) to deal with the problem of unbalanced data to identify the most effective prediction features. The proposed model run typical tests on two real databases of public credit card transactions, including fraudulent and official ones. Based on the comparison of other ML methods, the extra tree classifier has performed better and achieved high efficiencies such as 96% of accuracy and 57.95% of F1-measure.","","978-1-6654-7971-4","10.1109/ICESC54411.2022.9885649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9885649","Credit Card Fraud Detection;Synthetic Minority Oversampling;Imbalance Dataset;Machine Learning;Recursive Feature Elimination","Adaptation models;Fitting;Predictive models;Credit cards;Feature extraction;Data models;Fraud","","2","","40","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"Natural Language Processing for Affective, Psychological, and Content Analysis","A. Kumar","University of London, UK; University of Delhi, India",Language Intelligence: Expanding Frontiers in Natural Language Processing,"","2025","","","159","221","Summary <p>This chapter investigates NLP's role in analyzing affective and psychological aspects of language, focusing on sentiment analysis, emotion recognition, and psychometric NLP. It also delves into specific content analysis tasks like sarcasm detection, humor recognition, and identifying distress indicators such as depression and anxiety. These applications highlight the capability of NLP to interpret human emotions and psychological states from textual data.</p>","","9781394297283","10.1002/9781394297290.ch7","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10830638.pdf&bkn=10830550&pdfType=chapter","","Sentiment analysis;Transfer learning;Analytical models;Transformers;Neural networks;Bayes methods;Zero shot learning;Reviews;Psychology;Organizations","","","","","","7 Jan 2025","","","IEEE","Wiley-IEEE Press eBook Chapters"
"1 Digital Forensics Meets AI: A Game-changer for the 4th Industrial Revolution","",,Artificial Intelligence and Blockchain in Digital Forensics,"","2022","","","1","20","Digital forensics is the science of detecting evidence from digital media like a computer, smart phone, server, or network. It provides the forensic team with the most beneficial methods to solve confused digital-related cases. AI and blockchain can be applied to solve online predatory chat cases and photo forensics cases, provide network service evidence, custody of digital files in forensic medicine, and identify roots of data scavenging. The increased use of PCs and extensive use of internet access, has meant easy availability of hacking tools. Over the past two decades, improvements in the information technology landscape have made the collection, preservation, and analysis of digital evidence extremely important. The traditional tools for solving cybercrimes and preparing court cases are making investigations difficult. We can use AI and blockchain design frameworks to make the digital forensic process efficient and straightforward. AI features help determine the contents of a picture, detect spam email messages and recognize swatches of hard drives that could contain suspicious files. Blockchain-based lawful evidence management schemes can supervise the entire evidence flow of all of the court data. This book can provide a wide-ranging overview of how AI and blockchain can be used to solve problems in digital forensics using advanced tools and applications available on the market.","","9788770226875","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9933568.pdf&bkn=9933495&pdfType=chapter","","","","","","","","31 Oct 2022","","","River Publishers","River eBook Chapters"
"Newton the Alchemist: Science, Enigma, and the Quest for Nature's "Secret Fire"","W. Newman",NA,"Newton the Alchemist: Science, Enigma, and the Quest for Nature's ""Secret Fire""","","2019","","","","","A book that finally demystifies Newton’s experiments in alchemyWhen Isaac Newton’s alchemical papers surfaced at a Sotheby’s auction in 1936, the quantity and seeming incoherence of the manuscripts were shocking. No longer the exemplar of Enlightenment rationality, the legendary physicist suddenly became “the last of the magicians.” Newton the Alchemist unlocks the secrets of Newton’s alchemical quest, providing a radically new understanding of the uncommon genius who probed nature at its deepest levels in pursuit of empirical knowledge.In this evocative and superbly written book, William Newman blends in-depth analysis of newly available texts with laboratory replications of Newton’s actual experiments in alchemy. He does not justify Newton’s alchemical research as part of a religious search for God in the physical world, nor does he argue that Newton studied alchemy to learn about gravitational attraction. Newman traces the evolution of Newton’s alchemical ideas and practices over a span of more than three decades, showing how they proved fruitful in diverse scientific fields. A precise experimenter in the realm of “chymistry,” Newton put the riddles of alchemy to the test in his lab. He also used ideas drawn from the alchemical texts to great effect in his optical experimentation. In his hands, alchemy was a tool for attaining the material benefits associated with the philosopher’s stone and an instrument for acquiring scientific knowledge of the most sophisticated kind.Newton the Alchemist provides rare insights into a man who was neither Enlightenment rationalist nor irrational magus, but rather an alchemist who sought through experiment and empiricism to alter nature at its very heart.","","9780691185033","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9453262.pdf&bkn=9453261&pdfType=book","Isaac Newton;nature;alchemy;chymistry;optical experimentation;scientific knowledge;Enlightenment;empiricism;reason;chymical research;secret fire;natural world;chymical studies;experimental philosophy;philosophers' stone;adept;exegesis;biblical prophecy;aurific art;ancient mythology;metals;mining;Eirenaeus Philalethes;Nicolas Flamel;education;Free Grammar School;Trinity College;Treatise of Chymistry;Robert Boyle;Benedictine Basilius Valentinus;optical research;optics;color theory;treatise;Humores minerales;Of Natures obvious laws & processes in vegetation;sea salt;niter;vegetability;Michael Sendivogius;antimony;lead;Philalethes;florilegium;Keynes 35;Johann de Monte–Snyders;alchemist;Johann de Monte-Snyders;Sendivogius;Keynes MS 58;Ramon Lull;Epistola ad Theodorum Mundanum;Opera;Nicolas Fatio de Duillier;experimental notebooks;chymical laboratory;CU Add. 3973;sophic sal ammoniac;antimonial sublimate;copper vitriol;sal ammoniac;our Venus;scientific collaboration;Three Mysterious Fires;Keynes 58;caduceus of Mercury;scythe of Saturn;Praxis;alchemical text;florilegium style;laboratory notebooks;chymist;Captain Hylliard;William Yworth;lorilegia;refraction theory;sulfur;Opticks;Hypothesis of Light;chymistry of light;refractive power;color;phlogiston theory;sophic mercury;gold;alchemists;chrysopoeia","","","","","","","13 Jul 2021","","","Princeton University Press","Princeton University Press eBooks"
"Current and Evolving Applications to Network Cybersecurity","D. Minoli; B. Occhiogrosso","Stevens Institute of Technology, AT&T, Red Bank, NJ; NA",AI Applications to Communications and Information Technologies: The Role of Ultra Deep Neural Networks,"","2024","","","347","405","This chapter focuses on cybersecurity challenges in the information and communications technology (ICT, also known as IT) arena and on machine learning (ML) methods to address some of these critical concerns. ML has many applications in cybersecurity including identifying network cyber threats and enhancing host antivirus software. Cybersecurity concerns deal with at least four environments: ICT corporate networks, intranets, computing resources/databases and websites; (home) personal devices including laptops, smart phones and intelligent appliances; distributed Internet of Things sensors and devices; and service provider networks of all types, also including cloud‐based services. Security Policy and Defense in Depth are two important elements to deal with corporate cybersecurity risks. A blockchain is a cryptographically‐linked list of blocks created by nodes, where each block has a header, the relevant transaction data to be protected, and relevant security metadata.","","9781394190027","10.1002/9781394190034.ch6","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10320143.pdf&bkn=10320140&pdfType=chapter","","Security;Data breach;Computer security;Information and communication technology;Grippers;Computer viruses;Computer hacking","","","","","","16 Nov 2023","","","IEEE","Wiley-IEEE Press eBook Chapters"
"Efficient Algorithm Design: Unlock the power of algorithms to optimize computer programming","M. Makrehchi",NA,Efficient Algorithm Design: Unlock the power of algorithms to optimize computer programming,"","2024","","","","","Master advanced algorithm design techniques to tackle complex programming challenges and optimize application performanceKey FeaturesDevelop advanced algorithm design skills to solve modern computational problemsLearn state-of-the-art techniques to deepen your understanding of complex algorithmsApply your skills to real-world scenarios, enhancing your expertise in today's tech landscapePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionEfficient Algorithm Design redefines algorithms, tracing the evolution of computer science as a discipline bridging natural science and mathematics. Author Masoud Makrehchi, PhD, with his extensive experience in delivering publications and presentations, explores the duality of computers as mortal hardware and immortal algorithms. The book guides you through essential aspects of algorithm design and analysis, including proving correctness and the importance of repetition and loops. This groundwork sets the stage for exploring algorithm complexity, with practical exercises in design and analysis using sorting and search as examples. Each chapter delves into critical topics such as recursion and dynamic programming, reinforced with practical examples and exercises that link theory with real-world applications. What sets this book apart is its focus on the practical application of algorithm design and analysis, equipping you to solve real programming challenges effectively. By the end of this book, you’ll have a deep understanding of algorithmic foundations and gain proficiency in designing efficient algorithms, empowering you to develop more robust and optimized software solutions. What you will learnGain skills in advanced algorithm design for better problem-solvingUnderstand algorithm correctness and complexity for robust softwareApply theoretical concepts to real-world scenarios for practical solutionsMaster sorting and search algorithms, understanding their synergyExplore recursion and recurrence for complex algorithmic structuresLeverage dynamic programming to optimize algorithmsGrasp the impact of data structures on algorithm efficiency and designWho this book is forIf you’re a software engineer, computer scientist, or a student in a related field looking to deepen your understanding of algorithm design and analysis, this book is tailored for you. A foundation in programming and a grasp of basic mathematical concepts is recommended. It's an ideal resource for those already familiar with the basics of algorithms who want to explore more advanced topics. Data scientists and AI developers will find this book invaluable for enhancing their algorithmic approaches in practical applications.","","9781835886830","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769243.pdf&bkn=10769242&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Decentralized Identity Explained: Embrace decentralization for a more secure and empowering digital experience","R. Pinto",NA,Decentralized Identity Explained: Embrace decentralization for a more secure and empowering digital experience,"","2024","","","","","Delve into the cutting-edge trends of decentralized identities, blockchains, and other digital identity management technologies and leverage them to craft seamless digital experiences for both your customers and employees Key FeaturesExplore decentralized identities and blockchain technology in depthGain practical insights for leveraging advanced digital identity management tools, frameworks, and solutionsDiscover best practices for integrating decentralized identity solutions into existing systemsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionLooking forward to mastering digital identity? This book will help you get to grips with complete frameworks, tools, and strategies for safeguarding personal data, securing online transactions, and ensuring trust in digital interactions in today's cybersecurity landscape. Decentralized Identity Explained delves into the evolution of digital identities, from their historical roots to the present landscape and future trajectories, exploring crucial concepts such as IAM, the significance of trust anchors and sources of truth, and emerging trends such as SSI and DIDs. Additionally, you’ll gain insights into the intricate relationships between trust and risk, the importance of informed consent, and the evolving role of biometrics in enhancing security within distributed identity management systems. Through detailed discussions on protocols, standards, and authentication mechanisms, this book equips you with the knowledge and tools needed to navigate the complexities of digital identity management in both current and future cybersecurity landscapes. By the end of this book, you’ll have a detailed understanding of digital identity management and best practices to implement secure and efficient digital identity frameworks, enhancing both organizational security and user experiences in the digital realm.What you will learnUnderstand the need for security, privacy, and user-centric methodsGet up to speed with the IAM security frameworkExplore the crucial role of sources of truth in identity data verificationDiscover best practices for implementing access control listsGain insights into the fundamentals of informed consentDelve into SSI and understand why it mattersExplore identity verification methods such as knowledge-based and biometricWho this book is forThis book is for cybersecurity professionals and IAM engineers/architects who want to learn how decentralized identity helps to improve security and privacy and how to leverage it as a trust framework for identity management. ","","9781804614549","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10803999.pdf&bkn=10803998&pdfType=book","","","","","","","","16 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"The Synthetic University: How Higher Education Can Benefit from Shared Solutions and Save Itself","J. L. Shulman",NA,The Synthetic University: How Higher Education Can Benefit from Shared Solutions and Save Itself,"","2023","","","","","A bold, collaborative vision for combatting the ever-rising cost of collegeUS colleges and universities have long been the envy of the world. Institutional autonomy has fostered creativity among faculty, students, and staff. But this autonomy means that colleges tend to create their own solutions for every need. As a result, higher education suffers from costly redundancies that drive tuitions ever upward, putting higher education, essential to the fabric of the country, at risk. Instead of wishful thinking about collaboration or miraculous subsidies, The Synthetic University describes intermediary organizations that can provide innovative, cost-effective solutions.Offering answers to challenges jointly faced by thousands of institutions, James Shulman lays out a compelling new vision of how to reduce spending while enabling schools to maintain their particular contributions. He explains why colleges are so resistant to change and presents illuminating case studies of mission-driven and market-supported entrepreneurial organizations—such as the student tracking infrastructure of the National Student Clearinghouse or the ambitious effort of classics professors to create a shared transinstitutional department. Mixing theory with lessons drawn from his own experience, he demonstrates how to finance and implement the organizations that can synthesize much-needed solutions.A road map for sustained institutional change, The Synthetic University shows how to overcome colleges’ do-it-yourself impulses, avoid the threat of disruption, and preserve the institutions that we need to conduct basic research, foster innovation, and prepare diverse students to lead meaningful and productive lives.","","9780691237626","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10172382.pdf&bkn=10172381&pdfType=book","","","","","","","","4 Jul 2023","","","Princeton University Press","Princeton University Press eBooks"
"The Best Writing on Mathematics 2018","",,The Best Writing on Mathematics 2018,"","2019","","","","","The year’s finest mathematical writing from around the worldThis annual anthology brings together the year’s finest mathematics writing from around the world. Featuring promising new voices alongside some of the foremost names in the field, The Best Writing on Mathematics 2018 makes available to a wide audience many pieces not easily found anywhere else—and you don’t need to be a mathematician to enjoy them. These essays delve into the history, philosophy, teaching, and everyday aspects of math, offering surprising insights into its nature, meaning, and practice—and taking readers behind the scenes of today’s hottest mathematical debates.James Grime shows how to build subtly mischievous dice for playing slightly unfair games and Michael Barany traces how our appreciation of the societal importance of mathematics has developed since World War II. In other essays, Francis Su extolls the inherent values of learning, doing, and sharing mathematics, and Margaret Wertheim takes us on a mathematical exploration of the mind and the world—with glimpses at science, philosophy, music, art, and even crocheting. And there’s much, much more.In addition to presenting the year’s most memorable math writing, this must-have anthology includes an introduction by the editor and a bibliography of other notable pieces on mathematics.This is a must-read for anyone interested in where math has taken us—and where it is headed.","","9780691188720","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9453374.pdf&bkn=9453373&pdfType=book","","","","","","","","13 Jul 2021","","","Princeton University Press","Princeton University Press eBooks"
"Privacy-Preserving Machine Learning: A use-case-driven approach to building and protecting ML pipelines from privacy and security threats","S. R. Aravilli; S. Hamilton",NA; NA,Privacy-Preserving Machine Learning: A use-case-driven approach to building and protecting ML pipelines from privacy and security threats,"","2024","","","","","Gain hands-on experience in data privacy and privacy-preserving machine learning with open-source ML frameworks, while exploring techniques and algorithms to protect sensitive data from privacy breaches Key FeaturesUnderstand machine learning privacy risks and employ machine learning algorithms to safeguard data against breachesDevelop and deploy privacy-preserving ML pipelines using open-source frameworksGain insights into confidential computing and its role in countering memory-based data attacksPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionPrivacy regulations are evolving each year and compliance with privacy regulations is mandatory for every enterprise. Machine learning engineers are required to not only analyze large amounts of data to gain crucial insights, but also comply with privacy regulations to protect sensitive data. This may seem quite challenging considering the large volume of data involved and lack of in-depth expertise in privacy-preserving machine learning. This book delves into data privacy, machine learning privacy threats, and real-world cases of privacy-preserving machine learning, as well as open-source frameworks for implementation. You’ll be guided through developing anti-money laundering solutions via federated learning and differential privacy. Dedicated sections also address data in-memory attacks and strategies for safeguarding data and ML models. The book concludes by discussing the necessity of confidential computation, privacy-preserving machine learning benchmarks, and cutting-edge research. By the end of this machine learning book, you’ll be well-versed in privacy-preserving machine learning and know how to effectively protect data from threats and attacks in the real world.What you will learnStudy data privacy, threats, and attacks across different machine learning phasesExplore Uber and Apple cases for applying differential privacy and enhancing data securityDiscover IID and non-IID data sets as well as data categoriesUse open-source tools for federated learning (FL) and explore FL algorithms and benchmarksUnderstand secure multiparty computation with PSI for large dataGet up to speed with confidential computation and find out how it helps data in memory attacksWho this book is forThis book is for data scientists, machine learning engineers, and privacy engineers who have working knowledge of mathematics as well as basic knowledge in any one of the ML frameworks (TensorFlow, PyTorch, or scikit-learn). ","","9781800564220","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10540165.pdf&bkn=10540164&pdfType=book","","","","","","","","28 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"IEEE Approved Standard for the Functional Verification Language e","",,"IEEE P1647/D3, April 2019","18 Jun 2019","2019","","","1","652","The e functional verification language is an application-specific programming language, aimed at automating the task of verifying a hardware or software design with respect to its specification. Verification environments written in e provide a model of the environment in which the design is expected to function, including the kinds of erroneous conditions the design needs to withstand. A typical verification environment is capable of generating user-controlled test inputs with statistically interesting characteristics. Such an environment can check the validity of the design responses. Functional coverage metrics are used to control the verification effort and gauge the quality of the design. e verification environments can be used throughout the design cycle, from a high-level architectural model to a fully realized system. A definition of the e language syntax and semantics and how tool developers and verification engineers should use them are contained in this standard.","","978-1-5044-5914-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8739153","assertion;concurrent programming;constraint;dynamic verification;functional coverage;functional verification;IEEE 1647;simulation;temporal logic;test generation","IEEE Standards;Functional programming;Formal verification","","","","","","18 Jun 2019","","","IEEE","IEEE Standards"
"AIdeal: Sentience and Ideology","D. Estrada","Department of Humanities and Social Sciences, New Jersey Institute of Technology, Newark, NJ, USA",Journal of Social Computing,"23 Jan 2024","2023","4","4","275","325","This paper addresses a set of ideological tensions involving the classification of agential kinds, which I see as the methodological and conceptual core of the sentience discourse. Specifically, I consider ideals involved in the classification of biological and artifactual kinds, and ideals related to agency, identity, and value. These ideals frame the background against which sentience in Artificial Intelligence (AI) is theorized and debated, a framework I call the AIdeal. To make this framework explicit, I review the historical discourse on sentience as it appears in ancient, early modern, and the 20th century philosophy, paying special attention to how these ideals are projected onto artificial agents. I argue that tensions among these ideals create conditions where artificial sentience is both necessary and impossible, resulting in a crisis of ideology. Moving past this crisis does not require a satisfying resolution among competing ideals, but instead requires a shift in focus to the material conditions and actual practices in which these ideals operate. Following Charles Mills, I sketch a nonideal approach to AI and artificial sentience that seeks to loosen the grip of ideology on the discourse. Specifically, I propose a notion of participation that deflates the sentience discourse in AI and shifts focus to the material conditions in which sociotechnical networks operate.","2688-5255","","10.23919/JSC.2023.0029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10412092","sentience;agency;artifacts;artificial intelligence;ideology;nonideal theory;natural kinds;participation","Social computing;Philosophical considerations;Biology;Artificial intelligence","","","","86","","23 Jan 2024","","","TUP","TUP Journals"
"Inside AI: Over 150 billion purchases per year use this author’s AI","A. Adjaoute",Manning Publications,Inside AI: Over 150 billion purchases per year use this author’s AI,"","2024","","","","","Separate AI truth from AI hype, and learn how to put this powerful technology to work. In Inside AI AI professor and entrepreneur Dr. Akli Adjaoute puts AI in perspective, with informed insights from 30 years spent in the field. His book lays out a pragmatic blueprint that every leader can utilize to drive innovation with artificial intelligence. In Inside AI you’ll learn how to:  Gain insight into diverse AI techniques and methodologies Learn from both successful and failed AI applications Identify the capabilities and limitations of AI systems Understand successful and failed uses of AI in business See where human cognition still exceeds AI Bust common myths like AI’s threat to jobs and civilization Manage AI projects effectively  Inside AI takes you on a journey through artificial intelligence, from AI’s origins in traditional expert systems all the way to deep learning and Large Language Models. There’s no hype here—you’ll get the grounded, evidence-based insights that are vital for making strategic decisions and preparing your business for the future.","","9781633437722","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10745303.pdf&bkn=10745302&pdfType=book","modern;DL;deep learning;diverse;LLMs;large language models;strategic;future;business;hype-free;pragmatic;techniques;methodologies;future","","","","","","","6 Nov 2024","","","Manning","Manning eBooks"
"Privacy and Security Issues in Deep Learning: A Survey","X. Liu; L. Xie; Y. Wang; J. Zou; J. Xiong; Z. Ying; A. V. Vasilakos","College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; Fujian Provincial Key Laboratory of Network Security and Cryptology, College of Mathematics and Informatics, Fujian Normal University, Fuzhou, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China",IEEE Access,"8 Jan 2021","2021","9","","4566","4593","Deep Learning (DL) algorithms based on artificial neural networks have achieved remarkable success and are being extensively applied in a variety of application domains, ranging from image classification, automatic driving, natural language processing to medical diagnosis, credit risk assessment, intrusion detection. However, the privacy and security issues of DL have been revealed that the DL model can be stolen or reverse engineered, sensitive training data can be inferred, even a recognizable face image of the victim can be recovered. Besides, the recent works have found that the DL model is vulnerable to adversarial examples perturbed by imperceptible noised, which can lead the DL model to predict wrongly with high confidence. In this paper, we first briefly introduces the four types of attacks and privacy-preserving techniques in DL. We then review and summarize the attack and defense methods associated with DL privacy and security in recent years. To demonstrate that security threats really exist in the real world, we also reviewed the adversarial attacks under the physical condition. Finally, we discuss current challenges and open problems regarding privacy and security issues in DL.","2169-3536","","10.1109/ACCESS.2020.3045078","National Natural Science Foundation of China(grant numbers:U1804263,61702105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9294026","Deep learning;DL privacy;DL security;model extraction attack;model inversion attack;adversarial attack;poisoning attack;adversarial defense;privacy-preserving","Security;Computational modeling;Privacy;Data models;Training;Training data;Face recognition","","151","","218","CCBY","15 Dec 2020","","","IEEE","IEEE Journals"
"Adaptive Radar Detection: Model-Based, Data-Driven and Hybrid Approaches","A. Coluccia",NA,"Adaptive Radar Detection: Model-Based, Data-Driven and Hybrid Approaches","","2022","","","","","This book shows you how to adopt data-driven techniques for the problem of radar detection, both per se and in combination with model-based approaches. In particular, the focus is on space-time adaptive target detection against a background of interference consisting of clutter, possible jammers, and noise. It is a handy, concise reference for many classic (model-based) adaptive radar detection schemes as well as the most popular machine learning techniques (including deep neural networks) and helps you identify suitable data-driven approaches for radar detection and the main related issues. You’ll learn how data-driven tools relate to, and can be coupled or hybridized with, traditional adaptive detection statistics; understand fundamental concepts, schemes, and algorithms from statistical learning, classification, and neural networks domains. The book also walks you through how these concepts and schemes have been adapted for the problem of radar detection in the literature and provides you with a methodological guide for the design, illustrating different possible strategies. You’ll be equipped to develop a unified view, under which you can exploit the new possibilities of the data-driven approach even using simulated data. This book is an excellent resource for Radar professionals and industrial researchers, postgraduate students in electrical engineering and the academic community.","","9781630819019","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10001750.pdf&bkn=10001749&pdfType=book","","","","","","","","28 Dec 2022","","","Artech","Artech Books"
"IEEE Standard for the Functional Verification Language e - Redline","",,IEEE Std 1647-2019 (Revision of IEEE Std 1647-2016) - Redline,"19 Nov 2019","2019","","","1","981","The e functional verification language is an application-specific programming language, aimed at automating the task of verifying a hardware or software design with respect to its specification. Verification environments written in e provide a model of the environment in which the design is expected to function, including the kinds of erroneous conditions the design needs to withstand. A typical verification environment is capable of generating user-controlled test inputs with statistically interesting characteristics. Such an environment can check the validity of the design responses. Functional coverage metrics are used to control the verification effort and gauge the quality of the design. e verification environments can be used throughout the design cycle, from a high-level architectural model to a fully realized system. A definition of the e language syntax and semantics and how tool developers and verification engineers should use them are contained in this standard.","","978-1-5044-6271-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8906261","assertion;concurrent programming;constraint;dynamic verification;functional coverage;functional verification;IEEE 1647;simulation;temporal logic;test generation","IEEE Standards;Concurrent programming;System verification;Simulation;Test generation","","","","","","19 Nov 2019","","","IEEE","IEEE Standards"
"Technology and the Rise of Great Powers: How Diffusion Shapes Economic Competition","J. Ding",NA,Technology and the Rise of Great Powers: How Diffusion Shapes Economic Competition,"","2024","","","","","A novel theory of how technological revolutions affect the rise and fall of great powersWhen scholars and policymakers consider how technological advances affect the rise and fall of great powers, they draw on theories that center the moment of innovation—the eureka moment that sparks astonishing technological feats. In this book, Jeffrey Ding offers a different explanation of how technological revolutions affect competition among great powers. Rather than focusing on which state first introduced major innovations, he investigates why some states were more successful than others at adapting and embracing new technologies at scale. Drawing on historical case studies of past industrial revolutions as well as statistical analysis, Ding develops a theory that emphasizes institutional adaptations oriented around diffusing technological advances throughout the entire economy.Examining Britain’s rise to preeminence in the First Industrial Revolution, America and Germany’s overtaking of Britain in the Second Industrial Revolution, and Japan’s challenge to America’s technological dominance in the Third Industrial Revolution (also known as the “information revolution”), Ding illuminates the pathway by which these technological revolutions influenced the global distribution of power and explores the generalizability of his theory beyond the given set of great powers. His findings bear directly on current concerns about how emerging technologies such as AI could influence the US-China power balance.","","9780691260372","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10614670.pdf&bkn=10614669&pdfType=book","artificial intelligence;China;economic productivity;engineering;general-purpose technology;great powers;human capital;industrial revolutions;Jeffrey Ding;leading sectors;political economy;rising powers;Technological;technological competition;Technology and the Rise of Great Powers: How Diffusion Shapes Economic Competition;United States;Power;Economic;Sectors;Industrial;Growth;Innovation;Diffusion;Productivity;Mechanism;Industries;Technologies;Gpts;Economy;Revolution;GPT diffusion;Transition;Period;Trajectory;Technology;Power transition;Infrastructure;Institutional;Chemical;Skill infrastructure;Leadership;Economic power;Machine;Technological change;Century;Data;Country;Steam;GPT skill infrastructure;LS mechanism;Analysis;Education;Iron;Computerization;Industrial revolution;Breakthroughs;Technical;Military;Economic power transition;Production;Software;Productivity growth;Machine tools;Steam engine;Scholars;GPT mechanism;Engineers;Cotton;Software engineering;Research","","","","","","","30 Jul 2024","","","Princeton University Press","Princeton University Press eBooks"
"Large image datasets: A pyrrhic win for computer vision?","A. Birhane; V. U. Prabhu","School of Computer Science, Lero & University College, Dublin, Ireland; UnifyID AI Labs, Redwood City, USA",2021 IEEE Winter Conference on Applications of Computer Vision (WACV),"14 Jun 2021","2021","","","1536","1546","In this paper we investigate problematic practices and consequences of large scale vision datasets (LSVDs). We examine broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an example, we perform a cross-sectional model-based quantitative census covering factors such as age, gender, NSFW content scoring, class- wise accuracy, human-cardinality-analysis, and the semanticity of the image class information in order to statistically investigate the extent and subtleties of ethical transgressions. We then use the census to help hand-curate a look-up-table of images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of verifiably pornographic: shot in a non-consensual setting (up-skirt), beach voyeuristic, and exposed private parts. We survey the landscape of harm and threats both the society at large and individuals face due to uncritical and ill-considered dataset curation practices. We then propose possible courses of correction and critique their pros and cons. We have duly open-sourced all of the code and the census meta-datasets generated in this endeavor for the computer vision community to build on. By unveiling the severity of the threats, our hope is to motivate the constitution of mandatory Institutional Review Boards (IRB) for large scale dataset curation.","2642-9381","978-1-6654-0477-8","10.1109/WACV48630.2021.00158","Science Foundation Ireland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9423393","","Computer vision;Conferences;IEEE Constitution;Faces","","93","","91","IEEE","14 Jun 2021","","","IEEE","IEEE Conferences"
"COVID-19-CT-CXR: A Freely Accessible and Weakly Labeled Chest X-Ray and CT Image Collection on COVID-19 From Biomedical Literature","Y. Peng; Y. Tang; S. Lee; Y. Zhu; R. M. Summers; Z. Lu","NCBI/NLM/NIH and Department of Population Health Sciences, Weill Cornell Medicine, New York, NY, USA; Radiology and Imaging Sciences Department, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health (NIH) Clinical Center, Bethesda, MD, USA; Radiology and Imaging Sciences Department, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health (NIH) Clinical Center, Bethesda, MD, USA; Radiology and Imaging Sciences Department, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health (NIH) Clinical Center, Bethesda, MD, USA; Radiology and Imaging Sciences Department, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health (NIH) Clinical Center, Bethesda, MD, USA; National Library of Medicine (NLM), National Center for Biotechnology Information (NCBI), National Institutes of Health (NIH), Bethesda, MD, USA",IEEE Transactions on Big Data,"1 Mar 2021","2021","7","1","3","12","The latest threat to global health is the COVID-19 outbreak. Although there exist large datasets of chest X-rays (CXR) and computed tomography (CT) scans, few COVID-19 image collections are currently available due to patient privacy. At the same time, there is a rapid growth of COVID-19-relevant articles in the biomedical literature, including those that report findings on radiographs. Here, we present COVID-19-CT-CXR, a public database of COVID-19 CXR and CT images, which are automatically extracted from COVID-19-relevant articles from the PubMed Central Open Access (PMC-OA) Subset. We extracted figures, associated captions, and relevant figure descriptions in the article and separated compound figures into subfigures. Because a large portion of figures in COVID-19 articles are not CXR or CT, we designed a deep-learning model to distinguish them from other figure types and to classify them accordingly. The final database includes 1,327 CT and 263 CXR images (as of May 9, 2020) with their relevant text. To demonstrate the utility of COVID-19-CT-CXR, we conducted four case studies. (1) We show that COVID-19-CT-CXR, when used as additional training data, is able to contribute to improved deep-learning (DL) performance for the classification of COVID-19 and non-COVID-19 CT. (2) We collected CT images of influenza, another common infectious respiratory illness that may present similarly to COVID-19, and fine-tuned a baseline deep neural network to distinguish a diagnosis of COVID-19, influenza, or normal or other types of diseases on CT. (3) We fine-tuned an unsupervised one-class classifier from non-COVID-19 CXR and performed anomaly detection to detect COVID-19 CXR. (4) From text-mined captions and figure descriptions, we compared 15 clinical symptoms and 20 clinical findings of COVID-19 versus those of influenza to demonstrate the disease differences in the scientific publications. Our database is unique, as the figures are retrieved along with relevant text with fine-grained descriptions, and it can be extended easily in the future. We believe that our work is complementary to existing resources and hope that it will contribute to medical image analysis of the COVID-19 pandemic. The dataset, code, and DL models are publicly available at https://github.com/ncbi-nlp/COVID-19-CT-CXR.","2332-7790","","10.1109/TBDATA.2020.3035935","Intramural Research Programs of the National Library of Medicine; National Institutes of Health; NLM(grant numbers:4R00LM013001); Google Cloud; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9248607","COVID-19;chest X-ray;CT","COVID-19;Computed tomography;Databases;Biomedical imaging;X-ray imaging;Compounds","","58","","48","CCBY","4 Nov 2020","","","IEEE","IEEE Journals"
"Spatio-Temporal Turbulence Mitigation: A Translational Perspective","X. Zhang; N. Chimitt; Y. Chi; Z. Mao; S. H. Chan","School of Electrical and Computer Engineering, Purdue University; School of Electrical and Computer Engineering, Purdue University; School of Electrical and Computer Engineering, Purdue University; Samsung Research America; School of Electrical and Computer Engineering, Purdue University",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","2889","2899","Recovering images distorted by atmospheric turbulence is a challenging inverse problem due to the stochastic nature of turbulence. Although numerous turbulence mitigation (TM) algorithms have been proposed, their efficiency and generalization to real-world dynamic scenarios remain severely limited. Building upon the intuitions of classical TM algorithms, we present the Deep Atmospheric TUrbulence Mitigation network (DATUM). DATUM aims to overcome major challenges when transitioning from classical to deep learning approaches. By carefully integrating the merits of classical multi-frame TM methods into a deep network structure, we demonstrate that DATUM can efficiently perform long-range temporal aggregation using a recurrent fashion, while deformable attention and temporal-channel attention seamlessly facilitate pixel registration and lucky imaging. With additional supervision, tilt and blur degradation can be Jointly mitigated. These inductive biases empower DATUM to significantly outperform existing methods while delivering a tenfold increase in processing speed. A large-scale training dataset, ATSyn, is presented as a co-invention to enable the generalization to real turbulence. Our code and datasets are available at http://xg416.github.io/DATUM","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.00279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10658372","turbulence mitigation;video restoration;synthetic dataset","Degradation;Deep learning;Training;Prevention and mitigation;Heuristic algorithms;Government;Stochastic processes","","3","","77","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"One-dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications","M. Lyu; Y. Yang; H. Hong; H. Chen; X. Jin; Y. He; H. Xue; J. Han; G. Ding",Tsinghua University; Tsinghua University; Alibaba Group; Tsinghua University; Alibaba Group; Alibaba Group; Alibaba Group; Tsinghua University; Tsinghua University,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","7559","7568","The prevalent use of commercial and open-source diffusion models (DMs) for text-to-image generation prompts risk mitigation to prevent undesired behaviors. Existing concept erasing methods in academia are all based on full parameter or specification-based fine-tuning, from which we observe the following issues: 1) Generation alteration towards erosion: Parameter drift during target elimination causes alterations and potential deformations across all generations, even eroding other concepts at varying degrees, which is more evident with multi-concept erased; 2) Transfer in-ability & deployment inefficiency: Previous model-specific erasure impedes the flexible combination of concepts and the training-free transfer towards other models, resulting in linear cost growth as the deployment scenarios increase. To achieve non-invasive, precise, customizable, and transferable elimination, we ground our erasing framework on one-dimensional adapters to erase multiple concepts from most DMs at once across versatile erasing applications. The concept-SemiPermeable structure is injected as a Membrane (SPM) into any DM to learn targeted erasing, and mean-time the alteration and erosion phenomenon is effectively mitigated via a novel Latent Anchoring fine-tuning strategy. Once obtained, SPMs can be flexibly combined and plug-and-play for other DMs without specific re-tuning, enabling timely and efficient adaptation to diverse scenarios. During generation, our Facilitated Transport mechanism dynamically regulates the permeability of each SPM to re-spond to different input prompts, further minimizing the impact on other concepts. Quantitative and qualitative results across ~40 concepts, 7 DMs and 4 erasing applications have demonstrated the superior erasing of SPM. Our code and pre-tuned SPMs are available on the project page https:/lyumengyao.github.io/projects/spm.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.00722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10656262","Diffusion Models;Concept Erasing","Deformable models;Adaptation models;Costs;Deformation;Text to image;Diffusion models;Permeability","","2","","52","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"On Identification and Retrieval of Near-Duplicate Biological Images: a New Dataset and Protocol","T. E. Koker; S. S. Chintapalli; S. Wang; B. A. Talbot; D. Wainstock; M. Cicconet; M. C. Walsh",Harvard Medical School; Harvard Medical School; Harvard Medical School; Harvard Medical School; Harvard Medical School; Harvard Medical School; Harvard Medical School,2020 25th International Conference on Pattern Recognition (ICPR),"5 May 2021","2021","","","3114","3121","Manipulation and re-use of images in scientific publications is a growing issue, not only for biomedical publishers, but also for the research community in general. In this work we introduce BINDER - Bio-Image Near-Duplicate Examples Repository, a novel dataset to help researchers develop, train, and test models to detect same-source biomedical images. BINDER contains 7,490 unique image patches for model training, 1,821 same-size patch duplicates for validation and testing, and 868 different-size image/patch pairs for image retrieval validation and testing. Except for the training set, patches already contain manipulations including rotation, translation, scale, perspective transform, contrast adjustment and/or compression artifacts. We further use the dataset to demonstrate how novel adaptations of existing image retrieval and metric learning models can be applied to achieve high-accuracy inference results, creating a baseline for future work. In aggregate, we thus present a supervised protocol for near-duplicate image identification and retrieval without any “real-world” training example. Our dataset and source code are available at hms-idac.github.io/BINDER.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9412849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9412849","","Training;Measurement;Adaptation models;Protocols;Biological system modeling;Image retrieval;Transforms","","2","","30","IEEE","5 May 2021","","","IEEE","IEEE Conferences"
"Temporally consistent video colorization with deep feature propagation and self-regularization learning","Y. Liu; H. Zhao; K. C. K. Chan; X. Wang; C. C. Loy; Y. Qiao; C. Dong","Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China; University of Chinese Academy of Sciences, Beijing 100049, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China; Department of Electrical & Computer Engineering, Nanyang Technological University, 50 Nanyang Avenue, Singapore 639798; Applied Research Center, Tencent PCG, Shenzhen, China; Department of Electrical & Computer Engineering, Nanyang Technological University, 50 Nanyang Avenue, Singapore 639798; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China",Computational Visual Media,"20 Feb 2025","2024","10","2","375","395","Video colorization is a challenging and highly ill-posed problem. Although recent years have witnessed remarkable progress in single image colorization, there is relatively less research effort on video colorization, and existing methods always suffer from severe flickering artifacts (temporal inconsistency) or unsatisfactory colorization. We address this problem from a new perspective, by jointly considering colorization and temporal consistency in a unified framework. Specifically, we propose a novel temporally consistent video colorization (TCVC) framework. TCVC effectively propagates frame-level deep features in a bidirectional way to enhance the temporal consistency of colorization. Furthermore, TCVC introduces a self-regularization learning (SRL) scheme to minimize the differences in predictions obtained using different time steps. SRL does not require any ground-truth color videos for training and can further improve temporal consistency. Experiments demonstrate that our method can not only provide visually pleasing colorized video, but also with clearly better temporal consistency than state-of-the-art methods. A video demo is provided at https://www.youtube.com/watch?v=c7dczMs-olE, while code is available at https://github.com/lyh-18/TCVC-Temporally-Consistent-Video-Colorization.","2096-0662","","10.1007/s41095-023-0342-8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897653","video colorization;temporal consistency;feature propagation;self-regularization","Image color analysis;Feature extraction;Gray-scale;Art;Training;Spatiotemporal phenomena;Indexes;Data mining;Coherence;Visualization","","1","","","","20 Feb 2025","","","TUP","TUP Journals"
"A Survey on Terrain Traversability Analysis for Autonomous Ground Vehicles: Methods, Sensors, and Challenges","P. V. K. Borges; T. Peynot; S. Liang; B. Arain; M. Wildie; M. G. Minareci; S. Lichman; G. Samvedi; I. Sa; N. Hudson; M. Milford; P. Moghadam; P. Corke","CSIRO, Brisbane, Australia; Queensland University of Technology, Brisbane, Australia; CSIRO, Brisbane, Australia; Queensland University of Technology, Brisbane, Australia; CSIRO, Brisbane, Australia; Queensland University of Technology, Brisbane, Australia; CSIRO, Brisbane, Australia; Queensland University of Technology, Brisbane, Australia; CSIRO, Brisbane, Australia; Amazon, Seattle, Washington; Queensland University of Technology, Brisbane, Australia; CSIRO, Brisbane, Australia; Queensland University of Technology, Brisbane, Australia",Field Robotics,"25 Feb 2025","2022","2","","1567","1627","Understanding the terrain in the upcoming path of a ground robot is one of the most challenging problems in field robotics. Terrain and traversability analysis is a multidisciplinary field combining robotics with image and signal processing, feature extraction, machine learning, three-dimensional (3D) mapping, and 3D geometry. Application scenarios range from autonomous vehicles on urban networks to agriculture, defence, exploration, mining, and search and rescue. Given the broad set of techniques available and the fast progress in this area, in this paper we organize and survey the corresponding literature, define unambiguous key terms, and discuss links among fundamental building blocks ranging from terrain classification to traversability regression. The advantages and the drawbacks of the methods are critically discussed, providing a comprehensive coverage of key aspects, including open code, available datasets for experimentation and comparisons, and important open research issues.","2771-3989","","10.55417/fr.2022049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10876033","perception;obstacle avoidance;terrestrial robotics;navigation","Robots;Robot sensing systems;Navigation;Surveys;Costs;Reviews;Taxonomy;Sensor phenomena and characterization;Machine learning;Laser radar","","1","","","CCBY","25 Feb 2025","","","FRPS","FRPS Journals"
"Genre Classification of Books on Spanish","J. A. Nolazco-Flores; A. V. Guerrero-Galván; C. Del-Valle-Soto; L. P. Garcia-Perera","School of Engineering and Science, Tecnologico de Monterrey, Monterrey, Nuevo León, Mexico; School of Engineering and Science, Tecnologico de Monterrey, Monterrey, Nuevo León, Mexico; Facultad de Ingeniería, Universidad Panamericana, Zapopan, Jalisco, Mexico; Computer Science Department, John Hopkins University, Baltimore, MD, USA",IEEE Access,"30 Nov 2023","2023","11","","132878","132892","Genre categorization of published titles is a common practice in publishing houses, libraries, and bookstores, as well as a fundamental element of editorial marketing. However, assigning subject codes to each title proves to be an arduous task for both publishers and data aggregators. The problem with automatic genre categorization is that some publishers use more than 200 categories, making it a highly complex task. Moreover, even though these publishers based their categorization on standards, they ofthen alter the names of these standards as they consider to be too technical. In this paper, we proposed Thema-based categorization as a tool to facilitate editors’ work by advancing the categorization process, allowing them to focus on finer category granularity. This categorization has four key features: first, it clusters the most important categories for Latin American publishers. Second, it stops grouping when the number of thematic categories remains practical for the purposes of the publishing business. Third, we assign names to these categories that resonate with Latin American stakeholders. Finally, the number of categories is optimized to provide reasonable classification performance. We worked on the description of books in Spanish of two publishers, and mapped them to this proposed categorization. This allowed us to created a database for train a model to automate categorization. After conducting our analysis, we determined that 26 thematic categories were an appropriate number that fulfilled the three features mentioned earlier. However, we recognized that classifying into 26 categories was still a complex task, so to overcome this challenge, we decided to augment data by back-translating it into Spanish using the translation function,  $T_{l}^{S}\left ({T_{S}^{l}\left ({S }\right) }\right)$ , where  $T_{S}^{l}\left ({S }\right)$  is the translation function from Spanish,  $s$ , to language,  $l$ ;  $T_{l}^{s}\left ({l }\right)$  is the translation function from language,  $l$ , to Spanish,  $S$ , and  $T_{S}^{l}\left ({S }\right)$  and  $T_{l}^{S}\left ({l }\right)$  are not-invertible functions. Experimental results, obtained using 5-fold cross-validation, were approximately 57%, 57%, 63.38%, and 65.26% for the F1-score of Support Vector Machine (SVM), Logist Regression (LR), BERT, and RoBERTa models, respectively. We utilized the F1-score metric because our categories were not perfectly balanced. The results achieved by RoBERTa outperform those reported in the literature. Furthermore, these results are built upon the foundation of the Thema standard for categorizing book genres. Additionally, the categories have been specifically designed to align with the preferences and needs of Latin American publishers.","2169-3536","","10.1109/ACCESS.2023.3332997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10318045","Genre categorization;text classification;book categorization;BERT;RoBERTa","Standards;Task analysis;Natural language processing;Databases;Training;Feature extraction;Support vector machines;Text categorization;Information analysis;Publishing","","","","37","CCBYNCND","14 Nov 2023","","","IEEE","IEEE Journals"
"CSBNet: Leveraging Edge Intelligence for Multi-Granularity Low-Light Image Enhancement","Y. Wang; L. Jiang; Z. Du; B. Li; W. Yang","School of Artificial Intelligence, Chongqing University of Technology, Chongqing, China; School of Artificial Intelligence, Chongqing University of Technology, Chongqing, China; School of Artificial Intelligence, Chongqing University of Technology, Chongqing, China; School of Artificial Intelligence, Chongqing University of Technology, Chongqing, China; Shenzhen International Graduate School/Department of Electronic Engineering, Tsinghua University, Shenzhen, China",IEEE Internet of Things Journal,"","2024","PP","99","1","1","Low-light conditions constantly restrict the performance of IoT image sensors, thereby impacting image quality and the precision of visual data analysis. The emerging edge intelligence is crucial for low-light image enhancement in improving image quality and data support reliability for IoT systems, which in turn fosters the intelligence and automation progress of the IoT. The enhancement of low-light images necessitates the restoration of both contextual information and spatial details, maintaining the semantic content of the original image and the point-to-point correspondence between inputs and outputs. However, existing methods predominantly concentrate on one aspect, either contextual information or spatial details, making it difficult to simultaneously balance both. To overcome this challenge, we introduce a novel two-branch network, the Context-Space Balance Network (CSBNet), tailored for low-light image enhancement. It comprises a Contextual Information Recovery Network (CIRNet), which adeptly extracts contextual information from multi-scale low-light images, and a Spatial Information Recovery Network (SIRNet), which is designed to preserve spatial details at the original resolution. We also implement a Context-Space Feature Fusion (CSFF) module to seamlessly integrate contextual information with spatial details. Qualitative and quantitative experimental results demonstrate that our CSBNet can better handle many quality degradation types in lowlight images compared with state-of-the-art solutions, outperforming them by PSNR=24.64dB, SSIM=0.867, and LPIPS=0.063 on the benchmark LOL dataset. The source code of CSBNet is available at https://github.com/Loong161/CSBNet.","2327-4662","","10.1109/JIOT.2024.3513545","Natural Science Foundation of Guangdong Province(grant numbers:No.2020A1515010711); National Natural Science Foundation of China(grant numbers:No.61771276); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10786260","Edge Intelligence;Low-light Image Enhancement;Internet of Things (IoT);Contextual Information;Spatial Details;Context-Space Feature Fusion","Internet of Things;Image enhancement;Image edge detection;Lighting;Image quality;Image restoration;Sensors;Noise;Image sensors;Gray-scale","","","","","IEEE","9 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Beyond Privacy: Generating Privacy-Preserving Faces Supporting Robust Image Authentication","T. Wang; W. Wen; X. Xiao; Z. Hua; Y. Zhang; Y. Fang","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Computing and Artificial Intelligence, Jiangxi University of Finance and Economics, Nanchang, China; School of Computing and Artificial Intelligence, Jiangxi University of Finance and Economics, Nanchang, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computing and Artificial Intelligence, Jiangxi University of Finance and Economics, Nanchang, China; School of Computing and Artificial Intelligence, Jiangxi University of Finance and Economics, Nanchang, China",IEEE Transactions on Information Forensics and Security,"4 Mar 2025","2025","20","","2564","2576","The prevalence of face capturing along with the advancement of face recognition poses a potential threat to individual privacy. To protect privacy, plenty of methods have been proposed to change identity in the face, thus blocking malicious face recognition. However, these methods fail to satisfy authentication requirements for special application scenarios, e.g., face authentication in surveillance capture. In this paper, we propose a novel face privacy protection model, which supports robust image authentication via information-conditional identity transformation. Specifically, we first introduce a basic face manipulation model (FMM), which can preserve identity-irrelevant attributes when manipulating identity. Based on FMM, we further design a lightweight protector called AIDPro, outputting a transformed identity which is different from the original one and is embedded a message presenting authentication information. Benefiting from the semantic robustness, our model does not require noise layers to achieve accurate message extraction after various image distortions. In addition, the message can be the condition to guide the identity transformation for privacy protection, which avoids extra resource consumption from supporting image authentication. Extensive experimental results demonstrate our model has comparable privacy protection performance, superior attribute preservation performance, and robust authentication performance especially in JPEG compression and screen shooting. Our code is available at https://github.com/daizigege/AIDPro.","1556-6021","","10.1109/TIFS.2025.3541859","Natural Science Foundation of China(grant numbers:62201233); Double Thousand Plan of Jiangxi Province(grant numbers:jxsq2023201118); Outstanding Youth Fund Program of Jiangxi Province(grant numbers:20232ACB212004); National Natural Science Foundation of China(grant numbers:U24A20220); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10884889","Image privacy;face protection;information hiding;robustness","Faces;Privacy;Authentication;Protection;Noise;Robustness;Surveillance;Perturbation methods;Face recognition;Data privacy","","","","57","IEEE","13 Feb 2025","","","IEEE","IEEE Journals"
"MAT: Multi-Range Attention Transformer for Efficient Image Super-Resolution","C. Xie; X. Zhang; L. Li; Y. Fu; B. Gong; T. Li; K. Zhang","School of Computing and Artificial intelligence, Southwest Jiaotong University, China; School of Computing and Artificial intelligence, Southwest Jiaotong University, China; School of Computing and Artificial intelligence, Southwest Jiaotong University, China; INSAIT, Sofia University ”St. Kliment Ohridski”, Bulgaria; Ant Group, China; School of Computing and Artificial intelligence, Southwest Jiaotong University, China; School of Intelligence Science and Technology, Nanjing University, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Image super-resolution (SR) has significantly advanced through the adoption of Transformer architectures. However, conventional techniques aimed at enlarging the self-attention window to capture broader contexts come with inherent drawbacks, especially the significantly increased computational demands. Moreover, the feature perception within a fixed-size window of existing models restricts the effective receptive field (ERF) and the intermediate feature diversity. We demonstrate that a flexible integration of attention across diverse spatial extents can yield significant performance enhancements. In line with this insight, we introduce Multi-Range Attention Transformer (MAT) for SR tasks. MAT leverages the computational advantages inherent in dilation operation, in conjunction with self-attention mechanism, to facilitate both multi-range attention (MA) and sparse multi-range attention (SMA), enabling efficient capture of both regional and sparse global features. Combined with local feature extraction, MAT adeptly capture dependencies across various spatial ranges, improving the diversity and efficacy of its feature representations. We also introduce the MSConvStar module, which augments the model’s ability for multi-range representation learning. Comprehensive experiments show that our MAT exhibits superior performance to existing state-of-the-art SR models with remarkable efficiency (∼ 3.3× faster than SRFormer-light). The codes are available at https://github.com/stella-von/MAT.","1558-2205","","10.1109/TCSVT.2025.3553135","Sichuan Province Science and Technology Support Program(grant numbers:2024NSFTD0036,2024ZHCG0166); The science fund program for distinguished Young scholars (Overseas); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10935664","Transformer;image super-resolution;multi-range attention;efficient","Feature extraction;Transformers;Image reconstruction;Convolution;Circuits and systems;Computer architecture;Windows;Training;Superresolution;Computational complexity","","","","","IEEE","21 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Boosting 3D Object Detection with Semantic-Aware Multi-Branch Framework","H. Jing; A. Wang; L. Zhao; Y. Yang; D. Bu; J. Zhang; Y. Zhang; J. Hou","School of Electronic Information Engineering, Taiyuan University of Science and Technology, No. 66 Waliu Road, Taiyuan, China; School of Electronic Information Engineering, Taiyuan University of Science and Technology, No. 66 Waliu Road, Taiyuan, China; School of Electronic Information Engineering, Taiyuan University of Science and Technology, No. 66 Waliu Road, Taiyuan, China; School of Electronic Information Engineering, Taiyuan University of Science and Technology, No. 66 Waliu Road, Taiyuan, China; School of Electronic Information Engineering, Taiyuan University of Science and Technology, No. 66 Waliu Road, Taiyuan, China; School of Electronic Information Engineering, Taiyuan University of Science and Technology, No. 66 Waliu Road, Taiyuan, China; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong, China; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","In autonomous driving, LiDAR sensors are vital for acquiring 3D point clouds, providing reliable geometric information. However, traditional sampling methods of preprocessing often ignore semantic features, leading to detail loss and ground point interference in 3D object detection. To address this, we propose a multi-branch two-stage 3D object detection framework using a Semantic-aware Multi-branch Sampling (SMS) module and multi-view consistency constraints. The SMS module includes random sampling, Density Equalization Sampling (DES) for enhancing distant objects, and Ground Abandonment Sampling (GAS) to focus on non-ground points. The sampled multi-view points are processed through a Consistent KeyPoint Selection (CKPS) module to generate consistent keypoint masks for efficient proposal sampling. The first-stage detector uses multi-branch parallel learning with multi-view consistency loss for feature aggregation, while the second-stage detector fuses multi-view data through a Multi-View Fusion Pooling (MVFP) module to precisely predict 3D objects. The experimental results on the KITTI dataset and Waymo Open Dataset show that our method achieves excellent detection performance improvement for a variety of backbones, especially for low-performance backbones with simple network structures. The code will be publicly available at https://github.com/HaoJing-SX/SMS.","1558-2205","","10.1109/TCSVT.2025.3527997","Taiyuan Key Core Technology Research ?Reveal the List? Project(grant numbers:20240027,2024TYJB0128); Shanxi Scholarship Council of China(grant numbers:2024-130); Hong Kong Innovation and Technology(grant numbers:ITS/164/23,MHP/117/21); the NSFC Excellent Young Scientists(grant numbers:6242211); National Natural Science Foundation of China(grant numbers:62072325,62202323,U23A20314); Shanxi Province ?Reveal the List? Major Project(grant numbers:202301156401007); Industrial Application of Shanxi Provincial Technology Innovation Center(grant numbers:IVASXTIC2022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10836835","Point Clouds;3D Object Detection;Sampling Method;Preprocessing","Three-dimensional displays;Semantics;Feature extraction;Proposals;Point cloud compression;Object detection;Detectors;Data preprocessing;Sampling methods;Interference","","","","","IEEE","10 Jan 2025","","","IEEE","IEEE Early Access Articles"
"DAT: Dual-branch Adapter-Tuning for Few-shot Recognition","J. Chen; G. Wu; H. Li; J. Chen; W. Zhang; W. Zheng; R. Wang","College of Computer Science and Engineering, Sun Yat-sen Univerisity, Guangzhou, China; College of Computer Science and Engineering, Sun Yat-sen Univerisity, Guangzhou, China; School of Electronic and Computer Engineering, Peking University, Shenzhen, China; College of Computer Science and Engineering, Sun Yat-sen Univerisity, Guangzhou, China; College of Computer Science and Engineering, Sun Yat-sen Univerisity, Guangzhou, China; College of Computer Science and Engineering, Sun Yat-sen Univerisity, Guangzhou, China; Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, Guangzhou, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Parameter-Efficient Fine-Tuning methods based on vision-language models (such as CLIP) for few-shot learning have recently received considerable attention. However, previous works only fine-tune either the image or text branch, breaking the alignment of the original two branches, meanwhile fine-tuning both branches of the CLIP would inevitably introduce more trainable parameters and likely cause more severe over-fitting due to the limited training data. In this study, we propose a novel Dual-branch Adapter-Tuning framework (DAT), which collaboratively trains the visual adapter and textual adapter added to the two branches of the original CLIP with multiple consistency constraints. By effectively utilizing the semantically detailed class-specific prompts and outputs of the original CLIP to guide the fine-tuning of both branches, our method gains exceptional adaptation ability to the downstream few-shot learning tasks and alleviates the over-fitting issue, meanwhile maximally preserving the generalization ability of the original CLIP model. Our proposed framework has achieved superior performance on diverse datasets under various few-shot learning settings compared to the existing approaches. The source code is available at https://github.com/SandyXi/DAT.","1558-2205","","10.1109/TCSVT.2025.3541960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10884900","vision-language model;parameter-efficient fine-tuning;few-shot learning","Visualization;Few shot learning;Training;Adaptation models;Circuits and systems;Tuning;Training data;Accuracy;Semantics;Metalearning","","","","","IEEE","13 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Personalizing Federated Instrument Segmentation With Visual Trait Priors in Robotic Surgery","J. Xu; J. Wang; L. Yu; D. Stoyanov; Y. Jin; E. B. Mazomenos","UCL Hawkes Institute and the Department of Medical Physics and Biomedical Engineering, University College London, London, U.K.; Department of Computer Science, School of Informatics, Xiamen University, Xiamen, China; Department of Statistics and Actuarial Science, The University of Hong Kong, Hong Kong, China; UCL Hawkes Institute and the Department of Medical Physics and Biomedical Engineering, University College London, London, U.K.; Department of Biomedical Engineering and the Department of Electrical and Computer Engineering, National University of Singapore, Singapore; UCL Hawkes Institute and the Department of Medical Physics and Biomedical Engineering, University College London, London, U.K.",IEEE Transactions on Biomedical Engineering,"","2025","PP","99","1","11","Personalized federated learning (PFL) for surgical instrument segmentation (SIS) is a promising approach. It enables multiple clinical sites to collaboratively train a series of models in privacy, with each model tailored to the individual distribution of each site. Existing PFL methods rarely consider the personalization of multi-headed self-attention, and do not account for appearance diversity and instrument shape similarity, both inherent in surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait priors for SIS, incorporating global-personalized disentanglement (GPD), appearance-regulation personalized enhancement (APE), and shape-similarity global enhancement (SGE), to boost SIS performance in each site. GPD represents the first attempt at head- wise assignment for multi-headed self-attention personalization. To preserve the unique appearance representation of each site and gradually leverage the inter-site difference, APE introduces appearance regulation and provides customized layer- wise aggregation solutions via hypernetworks for each site's personalized parameters. The mutual shape information of instruments is maintained and shared via SGE, which enhances the cross-style shape consistency on the image level and computes the shape-similarity contribution of each site on the prediction level for updating the global parameters. PFedSIS outperforms state-of-the-art methods with +1.51% Dice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding code and models are available at https://github.com/wzjialang/PFedSIS.","1558-2531","","10.1109/TBME.2025.3526667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10830588","Personalized federated learning;multi-headed self-attention;hypernetwork;appearance regulation;shape similarity","Instruments;Shape;Surgery;Training;Servers;Federated learning;Data models;Biomedical engineering;Visualization;Regulation","","","","","IEEE","7 Jan 2025","","","IEEE","IEEE Early Access Articles"
"Data Storytelling with Altair and AI","A. Lo Duca",Manning Publications,Data Storytelling with Altair and AI,"","2024","","","","","Great data presentations tell a story. Learn how to organize, visualize, and present data using Python, generative AI, and the cutting-edge Altair data visualization toolkit. Take the fast track to amazing data presentations! Data Storytelling with Altair and AI introduces a stack of useful tools and tried-and-tested methodologies that will rapidly increase your productivity, streamline the visualization process, and leave your audience inspired. In Data Storytelling with Altair and AI you’ll discover:  Using Python Altair for data visualization Using Generative AI tools for data storytelling The main concepts of data storytelling Building data stories with the DIKW pyramid approach Transforming raw data into a data story  Data Storytelling with Altair and AI teaches you how to turn raw data into effective, insightful data stories. You’ll learn exactly what goes into an effective data story, then combine your Python data skills with the Altair library and AI tools to rapidly create amazing visualizations. Your bosses and decision-makers will love your new presentations—and you’ll love how quick Generative AI makes the whole process!","","9781633437920","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10745331.pdf&bkn=10745330&pdfType=book","DIKW pyramid;information;knowledge;wisdom;visualization;Python;Copilot;ChatGPT;Vega;DALL-E;intuitive;prompt;transform;chart;presentation;encodings","","","","","","","6 Nov 2024","","","Manning","Manning eBooks"
"EI2Det: Edge-Guided Illumination-Aware Interactive Learning for Visible-Infrared Object Detection","K. Hu; Y. He; Y. Li; J. Zhao; S. Chen; Y. Kang","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","The complementary characteristics of visible (VIS) and infrared (IR) modalities play a crucial role in scene perception for autonomous driving, especially under poor lighting conditions. However, effectively leveraging the complementary information from visible and infrared images to further enhance perception performance remains a challenging task. These challenges stem from the difficulty of adaptively balancing the contributions of visible and infrared information under dynamic illumination conditions, the reliance on static fusion strategies that fail to fully utilize cross-modal complementarities, and the limitations of existing datasets in terms of diverse scenes, fine-grained illumination annotations, and high imaging quality. To address the challenges, we propose an Edge-guided Illumination-aware Interactive learning-based Detector (EI2Det). It includes three novel modules. The cross-modal interaction module uses visible-priority and infrared-priority multi-head cross-attention mechanisms to refine inter-modality and intra-modality feature representations, improving the model’s robustness and adaptability. The illumination-aware weighting module predicts illumination intensity levels to dynamically adjust the contributions of visible and infrared features, ensuring effective fusion under various lighting conditions. The edge-guided fusion module leverages critical edge information to guide the detector’s attention to object boundaries, significantly enhancing its localization capability. Additionally, we introduce a Multi-modality Full-time dataset for Autonomous Driving (MFAD), featuring 12,370 image pairs with fine-grained annotations of illumination intensity, covering diverse driving scenarios and weather conditions. Extensive experiments on the public M3FD, KAIST, FLIR, LLVIP, and our MFAD datasets demonstrate superior performance and generalization ability of our approach. The code and dataset will be available at https://github.com/hukefy/EI2Det.","1558-2205","","10.1109/TCSVT.2025.3539625","National Natural Science Foundation of China(grant numbers:92473114); University Synergy Innovation Program of Anhui Province(grant numbers:GXXT-2023-003); Chinese Academy of Sciences(grant numbers:XDB0660000,YSBR-029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10877920","Visible and infrared;object detection;dataset and benchmark;autonomous driving","Lighting;Object detection;Image edge detection;Feature extraction;Autonomous vehicles;Meteorology;Image fusion;Clouds;Circuits and systems;Training","","","","","IEEE","7 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Disentangle and Then Fuse: A Cross-Modal Network for Synthesizing Gadolinium-Enhanced Brain MR Images","Z. Che; Z. Zhang; Y. Wu; M. Wang","School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Department of Radiology, Henan Provincial People’s Hospital, Zhengzhou, Henan, China; Department of Radiology, Henan Provincial People’s Hospital, Zhengzhou, Henan, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Despite the widespread use of gadolinium-based contrast agents in clinical MRI examinations due to their significant advantages in structural localization and tumor identification, there is a risk of brain deposition and nephrogenic systemic fibrosis. Cross-modal image synthesis methods offer a new alternative, yet lesion synthesis remains challenging. On one hand, brain lesions vary significantly in location, shape, and size. On the other hand, the high background ratio associated with brain lesions makes their synthesis more difficult. To address these issues, we first introduce a Multi-Objective Local Perception Module (M-OLPM), which utilizes edge generation and lesion segmentation tasks to prioritize local lesions from the disentangled local perceptual feature subspaces. To better extend to multi-objective local perception, we propose a ’Disentangle and Then Fuse’ learning strategy, including a Feature Disentanglement Module (FDM) and a Global Fusion Module (GFM). The FDM decouples multimodal deep features into low-frequency semantic features and high-frequency edge features, alleviating feature conflicts from weakly related perception tasks. To enhance feature interaction among multiple perception tasks, the GFM progressively integrates these local perceptual features and underlying detail features through an attention mechanism, further refining the global image quality. Evaluated on the publicly available BRaTS2020, BRaTS2021 datasets, and the private HPPH dataset, our method significantly outperforms the existing technology in both visual and quantitative assessments of gadolinium-enhanced MRI images in global and localized lesion areas, providing a safe alternative to gadolinium enhancement. The source code is publicly available at https://github.com/zengyangche/DTF-Net.","1558-2205","","10.1109/TCSVT.2025.3528981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10839402","medical image synthesis;cross-modal;MRI;GAN;gadolinium-enhanced","Lesions;Magnetic resonance imaging;Image edge detection;Semantics;Image synthesis;Biomedical imaging;Frequency division multiplexing;Feature extraction;Shape;Interference","","","","","IEEE","13 Jan 2025","","","IEEE","IEEE Early Access Articles"
"Writing with Pleasure","H. Sword; S. T. Marsh",NA; NA,Writing with Pleasure,"","2023","","","","","An essential guide to cultivating joy in your professional and personal writingWriting should be a pleasurable challenge, not a painful chore. Writing with Pleasure empowers academic, professional, and creative writers to reframe their negative emotions about writing and reclaim their positive ones. By learning how to cast light on the shadows, you will soon find yourself bringing passion and pleasure to everything you write.Acclaimed international writing expert Helen Sword invites you to step into your “WriteSPACE”—a space of pleasurable writing that is socially balanced, physically engaged, aesthetically nourishing, creatively challenging, and emotionally uplifting. Sword weaves together cutting-edge findings in the sciences and social sciences with compelling narratives gathered from nearly six hundred faculty members and graduate students from across the disciplines and around the world. She provides research-based principles, hands-on strategies, and creative “pleasure prompts” designed to help you ramp up your productivity and enhance the personal rewards of your writing practice. Whether you’re writing a scholarly article, an administrative email, or a love letter, this book will inspire you to find delight in even the most mundane writing tasks and a richer, deeper pleasure in those you already enjoy.Exuberantly illustrated by prizewinning graphic memoirist Selina Tusitala Marsh, Writing with Pleasure is an indispensable resource for academics, students, professionals, and anyone for whom writing has come to feel like a burden rather than a joy.","","9780691229416","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10172362.pdf&bkn=10172361&pdfType=book","Writing with Pleasure;helen sword;Selina Tusitala Marsh;essential guide;professional writing;learning how to write;how to write;personal writing;skills for scholars;princeton university press;creative writing;compelling narratives;scholarly article;an administrative email;a love letter;illustrated by prizewinning graphic memoirist;WriteSPACE;pleasurable writing;creatively challenging;research-based principles;hands-on strategies;writing prompts;writing skills;learn to write;make writing fun;how to make writing fun;creative space;be a better writer;books about writing","","","","","","","4 Jul 2023","","","Princeton University Press","Princeton University Press eBooks"
"Creativity Support in AI Co-creative Tools: Current Research, Challenges and Opportunities","B. Ning; F. Liu; Z. Liu","School of Design, Hunan University, Changsha, China; School of Design, Hunan University, Changsha, China; School of Design, Hunan University, Changsha, China",2023 26th International Conference on Computer Supported Cooperative Work in Design (CSCWD),"22 Jun 2023","2023","","","5","10","Artificial Intelligence technology-driven Creativity Support Tools (AI-CSTs) provide specific field capability support for human creative activities. In this paper, we compare and analyze the current situation and trend of AI-CSTs design space in four aspects: creative stage, support form, support technology, and role diversity. Through a coding study and comparative analysis of 50 AI-CSTs cases, we discuss the impact of AI-CSTs on traditional workflows, the boundaries of AI-CSTs as co-creators, and how to treat AI errors, which provides insights for future AI-CSTs design. We summarize the collaboration framework in AI-CSTs. Finally, this paper also studies the information technology requirements and challenges of AI-CSTs research, which provides a new perspective to understanding the landscape of AI-CSTs.","2768-1904","979-8-3503-3168-4","10.1109/CSCWD57460.2023.10152832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10152832","Creativity Support Tools;Computational Creativity;Artificial Intelligence;Collaboration framework","Systematics;Federated learning;Collaboration;Market research;Encoding;Information technology;Creativity","","","","35","IEEE","22 Jun 2023","","","IEEE","IEEE Conferences"
"Foundations of Probabilistic Logic Programming: Languages, Semantics, Inference and Learning","F. Riguzzi",NA,"Foundations of Probabilistic Logic Programming: Languages, Semantics, Inference and Learning","","2020","","","i","xxxv","Probabilistic Logic Programming extends Logic Programming by enabling the representation of uncertain information by means of probability theory. Probabilistic Logic Programming is at the intersection of two wider research fields: the integration of logic and probability and Probabilistic Programming. Logic enables the representation of complex relations among entities while probability theory is useful for model uncertainty over attributes and relations. Combining the two is a very active field of study. Probabilistic Programming extends programming languages with probabilistic primitives that can be used to write complex probabilistic models. Algorithms for the inference and learning tasks are then provided automatically by the system. Probabilistic Logic programming is at the same time a logic language, with its knowledge representation capabilities, and a Turing complete language, with its computation capabilities, thus providing the best of both worlds. Since its birth, the field of Probabilistic Logic Programming has seen a steady increase of activity, with many proposals for languages and algorithms for inference and learning. Foundations of Probabilistic Logic Programming aims at providing an overview of the field with a special emphasis on languages under the Distribution Semantics, one of the most influential approaches. The book presents the main ideas for semantics, inference, and learning and highlights connections between the methods. Many examples of the book include a link to a page of the web application http://cplint.eu where the code can be run online.","","9788770220170","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9226728.pdf&bkn=9218850&pdfType=chapter","","","","","","","","20 Oct 2020","","","River Publishers","River eBook Chapters"
"Academic Performance Prediction Using Machine Learning Approaches: A Survey","J. Pan; Z. Zhao; D. Han","Department of Mechanical and Automation Engineering, Chinese University of Hong Kong, Hong Kong SAR; School of Humanities and Social Science, Chinese University of Hong Kong (Shenzhen), Hong Kong SAR; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong SAR",IEEE Transactions on Learning Technologies,"","2025","PP","99","1","18","Properly predicting students' academic performance is crucial for elevating educational outcomes in various disciplines. Through precise performance prediction, schools can quickly pinpoint students facing challenges and provide customized educational materials suited to their specific learning needs. The reliance on teachers' experience to predict students' academic performance has proven to be less accurate and efficient than desired. Consequently, the past decade has witnessed a marked surge in employing machine learning and data mining techniques to forecast students' performance. However, the academic community has yet to agree on the most effective algorithm for predicting academic outcomes. Nonetheless, conducting an analysis and comparison of the existing algorithms in this field remains meaningful. Furthermore, recommendations for selecting an appropriate algorithm will be provided to interested researchers and educators based on their specific requirements. This paper reviews the state-of-the-art literature on academic performance predictions using machine-learning approaches in recent years. It details the variables analyzed, the algorithms implemented, the datasets utilized, and the evaluation metrics applied to assess model efficacy. What makes this work different is that relevant surveys in the past 10 years are also analyzed and compared, highlighting their contributions and review methods. Additionally, we compared the accuracy of various machine learning models using popular open-access datasets and determined the best-performing algorithms among them. Our dataset and source codes are released for future algorithm comparisons and evaluations in this community.","1939-1382","","10.1109/TLT.2025.3554174","Hong Kong Teaching Development and Language Enhancement(grant numbers:TDLEG-4170989); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10938259","Survey;Performance prediction;Learning Analytics;AI in Education;Machine learning;Students' performance;Supervised learning;Neural networks","Reviews;Prediction algorithms;Surveys;Machine learning;Data mining;Machine learning algorithms;Random forests;Artificial intelligence;Accuracy;Logistic regression","","","","","IEEE","24 Mar 2025","","","IEEE","IEEE Early Access Articles"
"SMART: Scene-Motion-Aware Human Action Recognition Framework for Mental Disorder Group","Z. Lai; J. Yang; S. Xia; Q. Wu; Z. Sun; W. Yu; L. Pei","School of Electronic Information and Electrical Engineering, Shanghai Key Laboratory of Navigation and Location-based Services, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Key Laboratory of Navigation and Location-based Services, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Key Laboratory of Navigation and Location-based Services, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Key Laboratory of Navigation and Location-based Services, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Key Laboratory of Navigation and Location-based Services, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Key Laboratory of Navigation and Location-based Services, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Key Laboratory of Navigation and Location-based Services, Shanghai Jiao Tong University, Shanghai, China",IEEE Internet of Things Journal,"","2024","PP","99","1","1","Patients with mental disorders often exhibit risky abnormal actions, such as climbing walls or hitting windows, necessitating intelligent video behavior monitoring for smart healthcare with the rising Internet of Things (IoT) technology. However, the development of vision-based Human Action Recognition (HAR) for these actions is hindered by the lack of specialized algorithms and datasets. In this paper, we innovatively propose to build a vision-based HAR dataset including abnormal actions often occurring in the mental disorder group and then introduce a novel Scene-Motion-aware Action Recognition Technology framework, named SMART, consisting of two technical modules. First, we propose a scene perception module to extract human motion trajectory and human-scene interaction features, which introduces additional scene information for a supplementary semantic representation of the above actions. Second, the multi-stage fusion module fuses the skeleton motion, motion trajectory, and human-scene interaction features, enhancing the semantic association between the skeleton motion and the above supplementary representation, thus generating a comprehensive representation with both human motion and scene information. The effectiveness of our proposed method has been validated on our self-collected HAR dataset (MentalHAD), achieving 94.9% and 93.1% accuracy in un-seen subjects and scenes and outperforming state-of-the-art approaches by 6.5% and 13.2%, respectively. The demonstrated subject-and scene-generalizability makes it possible for SMART’s migration to practical deployment in smart healthcare systems for mental disorder patients in medical settings. The code and dataset will be released publicly for further research: https://github.com/Inowlzy/SMART.git.","2327-4662","","10.1109/JIOT.2024.3509458","National Natural Science Foundation of China(grant numbers:62273229); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771963","Action Recognition;Mental Disorders;Multi-stage Fusion;Scene Semantic Understanding;Smart Healthcare","Feature extraction;Skeleton;Semantics;Mental disorders;Internet of Things;Trajectory;Data mining;Medical services;Human activity recognition;Pipelines","","","","","IEEE","2 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Data-Driven and Physics-Assisted Machine Learning Approach for Warpage Classification and Process Parameter Optimization in a 3-D-Printed BeltClip","T. S. Tamir; X. Hua; J. Jiang; J. Leng; G. Xiong; Z. Shen; Q. Liu","State Key Laboratory of Precision Electronic Manufacturing Technology and Equipment, and Guangdong Provincial Key Laboratory of Computer Integrated Manufacturing, Guangdong University of Technology, Guangzhou, China; Department of Engineering, Faculty of Environment, Science and Economy, University of Exeter, Exeter, U.K.; Department of Engineering, Faculty of Environment, Science and Economy, University of Exeter, Exeter, U.K.; State Key Laboratory of Precision Electronic Manufacturing Technology and Equipment, and Guangdong Provincial Key Laboratory of Computer Integrated Manufacturing, Guangdong University of Technology, Guangzhou, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Beijing Engineering Research Center of Intelligent Systems and Technology, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Beijing Engineering Research Center of Intelligent Systems and Technology, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Precision Electronic Manufacturing Technology and Equipment, and Guangdong Provincial Key Laboratory of Computer Integrated Manufacturing, Guangdong University of Technology, Guangzhou, China",IEEE Transactions on Computational Social Systems,"","2024","PP","99","1","16","3-D printing, or additive manufacturing (AM), leverages 3-D computer-aided design models and numerical control to produce objects layer-by-layer, playing a key role in Industry 4.0 and Industry 5.0. Despite its potential to revolutionize manufacturing by creating complex structures more efficiently and cost-effectively, 3-D printing still faces quality issues due to a lack of sufficient data, resulting in improper process parameter settings and poor analyzability. This work introduces a data-driven and physics-assisted machine learning (DP-ML) approach for a 3-D-printed BeltClip object, integrating finite element analysis (FEA) and physics-informed machine learning (PIML). The proposed DP-ML framework provides a cost-effective and time-efficient data collection method using Digimat-AM and a warpage classification algorithm. The data collection begins with obtaining the STereoLithography (STL) file of the BeltClip object from Thingiverse and slicing it in Ultimaker©, Cura, considering process parameters such as infill amount, toolpath pattern, layer height, print speed, and extrusion temperature. The resulting G-code file is then input into Digimat-AM for further parameter setting and analysis. In Digimat-AM, glass fiber-filled and unfilled material types are set, undergoing the virtual 3-D printing process, followed by a warpage analysis of the printed BeltClip. The collected 3-D printing data is used to build ML models—deep neural network (DNN), decision tree (DT), support vector machine (SVM), logistic regression (LR), and random forest. The DNN contains three architectures—DNN-1, DNN-2, and DNN-3. Based on the metrics of precision, recall, F1-score, and accuracy, DNN-3 outperforms the others and is chosen for the warpage classification algorithm. The presented DP-ML approach is compared with the state-of-the-art methods and shows a promising capability to predicting warpage, optimizing process parameters, and improving the overall quality and efficiency of a 3-D-printed BeltClip.","2329-924X","","10.1109/TCSS.2024.3512614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10819292","3-D printing;data-driven;digital manufacturing;optimal process parameters;physics-assisted machine learning;warpage analysis","Three-dimensional printing;Solid modeling;Manufacturing;Data collection;Monitoring;Artificial neural networks;Data models;Computational modeling;Support vector machines;Design automation","","","","","IEEE","31 Dec 2024","","","IEEE","IEEE Early Access Articles"
"StegMamba: Distortion-free Immune-Cover for Multi-Image Steganography with State Space Model","T. Luo; Y. Zhou; Z. He; G. Jiang; H. Xu; S. Qi; Y. Zhang","College of Science and Technology, Ningbo University, Ningbo, China; College of Science and Technology, Ningbo University, Ningbo, China; College of Science and Technology, Ningbo University, Ningbo, China; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China; Department of Mathematics, The Chinese University of Hong Kong, Hong Kong, China; Department of Mathematics, The Chinese University of Hong Kong, Hong Kong, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2024","PP","99","1","1","Multi-image steganography ensures privacy protection while avoiding suspicion from third parties by embedding multiple secret images within a cover image. However, existing multi-image steganographic methods fail to model global spatial correlations to reduce image damage at the low computation cost. Moreover, they do not account for the anti-distortion capability of the cover image, which is crucial for achieving imperceptible and ensuring security. To overcome these limitations, we propose StegMamba, a distortion-free immune-cover for multi-image steganography architecture with a state space model. Specifically, we first explore the potential of the linear computational cost model Mamba for data hiding tasks through a steganography Mamba block (SMB), whose efficiency makes it suitable for real-time applications. Subsequently, considering that images with distortion resistance reduce embedding damage, the original cover image is reconstructed through immune-cover construction module (ICCM) and associated with the steganography task. Moreover, well-coupled features facilitate fusion, and thus a wavelet-based interaction module (WIM) is designed for effective communication between the immune-cover and the secret images. Compared with the state-of-the-art global attention-based methods, the proposed StegMamba obtains PSNR gains of 3.30 dB, 1.37 dB, and 1.92 dB for the stego image, and two secret recovery images, respectively, and the reduction of 2.87% in detection accuracy for anti-steganalysis. This code is available at https://github.com/YuhangZhouCJY/StegMamba.","1558-2205","","10.1109/TCSVT.2024.3515652","Zhejiang Provincial Natural Science Foundation of China(grant numbers:No. LY22F020020); National Natural Science Foundation of China(grant numbers:No. 62171243,No. 62271276,No. 62471264); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10794530","Image steganography;State space model;Immune-cover;Distortion resistance","Steganography;Distortion;Immune system;Feature extraction;Optimization;Computational modeling;Circuits and systems;Resistance;Couplings;Image color analysis","","","","","IEEE","11 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Cyberwarfare: An Introduction to Information-Age Conflict","I. Porche",NA,Cyberwarfare: An Introduction to Information-Age Conflict,"","2019","","","","","Conflict in cyberspace is becoming more prevalent in all public and private sectors and is of concern on many levels. As a result, knowledge of the topic is becoming essential across most disciplines. This book reviews and explains the technologies that underlie offensive and defensive cyber operations, which are practiced by a range of cyber actors including state actors, criminal enterprises, activists, and individuals. It explains the processes and technologies that enable the full spectrum of cyber operations. Readers will learn how to use basic tools for cyber security and pen-testing, and also be able to quantitatively assess cyber risk to systems and environments and discern and categorize malicious activity. The book provides key concepts of information age conflict technical basics/fundamentals needed to understand more specific remedies and activities associated with all aspects of cyber operations. It explains techniques associated with offensive cyber operations, with careful distinctions made between cyber ISR, cyber exploitation, and cyber attack. It explores defensive cyber operations and includes case studies that provide practical information, making this book useful for both novice and advanced information warfare practitioners.","","9781630815783","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9098747.pdf&bkn=9098746&pdfType=book","","","","","","","","16 Jun 2020","","","Artech","Artech Books"
"Dynamic Appearance Particle Neural Radiance Field","A. Lin; Y. Xiang; J. Li; M. Prasad","Australian Artificial Intelligence Institute (AAII), School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia; School of Automobile, Chang’an University, Xian, China; Australian Artificial Intelligence Institute (AAII), School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia; Australian Artificial Intelligence Institute (AAII), School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Neural Radiance Fields (NeRFs) have shown great potential in modeling 3D scenes. Dynamic NeRFs extend this model by capturing time-varying elements, typically using deformation fields. The existing dynamic NeRFs employ a similar Eulerian representation for both light radiance and deformation fields. This leads to a close coupling of appearance and motion and lacks a physical interpretation. In this work, we propose Dynamic Appearance Particle Neural Radiance Field (DAP-NeRF), which introduces particle-based representation to model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists of the superposition of a static field and a dynamic field. The dynamic field is quantized as a collection of appearance particles, which carries the visual information of a small dynamic element in the scene and is equipped with a motion model. All components, including the static field, the visual features and the motion models of particles, are learned from monocular videos without any prior geometric knowledge of the scene. We develop an efficient computational framework for the particle-based model. We also construct a new dataset to evaluate motion modeling. Experimental results show that DAP-NeRF is an effective technique to capture not only the appearance but also the physically meaningful motions in a 3D dynamic scene. Code is available at: https://github.com/Cenbylin/DAP-NeRF.","1558-2205","","10.1109/TCSVT.2025.3540792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10879537","Neural Radiance Field;Dynamic Scene modeling;3D Reconstruction;View Synthesis","Neural radiance field;Dynamics;Three-dimensional displays;Deformation;Solid modeling;Deformable models;Vehicle dynamics;Computational modeling;Visualization;Rendering (computer graphics)","","","","","CCBYNCND","11 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Geometry-aware 3D pose transfer using transformer autoencoder","S. Liu; S. Gai; F. Da; F. Waris","Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, 210096, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, 210096, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, 210096, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, 210096, China",Computational Visual Media,"13 Feb 2025","2024","10","6","1063","1078","3D pose transfer over unorganized point clouds is a challenging generation task, which transfers a source's pose to a target shape and keeps the target's identity. Recent deep models have learned deformations and used the target's identity as a style to modulate the combined features of two shapes or the aligned vertices of the source shape. However, all operations in these models are point-wise and independent and ignore the geometric information on the surface and structure of the input shapes. This disadvantage severely limits the generation and generalization capabilities. In this study, we propose a geometry-aware method based on a novel transformer autoencoder to solve this problem. An efficient self-attention mechanism, that is, cross-covariance attention, was utilized across our framework to perceive the correlations between points at different distances. Specifically, the transformer encoder extracts the target shape's local geometry details for identity attributes and the source shape's global geometry structure for pose information. Our transformer decoder efficiently learns deformations and recovers identity properties by fusing and decoding the extracted features in a geometry attentional manner, which does not require corresponding information or modulation steps. The experiments demonstrated that the geometry-aware method achieved state-of-the-art performance in a 3D pose transfer task. The implementation code and data are available at https://github.com/SEULSH/Geometry-Aware-3D-Pose-Transfer-Using-Transformer-Autoencoder.","2096-0662","","10.1007/s41095-023-0379-8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10884991","3D pose transfer;geometry-aware;transformer autoencoder;cross-covariance attention","Shape;Transformers;Three-dimensional displays;Deformation;Autoencoders;Decoding;Feature extraction;Solid modeling;Deformable models;Topology","","","","","","13 Feb 2025","","","TUP","TUP Journals"
"A Comprehensive Study on Colorectal Polyp Segmentation With ResUNet++, Conditional Random Field and Test-Time Augmentation","D. Jha; P. H. Smedsrud; D. Johansen; T. de Lange; H. D. Johansen; P. Halvorsen; M. A. Riegler","SimulaMet, 0167 Oslo and UiT The Arctic University of Norway, Tromsø; SimulaMet, 0167 Oslo and University of Oslo, Oslo; UiT The Arctic University of Norway, Tromsø; Medical Department Sahlgrenska University Hospital – Mölndal, Region Västra Götaland, Mölndal, Sweden; UiT The Arctic University of Norway, Tromsø; NA; NA",IEEE Journal of Biomedical and Health Informatics,"3 Jun 2021","2021","25","6","2029","2040","Colonoscopy is considered the gold standard for detection of colorectal cancer and its precursors. Existing examination methods are, however, hampered by high overall miss-rate, and many abnormalities are left undetected. Computer-Aided Diagnosis systems based on advanced machine learning algorithms are touted as a game-changer that can identify regions in the colon overlooked by the physicians during endoscopic examinations, and help detect and characterize lesions. In previous work, we have proposed the ResUNet++ architecture and demonstrated that it produces more efficient results compared with its counterparts U-Net and ResUNet. In this paper, we demonstrate that further improvements to the overall prediction performance of the ResUNet++ architecture can be achieved by using Conditional Random Field (CRF) and Test-Time Augmentation (TTA). We have performed extensive evaluations and validated the improvements using six publicly available datasets: Kvasir-SEG, CVC-ClinicDB, CVC-ColonDB, ETIS-Larib Polyp DB, ASU-Mayo Clinic Colonoscopy Video Database, and CVC-VideoClinicDB. Moreover, we compare our proposed architecture and resulting model with other state-of-the-art methods. To explore the generalization capability of ResUNet++ on different publicly available polyp datasets, so that it could be used in a real-world setting, we performed an extensive cross-dataset evaluation. The experimental results show that applying CRF and TTA improves the performance on various polyp segmentation datasets both on the same dataset and cross-dataset. To check the model's performance on difficult to detect polyps, we selected, with the help of an expert gastroenterologist, 196 sessile or flat polyps that are less than ten millimeters in size. This additional data has been made available as a subset of Kvasir-SEG. Our approaches showed good results for flat or sessile and smaller polyps, which are known to be one of the major reasons for high polyp miss-rates. This is one of the significant strengths of our work and indicates that our methods should be investigated further for use in clinical practice.","2168-2208","","10.1109/JBHI.2021.3049304","Research Council of Norway Project(grant numbers:263248); Norges Forskningsråd(grant numbers:270053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9314114","Colonoscopy;polyp segmentation;ResUNet++;conditional random field;test-time augmentation;generalization","Image segmentation;Colonoscopy;Cancer;Hospitals;Computer architecture;Training;Task analysis","Algorithms;Colonic Polyps;Colonoscopy;Diagnosis, Computer-Assisted;Humans;Neural Networks, Computer","190","","61","CCBY","5 Jan 2021","","","IEEE","IEEE Journals"
"Large AI Models in Health Informatics: Applications, Challenges, and the Future","J. Qiu; L. Li; J. Sun; J. Peng; P. Shi; R. Zhang; Y. Dong; K. Lam; F. P. . -W. Lo; B. Xiao; W. Yuan; N. Wang; D. Xu; B. Lo","Precision Robotics (Hong Kong) Ltd., Hong Kong; Department of Informatics, King's College London, London, U.K.; School of Engineering, Stanford University, Stanford, CA, USA; Department of Engineering Science, University of Oxford, Oxford, U.K.; Department of Biomedical Engineering, The Chinese University of Hong Kong, Hong Kong; Precision Robotics (Hong Kong) Ltd., Hong Kong; Faculty of Engineering, The University of Hong Kong, Hong Kong; Department of Surgery and Cancer, Imperial College London, London, U.K.; Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.; Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.; Department of Biomedical Engineering, The Chinese University of Hong Kong, Hong Kong; Beijing Tongren Eye Center, Beijing Tongren Hospital, Capital Medical University, Beijing, China; Department of Electrical Engineering, University of Missouri, Columbia, MO, USA; Facualty of Medicine, Imperial College London, London, U.K.",IEEE Journal of Biomedical and Health Informatics,"5 Dec 2023","2023","27","12","6074","6087","Large AI models, or foundation models, are models recently emerging with massive scales both parameter-wise and data-wise, the magnitudes of which can reach beyond billions. Once pretrained, large AI models demonstrate impressive performance in various downstream tasks. A prime example is ChatGPT, whose capability has compelled people's imagination about the far-reaching influence that large AI models can have and their potential to transform different domains of our lives. In health informatics, the advent of large AI models has brought new paradigms for the design of methodologies. The scale of multi-modal data in the biomedical and health domain has been ever-expanding especially since the community embraced the era of deep learning, which provides the ground to develop, validate, and advance large AI models for breakthroughs in health-related areas. This article presents a comprehensive review of large AI models, from background to their applications. We identify seven key sectors in which large AI models are applicable and might have substantial influence, including: 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics. We examine their challenges, followed by a critical discussion about potential future directions and pitfalls of large AI models in transforming the field of health informatics.","2168-2208","","10.1109/JBHI.2023.3316750","Research Grants Council (RGC) of Hong Kong SAR(grant numbers:ECS24211020,GRF14203821,GRF14216222); Innovation and Technology Fund (ITF) of Hong Kong SAR(grant numbers:ITS/240/21); Science, Technology and Innovation Commission (STIC) of Shenzhen Municipality(grant numbers:SGDX20220530111005039); Bill & Melinda Gates Foundation(grant numbers:OPP1171395); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261199","Artificial intelligence;bioinformatics;biomedicine;deep learning;foundation model;health informatics;healthcare;medical imaging","Data models;Artificial intelligence;Biological system modeling;Task analysis;Bioinformatics;Medical diagnostic imaging;Chatbots","Humans;Medical Informatics;Computational Biology;Imagination;Public Health;Robotics","55","","190","CCBY","22 Sep 2023","","","IEEE","IEEE Journals"
"Dynamic Service Placement in Multi-Access Edge Computing: A Systematic Literature Review","H. Tabatabaee Malazi; S. R. Chaudhry; A. Kazmi; A. Palade; C. Cabrera; G. White; S. Clarke","Department of Computer Science, Maynooth University, Maynooth, Co. Kildare, Ireland; Department of Computer Science, Munster Technological University, Cork, Ireland; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland; Department of Computer Science and Technology, University of Cambridge, Cambridge, U.K.; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland",IEEE Access,"30 Mar 2022","2022","10","","32639","32688","The advent of new cloud-based applications such as mixed reality, online gaming, autonomous driving, and healthcare has introduced infrastructure management challenges to the underlying service network. Multi-access edge computing (MEC) extends the cloud computing paradigm and leverages servers near end-users at the network edge to provide a cloud-like environment. The optimum placement of services on edge servers plays a crucial role in the performance of such service-based applications. Dynamic service placement problem addresses the adaptive configuration of application services at edge servers to facilitate end-users and those devices that need to offload computation tasks. While reported approaches in the literature shed light on this problem from a particular perspective, a panoramic study of this problem reveals the research gaps in the big picture. This paper introduces the dynamic service placement problem and outline its relations with other problems such as task scheduling, resource management, and caching at the edge. We also present a systematic literature review of existing dynamic service placement methods for MEC environments from networking, middleware, applications, and evaluation perspectives. In the first step, we review different MEC architectures and their enabling technologies from a networking point of view. We also introduce different cache deployment solutions in network architectures and discuss their design considerations. The second step investigates dynamic service placement methods from a middleware viewpoint. We review different service packaging technologies and discuss their trade-offs. We also survey the methods and identify eight research directions that researchers follow. Our study categorises the research objectives into six main classes, proposing a taxonomy of design objectives for the dynamic service placement problem. We also investigate the reported methods and devise a solutions taxonomy comprising six criteria. In the third step, we concentrate on the application layer and introduce the applications that can take advantage of dynamic service placement. The fourth step investigates evaluation environments used to validate the solutions, including simulators and testbeds. We introduce real-world datasets such as edge server locations, mobility traces, and service requests used to evaluate the methods. We compile a list of open issues and challenges categorised by various viewpoints in the last step.","2169-3536","","10.1109/ACCESS.2022.3160738","Science Foundation Ireland (SFI), Enable Project(grant numbers:16/SP/3804); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9738624","Mobile edge computing;decentralised cloud;MEC server;service caching;service offloading;computational offloading;service deployment;resource management;service orchestration","Cloud computing;Servers;Vehicle dynamics;Resource management;Wireless fidelity;Taxonomy;Task analysis","","51","","284","CCBY","21 Mar 2022","","","IEEE","IEEE Journals"
"A Comprehensive Survey: Evaluating the Efficiency of Artificial Intelligence and Machine Learning Techniques on Cyber Security Solutions","M. Ozkan-Okay; E. Akin; Ö. Aslan; S. Kosunalp; T. Iliev; I. Stoyanov; I. Beloev","Department of Computer Engineering, Ankara University, Gölbaşı, Ankara, Turkey; Department of Computer Engineering, Bitlis Eren University, Merkez, Bitlis, Turkey; Department of Software Engineering, Bandırma Onyedi Eylül University, Bandırma, Balıkesir, Turkey; Department of Computer Technologies, Gönen Vocational School, Bandırma Onyedi Eylül University, Bandırma, Balıkesir, Turkey; Department of Telecommunication, University of Ruse, Ruse, Bulgaria; Department of Electrical and Power Engineering, University of Ruse, Ruse, Bulgaria; Department of Transport, University of Ruse, Ruse, Bulgaria",IEEE Access,"25 Jan 2024","2024","12","","12229","12256","Given the continually rising frequency of cyberattacks, the adoption of artificial intelligence methods, particularly Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL), has become essential in the realm of cybersecurity. These techniques have proven to be effective in detecting and mitigating cyberattacks, which can cause significant harm to individuals, organizations, and even countries. Machine learning algorithms use statistical methods to identify patterns and anomalies in large datasets, enabling security analysts to detect previously unknown threats. Deep learning, a subfield of ML, has shown great potential in improving the accuracy and efficiency of cybersecurity systems, particularly in image and speech recognition. On the other hand, RL is again a subfield of machine learning that trains algorithms to learn through trial and error, making it particularly effective in dynamic environments. We also evaluated the usage of ChatGPT-like AI tools in cyber-related problem domains on both sides, positive and negative. This article provides an overview of how ML, DL, and RL are applied in cybersecurity, including their usage in malware detection, intrusion detection, vulnerability assessment, and other areas. The paper also specifies several research questions to provide a more comprehensive framework to investigate the efficiency of AI and ML models in the cybersecurity domain. The state-of-the-art studies using ML, DL, and RL models are evaluated in each Section based on the main idea, techniques, and important findings. It also discusses these techniques’ challenges and limitations, including data quality, interpretability, and adversarial attacks. Overall, the use of ML, DL, and RL in cybersecurity holds great promise for improving the effectiveness of security systems and enhancing our ability to protect against cyberattacks. Therefore, it is essential to continue developing and refining these techniques to address the ever-evolving nature of cyber threats. Besides, some promising solutions that rely on machine learning, deep learning, and reinforcement learning are susceptible to adversarial attacks, underscoring the importance of factoring in this vulnerability when devising countermeasures against sophisticated cyber threats. We also concluded that ChatGPT can be a valuable tool for cybersecurity, but it should be noted that ChatGPT-like tools can also be manipulated to threaten the integrity, confidentiality, and availability of data.","2169-3536","","10.1109/ACCESS.2024.3355547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10403908","Cyberattacks and solutions;deep learning;machine learning;reinforcement learning;AI tools","Computer security;Deep learning;Security;Fraud;Prediction algorithms;Telecommunication traffic;Reinforcement learning;Machine learning;Artificial intelligence","","27","","180","CCBYNCND","18 Jan 2024","","","IEEE","IEEE Journals"
"2022 Review of Data-Driven Plasma Science","R. Anirudh; R. Archibald; M. S. Asif; M. M. Becker; S. Benkadda; P. -T. Bremer; R. H. S. Budé; C. S. Chang; L. Chen; R. M. Churchill; J. Citrin; J. A. Gaffney; A. Gainaru; W. Gekelman; T. Gibbs; S. Hamaguchi; C. Hill; K. Humbird; S. Jalas; S. Kawaguchi; G. -H. Kim; M. Kirchen; S. Klasky; J. L. Kline; K. Krushelnick; B. Kustowski; G. Lapenta; W. Li; T. Ma; N. J. Mason; A. Mesbah; C. Michoski; T. Munson; I. Murakami; H. N. Najm; K. E. J. Olofsson; S. Park; J. L. Peterson; M. Probst; D. Pugmire; B. Sammuli; K. Sawlani; A. Scheinker; D. P. Schissel; R. J. Shalloo; J. Shinagawa; J. Seong; B. K. Spears; J. Tennyson; J. Thiagarajan; C. M. Ticoş; J. Trieschmann; J. v. Dijk; B. V. Essen; P. Ventzek; H. Wang; J. T. L. Wang; Z. Wang; K. Wende; X. Xu; H. Yamada; T. Yokoyama; X. Zhang","Lawrence Livermore National Laboratory, Livermore, CA, USA; Department of Electrical and Computer Engineering, Oak Ridge National Laboratory, Oak Ridge, TN, USA; University of California at Riverside, Riverside, CA, USA; Leibniz Institute for Plasma Science and Technology (INP), Greifswald, Germany; CNRS, PIIM UMR 7345, Aix-Marseille University, Marseille, France; Lawrence Livermore National Laboratory, Livermore, CA, USA; Department of Applied Physics, Eindhoven University of Technology, Eindhoven, The Netherlands; Princeton Plasma Physics Laboratory, Princeton, NJ, USA; Institute of Ion Physics and Applied Physics, University of Innsbruck, Innsbruck, Austria; Princeton Plasma Physics Laboratory, Princeton, NJ, USA; Dutch Institute for Fundamental Energy Research (DIFFER), Eindhoven, The Netherlands; Lawrence Livermore National Laboratory, Livermore, CA, USA; Department of Electrical and Computer Engineering, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Department of Physics and Astronomy, University of California at Los Angeles, Los Angeles, CA, USA; NVIDIA, Santa Clara, CA, USA; Center for Atomic and Molecular Technologies, Graduate School of Engineering, Osaka University, Osaka, Japan; Department of Nuclear Sciences and Applications, International Atomic Energy Agency, Vienna, Austria; Lawrence Livermore National Laboratory, Livermore, CA, USA; Center for Free-Electron Laser Science and the Department of Physics, Universität Hamburg, Hamburg, Germany; Division of Information and Electronic Engineering, Graduate School of Engineering, Muroran Institute of Technology, Muroran, Japan; Department of Nuclear Engineering, Seoul National University, Seoul, Republic of Korea; Deutsches Elektronen-Synchrotron (DESY), Hamburg, Germany; Department of Electrical and Computer Engineering, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Los Alamos National Laboratory, Los Alamos, NM, USA; Center for Ultrafast Optical Science, University of Michigan, Ann Arbor, MI, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Department of Mathematics, University of Leuven (KU Leuven), Leuven, Belgium; Los Alamos National Laboratory, Los Alamos, NM, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Department of Physical Sciences, University of Kent, Canterbury, U.K.; Department of Chemical and Biomolecular Engineering, University of California at Berkeley, Berkeley, CA, USA; Oden Institute for Computational Engineering and Sciences, The University of Texas at Austin, Austin, TX, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; National Institute for Fusion Science, National Institutes of Natural Sciences, Gifu, Japan; Sandia National Laboratories, Livermore, CA, USA; General Atomics, San Diego, CA, USA; Samsung Display Company Ltd., Asan-si, Republic of Korea; Lawrence Livermore National Laboratory, Livermore, CA, USA; Institute of Ion Physics and Applied Physics, University of Innsbruck, Innsbruck, Austria; Department of Electrical and Computer Engineering, Oak Ridge National Laboratory, Oak Ridge, TN, USA; General Atomics, San Diego, CA, USA; Lam Research Corporation, Fremont, CA, USA; Los Alamos National Laboratory, Los Alamos, NM, USA; General Atomics, San Diego, CA, USA; Deutsches Elektronen-Synchrotron (DESY), Hamburg, Germany; Tokyo Electron America, Inc., Austin, TX, USA; Samsung Display Company Ltd., Asan-si, Republic of Korea; Lawrence Livermore National Laboratory, Livermore, CA, USA; Department of Physics and Astronomy, University College London, London, U.K.; Lawrence Livermore National Laboratory, Livermore, CA, USA; National Institute for Laser, Plasma and Radiation Physics, Măgurele, Romania; Theoretical Electrical Engineering, Faculty of Engineering, Kiel University, Kiel, Germany; Department of Applied Physics, Eindhoven University of Technology, Eindhoven, The Netherlands; Lawrence Livermore National Laboratory, Livermore, CA, USA; Tokyo Electron America, Inc., Austin, TX, USA; Institute for Space Weather Sciences, New Jersey Institute of Technology, Newark, NJ, USA; Institute for Space Weather Sciences, New Jersey Institute of Technology, Newark, NJ, USA; Los Alamos National Laboratory, Los Alamos, NM, USA; Leibniz Institute for Plasma Science and Technology (INP), Greifswald, Germany; Lawrence Livermore National Laboratory, Livermore, CA, USA; Graduate School of Frontier Sciences, The University of Tokyo, Chiba, Japan; Graduate School of Frontier Sciences, The University of Tokyo, Chiba, Japan; Los Alamos National Laboratory, Los Alamos, NM, USA",IEEE Transactions on Plasma Science,"21 Aug 2023","2023","51","7","1750","1838","Data-driven science and technology offer transformative tools and methods to science. This review article highlights the latest development and progress in the interdisciplinary field of data-driven plasma science (DDPS), i.e., plasma science whose progress is driven strongly by data and data analyses. Plasma is considered to be the most ubiquitous form of observable matter in the universe. Data associated with plasmas can, therefore, cover extremely large spatial and temporal scales, and often provide essential information for other scientific disciplines. Thanks to the latest technological developments, plasma experiments, observations, and computation now produce a large amount of data that can no longer be analyzed or interpreted manually. This trend now necessitates a highly sophisticated use of high-performance computers for data analyses, making artificial intelligence and machine learning vital components of DDPS. This article contains seven primary sections, in addition to the introduction and summary. Following an overview of fundamental data-driven science, five other sections cover widely studied topics of plasma science and technologies, i.e., basic plasma physics and laboratory experiments, magnetic confinement fusion, inertial confinement fusion and high-energy-density physics, space and astronomical plasmas, and plasma technologies for industrial and other applications. The final Section before the summary discusses plasma-related databases that could significantly contribute to DDPS. Each primary Section starts with a brief introduction to the topic, discusses the state-of-the-art developments in the use of data and/or data-scientific approaches, and presents the summary and outlook. Despite the recent impressive signs of progress, the DDPS is still in its infancy. This article attempts to offer a broad perspective on the development of this field and identify where further innovations are required.","1939-9375","","10.1109/TPS.2023.3268170","U.S. Department of Energy, the Office of Science; Office of Advanced Scientific Computing Research, SciDAC Program, through the FASTMath Institute, Oak Ridge National Laboratory(grant numbers:DEAC05-00OR22725); United States Department of Energy (DOE)/High Energy Physics (HEP)(grant numbers:DE-SC0016804); U.S. DOE through the Lawrence Livermore National Laboratory(grant numbers:DE-AC52-07NA27344); European Union’s Horizon 2020 Research and Innovation Programme(grant numbers:776262); Artificial Intelligence and Data Analytics Lab, Prince Sultan University; European Union via the Euratom Research and Training Programme(grant numbers:101052200—EUROfusion); JSPS Core-to-Core Program(grant numbers:JPJSCCA2019002); National Research Council of Science and Technology (NST); Korea Government (MSIT)(grant numbers:CRC-20-01-NFRI); German Federal Ministry of Education and Research (BMBF)(grant numbers:16FDM005,16QK03A); U.S. Air Force Office of Scientific Research Award(grant numbers:FA9550-21-1-0330); CNRS International Research Project (IRP) FJ-IPL; PPS-Contribution Research and Innovation of the Ministry of Economic Affairs and Climate Policy (The Netherlands); ASML; U.S. DOE Office of Fusion Energy Science and the Office of Advanced Computing Research(grant numbers:DE-AC02-09CH11466); Princeton University on behalf of the Princeton Plasma Physics Laboratory, the SciDAC Partnership Center for High-Performance Boundary Plasma Simulation; Theory Department; Japan Society of Promotion of Science (JSPS) Grants-in-Aid for Scientific Research(grant numbers:(S) 15H05736,(A) 21H04453); JSPS Core-to-Core Program(grant numbers:JPJSCCA2019002); Osaka University International Joint Research Promotion Programs; U.S. Department of Energy, the Office of Science; Office of Advanced Scientific Computing Research, SciDAC Program, through the FASTMath Institute, Argonne National Laboratory(grant numbers:DE-AC02-06CH11357); U.S. Department of Energy, the Office of Science; Office of Advanced Scientific Computing Research, SciDAC Program; Deutsche Forschungsgemeinschaft (DFG), German Research Foundation(grant numbers:138690629—TRR 87,434434223—SFB 1461); Tokyo Electron Technology Solutions Ltd; U.S. Department of Energy through the Los Alamos National Laboratory; German Ministry of Education and Research (BMBF)(grant numbers:03Z22DN12,03Z22D511); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214236","Artificial intelligence;data-driven plasma science;machine learning;nuclear fusion;plasma control;plasma diagnostics;plasma processing;plasma simulation","Plasmas;Physics;Scientific computing;Research and development;Data science;Contracts;Technological innovation","","18","","744","CCBY","9 Aug 2023","","","IEEE","IEEE Journals"
"Exploring the Reactions of Early Users of ChatGPT to the Tool using Twitter Data: Sentiment and Topic Analyses","A. Tounsi; S. Elkefi; S. L. Bhar","Department of Civil, Environmental & Ocean Engineering, Stevens Institute of Technology, Hoboken, USA; School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, USA; LR-OASIS, National Engineering School of Tunis, Tunis, Tunisia",2023 IEEE International Conference on Advanced Systems and Emergent Technologies (IC_ASET),"20 Jun 2023","2023","","","1","6","Recently, large language models have gained considerable interest due to their impressive performance on different tasks. OpenAl's ChatGPT is a revolutionary language model that has quickly become popular among early adopters. Many users have gone so far as to call it a disruptive technology in many different domains. Its large, pre-trained model provides powerful capabilities already being applied to various use cases. Gaining an understanding of the thoughts and feelings of those who are quick to embrace new technologies is essential, as it can offer valuable insights into the technology's potential successes, failures, and other characteristics. In this paper, a mixed-method study was developed using 463,983 tweets from early ChatGPT users. We first apply topic modeling to identify the maj or topics and then conduct an extensive qualitative sentiment analysis. We observed that most people were pleased with the tool, but some expressed apprehension about its potential ethical and legal implications. After its launch, however, the fears were largely alleviated as people began to use it and recognize its benefits for personal and business purposes. It is important to be aware of any new technology's possible drawbacks and use it responsibly.","","979-8-3503-2102-9","10.1109/IC_ASET58101.2023.10150870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10150870","ChatGPT;Artificial Intelligence;OpenAI;Sentiment analysis;Topic Analysis","Ethics;Sentiment analysis;Analytical models;Social networking (online);Law;Blogs;Chatbots","","5","","21","IEEE","20 Jun 2023","","","IEEE","IEEE Conferences"
"A Review of Knowledge Distillation in Object Detection","ShengJieCheng; QiuxiaZhao; XinYunZhang; N. Yadikar; K. Ubul","School of Information Science and Engineering, Xinjiang University, Ürümqi, China; School of Software, Xinjiang University, Ürümqi, China; Key Laboratory of Xinjiang Multilingual Information Technology, Xinjiang University, Ürümqi, China; School of Information Science and Engineering, Xinjiang University, Ürümqi, China; School of Information Science and Engineering, Xinjiang University, Ürümqi, China",IEEE Access,"","2023","PP","99","1","1","Target detection is a revolutionary advancement in computer vision that provides the ability to identify specific targets in images for a wide variety of applications, including but not limited to video surveillance, face recognition, and autonomous driving. Although target detection has been developed to a high level and can be deployed for applications in several fields, there are still some problems in practice, such as the two-phase detection algorithm has high detection accuracy but slow detection speed, while the one-phase detection algorithm is fast, but its accuracy is poor. We need to combine their respective advantages further for related algorithm research, and we need to reach a balance between detection speed and detection accuracy so that the algorithms can be deployed on edge devices with limited computational power. Knowledge Refinement, as one of the common means of model compression, can solve the above problems, and it reduces the deployment difficulty of the algorithms. In this paper, we summarize the use of knowledge compression on target detection. We conclude the methods mentioned in the paper from an objective and unbiased perspective and suggest possible improvement directions, and we provide an outlook on the future trend of combining distillation learning and target detection. This paper provides a clear overview of the field of target detection and provides an idea of future trends.","2169-3536","","10.1109/ACCESS.2023.3288692","National Natural Science Foundation of China(grant numbers:61862061,62061045,62266044); National Key Research and Development Program of China(grant numbers:2021YFB2802100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10159388","target detection;knowledge distillation;deep learning","Feature extraction;Object detection;Targeting;Detectors;Knowledge engineering;Deep learning;Prediction algorithms","","3","","","CCBYNCND","22 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Legal Natural Language Processing From 2015 to 2022: A Comprehensive Systematic Mapping Study of Advances and Applications","E. Quevedo; T. Cerny; A. Rodriguez; P. Rivas; J. Yero; K. Sooksatra; A. Zhakubayev; D. Taibi","Department of Computer Science, School of Engineering and Computer Science, Baylor University, Waco, TX, USA; Systems and Industrial Engineering, The University of Arizona, Tucson, AZ, USA; Department of Computer Science, School of Engineering and Computer Science, Baylor University, Waco, TX, USA; Department of Computer Science, School of Engineering and Computer Science, Baylor University, Waco, TX, USA; Department of Computer Science, School of Engineering and Computer Science, Baylor University, Waco, TX, USA; Department of Computer Science, School of Engineering and Computer Science, Baylor University, Waco, TX, USA; Department of Computer Science, School of Engineering and Computer Science, Baylor University, Waco, TX, USA; M3S Research Unit, University of Oulu, Oulu, Finland",IEEE Access,"17 Oct 2024","2024","12","","145286","145317","The surge in legal text production has amplified the workload for legal professionals, making many tasks repetitive and time-consuming. Furthermore, the complexity and specialized language of legal documents pose challenges not just for those in the legal domain but also for the general public. This emphasizes the potential role and impact of Legal Natural Language Processing (Legal NLP). Although advancements have been made in this domain, particularly after 2015 with the advent of Deep Learning and Large Language Models (LLMs), a systematic exploration of this progress until 2022 is nonexistent. In this research, we perform a Systematic Mapping Study (SMS) to bridge this gap. We aim to provide a descriptive statistical analysis of the Legal NLP research between 2015 and 2022. Categorize and sub-categorize primary publications based on their research problems. Identify limitations and areas of improvement in current research. Using a robust search methodology across four reputable indexers, we filtered 536 papers down to 75 pivotal articles. Our findings reveal the diverse methods employed for tasks such as Multiclass Classification, Summarization, and Question Answering in the Legal NLP field. We also highlight resources, challenges, and gaps in current methodologies and emphasize the need for curated datasets, ontologies, and a focus on inherent difficulties like data accessibility. As the legal sector gradually embraces Natural Language Processing (NLP), understanding the capabilities and limitations of Legal NLP becomes vital for ensuring efficient and ethical application. The research offers insights for both Legal NLP researchers and the broader legal community, advocating for continued advancements in automation while also addressing ethical concerns.","2169-3536","","10.1109/ACCESS.2023.3333946","National Science Foundation(grant numbers:2409933,2039678,2136961,2210091); Research Council of Finland(grant numbers:349488); MuFAno; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10320368","Systematic-mapping-study;legal-NLP;deep learning","Law;Natural language processing;Task analysis;Systematics;Information retrieval;Surveys;Search problems;Deep learning","","2","","125","CCBY","16 Nov 2023","","","IEEE","IEEE Journals"
"CSAT: Contrastive Sampling-Aggregating Transformer for Community Detection in Attribute-Missing Networks","M. Li; Y. Zhang; W. Zhang; S. Zhao; X. Piao; B. Yin","Department of Information Science, Beijing Key Laboratory of Multimedia and Intelligent Software Technology, Beijing Institute of Artificial Intelligence, Beijing University of Technology, Beijing, China; Department of Information Science, Beijing Key Laboratory of Multimedia and Intelligent Software Technology, Beijing Institute of Artificial Intelligence, Beijing University of Technology, Beijing, China; College of Economics and Management, China Agricultural University, Beijing, China; Department of Information Science, Beijing Key Laboratory of Multimedia and Intelligent Software Technology, Beijing Institute of Artificial Intelligence, Beijing University of Technology, Beijing, China; Department of Information Science, Beijing Key Laboratory of Multimedia and Intelligent Software Technology, Beijing Institute of Artificial Intelligence, Beijing University of Technology, Beijing, China; Department of Information Science, Beijing Key Laboratory of Multimedia and Intelligent Software Technology, Beijing Institute of Artificial Intelligence, Beijing University of Technology, Beijing, China",IEEE Transactions on Computational Social Systems,"2 Apr 2024","2024","11","2","2277","2290","Community detection aims to identify dense subgroups of nodes within a network. However, in real-world networks, node attributes are often missing, making traditional methods less effective. In networks with missing attributes, the main challenge of community detection is to deal with the missing attribute information efficiently and use network structure information to make accurate predictions. This article proposes an innovative method called contrastive sampling-aggregating transformer (CSAT) for community detection in attribute-missing networks. CSAT incorporates the contrastive learning principle to capture hidden patterns among nodes and to aggregate information from different samples to create a more robust and accurate methodology for community detection. Specifically, CSAT utilizes a sampling and propagation strategy to obtain different samples and smooth attribute features of the network structure and leverages the Transformer architecture to model the pairwise relationships between nodes. Therefore, our method can address the attribute-missing issue by integrating the auxiliary information from both the network structure and other sources. Extensive experiments on several benchmark datasets demonstrate CSAT’s superior performance compared to the state-of-the-art methods for community detection.","2329-924X","","10.1109/TCSS.2023.3292145","National Key Research and Development Program of China(grant numbers:2021ZD0111902); National Natural Science Foundation of China(grant numbers:62072015,U21B2038,U19B2039); Beijing Natural Science Foundation(grant numbers:4222021); Research and Development Program of Beijing Municipal Education Commission(grant numbers:KZ202210005008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10210121","Attribute-missing graph;community detection;deep graph representation learning;graph contrastive learning;Transformer","Transformers;Feature extraction;Representation learning;Image edge detection;Network analyzers;Interpolation;Task analysis","","1","","51","IEEE","7 Aug 2023","","","IEEE","IEEE Journals"
"Detection and Visualization of Neglected Tropical Skin Diseases Using EfficientNet and Grad-CAM","P. Surasinghe; P. Sabapathippillai; K. Thanikasalam","Department of Physical Science, University of Vavuniya, Sri Lanka; Department of Physical Science, University of Vavuniya, Sri Lanka; Department of Computer Science, University of Jaffna, Sri Lanka",2023 5th International Conference on Advancements in Computing (ICAC),"12 Feb 2024","2023","","","472","477","Early skin disease detection is crucial for both effective treatment and the prevention of spreading to others. Neglected Tropical Skin Diseases (skin-NTDs) primarily affect low-income and developing countries, receiving inadequate attention and resources in comparison to other health issues. In this study, an automated system has been developed for detecting five skin-NTDs, capable of recognizing the diseases from raw lesion images without requiring any pre-processing. A dataset was created in the initial phase of the study since no publicly available benchmarks are available. The EfficientNet family of pre-trained models was utilized to train the classifier, and the EfficientNet-B3 was selected based on the experimental results. Additionally, the proposed work has developed a Grad-CAM based visualization technique to identify the most influential regions within images for the classification of specific diseases. The proposed model exhibited an overall classification accuracy of 91.53% on the test data. The proposed work will offer benefits to frontline medical staff and local residents in low-income countries.","2837-5424","979-8-3503-5813-1","10.1109/ICAC60630.2023.10417550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10417550","EfficientNet;Grad-CAM;Neglected Tropical Skin Diseases","Data visualization;Benchmark testing;Skin;Data models;Lesions;Diseases;Biomedical imaging","","1","","31","IEEE","12 Feb 2024","","","IEEE","IEEE Conferences"
"A Survey on Geolocation on the Internet","A. Zilberman; A. Offer; B. Pincu; Y. Glickshtein; R. Kant; O. Brodt; A. Otung; R. Puzis; A. Shabtai; Y. Elovici","and Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel; and Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel; and Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel; and Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel; and Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel; and Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel; Fujitsu, Slough, United Kingdom; and Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel; and Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel; and Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel",IEEE Communications Surveys & Tutorials,"","2024","PP","99","1","1","In the interconnected world of the Internet, IP geolocation -identifying the geographic location of a device, user, or data source given their IP -plays an essential role in numerous applications and services. Over the years, IP geolocation technology has evolved, and it is now a cornerstone of the digital age. However, attaining accurate IP geolocation is not without its challenges. Our comprehensive survey explores IP geolocation, focusing on emerging challenges in security, data geolocation in the cloud, and coping with proxies and VPNs that are used to hide the real IP geolocation. These topics have gained prominence as organizations grapple with safeguarding the location of sensitive information, complying with data privacy regulations, and adapting to the dynamic digital landscape. By synthesizing research, analyzing industry practices, and exploring trends, we equip readers with a taxonomy and insights into the multifaceted world of geolocation and its vital role in shaping the digital future. This comprehensive survey not only enriches understanding of IP geolocation and its challenges but also provides a roadmap for future research, fostering increased awareness and the development of informed strategies for addressing the evolving landscape of geospatial security on the Internet.","1553-877X","","10.1109/COMST.2024.3518398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802881","Geolocation;IP-Geolocation;Passive Geolocation;Active Geolocation;Probes;Measurements;Delay;Landmarks;Localization;Multilateration;Triangulation;Single Radius;RIPE Atlas;Point of Presence;Cloud Geolocation;Data Geolocation;Survey","Geology;Surveys;IP networks;Cloud computing;Tutorials;Industries;Virtual private networks;Market research;User experience;Probes","","1","","","IEEE","16 Dec 2024","","","IEEE","IEEE Early Access Articles"
"The Language of Infographics: Toward Understanding Conceptual Metaphor Use in Scientific Storytelling","H. Pokojná; T. Isenberg; S. Bruckner; B. Kozlíková; L. Garrison","Visitlab, Faculty of Informatics, Masaryk University, Czech Republic; CNRS, Inria, LISN, Inria, Université Paris-Saclay, France; Institute for Visual and Analytic Computing, University of Rostock, Germany; Visitlab, Faculty of Informatics, Masaryk University, Czech Republic; Department of Informatics & Mohn Medical Imaging and Visualization Centre, Department of Radiology, Haukeland University Hospital, University of Bergen, Norway",IEEE Transactions on Visualization and Computer Graphics,"25 Nov 2024","2025","31","1","371","381","We apply an approach from cognitive linguistics by mapping Conceptual Metaphor Theory (CMT) to the visualization domain to address patterns of visual conceptual metaphors that are often used in science infographics. Metaphors play an essential part in visual communication and are frequently employed to explain complex concepts. However, their use is often based on intuition, rather than following a formal process. At present, we lack tools and language for understanding and describing metaphor use in visualization to the extent where taxonomy and grammar could guide the creation of visual components, e.g., infographics. Our classification of the visual conceptual mappings within scientific representations is based on the breakdown of visual components in existing scientific infographics. We demonstrate the development of this mapping through a detailed analysis of data collected from four domains (biomedicine, climate, space, and anthropology) that represent a diverse range of visual conceptual metaphors used in the visual communication of science. This work allows us to identify patterns of visual conceptual metaphor use within the domains, resolve ambiguities about why specific conceptual metaphors are used, and develop a better overall understanding of visual metaphor use in scientific infographics. Our analysis shows that ontological and orientational conceptual metaphors are the most widely applied to translate complex scientific concepts. To support our findings we developed a visual exploratory tool based on the collected database that places the individual infographics on a spatio-temporal scale and illustrates the breakdown of visual conceptual metaphors.","1941-0506","","10.1109/TVCG.2024.3456327","University of Bergen; Trond Mohn Foundation in Bergen; Visualizing Data Science for Large Scale Hypothesis Management in Imaging Biomarker Discovery (VIDI) Project(grant numbers:813558); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10682437","Visualization;visual metaphors;science communication;conceptual metaphors;visual communication","Visualization;Linguistics;Libraries;Biomedical imaging;Visual communication;Uncertainty;Shape","","","","92","IEEE","17 Sep 2024","","","IEEE","IEEE Journals"
"2 NOTHING ABOUT US WITHOUT US: CO-CREATION WITHIN COMMUNITIES","K. Cizek; W. Uricchio; J. Anderson; M. A. Carter; D. N. Agency; T. A. Harris; M. K. Holmes; R. Lachman; L. Massiah; C. Mertes; S. Rafsky; M. Stephenson; A. Winger-Bearskin; S. Wolozin",MIT Open Documentary Lab; Massachusetts Institute of Technology; Wayne State University; Emerson College; NA; Family Pictures USA; BlackStar Film Festival; Ryerson University; Scribe Video Center; Ford Foundation; NA; The Rada Film Group; NA; MIT Open Documentary Lab,Collective Wisdom: Co-Creating Media for Equity and Justice,"","2022","","","75","147","Community-based methods are the most commonly identified forms of media co- creation, both in real life and online. These are projects that put people with firsthand experience at the center of a practice rather than the artistic vision or agenda of a (usually) professional media maker who extracts stories from ""subjects"" and then displays them to ""audiences."" Community co-creation makes its watchword ""nothing about us without us,"" from the Latin phrase nihil de nobis, sine nobis, an expression that for ancient Roman regimes meant that no policy should be created without the participation of the people who would be affected.","","9780262369862","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9986289.pdf&bkn=9975235&pdfType=chapter","","","","","","","","15 Dec 2022","","","MIT Press","MIT Press eBook Chapters"
"Artificial Intelligence-Based Cybersecurity for the Metaverse: Research Challenges and Opportunities","A. Awadallah; K. Eledlebi; J. Zemerly; D. Puthal; E. Damiani; K. Taha; T. -Y. Kim; P. D. Yoo; K. -K. R. Choo; M. -S. Yim; C. Y. Yeun","Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; NA; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Indian Institute of Management Bodh Gaya, India; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Civil Infrastructure and Environmental Engineering Department, Khalifa University, Abu Dhabi, United Arab Emirates; University of London, London, United Kingdom; Department of Information Systems and Cyber Security, The University of Texas, San Antonio, TX, USA; Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates",IEEE Communications Surveys & Tutorials,"","2024","PP","99","1","1","The metaverse, known as the next-generation 3D Internet, represents virtual environments that mirror the physical world. It is supported by innovative technologies such as digital twins and extended reality (XR), which elevate user experiences across various fields. However, the metaverse also introduces significant cybersecurity and privacy challenges that remain underexplored. Due to its complex multi-tech infrastructure, the metaverse requires sophisticated, automated, and intelligent cybersecurity measures to mitigate emerging threats effectively. Therefore, this paper is the first to explore Artificial Intelligence (AI)-driven cybersecurity techniques for the metaverse, examining academic and industrial perspectives. First, we provide an overview of the metaverse, presenting a detailed system model, diverse use cases, and insights into its current industrial status. We then present attack models and cybersecurity threats derived from the unique characteristics and technologies of the metaverse. Next, we review AI-driven cybersecurity solutions based on three critical aspects: User authentication, intrusion detection systems (IDS), and the security of digital assets, specifically for Blockchain and Non-fungible Tokens (NFTs). Finally, we highlight challenges and suggest future research opportunities to enhance metaverse security, privacy, and digital asset transactions.","1553-877X","","10.1109/COMST.2024.3442475","Center for Cyber-Physical Systems, Khalifa University(grant numbers:Grant 8474999137-RC1-C2PS-T5); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10634174","Artificial Intelligence;biometrics;continuous authentication;cybersecurity;Digital Twins;intrusion detection;metaverse;multimodality;NFTs","Metaverse;Computer security;Artificial intelligence;Surveys;Security;Reviews;Privacy","","","","","CCBY","12 Aug 2024","","","IEEE","IEEE Early Access Articles"
"Extractive Text Summarization of Clinical Text Using Deep Learning Models","C. S. G; S. T. K; N. D. D; G. S. A. P; K. S. P. M","Computer Science Engineering, SRM University, Amaravathi, India; Computer Science Engineering, SRM University, Amaravathi, India; Computer Science Engineering, SRM University, Amaravathi, India; Computer Science Engineering, SRM University, Amaravathi, India; Computer Science Engineering, SRM University, Amaravathi, India",2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE),"18 Apr 2024","2024","","","1","6","This project focused on using clinical text data from the PubMed dataset to train transformer models and deep learning models for text summarization. The primary goal was to develop a system capable of identifying and extracting meaningful information from large clinical texts. Using transformer models and deep learning techniques, the goal was to improve the search for information in the medical literature. The ROUGE score, a widely accepted metric for automated summary assessment, was used to analyze the performance of the trained models. This project involved not only training and optimizing transformer and deep learning models to obtain a comprehensive summary, but also comparing their ROUGE scores to determine which model outperformed the others. This comparative analysis was necessary to determine the most effective model for extracting important insights from clinical texts. The findings have the potential to significantly impact information in the clinical domain, providing researchers and healthcare professionals with faster access to critical information.","","979-8-3503-2820-2","10.1109/ic-ETITE58242.2024.10493738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493738","Text Summarization;NLP;BERT;BART","Deep learning;Training;Measurement;Analytical models;Medical services;Transformers;Market research","","","","22","IEEE","18 Apr 2024","","","IEEE","IEEE Conferences"
"An Adaptation of DigComp for the South African Context","C. D. Preez; D. Van Greunen; C. Foxcroft","Nelson Mandela University, University Way, Summerstrand, Gqeberha, South Africa; Nelson Mandela University, University Way, Summerstrand, Gqeberha, South Africa; Nelson Mandela University, University Way, Summerstrand, Gqeberha, South Africa",2024 IST-Africa Conference (IST-Africa),"1 Jul 2024","2024","","","1","9","The rapid rise of disruptive technologies is fundamentally changing how we live, work, communicate, and relate to each other. Governments are implementing digital transformation strategies to improve service delivery, citizen participation, and information-sharing mechanisms. South Africa has identified digital skills as a critical challenge to becoming a professional and globally competitive knowledge society. However, the 2016 Global IT Report of the World Economic Forum ranks South Africa 95th out of 139 countries on the skills pillar due to poor education quality in math and science and a lack of effective use of digital opportunities. This paper is based on a PhD study aimed at measuring the digital competence of South African citizens. A unique definition of digital competence has been derived from the literature in pursuit of understanding the characteristics of digital competence. Furthermore, models and frameworks that measure digital competence were evaluated for their suitability to the South African context, which resulted in the selection of the EU DigComp framework. An adaptation of the same was developed for the South African context.","2576-8581","978-1-905824-73-1","10.23919/IST-Africa63983.2024.10569509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10569509","Digital Competence;Digital Competence Assessment Model;Citizens;DigComp;ZA Adapted DigComp Framework","Economics;Adaptation models;Biological system modeling;Digital transformation;Government;Education;Medical services","","","","20","","1 Jul 2024","","","IEEE","IEEE Conferences"
"Novel Learning-Based Multi-User Detection Algorithms for Spatially Correlated MTC","T. Sivalingam; S. Gunarathne; N. H. Mahmood; S. Ali; N. Rajatheva; M. Latva-aho","Faculty of Information Technology and Electrical Engineering, Center for Wireless Communications, University of Oulu, Finland; Faculty of Information Technology and Electrical Engineering, Center for Wireless Communications, University of Oulu, Finland; Faculty of Information Technology and Electrical Engineering, Center for Wireless Communications, University of Oulu, Finland; Faculty of Information Technology and Electrical Engineering, Center for Wireless Communications, University of Oulu, Finland; Faculty of Information Technology and Electrical Engineering, Center for Wireless Communications, University of Oulu, Finland; Faculty of Information Technology and Electrical Engineering, Center for Wireless Communications, University of Oulu, Finland",IEEE Internet of Things Journal,"","2025","PP","99","1","1","Emerging massive machine-type communications service class needs to support many devices while ensuring that scarce radio resources are utilized efficiently. Non-orthogonal multiple access is proposed to minimize the signaling overhead and optimize resource allocation. However, during the initial access, the base station is presented with the challenge of identifying sparsely active devices in the absence of knowledge about the sparsity and channel state information. The user channels in most practical scenarios have common reflection paths, making them partially correlated, which can be exploited to improve the detection performance at the base station. In this context, we formulate a novel multi-user detection (MUD) problem in spatially correlated Rician channels, which we reformulate as a multi-label classification problem utilizing deep learning techniques. We propose two diverse approaches to tackle this problem: ViT-Net, a vision transformer-based architecture, and FAR-Net, a fully activated deep neural network featuring residual connections. Our analysis highlights the significance of spatial correlation for MUD, which can accord around 13% higher overloading ratio compared to the non-correlated scenario. Numerical evaluations demonstrate the effectiveness of the proposed model in addressing spatial correlation compared to the existing deep learning models.","2327-4662","","10.1109/JIOT.2025.3552215","Research Council of Finland(grant numbers:369116); Business Finland(grant numbers:8002/31/2022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10930453","Multiple user detection;Rician channels;SCMA;spatial correlation;vision-transformer","Multiuser detection;Correlation;Transformers;NOMA;Rician channels;Deep learning;Iterative methods;Convergence;Computer architecture;6G mobile communication","","","","","CCBY","17 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Neuro-Symbolic AI: Design transparent and trustworthy systems that understand the world as you do","A. Dingli; D. Farrugia",NA; NA,Neuro-Symbolic AI: Design transparent and trustworthy systems that understand the world as you do,"","2023","","","","","Explore the inner workings of AI along with its limitations and future developments and create your first transparent and trustworthy neuro-symbolic AI system Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesUnderstand symbolic and statistical techniques through examples and detailed explanationsExplore the potential of neuro-symbolic AI for future developments using case studiesDiscover the benefits of combining symbolic AI with modern neural networks to build transparent and high-performance AI solutionsBook DescriptionNeuro-symbolic AI offers the potential to create intelligent systems that possess both the reasoning capabilities of symbolic AI along with the learning capabilities of neural networks. This book provides an overview of AI and its inner mechanics, covering both symbolic and neural network approaches. You’ll begin by exploring the decline of symbolic AI and the recent neural network revolution, as well as their limitations. The book then delves into the importance of building trustworthy and transparent AI solutions using explainable AI techniques. As you advance, you’ll explore the emerging field of neuro-symbolic AI, which combines symbolic AI and modern neural networks to improve performance and transparency. You’ll also learn how to get started with neuro-symbolic AI using Python with the help of practical examples. In addition, the book covers the most promising technologies in the field, providing insights into the future of AI. Upon completing this book, you will acquire a profound comprehension of neuro-symbolic AI and its practical implications. Additionally, you will cultivate the essential abilities to conceptualize, design, and execute neuro-symbolic AI solutions.What you will learnGain an understanding of the intuition behind neuro-symbolic AIDetermine the correct uses that can benefit from neuro-symbolic AIDifferentiate between types of explainable AI techniquesThink about, design, and implement neuro-symbolic AI solutionsCreate and fine-tune your first neuro-symbolic AI systemExplore the advantages of fusing symbolic AI with modern neural networks in neuro-symbolic AI systemsWho this book is forThis book is ideal for data scientists, machine learning engineers, and AI enthusiasts who want to explore the emerging field of neuro-symbolic AI and discover how to build transparent and trustworthy AI solutions. A basic understanding of AI concepts and familiarity with Python programming are needed to make the most of this book.","","9781804616956","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251250.pdf&bkn=10251249&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Joint 3D facial shape reconstruction and texture completion from a single image","X. Zeng; Z. Wu; X. Peng; Y. Qiao","Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; University of Chinese Academy of Sciences, Beijing, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China",Computational Visual Media,"20 Feb 2025","2022","8","2","239","256","Recent years have witnessed significant progress in image-based 3D face reconstruction using deep convolutional neural networks. However, current reconstruction methods often perform improperly in self-occluded regions and can lead to inaccurate correspondences between a 2D input image and a 3D face template, hindering use in real applications. To address these problems, we propose a deep shape reconstruction and texture completion network, SRTC-Net, which jointly reconstructs 3D facial geometry and completes texture with correspondences from a single input face image. In SRTC-Net, we leverage the geometric cues from completed 3D texture to reconstruct detailed structures of 3D shapes. The SRTC-Net pipeline has three stages. The first introduces a correspondence network to identify pixel-wise correspondence between the input 2D image and a 3D template model, and transfers the input 2D image to a U-V texture map. Then we complete the invisible and occluded areas in the U-V texture map using an inpainting network. To get the 3D facial geometries, we predict coarse shape (U-V position maps) from the segmented face from the correspondence network using a shape network, and then refine the 3D coarse shape by regressing the U-V displacement map from the completed U-V texture map in a pixel-to-pixel way. We examine our methods on 3D reconstruction tasks as well as face frontalization and pose invariant face recognition tasks, using both in-the-lab datasets (MICC, MultiPIE) and in-the-wild datasets (CFP). The qualitative and quantitative results demonstrate the effectiveness of our methods on inferring 3D facial geometry and complete texture; they outperform or are comparable to the state-of-the-art.","2096-0662","","10.1007/s41095-021-0238-4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897482","3D face reconstruction;U-V completion;pose invariant face recognition;deep learning","Three-dimensional displays;Face recognition;Image reconstruction;Shape;Solid modeling;Faces;Image segmentation;Geometry;Pipelines;Predictive models","","","","","","20 Feb 2025","","","TUP","TUP Journals"
"FocusCLIP: Focusing on Anomaly Regions by Visual-Text Discrepancies","Y. Zhao; J. Sun; L. Zhang; H. Lu","School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2024","PP","99","1","1","Few-shot anomaly detection aims to detect defects with only a limited number of normal samples for training. Recent few-shot methods typically focus on object-level features rather than subtle defects within objects, as pretrained models are generally trained on classification or image-text matching datasets. However, object-level features are often insufficient to detect defects, which are characterized by fine-grained texture variations. To address this, we propose FocusCLIP, which consists of a vision-guided branch and a language-guided branch. FocusCLIP leverages the complementary relationship between visual and text modalities to jointly emphasize discrepancies in fine-grained textures of defect regions. Specifically, we design three modules to mine these discrepancies. In the vision-guided branch, we propose the Bidirectional Self-knowledge Distillation (BSD) structure, which identifies anomaly regions through inconsistent representations and accumulates these discrepancies. Within this structure, the Anomaly Capture Module (ACM) is designed to refine features and detect more comprehensive anomalies by leveraging semantic cues from multi-head self-attention. In the language-guided branch, Multi-level Adversarial Class Activation Mapping (MACAM) utilizes foreground-invariant responses to adversarial text prompts, reducing interference from object regions and further focusing on defect regions. Our approach outperforms the state-of-the-art methods in few-shot anomaly detection. Additionally, the language-guided branch within FocusCLIP also demonstrates competitive performance in zero-shot anomaly detection, further validating the effectiveness of our proposed method.","1558-2205","","10.1109/TCSVT.2024.3524784","National Natural Science Foundation of China(grant numbers:62431004); Dalian Science and Technology Innovation Foundation(grant numbers:2023JJ12GX015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10819451","Anomaly detection;Self-knowledge distillation;Anomaly segmentation;Class activation mapping","Anomaly detection;Feature extraction;Semantics;Training;Focusing;Dispersion;Visualization;Interference;Image segmentation;Overfitting","","","","","IEEE","31 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Delving into Instance Modeling for Weakly Supervised Video Anomaly Detection","S. Sun; J. Hua; J. Feng; D. Wei; B. Lai; X. Gong","College of Information Science & Electronic Engineering, Zhejiang University, Hangzhou, China; Alibaba Cloud, Hangzhou, China; Alibaba Cloud, Hangzhou, China; Alibaba Cloud, Hangzhou, China; Alibaba Cloud, Hangzhou, China; College of Information Science & Electronic Engineering, Zhejiang University, Hangzhou, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Weakly-supervised video anomaly detection (WS-VAD) aims to identify fine-grained anomalies from sparse video-level labels, which has gained increasing attention in recent years due to its various applications such as disaster warning and public security. Recent studies typically formulate WS-VAD as a multi-instance learning (MIL) problem. However, they neglect the instance creation process and simply apply a uniform temporal pooling (UTP) operation to obtain the training instances, leading to severe anomaly contamination and dilution. In this paper, we emphasize the importance of the instance modeling procedure and propose two simple yet effective modules, i.e., the dynamic segment merging (DSM) module and the retrieval-augmented anomaly restoration (RA2R) module, to tackle the problem from segment-level and feature-level, respectively. We equip various state-of-the-art WS-VAD models with the proposed methods and conduct thorough experiments on the challenging datasets, e.g., UCF-Crime, and XD-Violence. Results demonstrate the proposed method brings consistent performance improvement and establishes new state-of-the-art.","1558-2205","","10.1109/TCSVT.2025.3546766","Zhejiang Province Pioneer Research and Development Project(grant numbers:2024C01017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10908237","Video anomaly detection;Multi-instance learning;Dynamic merging","Training;Feature extraction;Merging;Anomaly detection;Annotations;Contamination;Data models;Prototypes;Circuits and systems;Termination of employment","","","","","IEEE","28 Feb 2025","","","IEEE","IEEE Early Access Articles"
"ContextVLM: Zero-Shot and Few-Shot Context Understanding for Autonomous Driving Using Vision Language Models","S. Sural; Naren; R. R. Rajkumar","Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA",2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC),"20 Mar 2025","2024","","","468","475","In recent years, there has been a notable increase in the development of autonomous vehicle (AV) technologies aimed at improving safety in transportation systems. While AVs have been deployed in the real-world to some extent, a full-scale deployment requires AVs to robustly navigate through challenges like heavy rain, snow, low lighting, construction zones and GPS signal loss in tunnels. To be able to handle these specific challenges, an AV must reliably recognize the physical attributes of the environment in which it operates. In this paper, we define context recognition as the task of accurately identifying environmental attributes for an AV to appropriately deal with them. Specifically, we define 24 environmental contexts capturing a variety of weather, lighting, traffic and road conditions that an AV must be aware of. Motivated by the need to recognize environmental contexts, we create a context recognition dataset called DrivingContexts with more than 1.6 million context-query pairs relevant for an AV. Since traditional supervised computer vision approaches do not scale well to a variety of contexts, we propose a framework called ContextVLM that uses vision-language models to detect contexts using zero- and few-shot approaches. ContextVLM is capable of reliably detecting relevant driving contexts with an accuracy of more than 95% on our dataset, while running in real-time on a 4GB Nvidia GeForce GTX 1050 Ti GPU on an AV with a latency of 10.5 ms per query.","2153-0017","979-8-3315-0592-9","10.1109/ITSC58415.2024.10920066","US Department of Transportation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10920066","","Rain;Accuracy;Roads;Lighting;Graphics processing units;Real-time systems;Reliability;Autonomous vehicles;Context modeling;Meteorology","","","","26","IEEE","20 Mar 2025","","","IEEE","IEEE Conferences"
"Leveraging Artificial Intelligence for Strategic Advancement: Opportunities and Initiatives at the Miller Center","W. A. Shakir","Department of Computer Science, California State University, Fresno, USA",2024 1st International Conference on Emerging Technologies for Dependable Internet of Things (ICETI),"11 Dec 2024","2024","","","1","17","The rise of advanced AI technologies, such as ChatGPT, has presented institutions like the Miller Center with significant opportunities to enhance impact and modernize operations. This paper identifies the key challenges the Center faces in integrating AI into its workflows, focusing on the lack of clarity around how to capitalize on AI’s potential while managing its risks. By exploring AI initiatives at the University of Virginia and evaluating successful AI applications, this paper proposes a range of AI projects tailored to the Miller Center’s needs. These projects, spanning from low-risk tools to cutting-edge applications, are designed to improve data accessibility, streamline research processes, and foster public engagement. The recommendations aim to spark strategic discussions around AI adoption, enabling the Miller Center to leverage AI for both internal efficiency and external impact.","","979-8-3315-3355-7","10.1109/ICETI63946.2024.10777212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10777212","","Productivity;Leadership;Vocabulary;Technological innovation;Pain;Scholarships;Sparks;Artificial intelligence;Portfolios;Semantic technology","","","","15","IEEE","11 Dec 2024","","","IEEE","IEEE Conferences"
"3D Gaussian Splatting: Survey, Technologies, Challenges, and Opportunities","Y. Bao; T. Ding; J. Huo; Y. Liu; Y. Li; W. Li; Y. Gao; J. Luo","State Key Laboratory for Novel Software Technology, Nanjing University, China; Applied Sciences Group, Microsoft Corporation, Redmond, USA; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; Department of Computer Science, University of Rochester, America",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","3D Gaussian Splatting (3DGS) has emerged as a prominent technique with the potential to become a mainstream method for 3D representations. It can effectively transform multi-view images into explicit 3D Gaussian through efficient training, and achieve real-time rendering of novel views. This survey aims to analyze existing 3DGS-related works from multiple intersecting perspectives, including related tasks, technologies, challenges, and opportunities. The primary objective is to provide newcomers with a rapid understanding of the field and to assist researchers in methodically organizing existing technologies and challenges. Specifically, we delve into the optimization, application, and extension of 3DGS, categorizing them based on their focuses or motivations. Additionally, we summarize and classify nine types of technical modules and corresponding improvements identified in existing works. Based on these analyses, we further examine the common challenges and technologies across various tasks, proposing potential research opportunities.","1558-2205","","10.1109/TCSVT.2025.3538684","Natural Science Foundation of Jiangsu Province(grant numbers:BK20221441); Young Elite Scientists Sponsorship Program by CAST(grant numbers:2023QNRC001); National Natural Science Foundation of China(grant numbers:62106100,62192783,62276128); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10870258","3D Representations;Rendering;3DGS","Three-dimensional displays;Training;Rendering (computer graphics);Image reconstruction;Covariance matrices;Surveys;Optimization;Electronic mail;Computational efficiency;Vectors","","","","","IEEE","4 Feb 2025","","","IEEE","IEEE Early Access Articles"
"A Review of AI-Assisted Impact Analysis for Software Requirements Change: Challenges and Future Directions","A. Samhan; S. AlHajHassan; S. A. Dabaa't; A. Elrashidi","Software Engineering Department, Faculty of Information Technology, Zarqa University, Zarqa, Jordan; Software Engineering Department, Faculty of Information Technology, Applied Science Private University, Amman, Jordan; Software Engineering Department, Faculty of Information Technology, Zarqa University, Zarqa, Jordan; Electrical Engineering Department, Faculty of Engineering, University of Business and Technology, Jeddah, Saudi Arabia",2024 25th International Arab Conference on Information Technology (ACIT),"17 Feb 2025","2024","","","1","13","This paper presents a review study that explores the landscape and challenges of AI-assisted impact analysis in software requirements engineering. The growth of complex and change-prone software systems has highlighted the need for AI techniques—especially machine learning (ML) and natural language processing (NLP)—to be harnessed to improve impact analysis scalability and efficiency. Key challenges identified include data quality issues, scalability constraints, and ethical problems related to bias and accountability. To investigate these challenges and provide insights, a review was conducted across IEEE Xplore, ACM Digital Library, and Google Scholar, cross referencing between keywords that include concepts from the domains of ""software requirements"", ""impact analysis"", and ""AI-Assisted approaches"". Inclusion criteria focused on studies that presents empirical data on AI-assisted impact analysis. This study highlights research gaps and presents insights for overcoming obstacles using emerging technologies such as edge AI in order to enhance post-impact analysis. Findings show that while AI improves traditional impact analysis, further work is necessary to deliver effective, ethical and practical implementations within evolving software contexts.","2831-4948","979-8-3315-4001-2","10.1109/ACIT62805.2024.10877072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10877072","Impact Analysis;Software Requirements;Artificial Intelligence;Natural Language Processing;Software Engineering;AI Challenges;Edge AI","Ethics;Accuracy;Reviews;Scalability;Machine learning;Edge AI;Software systems;Natural language processing;Artificial intelligence;Software engineering","","","","51","IEEE","17 Feb 2025","","","IEEE","IEEE Conferences"
"UniSTAD: An Unified Triple-Tower Students-Teacher Model for Multi-Class Anomaly Detection and Localization","H. Liu; J. Sun","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2024","PP","99","1","1","Despite the rapid advancements in the unsupervised anomaly detection and localization, most existing methods require to train different models for different categories, leading to increased computational and memory demands for real application with the number of classes grows. A more practical task is to detect anomalies from different categories using one unified model. However, this unified setting is challenging for modeling the multi-class normal feature representation due to the diversity of data categories, and the existing methods often drop in performance under this setting. In this work, we propose UniSTAD, a novel and effective unified method for multi-class anomaly detection and localization, using a transformer-based triple-tower students-teacher model. The triple-tower design contains global and local student models, respectively predicting features from global and local context features. UniSTAD learns the feature representation of normal data by joint distilling features to pre-trained teacher model, and enforcing the global/local context-based feature reconstruction and consistency. In the inference stage, UniSTAD identifies anomalous regions where expected feature consistencies are broken. Additionally, we integrate an untrained, category-agnostic localization refinement module, further improving multi-class anomaly detection and localization performance. Evaluated on real-world industrial datasets, UniSTAD demonstrates the state-of-the-art performance, validating its efficacy for multi-class anomaly detection and localization.","1558-2205","","10.1109/TCSVT.2024.3507097","National Natural Science Foundation of China(grant numbers:12125104,12326615,U20B2075); the Key-Area Research and Development Program of Guangdong Province(grant numbers:2022B0303020003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10769545","Unified anomaly detection;knowledge distillation;self-supervised learning;vision transformer","Anomaly detection;Location awareness;Feature extraction;Computational modeling;Image reconstruction;Context modeling;Transformers;Predictive models;Data models;Circuits and systems","","","","","IEEE","27 Nov 2024","","","IEEE","IEEE Early Access Articles"
"Developing Digital RF Memories and Transceiver Technologies for Electromagnetic Warfare","",,Developing Digital RF Memories and Transceiver Technologies for Electromagnetic Warfare,"","2022","","","","","This book provides a comprehensive resource and thorough treatment in the latest development of Digital RF Memory (DRFM) technology and their key role in maintaining dominance over the electromagnetic spectrum. Part I discusses the use of advanced technology to design transceivers for spectrum sensing using unmanned systems to dominate the electromagnetic spectrum. Part II uses artificial intelligence and machine learning to enable modern spectrum sensing and detection signal processing for electronic support and electronic attack. Another key contribution is examination of counter-DRFM techniques. DRFM and transceiver design details and examples are provided along with the MATLAB software allowing the reader to construct their own embedded DRFM transceivers for unmanned systems. It examines the design trade-offs in developing multiple, structured, false target synthesis DRFM architectures and aids in developing counter-DRFM techniques and distinguish false target from real ones. Written by an expert in the field, and including MATLAB™ design software, this is the only comprehensive book written on the subject of DRFM.","","9781630816988","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9826856.pdf&bkn=9826835&pdfType=book","","","","","","","","12 Jul 2022","","","Artech","Artech Books"
"Data Engineering Best Practices: Architect robust and cost-effective data solutions in the cloud era","R. J. Schiller; D. Larochelle",NA; NA,Data Engineering Best Practices: Architect robust and cost-effective data solutions in the cloud era,"","2024","","","","","Explore modern data engineering techniques and best practices to build scalable, efficient, and future-proof data processing systems across cloud platformsKey FeaturesArchitect and engineer optimized data solutions in the cloud with best practices for performance and cost-effectivenessExplore design patterns and use cases to balance roles, technology choices, and processes for a future-proof designLearn from experts to avoid common pitfalls in data engineering projectsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionRevolutionize your approach to data processing in the fast-paced business landscape with this essential guide to data engineering. Discover the power of scalable, efficient, and secure data solutions through expert guidance on data engineering principles and techniques. Written by two industry experts with over 60 years of combined experience, it offers deep insights into best practices, architecture, agile processes, and cloud-based pipelines. You’ll start by defining the challenges data engineers face and understand how this agile and future-proof comprehensive data solution architecture addresses them. As you explore the extensive toolkit, mastering the capabilities of various instruments, you’ll gain the knowledge needed for independent research. Covering everything you need, right from data engineering fundamentals, the guide uses real-world examples to illustrate potential solutions. It elevates your skills to architect scalable data systems, implement agile development processes, and design cloud-based data pipelines. The book further equips you with the knowledge to harness serverless computing and microservices to build resilient data applications. By the end, you'll be armed with the expertise to design and deliver high-performance data engineering solutions that are not only robust, efficient, and secure but also future-ready.What you will learnArchitect scalable data solutions within a well-architected frameworkImplement agile software development processes tailored to your organization's needsDesign cloud-based data pipelines for analytics, machine learning, and AI-ready data productsOptimize data engineering capabilities to ensure performance and long-term business valueApply best practices for data security, privacy, and complianceHarness serverless computing and microservices to build resilient, scalable, and trustworthy data pipelinesWho this book is forIf you are a data engineer, ETL developer, or big data engineer who wants to master the principles and techniques of data engineering, this book is for you. A basic understanding of data engineering concepts, ETL processes, and big data technologies is expected. This book is also for professionals who want to explore advanced data engineering practices, including scalable data solutions, agile software development, and cloud-based data processing pipelines.","","9781803247366","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10740985.pdf&bkn=10740984&pdfType=book","","","","","","","","1 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"IEEE Standard for the Functional Verification Language e","",,IEEE Std 1647-2019 (Revision of IEEE Std 1647-2016),"9 Aug 2019","2019","","","1","622","The e functional verification language is an application-specific programming language, aimed at automating the task of verifying a hardware or software design with respect to its specification. Verification environments written in e provide a model of the environment in which the design is expected to function, including the kinds of erroneous conditions the design needs to withstand. A typical verification environment is capable of generating user-controlled test inputs with statistically interesting characteristics. Such an environment can check the validity of the design responses. Functional coverage metrics are used to control the verification effort and gauge the quality of the design. e verification environments can be used throughout the design cycle, from a high-level architectural model to a fully realized system. A definition of the e language syntax and semantics and how tool developers and verification engineers should use them are contained in this standard.;The e functional verification language is an application-specific programming language, aimed at automating the task of verifying a hardware or software design with respect to its specification. Verification environments written in e provide a model of the environment in which the design is expected to function, including the kinds of erroneous conditions the design needs to withstand. A typical verification environment is capable of generating user-controlled test inputs with statistically interesting characteristics. Such an environment can check the validity of the design responses. Functional coverage metrics are used to control the verification effort and gauge the quality of the design. e verification environments can be used throughout the design cycle, from a high-level architectural model to a fully realized system. A definition of the e language syntax and semantics and how tool developers and verification engineers should use them are contained in this standard.","","978-1-5044-5977-8","10.1109/IEEESTD.2019.8793253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793253","assertion;concurrent programming;constraint;dynamic verification;functional coverage;functional verification;IEEE 1647;simulation;temporal logic;test generation","IEEE Standards;Functional programming;System verification;Dynamic programming;Temporal logic","","1","","8","","9 Aug 2019","","","IEEE","IEEE Standards"
"10 Machine Learning Blueprints You Should Know for Cybersecurity: Protect your systems and boost your defenses with cutting-edge AI techniques","R. Oak",NA,10 Machine Learning Blueprints You Should Know for Cybersecurity: Protect your systems and boost your defenses with cutting-edge AI techniques,"","2023","","","","","Work on 10 practical projects, each with a blueprint for a different machine learning technique, and apply them in the real world to fight against cybercrime Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn how to frame a cyber security problem as a machine learning problemExamine your model for robustness against adversarial machine learningBuild your portfolio, enhance your resume, and ace interviews to become a cybersecurity data scientistBook DescriptionMachine learning in security is harder than other domains because of the changing nature and abilities of adversaries, high stakes, and a lack of ground-truth data. This book will prepare machine learning practitioners to effectively handle tasks in the challenging yet exciting cybersecurity space. The book begins by helping you understand how advanced ML algorithms work and shows you practical examples of how they can be applied to security-specific problems with Python – by using open source datasets or instructing you to create your own. In one exercise, you’ll also use GPT 3.5, the secret sauce behind ChatGPT, to generate an artificial dataset of fabricated news. Later, you’ll find out how to apply the expert knowledge and human-in-the-loop decision-making that is necessary in the cybersecurity space. This book is designed to address the lack of proper resources available for individuals interested in transitioning into a data scientist role in cybersecurity. It concludes with case studies, interview questions, and blueprints for four projects that you can use to enhance your portfolio. By the end of this book, you’ll be able to apply machine learning algorithms to detect malware, fake news, deep fakes, and more, along with implementing privacy-preserving machine learning techniques such as differentially private ML.What you will learnUse GNNs to build feature-rich graphs for bot detection and engineer graph-powered embeddings and featuresDiscover how to apply ML techniques in the cybersecurity domainApply state-of-the-art algorithms such as transformers and GNNs to solve security-related issuesLeverage ML to solve modern security issues such as deep fake detection, machine-generated text identification, and stylometric analysisApply privacy-preserving ML techniques and use differential privacy to protect user data while training ML modelsBuild your own portfolio with end-to-end ML projects for cybersecurityWho this book is forThis book is for machine learning practitioners interested in applying their skills to solve cybersecurity issues. Cybersecurity workers looking to leverage ML methods will also find this book useful. An understanding of the fundamental machine learning concepts and beginner-level knowledge of Python programming are needed to grasp the concepts in this book. Whether you’re a beginner or an experienced professional, this book offers a unique and valuable learning experience that’ll help you develop the skills needed to protect your network and data against the ever-evolving threat landscape.","","9781804611975","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251230.pdf&bkn=10251229&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"The Mathematical Radio: Inside the Magic of AM, FM, and Single-Sideband","P. J. Nahin; A. Simoson",NA; NA,"The Mathematical Radio: Inside the Magic of AM, FM, and Single-Sideband","","2024","","","","","How a modern radio works, told through mathematics, history, and selected puzzlesThe modern radio is a wonder, and behind that magic is mathematics. In The Mathematical Radio, Paul Nahin explains how radios work, deploying mathematics and historical discussion, accompanied by a steady stream of intriguing puzzles for math buffs to ponder. Beginning with oscillators and circuits, then moving on to AM, FM, and single-sideband radio, Nahin focuses on the elegant mathematics underlying radio technology rather than the engineering. He explores and explains more than a century of key developments, placing them in historical and technological context.Nahin, a prolific author of books on math for the general reader, describes in fascinating detail the mathematical underpinnings of a technology we use daily. He explains and solves, for example, Maxwell’s equations for the electromagnetic field. Readers need only a familarity with advanced high school–level math to follow Nahin’s mathematical discussions. Writing with the nonengineer in mind, Nahin examines topics including impulses in time and frequency, spectrum shifting at the transmitter, the superheterodyne, the physics of single-sideband radio, and FM sidebands. Chapters end with “challenge problems” and an appendix offers solutions, partial answers, and hints. Readers will come away with a new appreciation for the beauty of even the most useful mathematics.","","9780691235325","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10293186.pdf&bkn=10293185&pdfType=book","AM/FM radio theory;applied mathematics;broadcast radio history;G. H. Hardy;The Mathematical Radio: Inside the Magic of AM, FM, Single-Sideband, and Wifi;Paul J. Nahin;princeton university press;math;mathematics;radio;history of knowledge;history of mathematics;singleband;wifi;electronic inventions;math behind radio;how does radio work;radio engineering;oscillators;circuits;pivotal technology;history important technology;mathematical underpinnings","","","","","","","24 Oct 2023","","","Princeton University Press","Princeton University Press eBooks"
"Quantitative Biosciences: Dynamics across Cells, Organisms, and Populations","J. S. Weitz",NA,"Quantitative Biosciences: Dynamics across Cells, Organisms, and Populations","","2024","","","","","A hands-on approach to quantitative reasoning in the life sciencesQuantitative Biosciences establishes the quantitative principles of how living systems work across scales, drawing on classic and modern discoveries to present a case study approach that links mechanisms, models, and measurements. Each case study is organized around a central question in the life sciences: Are mutations dependent on selection? How do cells respond to fluctuating signals in the environment? How do organisms move in flocks given local sensing? How does the size of an epidemic depend on its initial speed of spread? Each question provides the basis for introducing landmark advances in the life sciences while teaching students—whether from the life sciences, physics, computational sciences, engineering, or mathematics—how to reason quantitatively about living systems given uncertainty.Draws on real-world case studies in molecular and cellular biosciences, organismal behavior and physiology, and populations and ecological communitiesStand-alone lab guides available in Python, R, and MATLAB help students move from learning in the classroom to doing research in practiceHomework exercises build on the lab guides, emphasizing computational model development and analysis rather than pencil-and-paper derivationsSuitable for capstone undergraduate classes, foundational graduate classes, or as part of interdisciplinary courses for students from quantitative backgroundsCan be used as part of conventional, flipped, or hybrid instruction formatsAdditional materials available to instructors, including lesson plans and homework solutions","","9780691256481","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10614703.pdf&bkn=10614702&pdfType=book","Dynamics;Cell;Population;Et;Mutation;Prey;Function;Equation;Predator;Mutants;Organisms;Scale;Probability;Gene;Total;Input;Interactions;Bacteria;Dynamical;Velocity;Transcription;Motion;Mrna;Stochastic;Voltage;Molecules;Relative;Evolutionary;Levels;Nonlinear;Constant;Coli;Mean;Frequency;Environment;Distribution;Game;Values;Current;Parameters;Concentration;Direction;Feedback;Expression;Event;Force;Neurons;Systems;Equilibrium;Disease;Models;Growth;Fitness;Proteins;Production;Exponentially;Range;Site;Signal;Data;Equivalent;Zero;Gene expression;Resistant;Degradation;Mechanisms;Components;Spread;Oscillations;Principles","","","","","","","30 Jul 2024","","","Princeton University Press","Princeton University Press eBooks"
"Ethics of the Algorithm: Digital Humanities and Holocaust Memory","T. Presner; A. Bonazzi; R. Deblinger; L. Fan; M. Lee; K. Rosen; C. Yamane",NA; NA; NA; NA; NA; NA; NA,Ethics of the Algorithm: Digital Humanities and Holocaust Memory,"","2024","","","","","How computational methods can expand how we see, read, and listen to Holocaust testimonyThe Holocaust is one of the most documented—and now digitized—events in human history. Institutions and archives hold hundreds of thousands of hours of audio and video testimony, composed of more than a billion words in dozens of languages, with millions of pieces of descriptive metadata. It would take several lifetimes to engage with these testimonies one at a time. Computational methods could be used to analyze an entire archive—but what are the ethical implications of “listening” to Holocaust testimonies by means of an algorithm? In this book, Todd Presner explores how the digital humanities can provide both new insights and humanizing perspectives for Holocaust memory and history.Presner suggests that it is possible to develop an “ethics of the algorithm” that mediates between the ethical demands of listening to individual testimonies and the interpretative possibilities of computational methods. He delves into thousands of testimonies and witness accounts, focusing on the analysis of trauma, language, voice, genre, and the archive itself. Tracing the affordances of digital tools that range from early, proto-computational approaches to more recent uses of automatic speech recognition and natural language processing, Presner introduces readers to what may be the ultimate expression of these methods: AI-driven testimonies that use machine learning to process responses to questions, offering a user experience that seems to replicate an actual conversation with a Holocaust survivor.With Ethics of the Algorithm, Presner presents a digital humanities argument for how big data models and computational methods can be used to preserve and perpetuate cultural memory.","","9780691258980","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10614711.pdf&bkn=10614710&pdfType=book","AI;algorithmic fabulation;archive;big data;cultural analytics;datafication;digital culture;digital media;digital technologies;ethical computation;Ethics of the Algorithm: Digital Humanities and Holocaust Memory;humanistic data science;natural language processing;survivor;Testimony;Todd Presner;virtual;witness;Data;Survivors;Narrative;Algorithm;Holocaust;Jewish;Human;Triplets;USC Shoah foundation;Mala;Jews;Visualization;Digital;Ethical;Semantic;Algorithmic;Memory;Ethics;Nazi;Database;Kimmelmann;Auschwitz;Corpus;War;Death;Trauma;Dimensions in Testimony (DiT);Network;Semantic triplets;History;Holocaust testimony;Judgment;Distant;Technologies;Cultural;Segments;Processes;Violence;Victims;Child;Population;Dutch;Fortunoff;Bomba;Ghetto;Silence;Media;Machine learning;Clusters;Police;Labor;Technology","","","","","","","30 Jul 2024","","","Princeton University Press","Princeton University Press eBooks"
"Virtual You: How Building Your Digital Twin Will Revolutionize Medicine and Change Your Life","P. Coveney; R. Highfield; V. Ramakrishnan",NA; NA; NA,Virtual You: How Building Your Digital Twin Will Revolutionize Medicine and Change Your Life,"","2023","","","","","The visionary science behind the digital human twins that will enhance our health and our futureVirtual You is a panoramic account of efforts by scientists around the world to build digital twins of human beings, from cells and tissues to organs and whole bodies. These virtual copies will usher in a new era of personalized medicine, one in which your digital twin can help predict your risk of disease, participate in virtual drug trials, shed light on the diet and lifestyle changes that are best for you, and help identify therapies to enhance your well-being and extend your lifespan—but thorny challenges remain.In this deeply illuminating book, Peter Coveney and Roger Highfield reveal what it will take to build a virtual, functional copy of a person in five steps. Along the way, they take you on a fantastic voyage through the complexity of the human body, describing the latest scientific and technological advances—from multiscale modeling to extraordinary new forms of computing—that will make “virtual you” a reality, while also considering the ethical questions inherent to realizing truly predictive medicine.With an incisive foreword by Nobel Prize–winning biologist Venki Ramakrishnan, Virtual You is science at its most astounding, showing how our virtual twins and even whole populations of virtual humans promise to transform our health and our lives in the coming decades.","","9780691223407","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10172356.pdf&bkn=10172355&pdfType=book","Protein;Molecule;Machine learning;Supercomputer;Simulation;Digital Twins;Prediction;Biology;Calculation;Metabolism;Result;Computer;Technology;Organism;Qubit;Mathematics;Bacteria;Big data;Enzyme;Disease;Parameter;Scientist;Thought;Virtual Cell;Liver;Sovereignty;Theory;Instance (computer science);Photon;Computer simulation;Differential equation;Quantity;Artificial neural network;Global commons;Action potential;Circulatory system;Gene;Human rights;Genomics;Mycoplasma;Stephen Wolfram;Heart;Metamaterial;Emergence;Logic;Cadaver;Digestion;Clinical trial;Antigen;Quantum mechanics;Aneurysm;Activation;Experimental data;Chemist;Mathematician;Measurement;Ordinary differential equation;Email;Chemical process;Receptor (biochemistry);Chemical reaction;Millimetre;Molecular machine;Desertification;Cloud;Cognitive test;Free parameter;Electron microscope;Victor Veselago;Time evolution","","","","","","","4 Jul 2023","","","Princeton University Press","Princeton University Press eBooks"
"Conference Digest","",,2020 IEEE Aerospace Conference,"21 Aug 2020","2020","","","1","310","full conference PDF","1095-323X","978-1-7281-2734-7","10.1109/AERO47225.2020.9172613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172613","","Space vehicles;Mars;Moon;Robots;Conferences;Space missions;Legged locomotion","","","","","IEEE","21 Aug 2020","","","IEEE","IEEE Conferences"
"Managing Machine Learning Projects: From design to deployment","S. Thompson",Manning Publications,Managing Machine Learning Projects: From design to deployment,"","2023","","","","","Guide machine learning projects from design to production with the techniques in this one-of-a-kind project management guide. No ML skills required In Managing Machine Learning Projects you’ll learn essential machine learning project management techniques, including:  Understanding an ML project’s requirements Setting up the infrastructure for the project and resourcing a team Working with clients and other stakeholders Dealing with data resources and bringing them into the project for use Handling the lifecycle of models in the project Managing the application of ML algorithms Evaluating the performance of algorithms and models Making decisions about which models to adopt for delivery Taking models through development and testing Integrating models with production systems to create effective applications Steps and behaviors for managing the ethical implications of ML technology  Managing Machine Learning Projects is an end-to-end guide for delivering machine learning applications on time and under budget. It lays out tools, approaches, and processes designed to handle the unique challenges of machine learning project management. You’ll follow an in-depth case study through a series of sprints and see how to put each technique into practice. The book’s strong consideration to data privacy, and community impact ensure your projects are ethical, compliant with global legislation, and avoid being exposed to failure from bias and other issues.","","9781633439023","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10443731.pdf&bkn=10443730&pdfType=book","ML;agile;devops;EDA;process;data;science;privacy;ethics;models;lifecycle;clients;success;practices;resources;stakeholders;sprint;infrastructure;system;security;survey;analysis;team;evaluate","","","","","","","22 Feb 2024","","","Manning","Manning eBooks"
"What Makes Us Smart: The Computational Logic of Human Cognition","S. Gershman",NA,What Makes Us Smart: The Computational Logic of Human Cognition,"","2021","","","","","How a computational framework can account for the successes and failures of human cognitionAt the heart of human intelligence rests a fundamental puzzle: How are we incredibly smart and stupid at the same time? No existing machine can match the power and flexibility of human perception, language, and reasoning. Yet, we routinely commit errors that reveal the failures of our thought processes. What Makes Us Smart makes sense of this paradox by arguing that our cognitive errors are not haphazard. Rather, they are the inevitable consequences of a brain optimized for efficient inference and decision making within the constraints of time, energy, and memory—in other words, data and resource limitations. Framing human intelligence in terms of these constraints, Samuel Gershman shows how a deeper computational logic underpins the “stupid” errors of human cognition.Embarking on a journey across psychology, neuroscience, computer science, linguistics, and economics, Gershman presents unifying principles that govern human intelligence. First, inductive bias: any system that makes inferences based on limited data must constrain its hypotheses in some way before observing data. Second, approximation bias: any system that makes inferences and decisions with limited resources must make approximations. Applying these principles to a range of computational errors made by humans, Gershman demonstrates that intelligent systems designed to meet these constraints yield characteristically human errors.Examining how humans make intelligent and maladaptive decisions, What Makes Us Smart delves into the successes and failures of cognition.","","9780691225999","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9547368.pdf&bkn=9547227&pdfType=book","","","","","","","","24 Sep 2021","","","Princeton University Press","Princeton University Press eBooks"
"A Survey on Semantic Communication Networks: Architecture, Security, and Privacy","S. Guo; Y. Wang; N. Zhang; Z. Su; T. H. Luan; Z. Tian; X. Shen","School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; Department of Electrical and Computer Engineering, University of Windsor, Windsor, Canada; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Computer Science, University of Technology Sydney, Sydney, Australia; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada",IEEE Communications Surveys & Tutorials,"","2024","PP","99","1","1","With the rapid advancement and deployment of intelligent agents and artificial general intelligence (AGI), a fundamental challenge for future networks is enabling efficient communications among agents. Unlike traditional human-centric, data-driven communication networks, the primary goal of agent-based communication is to facilitate coordination among agents. Therefore, task comprehension and collaboration become the key objectives of communications, rather than data synchronization. Semantic communication (SemCom) aims to align information and knowledge among agents to expedite task comprehension. While significant research has been conducted on SemCom for two-agent systems, the development of semantic communication networks (SemComNet) for multi-agent systems remains largely unexplored. In this paper, we provide a comprehensive and up-to-date survey of SemComNet, focusing on their fundamentals, security, and privacy aspects. We introduce a novel three-layer architecture for multi-agent interaction, comprising the control layer, semantic transmission layer, and cognitive sensing layer. We explore working modes and enabling technologies, and present a taxonomy of security and privacy threats, along with state-of-the-art defense mechanisms. Finally, we outline future research directions, paving the way toward intelligent, robust, and energy-efficient SemComNet. This survey represents the first comprehensive analysis of SemComNet, offering detailed insights into its core principles as well as associated security and privacy challenges.","1553-877X","","10.1109/COMST.2024.3516819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10798108","Semantic communication;artificial intelligence;security;privacy;trust","Security;Surveys;Privacy;Artificial intelligence;Knowledge based systems;Collaboration;Training;Sensors;Wireless communication;Computer hacking","","","","","IEEE","13 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Security Issues in Metaverse","Y. Wang; Z. Su; N. Zhang; D. Liu; R. Xing; T. H. Luan; X. Shen","School of Cyber Science and Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; School of Cyber Science and Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; School of Cyber Science and Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; School of Cyber Science and Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada","Metaverse Communication and Computing Networks: Applications, Technologies, and Approaches","","2024","","","205","239","In spite of the promising sign of Metaverse, security and privacy issues are the prime concerns that hinder its further development. A wide range of security breaches and privacy invasions may arise in the Metaverse from the management of massive data streams, pervasive user profiling activities, unfair outcomes of artificial intelligence algorithms, to the safety of physical infrastructures and human bodies. This chapter summarizes the key challenges and existing/potential solutions to build the secure and privacy‐preserving Metaverse from the following seven aspects (i.e. authentication and access control, data management, privacy, network, economy, governance, and physical/social effects). In the Metaverse, identity authentication and access control play a vital role for massive users/avatars in Metaverse service offering. The threats occurring in the Metaverse may also affect the physical world and threaten human society.","","9781394159994","10.1002/9781394160013.ch9","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10286219.pdf&bkn=10286201&pdfType=chapter","","Metaverse;Data privacy;Authentication;Access control;Safety;Avatars;Wearable computers","","","","","","16 Oct 2023","","","IEEE","Wiley-IEEE Press eBook Chapters"
"AWS Certified Security – Specialty (SCS-C02) Exam Guide: Get all the guidance you need to pass the AWS (SCS-C02) exam on your first attempt","A. Book; S. Scott",NA; NA,AWS Certified Security – Specialty (SCS-C02) Exam Guide: Get all the guidance you need to pass the AWS (SCS-C02) exam on your first attempt,"","2024","","","","","Become an AWS certified security specialist, strengthen your cloud defenses, and unlock advanced techniques for incident response, logging, identity management, and moreKey FeaturesStay updated with the most current SCS-C02 exam syllabusGain modern cloud security skills to build robust security solutionsAccess online exam prep resources like mock exams, flashcards, and exam tips to help with preparationPurchase of this book unlocks access to web-based exam prep resources such as mock exams and flashcardsBook DescriptionThe AWS Certified Security – Specialty exam validates your expertise in advanced cloud security, a crucial skill set in today's cloud market. With the latest updates and revised study material, this second edition provides an excellent starting point for your exam preparation. You’ll learn the fundamentals of core services, which are essential prerequisites before delving into the six domains covered in the exam. The book addresses various security threats, vulnerabilities, and attacks, such as DDoS attacks, offering insights into effective mitigation strategies at different layers. You’ll learn different tools available in Amazon Web Services (AWS) to secure your Virtual Private Cloud and allow the correct traffic to travel securely to your workloads. As you progress, you’ll explore the intricacies of AWS EventBridge and IAM services. Additionally, you’ll get lifetime access to supplementary online resources, including mock exams with exam-like timers, detailed solutions, interactive flashcards, and invaluable exam tips, all accessible across various devices such as PCs, tablets, and smartphones. Ultimately, armed with the knowledge and skills acquired from this AWS security guide, you'll be well-prepared to pass the exam and design secure AWS solutions with confidence.What you will learnApply cutting-edge AWS security techniques for robust cloud defensesImplement the AWS shared responsibility model effectivelyConfigure AWS resources to meet specific security requirementsConfigure and manage access controls and policies in AWSManage environments with AWS Security Hub and GuardDutyMonitor and log tasks efficiently using AWS logging and monitoring servicesCreate bucket policies for users with predefined permissions to accessCreate and manage private certificate authorities in AWS ACMWho this book is forThis book is for system administrators or security professionals looking to gain AWS security certification. Prior experience in securing cloud environments is necessary to get the most out of this book.","","9781837635924","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769265.pdf&bkn=10769264&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"AWS Certified Machine Learning - Specialty (MLS-C01) Certification Guide: The ultimate guide to passing the MLS-C01 exam on your firs atttempt","S. Nanda; W. Moura",NA; NA,AWS Certified Machine Learning - Specialty (MLS-C01) Certification Guide: The ultimate guide to passing the MLS-C01 exam on your firs atttempt,"","2024","","","","","Prepare confidently for the AWS MLS-C01 certification with this comprehensive and up-to-date exam guide, accompanied by web-based tools such as mock exams, flashcards, and exam tipsKey FeaturesGain proficiency in AWS machine learning services to excel in the MLS-C01 examBuild model training and inference pipelines and deploy machine learning models to the AWS cloudPractice on the go with the mobile-friendly bonus website, accessible with the bookPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe AWS Certified Machine Learning Specialty (MLS-C01) exam evaluates your ability to execute machine learning tasks on AWS infrastructure. This comprehensive book aligns with the latest exam syllabus, offering practical examples to support your real-world machine learning projects on AWS. Additionally, you'll get lifetime access to supplementary online resources, including mock exams with exam-like timers, detailed solutions, interactive flashcards, and invaluable exam tips, all accessible across various devices—PCs, tablets, and smartphones. Throughout the book, you’ll learn data preparation techniques for machine learning, covering diverse methods for data manipulation and transformation across different variable types. Addressing challenges such as missing data and outliers, the book guides you through an array of machine learning tasks including classification, regression, clustering, forecasting, anomaly detection, text mining, and image processing, accompanied by requisite machine learning algorithms essential for exam success. The book helps you master the deployment of models in production environments and their subsequent monitoring. Equipped with insights from this book and the accompanying mock exams, you'll be fully prepared to achieve the AWS MLS-C01 certification.What you will learnIdentify ML frameworks for specific tasksApply CRISP-DM to build ML pipelinesCombine AWS services to build AI/ML solutionsApply various techniques to transform your data, such as one-hot encoding, binary encoder, ordinal encoding, binning, and text transformationsVisualize relationships, comparisons, compositions, and distributions in the dataUse data preparation techniques and AWS services for batch and real-time data processingCreate training and inference ML pipelines with Sage MakerDeploy ML models in a production environment efficientlyWho this book is forThis book is designed for both students and professionals preparing for the AWS Certified Machine Learning Specialty exam or enhance their understanding of machine learning, with a specific emphasis on AWS. Familiarity with machine learning basics and AWS services is recommended to fully benefit from this book.","","9781835082904","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769766.pdf&bkn=10769765&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Enterprise-Grade Hybrid and Multi-Cloud Strategies: Proven strategies to digitally transform your business with hybrid and multi-cloud solutions","S. AG; K. Das",NA; NA,Enterprise-Grade Hybrid and Multi-Cloud Strategies: Proven strategies to digitally transform your business with hybrid and multi-cloud solutions,"","2024","","","","","Leverage cloud technologies, proven strategies, and effective frameworks to drive seamless digital transformation. Key FeaturesUnderstand the challenges enterprises face with cloud adoption and the importance of leadership visionLearn how to build the foundation for a vendor agnostic cloud-ready enterpriseDiscover best practices to architect an enterprise cloud strategy and responsibly innovate with emerging technologiesPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionIn the past decade, cloud technology has evolved from a mere deployment platform into a driving force of innovation. However, navigating the complexities of cloud adoption, especially with a hybrid approach, presents significant challenges. Solving Hybrid Cloud Challenges for Enterprises is your trusted guide to overcome the problems encountered in this process. Written by a principal architect at Google with 15+ years of experience, this vendor agnostic book begins by exploring the case studies of enterprises stepping into the world of the cloud, highlighting the pivotal role of leadership vision and mindset in driving digital transformation. You’ll explore the basics of cloud technology, its impact on various industries, and the challenges of cloud adoption. As you dive deeper, you’ll find real-world use cases of enterprises that have digitally disrupted their respective industries by innovating in the cloud. From assessing the cloud maturity of an organization and designing a cloud strategy to exploring the various facets of cloud transformation, this book will guide you at every step of the way. Finally, you’ll learn how to lead your organization’s cloud transformation journey with emerging technologies. By the end, you'll be well-equipped to design and architect a scalable, cloud-first IT organization. What you will learnUnderstand the hybrid cloud and multi-cloud paradigmsCultivate leadership will and mindset for crafting successful cloud transformationDesign and architect a scalable and open foundation for a cloud-first IT organizationApply open standards and frameworks to design a vendor-neutral cloud foundationUnderstand the cloud adoption frameworks and conduct maturity assessmentsRealize tangible business value through cloud adoption initiativesWho this book is forThis book is for cloud architects and engineers responsible for and seeking to digitally transform their business through cloud. Enterprise IT leaders will be able to successfully navigate the enterprise cloud transformation complexities with cloud migration strategies, prescriptive frameworks, and practical real-world examples. A basic understanding of enterprise IT functions and operations is assumed. ","","9781804611623","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769336.pdf&bkn=10769335&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Responsible AI in the Enterprise: Practical AI risk management for explainable, auditable, and safe models with hyperscalers and Azure OpenAI","A. Masood; H. Dawe; D. E. Adeli",NA; NA; NA,"Responsible AI in the Enterprise: Practical AI risk management for explainable, auditable, and safe models with hyperscalers and Azure OpenAI","","2023","","","","","Build and deploy your AI models successfully by exploring model governance, fairness, bias, and potential pitfalls Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn ethical AI principles, frameworks, and governanceUnderstand the concepts of fairness assessment and bias mitigationIntroduce explainable AI and transparency in your machine learning modelsBook DescriptionResponsible AI in the Enterprise is a comprehensive guide to implementing ethical, transparent, and compliant AI systems in an organization. With a focus on understanding key concepts of machine learning models, this book equips you with techniques and algorithms to tackle complex issues such as bias, fairness, and model governance. Throughout the book, you’ll gain an understanding of FairLearn and InterpretML, along with Google What-If Tool, ML Fairness Gym, IBM AI 360 Fairness tool, and Aequitas. You’ll uncover various aspects of responsible AI, including model interpretability, monitoring and management of model drift, and compliance recommendations. You’ll gain practical insights into using AI governance tools to ensure fairness, bias mitigation, explainability, privacy compliance, and privacy in an enterprise setting. Additionally, you’ll explore interpretability toolkits and fairness measures offered by major cloud AI providers like IBM, Amazon, Google, and Microsoft, while discovering how to use FairLearn for fairness assessment and bias mitigation. You’ll also learn to build explainable models using global and local feature summary, local surrogate model, Shapley values, anchors, and counterfactual explanations. By the end of this book, you’ll be well-equipped with tools and techniques to create transparent and accountable machine learning models.What you will learnUnderstand explainable AI fundamentals, underlying methods, and techniquesExplore model governance, including building explainable, auditable, and interpretable machine learning modelsUse partial dependence plot, global feature summary, individual condition expectation, and feature interactionBuild explainable models with global and local feature summary, and influence functions in practiceDesign and build explainable machine learning pipelines with transparencyDiscover Microsoft FairLearn and marketplace for different open-source explainable AI tools and cloud platformsWho this book is forThis book is for data scientists, machine learning engineers, AI practitioners, IT professionals, business stakeholders, and AI ethicists who are responsible for implementing AI models in their organizations.","","9781803249667","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251168.pdf&bkn=10251167&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"AI Security: Cyber Threats and Threat-Informed Defense","K. Singh; R. Saxena; B. Kumar","FET, MRIIRS & Member, IEEE, London, United Kingdom; Telecom and Cyber Security Architect Member, IEEE, Dubai, UAE; Academics FET,MRIIRS, Faridabad, India",2024 8th Cyber Security in Networking Conference (CSNet),"28 Jan 2025","2024","","","305","312","Artificial intelligence has emerged as a revolution-ary technology offering substantial advances over traditional information and communication systems. However, the increasing prevalence of AI introduces new vulnerabilities, making AI-driven systems more susceptible to cybercriminal activities and security threats aimed at disrupting their operations. This study comprehensively examines the cybersecurity challenges and threats associated with AI applications, emphasizing the core principles of information security. Confidentiality, Integrity, and Availability. The study categorizes AI-related threats into two key areas: first, threats targeting critical AI components such as data, models, and algorithms, and second, the malicious exploitation of AI to conduct sophisticated, large-scale cyberattacks. This analysis contributes to a threat-informed defense by examining risk assessment methodologies to address these challenges, under-scoring the need for robust security frameworks. Furthermore, it leverages the Adversarial Threat Landscape for Artificial Intelligence Systems (ATLAS) guidelines, offering future research directions to enhance AI security, and providing practical recom-mendations for securing AI across diverse deployment scenarios.","2768-0029","979-8-3315-3410-3","10.1109/CSNet64211.2024.10851770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10851770","Artificial Intelligence;AI Security;Machine Learning;Security for AI;Security Risk Management","Communication systems;Information security;Data models;Risk management;Artificial intelligence;Computer crime;Guidelines","","","","32","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"A Survey on Metaverse: Fundamentals, Security, and Privacy","Y. Wang; Z. Su; N. Zhang; R. Xing; D. Liu; T. H. Luan; X. Shen","School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; Department of Electrical and Computer Engineering, University of Windsor, Windsor, Canada; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada",IEEE Communications Surveys & Tutorials,"23 Feb 2023","2023","25","1","319","352","Metaverse, as an evolving paradigm of the next-generation Internet, aims to build a fully immersive, hyper spatiotemporal, and self-sustaining virtual shared space for humans to play, work, and socialize. Driven by recent advances in emerging technologies such as extended reality, artificial intelligence, and blockchain, metaverse is stepping from science fiction to an upcoming reality. However, severe privacy invasions and security breaches (inherited from underlying technologies or emerged in the new digital ecology) of metaverse can impede its wide deployment. At the same time, a series of fundamental challenges (e.g., scalability and interoperability) can arise in metaverse security provisioning owing to the intrinsic characteristics of metaverse, such as immersive realism, hyper spatiotemporality, sustainability, and heterogeneity. In this paper, we present a comprehensive survey of the fundamentals, security, and privacy of metaverse. Specifically, we first investigate a novel distributed metaverse architecture and its key characteristics with ternary-world interactions. Then, we discuss the security and privacy threats, present the critical challenges of metaverse systems, and review the state-of-the-art countermeasures. Finally, we draw open research directions for building future metaverse systems.","1553-877X","","10.1109/COMST.2022.3202047","NSFC(grant numbers:U20A20175,U1808207); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880528","Metaverse;security;privacy;distributed virtual worlds;extended reality;artificial intelligence;blockchain","Metaverse;Security;Privacy;Artificial intelligence;Blockchains;Spatiotemporal phenomena;Scalability","","564","","177","IEEE","7 Sep 2022","","","IEEE","IEEE Journals"
"Things Fall Together: A Guide to the New Materials Revolution","S. Tibbits",NA,Things Fall Together: A Guide to the New Materials Revolution,"","2021","","","","","From the visionary founder of the Self-Assembly Lab at MIT, a manifesto for the dawning age of active materialsThings in life tend to fall apart. Cars break down. Buildings fall into disrepair. Personal items deteriorate. Yet today's researchers are exploiting newly understood properties of matter to program materials that physically sense, adapt, and fall together instead of apart. These materials open new directions for industrial innovation and challenge us to rethink the way we build and collaborate with our environment. Things Fall Together is a provocative guide to this emerging, often mind-bending reality, presenting a bold vision for harnessing the intelligence embedded in the material world.Drawing on his pioneering work on self-assembly and programmable material technologies, Skylar Tibbits lays out the core, frequently counterintuitive ideas and strategies that animate this new approach to design and innovation. From furniture that builds itself to shoes printed flat that jump into shape to islands that grow themselves, he describes how matter can compute and exhibit behaviors that we typically associate with biological organisms, and challenges our fundamental assumptions about what physical materials can do and how we can interact with them. Intelligent products today often rely on electronics, batteries, and complicated mechanisms. Tibbits offers a different approach, showing how we can design simple and elegant material intelligence that may one day animate and improve itself—and along the way help us build a more sustainable future.Compelling and beautifully designed, Things Fall Together provides an insider's perspective on the materials revolution that lies ahead, revealing the spectacular possibilities for designing active materials that can self-assemble, collaborate, and one day even evolve and design on their own.","","9780691189710","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9519810.pdf&bkn=9519809&pdfType=book","","","","","","","","20 Aug 2021","","","Princeton University Press","Princeton University Press eBooks"
"AWS for Solutions Architects: The definitive guide to AWS Solutions Architecture for migrating to, building, scaling, and succeeding in the cloud","S. Shrivastava; N. Srivastav; A. Artasanchez; I. Sayed; D. S. C. Ph.D",NA; NA; NA; NA; NA,"AWS for Solutions Architects: The definitive guide to AWS Solutions Architecture for migrating to, building, scaling, and succeeding in the cloud","","2023","","","","","Become a master Solutions Architect with this comprehensive guide, featuring cloud design patterns and real-world solutions for building scalable, secure, and highly available systems Purchase of the print or Kindle book includes a free eBook in PDF format.Key FeaturesGain expertise in automating, networking, migrating, and adopting cloud technologies using AWSUse streaming analytics, big data, AI/ML, IoT, quantum computing, and blockchain to transform your businessUpskill yourself as an AWS solutions architect and explore details of the new AWS certificationBook DescriptionAre you excited to harness the power of AWS and unlock endless possibilities for your business? Look no further than the second edition of AWS for Solutions Architects! Packed with all-new content, this book is a must-have guide for anyone looking to build scalable cloud solutions and drive digital transformation using AWS.  This updated edition offers in-depth guidance for building cloud solutions using AWS. It provides detailed information on AWS well-architected design pillars and cloud-native design patterns. You'll learn about networking in AWS, big data and streaming data processing, CloudOps, and emerging technologies such as machine learning, IoT, and blockchain. Additionally, the book includes new sections on storage in AWS, containers with ECS and EKS, and data lake patterns, providing you with valuable insights into designing industry-standard AWS architectures that meet your organization's technological and business requirements. Whether you're an experienced solutions architect or just getting started with AWS, this book has everything you need to confidently build cloud-native workloads and enterprise solutions.What you will learnOptimize your Cloud Workload using the AWS Well-Architected FrameworkLearn methods to migrate your workload using the AWS Cloud Adoption FrameworkApply cloud automation at various layers of application workload to increase efficiencyBuild a landing zone in AWS and hybrid cloud setups with deep networking techniquesSelect reference architectures for business scenarios, like data lakes, containers, and serverless appsApply emerging technologies in your architecture, including AI/ML, IoT and blockchainWho this book is forThis book is for application and enterprise architects, developers, and operations engineers who want to become well versed with AWS architectural patterns, best practices, and advanced techniques to build scalable, secure, highly available, highly tolerant, and cost-effective solutions in the cloud. Existing AWS users are bound to learn the most, but it will also help those curious about how leveraging AWS can benefit their organization. Prior knowledge of any computing language is not needed, and there’s little to no code. Prior experience in software architecture design will prove helpful.","","9781803244822","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251174.pdf&bkn=10251173&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"7 Robust AI Systems","D. R. Martinez; B. M. Kifle",Massachusetts Institute of Technology; Microsoft,Artificial Intelligence: A Systems Approach from Architecture Principles to Deployment,"","2024","","","223","257","As we will discuss in this chapter, artificial intelligence (AI) outputs are very susceptible to minor changes to machine learning (ML) models. Unfortunately, what goes on inside these models, in terms of learning from data, is difficult to explain. There are significant research efforts dedicated to understanding AI system vulnerabilities caused by either intentional or unintentional means. There are also approaches proposed toward explainability of the ML results—explainable AI (XAI), as well as mitigation techniques against AI system vulnerabilities.","","9780262378703","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10710525.pdf&bkn=10701030&pdfType=chapter","","","","","","","","9 Oct 2024","","","MIT Press","MIT Press eBook Chapters"
"RESPECT 2020 Prepint proceedings","",,"2020 Research on Equity and Sustained Participation in Engineering, Computing, and Technology (RESPECT)","4 Dec 2020","2020","1","","1","246","Full conference PDF.","","978-1-7281-7172-2","10.1109/RESPECT49803.2020.9272473","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272473","","","","","","","IEEE","4 Dec 2020","","","IEEE","IEEE Conferences"
"Robust Machine Learning Systems: Challenges,Current Trends, Perspectives, and the Road Ahead","M. Shafique; M. Naseer; T. Theocharides; C. Kyrkou; O. Mutlu; L. Orosa; J. Choi","Technische Universität Wien (TU Wien), 1040 Vienna, Austria; Technische Universität Wien (TU Wien); University of Cyprus, 1678 Nicosia, Cyprus; University of Cyprus; ETH Zürich; ETH Zürich; Hanyang University",IEEE Design & Test,"21 Apr 2020","2020","37","2","30","57","Currently, machine learning (ML) techniques are at the heart of smart cyber-physical systems (CPSs) and Internet-of-Things (loT). This article discusses various challenges and probable solutions for security attacks on these ML-inspired hardware and software techniques.","2168-2364","","10.1109/MDAT.2020.2971217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8979377","","Training data;Artificial neural networks;Reliability;Smart devices;Hardware;Machine learning","","81","","180","IEEE","3 Feb 2020","","","IEEE","IEEE Magazines"
"An Overview of Blockchain for Industry 5.0: Towards Human-Centric, Sustainable and Resilient Applications","P. Fraga-Lamas; T. M. Fernández-Caramés; A. M. Rosado da Cruz; S. I. Lopes","Department of Computer Engineering, Faculty of Computer Science, Universidade da Coruña, A Coruña, Spain; Department of Computer Engineering, Faculty of Computer Science, Universidade da Coruña, A Coruña, Spain; ADiT-Lab, Instituto Politécnico de Viana do Castelo, Viana do Castelo, Portugal; ADiT-Lab, Instituto Politécnico de Viana do Castelo, Viana do Castelo, Portugal",IEEE Access,"29 Aug 2024","2024","12","","116162","116201","Industry 5.0 is an evolving concept that aims to enhance the way modern factories operate by seeking long-term growth, production efficiency and the well-being of industrial workers. Human-centricity, sustainability and resilience are the three pillars of Industry 5.0, which are developed on Industry 4.0 enabling technologies. One of the most compelling technologies to help implement the communications architecture proposed by Industry 5.0 is blockchain, which can provide trustworthy, secured and decentralized information to different industrial domains. This article provides an analysis of the transition between Industry 4.0 and Industry 5.0 paradigms. Moreover, it examines the benefits and challenges that arise when using blockchain to develop Industry 5.0 applications and analyzes the design factors that should be considered when developing this type of applications. Furthermore, it presents a thorough review on the most relevant blockchain-based applications for Industry 5.0 pillars. Therefore, the main goal of this article is to provide a comprehensive and detailed guide for future Industry 5.0 developers that allows for determining how blockchain might benefit the next generation of human-centric, sustainable, and resilient applications.","2169-3536","","10.1109/ACCESS.2024.3435374","Xunta de Galicia(grant numbers:ED431C 2020/15,PID2020-118857RA-100 (ORBALLO),TED2021-129433A-C22 (HELENE),MCIN/AEI/10.13039/501100011033); European Union NextGenerationEU/PRTR(grant numbers:NORTE-01-0145-FEDER-000043); TECH–Technology, Environment, Creativity and Health; Norte Portugal Regional Operational Program (NORTE 2020); PORTUGAL 2020 Partnership Agreement; European Regional Development Fund (ERDF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10614164","Industry 5.0;blockchain;human-centricity;sustainability;resilience;smart factories;Society 5.0","Fifth Industrial Revolution;Blockchains;Fourth Industrial Revolution;Industries;Production facilities;Europe;Sustainable development;Human factors;Sustainable development;Smart manufacturing","","1","","370","CCBY","29 Jul 2024","","","IEEE","IEEE Journals"
"EuMC 2018 Abstract Cards","",,2018 48th European Microwave Conference (EuMC),"22 Nov 2018","2018","","","1","413","Provides an abstract for each of the keynote presentations and may include a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.","","978-2-87487-051-4","10.23919/EuMC.2018.8541765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8541765","","Power generation;Linearity;Varactors;Power transmission lines;MODFETs;HEMTs;Wide band gap semiconductors","","","","","","22 Nov 2018","","","IEEE","IEEE Conferences"
"Artificial Intelligence in Science and Society: The Vision of USERN","T. Dorigo; G. D. Brown; C. Casonato; A. Cerdà; J. Ciarrochi; M. da Lio; N. D’Souza; N. R. Gauger; S. C. Hayes; S. G. Hofmann; R. Johansson; M. Liwicki; F. Lotte; J. J. Nieto; G. Olivato; P. Parnes; G. Perry; A. Plebe; I. M. Rao; N. Rezaei; F. Sandin; A. Ustyuzhanin; G. Vallortigara; P. Vischia; N. Yazdanpanah","Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Luleå, Sweden; Bush School of Government and Public Service, Texas A&M University, College Station, TX, USA; Faculty of Law, University of Trento, Trento, Italy; Soil Erosion and Degradation Research Group, Departament de Geografia, Universitat de Valéncia, Valéncia, Spain; Institute of Positive Psychology and Education, Australian Catholic University, Sydney, Australia; Department of Industrial Engineering, Università degli Studi di Trento, Trento, Italy; Department of Neuroscience, College of Humanities, Arts, and Social Sciences (CHASS), University of California at Riverside, Riverside, CA, USA; Chair for Scientific Computing, University of Kaiserslautern-Landau (RPTU), Kaiserslautern, Germany; Universal Scientific Education and Research Network (USERN), Tehran, Iran; Department of Psychology, Philipps University Marburg, Marburg, Germany; Department of Psychology, Stockholm University, Stockholm, Sweden; Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Luleå, Sweden; Universal Scientific Education and Research Network (USERN), Tehran, Iran; Universal Scientific Education and Research Network (USERN), Tehran, Iran; Faculty of Law, University of Trento, Trento, Italy; Department of Computer Science, Electrical and Space Engineering, ArcTech Learning Laboratory, Pervasive and Mobile Computing, Luleå University of Technology, Luleå, Sweden; Universal Scientific Education and Research Network (USERN), Tehran, Iran; Department of Computer Science, University College London, London, U.K.; Universal Scientific Education and Research Network (USERN), Tehran, Iran; Universal Scientific Education and Research Network (USERN), Tehran, Iran; Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Luleå, Sweden; Department of Computer Science, Electrical and Space Engineering, Constructor University, Bremen, Germany; Centre for Mind/Brain Sciences, University of Trento, Rovereto, Italy; Universal Scientific Education and Research Network (USERN), Tehran, Iran; Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Luleå, Sweden",IEEE Access,"28 Jan 2025","2025","13","","15993","16054","The recent rise in relevance and diffusion of Artificial Intelligence (AI)-based systems and the increasing number and power of applications of AI methods invites a profound reflection on the impact of these innovative systems on scientific research and society at large. The Universal Scientific Education and Research Network (USERN), an organization that promotes initiatives to support interdisciplinary science and education across borders and actively works to improve science policy, collects here the vision of its Advisory Board members, together with a selection of AI experts, to summarize how we see developments in this exciting technology impacting science and society in the foreseeable future. In this review, we first attempt to establish clear definitions of intelligence and consciousness, then provide an overview of AI’s state of the art and its applications. A discussion of the implications, opportunities, and liabilities of the diffusion of AI for research in a few representative fields of science follows this. Finally, we address the potential risks of AI to modern society, suggest strategies for mitigating those risks, and present our conclusions and recommendations.","2169-3536","","10.1109/ACCESS.2025.3529357","MUR PNRR Project FAIR-Future AI Research funded by the Next Generation EU(grant numbers:PE00000013.13039/501100011033,PID2020-113275GB-I00); ERDF A way of making Europe, European Union, and Xunta de Galicia, for Competitive Reference Research Groups (2023–2026)(grant numbers:ED431C 2023/12.13039/501100011033,RYC2021-033305-I); European Union NextGenerationEU/PRTR; Agencia Estatal de Investigacion (AEI) of Spain(grant numbers:MCIN/AEI/10); European Union—FSE REACT-EU, PON Ricerca e Innovazione 2014–2020; Ministry of Education, Singapore, through the Research Centre of Excellence Institute for Functional Intelligence Materials; National Research Foundation, Singapore, under its AI Singapore Program (AISG)(grant numbers:AISG3-RP-2022-028); “Ramón y Cajal” Program(grant numbers:MCIN/AEI/10); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10839366","Artificial intelligence;scientific research;science ethics;computer science;physics;medicine;psychology;mathematics;geography;agriculture","Artificial intelligence;Earth;Large language models;Hands;Surveys;Stress;Planetary orbits;Focusing;Volcanoes;Uncertainty","","","","417","CCBY","13 Jan 2025","","","IEEE","IEEE Journals"
"Running Out: In Search of Water on the High Plains","L. Bessire",NA,Running Out: In Search of Water on the High Plains,"","2021","","","","","An intimate reckoning with aquifer depletion in America's heartlandThe Ogallala aquifer has nourished life on the American Great Plains for millennia. But less than a century of unsustainable irrigation farming has taxed much of the aquifer beyond repair. The imminent depletion of the Ogallala and other aquifers around the world is a defining planetary crisis of our times. Running Out offers a uniquely personal account of aquifer depletion and the deeper layers through which it gains meaning and force.Anthropologist Lucas Bessire journeyed back to western Kansas, where five generations of his family lived as irrigation farmers and ranchers, to try to make sense of this vital resource and its loss. His search for water across the drying High Plains brings the reader face to face with the stark realities of industrial agriculture, eroding democratic norms, and surreal interpretations of a looming disaster. Yet the destination is far from predictable, as the book seeks to move beyond the words and genres through which destruction is often known. Instead, this journey into the morass of eradication offers a series of unexpected discoveries about what it means to inherit the troubled legacies of the past and how we can take responsibility for a more inclusive, sustainable future.An urgent and unsettling meditation on environmental change, Running Out is a revelatory account of family, complicity, loss, and what it means to find your way back home.","","9780691212654","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9519678.pdf&bkn=9519677&pdfType=book","belonging;conservative America;conservative values;environment;environmental crisis;ethnography;exclusion;family histories;farm families;global warming;Great Plains;groundwater depletion;Heartland;Julene Bair;memoir;memory;Ogallala Blue;partisanship;polarization;resource depletion;rural life;Sarah Smarsh;sustainability;the best memoirs;The Ogallala Road;water crisis;water wars;William Ashworth","","","","","","","20 Aug 2021","","","Princeton University Press","Princeton University Press eBooks"
"Beyond Fairness in Computer Vision: A Holistic Approach to Mitigating Harms and Fostering Community-Rooted Computer Vision Research","T. Gebru; R. Denton",NA; NA,Beyond Fairness in Computer Vision: A Holistic Approach to Mitigating Harms and Fostering Community-Rooted Computer Vision Research,"","2024","","","","","The field of computer vision is now a multi-billion dollar enterprise, with its use in surveillance applications driving this large market share. In the last six years, computer vision researchers have started to discuss the risks and harms of some of these systems, mostly using the lens of fairness introduced in the machine learning literature to perform this analysis. While this lens is useful to uncover and mitigate a narrow segment of the harms that can be enacted through computer vision systems, it is only one of the toolkits that researchers have available to uncover and mitigate the harms of the systems they build. In this monograph, a wide range of risks and harms that can be enacted through the development and deployment of computer vision systems are discussed, in addition to some existing technical approaches to mitigating these harms and the shortcomings of these mitigation strategies. Thereafter, computer vision researchers are introduced to harm mitigation strategies proposed by journalists, human rights activists, individuals harmed by computer vision systems, and researchers in disciplines ranging from sociology to physics. The monograph concludes by listing principles that researchers can follow to build community-rooted computer vision tools in the public interest. The authors hope that this monograph can serve as a starting point for researchers exploring the harms of current computer vision systems and attempting to steer the field into community-rooted work.","","9781638283553","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10702448.pdf&bkn=10702447&pdfType=book","","","","","","","","2 Oct 2024","","","now","Now Foundations and Trends Books"
"Practical Guide to Azure Cognitive Services: Leverage the power of Azure OpenAI to optimize operations, reduce costs, and deliver cutting-edge AI solutions","C. Seferlis; C. Nellis; A. Roberts",NA; NA; NA,"Practical Guide to Azure Cognitive Services: Leverage the power of Azure OpenAI to optimize operations, reduce costs, and deliver cutting-edge AI solutions","","2023","","","","","Streamline your complex processes and optimize your organization's operational efficiency, cost-effectiveness, and customer experience by unlocking the potential of Microsoft Azure Cognitive Services and OpenAI Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesMinimize costs and maximize operations by automating mundane activities using AI toolsIdeate solutions using real-world examples for manufacturing process improvement with AIMaster TCO and ROI analysis for implementing AI solutions, automating operations, and ideating innovative manufacturing solutions with real-world examplesBook DescriptionAzure Cognitive Services and OpenAI are a set of pre-built artificial intelligence (AI) solution APIs that can be leveraged from existing applications, allowing customers to take advantage of Microsoft’s award-winning Vision, Speech, Text, Decision, and GPT-4 AI capabilities. With Practical Guide to Azure Cognitive Services, you’ll work through industry-specific examples of implementations to get a head-start in your production journey. You’ll begin with an overview of the categorization of Azure Cognitive Services and the benefits of embracing AI solutions for practical business applications. After that, you’ll explore the benefits of using Azure Cognitive Services to optimize efficiency and improve predictive capabilities. Then, you’ll learn how to leverage Vision capabilities for quality control, Form Recognizer to streamline supply chain nuances, language understanding to improve customer service, and Cognitive Search for next-generation knowledge-mining solutions. By the end of this book, you’ll be able to implement various Cognitive Services solutions that will help you enhance efficiency, reduce costs, and improve the customer experience at your organization. You’ll also be well equipped to automate mundane tasks by reaping the full potential of OpenAI.What you will learnMaster cost-effective deployment of Azure Cognitive ServicesDevelop proven solutions from an architecture and development standpointUnderstand how Cognitive Services are deployed and customizedEvaluate various uses of Cognitive Services with different mediumsDisseminate Azure costs for Cognitive Services workloads smoothlyDeploy next-generation Knowledge Mining solutions with Cognitive SearchExplore the current and future journey of OpenAIUnderstand the value proposition of different AI projectsWho this book is forThis book is for data scientists, technology leaders, and software engineers looking to implement Azure Cognitive Services with the help of sample use cases derived from success stories. Experience with Python as well as an overall understanding of the Azure Portal with related services such as Azure Data Lake Storage and Azure Functions will help you make the most of this book.","","9781801810609","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251364.pdf&bkn=10251363&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"The Sounds of Life: How Digital Technology Is Bringing Us Closer to the Worlds of Animals and Plants","K. Bakker",NA,The Sounds of Life: How Digital Technology Is Bringing Us Closer to the Worlds of Animals and Plants,"","2022","","","","","An amazing journey into the hidden realm of nature’s soundsThe natural world teems with remarkable conversations, many beyond human hearing range. Scientists are using groundbreaking digital technologies to uncover these astonishing sounds, revealing vibrant communication among our fellow creatures across the Tree of Life.At once meditative and scientific, The Sounds of Life shares fascinating and surprising stories of nonhuman sound, interweaving insights from technological innovation and traditional knowledge. We meet scientists using sound to protect and regenerate endangered species from the Great Barrier Reef to the Arctic and the Amazon. We discover the shocking impacts of noise pollution on both animals and plants. We learn how artificial intelligence can decode nonhuman sounds, and meet the researchers building dictionaries in East African Elephant and Sperm Whalish. At the frontiers of innovation, we explore digitally mediated dialogues with bats and honeybees. Technology often distracts us from nature, but what if it could reconnect us instead?The Sounds of Life offers hope for environmental conservation and affirms humanity’s relationship with nature in the digital age. After learning about the unsuspected wonders of nature’s sounds, we will never see walks outdoors in the same way again.","","9780691240985","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9969969.pdf&bkn=9969968&pdfType=book","Scientist;Soundscape;Coral reef;Technology;Vibration;Larva;Whaling;Language;Insect;Ecosystem;Noise pollution;Marine biology;Honey bee;Bowhead whale;Ethology;Listening;Hatchling;Organism;Bird;Biologist;Animal echolocation;Microphone;Social behavior;Consciousness;Environmental noise;Seagrass;Bioacoustics;SOSUS;Contemporary society;Ecology;Sensory ecology;Hearing range;Marine mammal;Vocal learning;Microbat;Analogy;Hydrophone;Umwelt;Waggle dance;Activism;Biodiversity;Pheromone;Physiology;Lifeway;Sociality;Charismatic megafauna;Sense and respond;Sound ranging;Odor;Mammal;Crustacean;Worker bee;Longevity;Form of life (philosophy);Human intelligence;Resonance;Human;Nutrient;Field recording;Finding;RoboBee;Louse;Plant;Turtle;Wildlife;Beehive fence;Whale vocalization;Artificial reef;Positive feedback;Loudness;Observation;Background noise;Machine learning;Coastal management;Human voice;Emerging technologies;Beautiful music;Sparrow;Karaoke;Behavior;Natural sounds;Sophistication;Shoaling and schooling;Interactivity;CITES;Detection;Propolis;Beehive;Collaboration;Honeyguide;Acoustics;Safety in numbers;Interspecies communication;Environmental protection;Cortisol;Phrase (music);Supervisor;Carl Safina;Cognitive ethology;Sound recording and reproduction","","","","","","","5 Dec 2022","","","Princeton University Press","Princeton University Press eBooks"
"Do Plants Know Math?: Unwinding the Story of Plant Spirals, from Leonardo da Vinci to Now","S. Douady; J. Dumais; C. Golé; N. Pick",NA; NA; NA; NA,"Do Plants Know Math?: Unwinding the Story of Plant Spirals, from Leonardo da Vinci to Now","","2024","","","","","A breathtakingly illustrated look at botanical spirals and the scientists who puzzled over themCharles Darwin was driven to distraction by plant spirals, growing so exasperated that he once begged a friend to explain the mystery “if you wish to save me from a miserable death.” The legendary naturalist was hardly alone in feeling tormented by these patterns. Plant spirals captured the gaze of Leonardo da Vinci and became Alan Turing’s final obsession. This book tells the stories of the physicists, mathematicians, and biologists who found themselves magnetically drawn to Fibonacci spirals in plants, seeking an answer to why these beautiful and seductive patterns occur in botanical forms as diverse as pine cones, cabbages, and sunflowers.Do Plants Know Math? takes you down through the centuries to explore how great minds have been captivated and mystified by Fibonacci patterns in nature. It presents a powerful new geometrical solution, little known outside of scientific circles, that sheds light on why regular and irregular spiral patterns occur. Along the way, the book discusses related plant geometries such as fractals and the fascinating way that leaves are folded inside of buds. Your neurons will crackle as you begin to see the connections. This book will inspire you to look at botanical patterns—and the natural world itself—with new eyes.Featuring hundreds of gorgeous color images, Do Plants Know Math? includes a dozen creative hands-on activities and even spiral-plant recipes, encouraging readers to explore and celebrate these beguiling patterns for themselves.","","9780691261089","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10614697.pdf&bkn=10614696&pdfType=book","Phyllotaxis;botany;flowers;leaves;biomathematics;spirals, patterns;Fibonacci;Leonardo da Vinci;Charles Darwin;Alan Turing;history of science;science activities;botanical illustrations;Do Plants Know Math?;Unwinding the Story of Plant Spirals, from Leonardo da Vinci to Now;Stéphane Douady;Jacques Dumais;Christophe Golé;Nancy Pick;Princeton University Press;plant patterns;plant spirals;phyllotaxis;biomath;biomathematics;nature patterns;history of science;botany;botanical patterns;patterns in nature;golden ratio;golden section;ferns, daisies;sunflowers;pineapples;interdisciplinary science;botanical math;nature curiosity;plant observation;plant photography","","","","","","","30 Jul 2024","","","Princeton University Press","Princeton University Press eBooks"
"Front Matter","",,2023 5th International Conference on Sustainable Technologies for Industry 5.0 (STI),"25 Mar 2024","2023","","","1","58","","","979-8-3503-9431-3","10.1109/STI59863.2023.10464827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10464827","","Fifth Industrial Revolution;Schedules;Artificial intelligence;Technological innovation;Sustainable development;Standards;Leadership","","","","","IEEE","25 Mar 2024","","","IEEE","IEEE Conferences"
"Machine Learning for Physics and Astronomy","V. Acquaviva",NA,Machine Learning for Physics and Astronomy,"","2023","","","","","A hands-on introduction to machine learning and its applications to the physical sciencesAs the size and complexity of data continue to grow exponentially across the physical sciences, machine learning is helping scientists to sift through and analyze this information while driving breathtaking advances in quantum physics, astronomy, cosmology, and beyond. This incisive textbook covers the basics of building, diagnosing, optimizing, and deploying machine learning methods to solve research problems in physics and astronomy, with an emphasis on critical thinking and the scientific method. Using a hands-on approach to learning, Machine Learning for Physics and Astronomy draws on real-world, publicly available data as well as examples taken directly from the frontiers of research, from identifying galaxy morphology from images to identifying the signature of standard model particles in simulations at the Large Hadron Collider.Introduces readers to best practices in data-driven problem-solving, from preliminary data exploration and cleaning to selecting the best method for a given taskEach chapter is accompanied by Jupyter Notebook worksheets in Python that enable students to explore key conceptsIncludes a wealth of review questions and quizzesIdeal for advanced undergraduate and early graduate students in STEM disciplines such as physics, computer science, engineering, and applied mathematicsAccessible to self-learners with a basic knowledge of linear algebra and calculusSlides and assessment questions (available only to instructors)","","9780691249537","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10172388.pdf&bkn=10172387&pdfType=book","Machine Learning;Physics;Data Science;Astronomy;Viviana Acquaviva;Princeton","","","","","","","4 Jul 2023","","","Princeton University Press","Princeton University Press eBooks"
"7 Machine Learning and Blockchain Integration for Security Applications","",,Big Data Analytics and Intelligent Systems for Cyber Threat Intelligence,"","2022","","","129","174","In recent years, a considerable amount of effort has been devoted to cyber-threat protection of computer systems which is one of the most critical cybersecurity tasks for single users and businesses since even a single attack can result in compromised data and sufficient losses. Massive losses and frequent attacks dictate the need for accurate and timely detection methods. Current static and dynamic methods do not provide efficient detection, especially when dealing with zero-day attacks. For this reason, big data analytics and machine intelligence-based techniques can be used. This book brings together researchers in the field of big data analytics and intelligent systems for cyber threat intelligence CTI and key data to advance the mission of anticipating, prohibiting, preventing, preparing, and responding to internal security. The wide variety of topics it presents offers readers multiple perspectives on various disciplines related to big data analytics and intelligent systems for cyber threat intelligence applications. Technical topics discussed in the book include: • Big data analytics for cyber threat intelligence and detection • Artificial intelligence analytics techniques • Real-time situational awareness • Machine learning techniques for CTI • Deep learning techniques for CTI • Malware detection and prevention techniques • Intrusion and cybersecurity threat detection and analysis • Blockchain and machine learning techniques for CTI","","9788770227773","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9950408.pdf&bkn=9950321&pdfType=chapter","","","","","","","","14 Nov 2022","","","River Publishers","River eBook Chapters"
"Explainable AI for 6G Use Cases: Technical Aspects and Research Challenges","S. Wang; M. A. Qureshi; L. Miralles-Pechuán; T. Huynh-The; T. R. Gadekallu; M. Liyanage","School of Computer Science, University College Dublin, Dublin 4, Ireland; ADAPT Centre, Explainable Analytics Group, Faculty of Business, Technological University Dublin, Dublin 2, Ireland; School of Computing, Technological University Dublin, Dublin 7, Ireland; Department of Computer and Communications Engineering, Ho Chi Minh City University of Technology and Education, Ho Chi Minh City, Vietnam; Division of Research and Development, Lovely Professional University, Phagwara, India; School of Computer Science, University College Dublin, Dublin 4, Ireland",IEEE Open Journal of the Communications Society,"1 May 2024","2024","5","","2490","2540","Around 2020, 5G began its commercialization journey, and discussions about the next-generation networks (such as 6G) emerged. Researchers predict that 6G networks will have higher bandwidth, coverage, reliability, energy efficiency, and lower latency, and will be an integrated “human-centric” network system powered by artificial intelligence (AI). This 6G network will lead to many real-time automated decisions, ranging from network resource allocation to collision avoidance for self-driving cars. However, there is a risk of losing control over decision-making due to the high-speed, data-intensive AI decision-making that may go beyond designers’ and users’ comprehension. To mitigate this risk, explainable AI (XAI) methods can be used to enhance the transparency of the black-box AI decision-making process. This paper surveys the application of XAI towards the upcoming 6G age, including 6G technologies (such as intelligent radio and zero-touch network management) and 6G use cases (such as industry 5.0). Additionally, the paper summarizes the lessons learned from recent attempts and outlines important research challenges in applying XAI for 6G use cases soon.","2644-125X","","10.1109/OJCOMS.2024.3386872","European Commission in SPATIAL(grant numbers:101021808); Academy of Finland in 6Genesis(grant numbers:318927); Science Foundation Ireland through CONNECT Phase 2 Project(grant numbers:13/RC/2077_P2); ADAPT Centre Phase 2 Project(grant numbers:13/RC/2106_P2); Industry Fellowship(grant numbers:21/IRDIF/9839); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499970","B5G;6G;AI;XAI;explainability","6G mobile communication;Artificial intelligence;5G mobile communication;Explainable AI;Resource management;Security;Closed box","","18","","303","CCBYNCND","16 Apr 2024","","","IEEE","IEEE Journals"
"GrOVe: Ownership Verification of Graph Neural Networks using Embeddings","A. Waheed; V. Duddu; N. Asokan",University of Waterloo; University of Waterloo; University of Waterloo,2024 IEEE Symposium on Security and Privacy (SP),"5 Sep 2024","2024","","","2460","2477","Graph neural networks (GNNs) have emerged as a state-of-the-art approach to model and draw inferences from large scale graph-structured data in various application settings such as social networking. The primary goal of a GNN is to learn an embedding for each graph node in a dataset that encodes both the node features and the local graph structure around the node.Prior work has shown that GNNs are prone to model extraction attacks. Model extraction attacks and defenses have been explored extensively in other non-graph settings. While detecting or preventing model extraction appears to be difficult, deterring them via effective ownership verification techniques offer a potential defense. In non-graph settings, fingerprinting models, or the data used to build them, have shown to be a promising approach toward ownership verification.We present GrOVe, a state-of-the-art GNN model fingerprinting scheme that, given a target model and a suspect model, can reliably determine if the suspect model was trained independently of the target model or if it is a surrogate of the target model obtained via model extraction. We show that GrOVe can distinguish between surrogate and independent models even when the independent model uses the same training dataset and architecture as the original target model.Using six benchmark datasets and three model architectures, we show that GrOVe consistently achieves low falsepositive and false-negative rates. We demonstrate that GrOVe is robust against known fingerprint evasion techniques while remaining computationally efficient.","2375-1207","979-8-3503-3130-1","10.1109/SP54263.2024.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646643","Graph Neural Networks;Model Extraction;Ownership Verification","Training;Social networking (online);Computational modeling;Computer architecture;Fingerprint recognition;Feature extraction;Data models","","3","","94","IEEE","5 Sep 2024","","","IEEE","IEEE Conferences"
"What is Skill? (and why does it matter?)","R. McDermott; M. Daniels","School of Computing, Robert Gordon University), Aberdeen, Scotland, UK; Dept. of Information Technology, Uppsala University, Uppsala, Sweden",2023 IEEE Frontiers in Education Conference (FIE),"5 Jan 2024","2023","","","1","9","This Research-to-Practice Full Paper seeks to investigate the concept of Skill within a Competency Framework, such as that described by the CC2020 document. The notion of skill is fundamental to modern educational discourse. As educators, we strive, not only to impart knowledge, but to help students acquire the skills that they need to flourish in the modern academic and professional environments. We admire skillful practitioners and strive to become more skilled at what we do, recognising that skill is tied to an aesthetic sense - that there is something attractive and deeply satisfying about the process and output of skillful practice. Together with knowledge and disposition, the term is also used to denote one of the constituent components of competence. In computing, for example, the CC2020 document proposes curricular development models which promote skills as key ontological elements and emphasises skill acquisition as a major focus in the educational process. While this is undoubtedly an important, evolutionary development in discipline-based pedagogical practice, we feel that there are still foundational questions to be asked about precisely what is meant by definitional terms that form the core vocabulary of this approach. In this paper, we look at the notion of skill and provide a conceptual analysis which tries to distinguish it from other related ideas. We provide an overview of how skill has been seen historically as both a philosophical and sociological construct and what this means for using the term in educational theory. We examine how to usefully define skill, discuss the part it plays in teaching and assessment, and make recommendations for how it can be viewed operationally within a competency framework, such as that proposed by CC2020.","2377-634X","979-8-3503-3642-9","10.1109/FIE58773.2023.10343520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10343520","competency model;knowledge;skill acquisition;demonstration","Economics;Vocabulary;Engineering profession;Biological system modeling;Computational modeling;Education;Anxiety disorders","","1","","54","IEEE","5 Jan 2024","","","IEEE","IEEE Conferences"
"The Ultimate Guide to Informed Wearable Technology: A hands-on approach for creating wearables from prototype to purpose using Arduino systems","C. Farion",NA,The Ultimate Guide to Informed Wearable Technology: A hands-on approach for creating wearables from prototype to purpose using Arduino systems,"","2022","","","","","Master wearable technology with this book including colored images and over 50 activities using Arduino and ESP32, build useful, stylish, and smart wearable devices, and create interactive circuits that react to us and our environmentKey FeaturesLearn wearable technology and build electronic circuits with fun activities using Arduino systemsGet an in-depth understanding of e-textiles and ESP32 microcontrollers to create interactive wearablesApply a design innovation approach and best practices to address real-world issuesBook DescriptionWearable circuits add interaction and purpose to clothing and other wearable devices that are currently widely used in medical, social, safety, entertainment, and sports fields. To develop useful and impressive prototypes and wearables, you’ll need to be skilled in designing electronic circuits and working with wearable technologies. This book takes you on an interesting journey through wearable technology, starting from electronic circuits, materials, and e-textile toolkits to using Arduino, which includes a variety of sensors, outputs, actuators, and microcontrollers such as Gemma M0 and ESP32. As you progress, you’ll be carefully guided through creating an advanced IoT project. You’ll learn by doing and create wearables with the help of practical examples and exercises. Later chapters will show you how to develop a hyper-body wearable and solder and sew circuits. Finally, you’ll discover how to build a culture-driven wearable to track data and provide feedback using a Design Innovation approach. After reading this book, you’ll be able to design interactive prototypes and sew, solder, and program your own Arduino-based wearable devices with a purpose.What you will learnConstruct sewable electronic circuits with conductive thread and materialsDiscover the features of LilyPad, Gemma, Circuit Playground, and other boardsUse various components for listening, moving, sensing actions, and visualizing outputsControl ESP32 development boards for IoT explorationUnderstand why and how to prototype to create interactive wearablesGet skilled in sewing and soldering sensors to Arduino-based circuitsDesign and build a hyper-body wearable that senses and reactsMaster a Design Innovation approach for creating wearables with a purposeWho this book is forThis book is for electronics engineers, embedded system engineers and designers, and R&D engineers, who are beginners in the wearable technology domain as well as makers and hobbyists who have an interest in creative computing. It will also be useful for teachers, students, and researchers, who are learning interaction design, physical computing, technology, fashion, or arts. Having a basic understanding of Arduino-based systems will help in easily comprehending the contents of the book.","","9781803244471","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163312.pdf&bkn=10163311&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Application of Patent Analysis in Technology Management: A Scoping Review","M. Srivastava; K. Jain","Shailesh J. Mehta School of Management, Indian Institute of Technology Bombay, Mumbai, India; Shailesh J. Mehta School of Management, Indian Institute of Technology Bombay, Mumbai, India",IEEE Transactions on Engineering Management,"15 Oct 2024","2024","71","","14897","14914","Effective management of technology has become the engine for development. However, the fuzzy nature of technology development and management renders decision-makers dependent on credible information for technology-related decisions. A patent, a techno-legal document granted by the government to inventors to protect their inventions from imitation, proves to be a rich source of information. Various analytical tools can be employed on patent data to gain insights about managing technology strategically. The article conducts a scoping review to assess the application of patent analysis in technology management. This article replenishes the absence of a comprehensive review, revealing the application of patent analysis in technology management. Also, this review provides a panoramic view of how the researchers have employed various tools and techniques on patent data to harness its immense potential to aid in technology-related decision-making.","1558-0040","","10.1109/TEM.2024.3470776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10700042","Decision making;patent analysis;scoping review;technology management","Patents;Technology management;Databases;Decision making;Citation analysis;Information analysis","","","","170","IEEE","30 Sep 2024","","","IEEE","IEEE Journals"
"Innovation Ecosystems in Retail: Uncovering Technological Trends and Collaboration Networks Through Patent Mining","N. Mohammadi; M. Maghsoudi; M. Soghi","Faculty of Commerce and Trade, College of Management, University of Tehran, Tehran, Iran; Management and Accounting Faculty, Shahid Beheshti University, Tehran, Iran; Master of International Business, Faculty of Commerce and Trade, University of Tehran, Tehran, Iran",IEEE Access,"17 Dec 2024","2024","12","","186753","186778","Retail technology adoption is essential for improving operational efficiency, enhancing customer engagement, and supporting sustainable growth in a highly competitive environment. Despite the growing importance of digital transformation in retail, significant research gaps remain in understanding collaborative innovation networks within this sector. This study addresses these gaps by utilizing Social Network Analysis (SNA) to examine patent registrations in retail technology, analyzing a dataset of 36,411 patents from 1995 to 2024. The methodology includes network construction, community detection, and keyword analysis, mapping out 8,225 entities and 14,805 collaborations. Results reveal IBM, Target Brands, and Procter & Gamble as pivotal entities with high network centrality, highlighting their influence across key technological domains like AI, digital commerce, and retail security. The research categorizes technologies into 15 domains—such as Core Retail Operations, Digital Commerce, and Retail Communication—exposing seven major clusters where innovation efforts are concentrated. These findings offer practical insights for corporations, SMEs, startups, and policymakers aiming to navigate technological change and optimize strategic investments in retail. By detailing the relationships between leading entities and emerging technology clusters, this study provides a comprehensive view of the retail innovation landscape, aiding stakeholders in decision-making around technology adoption, collaborative opportunities, and competitive positioning in an increasingly digital retail ecosystem.","2169-3536","","10.1109/ACCESS.2024.3515290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10792909","Retail technology;patent analysis;social network analysis (SNA);digital commerce;retail industry collaboration;data-driven decision-making","Patents;Technological innovation;Industries;Market research;Supply chains;Ecosystems;Blockchains;Artificial intelligence;Companies;Collaboration","","","","131","CCBY","11 Dec 2024","","","IEEE","IEEE Journals"
"Software Architecture with Kotlin: Combine various architectural styles to create sustainable and scalable software solutions","J. (. S. Chow",NA,Software Architecture with Kotlin: Combine various architectural styles to create sustainable and scalable software solutions,"","2024","","","","","Develop innovative architectural styles by analyzing and merging various approaches, focusing on making trade-offs and mitigating risks to solve real-world problemsKey FeaturesLearn how to analyze and dissect various architectural styles into building blocksCombine existing ideas with your own to create custom solutionsMake informed decisions by navigating trade-offs and compromisesPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionSoftware Architecture with Kotlin explores the various styles of software architecture with a focus on using the Kotlin programming language. The author draws on their 20+ years of industry experience in developing large-scale enterprise distributed systems to help you grasp the principles, practices, and patterns that shape the architectural landscape of modern software systems. The book establishes a strong foundation in software architecture, explaining key concepts such as architectural qualities and principles, before teaching you how architectural decisions impact the quality of a system, such as scalability, reliability, and extendability. The chapters address modern architecture topics such as microservices, serverless, and event-driven architectures, providing insights into the challenges and trade-offs involved in adopting these architectural styles. You’ll also discover practical tools that’ll help you make informed decisions and mitigate risks. All architectural patterns in this book are demonstrated using Kotlin. By the end of this book, you’ll have gained practical expertise by using real-world examples, along with a solid understanding of Kotlin, to become a more proficient and impactful software architect.What you will learnMaster the fundamental principles of architecture and designExplore common architectural styles and their applicable scenariosAnalyze, break down, compare, and design architectural styles to solve practical problemsReason, negotiate, and make difficult choices in the absence of ideal solutionsMitigate risks when making compromises and trade-offsCreate scalable, sustainable, maintainable, and extendable software systemsUse the Kotlin programming language to achieve your architectural goalsWho this book is forThis book is for developers with basic Kotlin knowledge seeking a deeper understanding of architecture, Kotlin Android developers who are starting to get involved in backend development, and Java developers transitioning to Kotlin. It's also ideal for software architects who are less experienced in Kotlin and want to enhance their skills, as well as those who enjoy discussing and exploring unique architectural concepts.","","9781835464960","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10848484.pdf&bkn=10848483&pdfType=book","","","","","","","","22 Jan 2025","","","Packt Publishing","Packt Publishing eBooks"
"4 Here Be Dragons","W. R. Neuman",New York University,Evolutionary Intelligence: How Technology Will Make Us Smarter,"","2023","","","87","120","Roman and medieval cartographers developed the tradition of drawing sea monsters and lion-like creatures to designate the unknown dangers of the uncharted lands and oceans at the edges of their maps. Mystery implies danger. Just to make it clear, some mapmakers even wrote out the text in the margins—hic sunt dracones, here be dragons. There are many unknowns about how the phenomenon of evolutionary intelligence (EI) will ultimately become a routine part of our lives. And just as it was with each preceding generation of technology, there will be individuals with malevolent or criminal intent who will try to harness the power of these technologies to do evil things. We would be remiss not to look more closely at these potential fault lines in the future.","","9780262376235","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10355169.pdf&bkn=10339000&pdfType=chapter","","","","","","","","12 Dec 2023","","","MIT Press","MIT Press eBook Chapters"
"Automated Generation of Benchmarks for Falsification of STL Specifications","Y. Yan; D. Lyu; Z. Zhang; P. Arcaini; J. Zhao","Graduate School and Faculty of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan; Graduate School and Faculty of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan; Graduate School and Faculty of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan; National Institute of Informatics, Tokyo, Japan; Graduate School and Faculty of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,"","2025","PP","99","1","1","Falsification, whose aim is to detect unsafe behaviors of cyber-physical systems (CPS) that violate signal temporal logic (STL) specifications, has been actively investigated in the past decade. Although numerous falsification approaches have been proposed, the falsification community suffers from a shortage of benchmarks that hinders a thorough assessment of those falsification approaches. In this paper, we bridge this gap by proposing an automated approach for generating falsification benchmarks. Our approach is data-driven: firstly, we generate different time-variant traces (acting as system output traces) that satisfy a given STL specification, and we associate these with corresponding system input traces; then, we use these input and output traces to train an LSTM model that generalizes them. These models can serve as benchmarks for assessing falsification approaches against the given specification. In the experimental evaluation, we validate the generated models by measuring their ability to differentiate the performance of different falsification approaches. Our generated models expose strengths and weaknesses of all the considered falsification approaches, which was not achieved by benchmarks currently used in the falsification community. These results demonstrate the usefulness of our approach and can potentially push forward subsequent research in falsification.","1937-4151","","10.1109/TCAD.2025.3550410","Japan Society for the Promotion of Science(grant numbers:JP23H03372,JP23K16865); JST-Mirai Program(grant numbers:JPMJMI20B8); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10922764","Falsification;Benchmarks;Signal temporal logic;Recurrent neural networks","Benchmark testing;Semantics;Logic;Integrated circuit modeling;Training;Design automation;Computational modeling;Software packages;Robustness;Long short term memory","","","","","IEEE","11 Mar 2025","","","IEEE","IEEE Early Access Articles"
"The Kubernetes Bible: The definitive guide to deploying and managing Kubernetes across cloud and on-prem environments","G. Madapparambath; R. McKendrick; E. Price",NA; NA; NA,The Kubernetes Bible: The definitive guide to deploying and managing Kubernetes across cloud and on-prem environments,"","2024","","","","","This completely revised edition equips you to secure, scale, and optimize your deployments like a K8s pro . Learn advanced techniques and cloud implementations for robust container orchestration and cloud-native domination. Purchase of the print or Kindle book includes a free eBook in PDF format.Key FeaturesComprehensive coverage of Kubernetes concepts - from deployment to cluster and resource managementGain insights into the latest cloud-native trends and how they impact your Kubernetes deploymentsTap into the collective wisdom of acclaimed Kubernetes expertsBook DescriptionKubernetes has become the go-to orchestration platform for containerized applications. As a Kubernetes user, you know firsthand how powerful yet complex this tool can be. The Kubernetes Bible cuts through the complexity, offering hands-on examples and expert advice to conquer containerization challenges With this new edition, you will master cutting edge security practices, deploy seamlessly and scale effortlessly, ensuring unwavering service availability. You will gain the expertise to craft production-grade applications, secure development environments, navigate complex deployments with ease, and become a security maestro. You will be able to optimize network communication and data management across major cloud platforms. Additionally, this book dives deep into these challenges, offering solutions such as multi-container Pods, advanced security techniques, and expert networking guidance. You will also explore persistent storage advancements, cloud-specific cluster management updates, and best practices for traffic routing By the end of this comprehensive guide, you will possess the skills and knowledge to orchestrate your containerized applications with precision, ensuring their optimal performance and scalability. Stop settling for basic container management. Order your copy today and orchestrate your containers to greatness.What you will learnSecure your Kubernetes clusters with advanced techniquesImplement scalable deployments and autoscaling strategiesDesign and learn to build production-grade containerized applicationsManage Kubernetes effectively on major cloud platforms (GKE, EKS, AKS)Utilize advanced networking and service management practicesUse Helm charts and Kubernetes Operators for robust security measuresOptimize in-cluster traffic routing with advanced configurationsEnhance security with techniques like Immutable ConfigMaps and RBACWho this book is forWhether you're a software developer, DevOps engineer, or an existing Kubernetes user, this Kubernetes book is your comprehensive guide to mastering container orchestration and services in the cloud. It empowers you to overcome challenges in building secure, scalable, and cloud-native applications using Kubernetes. With a foundational understanding of Kubernetes, Docker, and leading cloud providers (AWS, Azure, GCP) recommended, this book equips you with the knowledge and skills needed to navigate complex deployments and master core Kubernetes concepts and architecture.","","9781835468241","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10780978.pdf&bkn=10780977&pdfType=book","","","","","","","","6 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference","A. Narayanan; S. Kapoor",NA; NA,"AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference","","2024","","","","","From two of TIME’s 100 Most Influential People in AI, what you need to know about AI—and how to defend yourself against bogus AI claims and productsConfused about AI and worried about what it means for your future and the future of the world? You’re not alone. AI is everywhere—and few things are surrounded by so much hype, misinformation, and misunderstanding. In AI Snake Oil, computer scientists Arvind Narayanan and Sayash Kapoor cut through the confusion to give you an essential understanding of how AI works and why it often doesn’t, where it might be useful or harmful, and when you should suspect that companies are using AI hype to sell AI snake oil—products that don’t work, and probably never will.While acknowledging the potential of some AI, such as ChatGPT, AI Snake Oil uncovers rampant misleading claims about the capabilities of AI and describes the serious harms AI is already causing in how it’s being built, marketed, and used in areas such as education, medicine, hiring, banking, insurance, and criminal justice. The book explains the crucial differences between types of AI, why organizations are falling for AI snake oil, why AI can’t fix social media, why AI isn’t an existential risk, and why we should be far more worried about what people will do with AI than about anything AI will do on its own. The book also warns of the dangers of a world where AI continues to be controlled by largely unaccountable big tech companies.By revealing AI’s limits and real risks, AI Snake Oil will help you make better decisions about whether and how to use AI at work and home.","","9780691249643","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10614695.pdf&bkn=10614694&pdfType=book","","","","","","","","30 Jul 2024","","","Princeton University Press","Princeton University Press eBooks"
"A Survey on Digital Twins: Architecture, Enabling Technologies, Security and Privacy, and Future Prospects","Y. Wang; Z. Su; S. Guo; M. Dai; T. H. Luan; Y. Liu","School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China",IEEE Internet of Things Journal,"23 Aug 2023","2023","10","17","14965","14987","By interacting, synchronizing, and cooperating with its physical counterpart in real time, digital twin (DT) is promised to promote an intelligent, predictive, and optimized modern city. Via interconnecting massive physical entities and their virtual twins with inter-twin and intra-twin communications, the Internet of DTs (IoDT) enables free data exchange, dynamic mission cooperation, and efficient information aggregation for composite insights across vast physical/virtual entities. However, as IoDT incorporates various cutting-edge technologies to spawn the new ecology, severe known/unknown security flaws, and privacy invasions of IoDT hinder its wide deployment. Besides, the intrinsic characteristics of IoDT, such as decentralized structure, information-centric routing, and semantic communications, entail critical challenges for security service provisioning in IoDT. To this end, this article presents an in-depth review of the IoDT with respect to system architecture, enabling technologies, and security/privacy issues. Specifically, we first explore a novel distributed IoDT architecture with cyber–physical interactions and discuss its key characteristics and communication modes. Afterward, we investigate the taxonomy of security and privacy threats in IoDT, discuss the key research challenges, and review the state-of-the-art defense approaches. Finally, we point out the new trends and open research directions related to IoDT.","2327-4662","","10.1109/JIOT.2023.3263909","NSFC(grant numbers:U22A2029,U20A20175,62101429); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10090432","Artificial intelligence (AI);blockchain;Internet of Digital Twins (IoDT);privacy;security;semantic communication","Digital twins;Security;Privacy;Semantics;Internet of Things;Internet;Synchronization","","71","","140","IEEE","3 Apr 2023","","","IEEE","IEEE Journals"
"A Survey of Security Challenges, Attacks Taxonomy and Advanced Countermeasures in the Internet of Things","S. Khanam; I. B. Ahmedy; M. Y. Idna Idris; M. H. Jaward; A. Q. Bin Md Sabri","Department of Computer System and Technology, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Department of Computer System and Technology, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Department of Computer System and Technology, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; School of Engineering, Monash University Malaysia, Bandar Sunway, Malaysia; Department of Artificial Intelligence, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia",IEEE Access,"15 Dec 2020","2020","8","","219709","219743","Internet of Things (IoT) facilitates the integration between objects and different sensors to provide communication among them without human intervention. However, the extensive demand for IoT and its various applications has continued to grow, coupled with the need to achieve foolproof security requirements. IoT produces a vast amount of data under several constraints such as low processor, power, and memory. These constraints, along with the invaluable data produced by IoT devices, make IoT vulnerable to various security attacks. This paper presents an overview of IoT, its well-known system architecture, enabling technologies, and discusses security challenges and goals. Furthermore, we analyze security vulnerabilities and provide state-of-the-art security taxonomy. The taxonomy of the most relevant and current IoT security attacks is presented for application, network, and physical layers. While most other surveys studied one of the areas of security measures, this study considers and reports on the most advanced security countermeasures within the areas of autonomic, encryption, and learning-based approaches. Additionally, we uncover security challenges that may be met by the research community regarding security implementation in heterogeneous IoT environment. Finally, we provide different visions about possible security solutions and future research directions.","2169-3536","","10.1109/ACCESS.2020.3037359","Impact-Oriented Interdisciplinary Research Grant Programme (IIRG)(grant numbers:IIRG003A-19IISS); Fundamental Research Grant Scheme (FRGS)(grant numbers:FP055-2019A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9256294","Attacks;countermeasures;encryption;Internet of Things;IoT architecture;learning-based algorithm;privacy;security;secure communications;taxonomy","Security;Internet of Things;Encryption;Taxonomy;Organizations;Protocols;Privacy","","70","","237","CCBY","11 Nov 2020","","","IEEE","IEEE Journals"
"A Survey of Adversarial Attack and Defense Methods for Malware Classification in Cyber Security","S. Yan; J. Ren; W. Wang; L. Sun; W. Zhang; Q. Yu","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Department of Mathematics and Theories, Peng Cheng Laboratory, Shenzhen, China; Department of Mathematics and Sciences, Peng Cheng Laboratory, Shenzhen, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Electrical Engineering and Telecommunications, The University of New South Wales, Sydney, NSW, Australia; Department of Mathematics and Sciences, Peng Cheng Laboratory, Shenzhen, China",IEEE Communications Surveys & Tutorials,"23 Feb 2023","2023","25","1","467","496","Malware poses a severe threat to cyber security. Attackers use malware to achieve their malicious purposes, such as unauthorized access, stealing confidential data, blackmailing, etc. Machine learning-based defense methods are applied to classify malware examples. However, such methods are vulnerable to adversarial attacks, where attackers aim to generate adversarial examples that can evade detection. Defenders also develop various approaches to enhance the robustness of malware classifiers against adversarial attacks. Both attackers and defenders evolve in the continuous confrontation of malware classification. In this paper, we firstly summarize a unified malware classification framework. Then, based on the framework, we systematically survey the Defense-Attack-Enhanced-Defense process and provide a comprehensive review of (i) machine learning-based malware classification, (ii) adversarial attacks on malware classifiers, and (iii) robust malware classification. Finally, we highlight the main challenges faced by both attackers and defenders and discuss some promising future work directions.","1553-877X","","10.1109/COMST.2022.3225137","National Research and Development Program of China(grant numbers:2020YFB1807503); NSFC Fund(grant numbers:U20A20156,61842202); Young Scientists Fund of the National Natural Science Foundation of China(grant numbers:62002342); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964330","Cyber security;malware;malware classification;adversarial examples;adversarial robustness","Malware;Feature extraction;Robustness;Security;Internet of Things;Data preprocessing;Cyberattack","","28","","155","IEEE","28 Nov 2022","","","IEEE","IEEE Journals"
"ChatGPT’s Security Risks and Benefits: Offensive and Defensive Use-Cases, Mitigation Measures, and Future Implications","M. Charfeddine; H. M. Kammoun; B. Hamdaoui; M. Guizani","REsearch Groups in Intelligent Machines (REGIM-Lab), National Engineering School of Sfax, University of Sfax, Sfax, Tunisia; REsearch Groups in Intelligent Machines (REGIM-Lab), National Engineering School of Sfax, University of Sfax, Sfax, Tunisia; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; Department of Machine Learning, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, United Arab Emirates",IEEE Access,"1 Mar 2024","2024","12","","30263","30310","ChatGPT has been acknowledged as a powerful tool that can radically boost productivity across a wide range of industries. It reveals potential in cybersecurity-related tasks such as social engineering. Nevertheless, this possibility raises important concerns regarding the thin line separating moral use of this technology from its harmful usage. It is imperative to address the challenges of distinguishing between legitimate and malevolent use of ChatGPT. This research paper investigates the many concerns of ChatGPT in cybersecurity, privacy and enterprise settings. It covers harmful attacker uses such as injecting malicious prompts, testing brute force attacks, preparing and developing ransomware attacks, etc. Defenders’ proactive activities are also addressed, highlighting ChatGPT’s significance in security operations and threat intelligence. These defensive operations are classified based on the National Institute of Standards and Technology cybersecurity framework. They involve analyzing configuration files, inquiring about authoritative server, improving security in various systems, etc. Moreover, secure enterprise practices and mitigations spread through five classes are proposed, with an emphasis on clear usage standards and guidelines establishment, personally identifiable information protection, adversarial attack prevention, watermarking generated content, etc. An integrated discussion digs into the interaction of offensive and defensive applications, covering ethical and practical concerns. Future attacks are also discussed, along with potential solutions such as content filtering and collaboration. Finally, a comparative analysis with recent research on ChatGPT security concerns is directed. The paper provides a thorough framework to comprehend the range of implications associated with ChatGPT, enabling the navigation of cybersecurity and privacy challenges.","2169-3536","","10.1109/ACCESS.2024.3367792","NSF(grant numbers:2003273); Tunisian Ministry of Higher Education and Scientific Research(grant numbers:LR11ES48); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10443401","Artificial intelligence;ChatGPT;computer crime;cyberattack;cyberethics;cybersecurity;defense industry;NLP;privacy","Chatbots;Security;Artificial intelligence;Computer security;Ethics;Guidelines;Privacy;Cyberattack;NIST Standards;Watermarking","","11","","87","CCBYNCND","21 Feb 2024","","","IEEE","IEEE Journals"
"Analog Integrated Circuit Routing Techniques: An Extensive Review","R. M. F. Martins; N. C. C. Lourenço","Instituto de Telecomunicações, Lisbon, Portugal; Instituto de Telecomunicações, Lisbon, Portugal",IEEE Access,"13 Apr 2023","2023","11","","35965","35983","Routing techniques for analog and radio-frequency (A/RF) integrated circuit (IC) design automation have been proposed in the literature for over three decades. On those, an extensive set of geometric constraints have already been covered as surrogates for routing quality, but also, performance-related criteria were progressively included. However, as A/RF design moved into advanced integration technology nodes, the increasing number of design rules/constraints, wire resistance, congestion, and interwire parasitic growth is constantly challenging existing automatic routing techniques and keeping pressure on their improvement. Fortunately, recent developments in modern workstations’ capabilities allowed the growth of sophisticated routing processes, including some assisted by the latest machine and deep learning methods, offering unprecedented solutions for the automation of this task. Still, as the correlation between routing-induced parasitic structures and the circuit’s functional behavior is far from simple, computational-intensive parasitic-inclusive and layout-aware synthesis techniques have also been proposed, where automatic routing techniques play a decisive role. This paper conducts an extensive review of A/RF IC routing techniques, from the digitally-inspired earliest approaches to state-of-the-art developments, providing a complete and comprehensive guide for circuit designers and design automation developers while defining research lines to facilitate more activities within this field.","2169-3536","","10.1109/ACCESS.2023.3265481","Fundação para a Ciência e a Tecnologia (FCT)/MCTES through national funds and when applicable co-funded European Union (EU) funds, through the Internal Research Project LAY(RF)2/X-0002-LX-20 and HAICAS/X-0009-LX-20(grant numbers:UIDB/50008/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10097479","Analog and radio-frequency;automatic routing;layout-aware synthesis;machine learning;parasitic-inclusive synthesis;path-finding algorithm;physical design","Routing;Radio frequency;Wires;Integrated circuits;Generators;Python;Task analysis;Machine learning","","10","","189","CCBYNCND","7 Apr 2023","","","IEEE","IEEE Journals"
"Privacy-preserving collaborative machine learning in biomedical applications","W. Kim; J. Seok","School of Electrical Engineering, Korea University, Seoul, South Korea; School of Electrical Engineering, Korea University, Seoul, South Korea",2022 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),"1 Mar 2022","2022","","","179","183","Machine learning (ML) algorithms are now widely used to tackle computational problems in diverse domains. In biomedicine, the rapidly growing amounts of experimental data increasingly necessitate the use of ML to discern complex data patterns. However, biomedical data is often considered sensitive, and the privacy of individuals behind the data is increasingly put at risk as a result. Traditional methods such as anonymization and pseudonymization are not always applicable and have limited effectiveness with respect to risk mitigation. Privacy researchers are actively developing alternative approaches to privacy protection, including strategies based on cryptography, such as homomorphic encryption and secure multiparty computation. This paper discusses recent advances in biomedical applications of these privacy techniques. We first review the key privacy techniques, then provide an overview of their applications in biomedical machine learning. Finally, we highlight the remaining challenges of current approaches and suggest directions for future work.","","978-1-6654-5818-4","10.1109/ICAIIC54071.2022.9722703","Korea Institute for Advancement of Technology; National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9722703","Privacy-Preserving Machine Learning;Collaborative Learning;Federated Learning;Secure Multi-party Computation","Privacy;Data privacy;Machine learning algorithms;Biological system modeling;Machine learning;Collaborative work;Multi-party computation","","8","","57","IEEE","1 Mar 2022","","","IEEE","IEEE Conferences"
"Enabling Intelligent Connectivity: A Survey of Secure ISAC in 6G Networks","X. Zhu; J. Liu; L. Lu; T. Zhang; T. Qiu; C. Wang; Y. Liu","School of Software Engineering, Beijing Jiaotong University, Beijing, China; School of Software Engineering, Beijing Jiaotong University, Beijing, China; School of Software Engineering, Beijing Jiaotong University, Beijing, China; School of Software Engineering, Beijing Jiaotong University, Beijing, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Cyber Security, Qilu University of Technology, Jinan, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",IEEE Communications Surveys & Tutorials,"","2024","PP","99","1","1","The rapid growth of intelligent sensing capabilities and super computation power in 6G mobile communication systems has facilitated their application in diverse domains such as smart health, smart factories, and the industrial Internet of Things. Integrated Sensing and Communication (ISAC), as a core technology, has merged with artificial intelligence (AI) to enable intelligent connectivity, leading to a paradigm shift in traditional communication modes. This paper presents a visionary design for an ISAC-oriented unified IoT architecture that integrates software-defined communication and super-intelligent agents. By leveraging dynamic adaptability, self-learning, and optimization, the ISAC system can intelligently and flexibly respond to evolving requirements and environments. The architecture is redefined into three layers: the hardware layer, the omniscient layer, and the application layer. Furthermore, a retrospective survey of ISAC technology development over the past decade is conducted, highlighting new design principles for AI-empowered networks and multi-modals that support “intelligent connectivity"" across various application scenarios and reinforce the security of ISAC. This paper categorizes the related works according to the different layer structures of the proposed architecture, and some important physical and machine learning models are introduced. Additionally, we summarize the current technological bottlenecks associated with ISAC and propose future research directions and potential solutions that lay the foundation for the future development of secure and intelligent communication networks.","1553-877X","","10.1109/COMST.2024.3432871","Talent Fund of Beijing Jiaotong University(grant numbers:2023XKRC016,2023XKRC050); China Postdoctoral Science Foundation(grant numbers:GZC20230223,GZC20230224); Fundamental Research Funds for the Central Universities(grant numbers:2021CZ102); Taishan Scholar Program of Shandong(grant numbers:tsqn202306251); Youth Innovation Team of Colleges and Universities in Shandong Province(grant numbers:2022KJ124); Open Project Fund of Key Laboratory of Computing Power Network and Information Security(grant numbers:2023ZD039); National Natural Science Foundation of China(grant numbers:62072030,92167204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10608156","6G;ISAC;AI;Software-defined communication;Super intelligent agent","Integrated sensing and communication;Computer architecture;Artificial intelligence;Surveys;6G mobile communication;Indexes;Wireless communication","","4","","","IEEE","24 Jul 2024","","","IEEE","IEEE Early Access Articles"
"7 Making Creative Systems Effective","O. Bown",University of New South Wales,Beyond the Creative Species: Making Machines That Make Art and Music,"","2021","","","269","294","How do we evaluate whether computational creativity systems are doing creative things, or in the language of distributed creativity, are effective and possibly active contributors in creative production? The radical multi-disciplinarity of computational creativity invites a wide range of methods for answering such questions. In engineering and computer science it is often the case that one can specify an objective goal for an AI system. In many creativity-related subtasks this may also be the case. Training a neural network to recognize objects in images requires that we have a dataset of annotated images, meaning that we necessarily have the information required to know exactly how well the system is performing—at least at the given subtask, if not at the wider objective of contributing to a creative task. Meanwhile, human-computer interaction tasks require evaluation of users' responses, and although this is less concrete, data—both qualitative and quantitative—can be gathered about the system's performance. In other cases, practice-based programmer-artists make work and self-evaluate. In the latter, the evidence becomes vaguer still, but the situation is largely similar to the human-computer interaction context. Systems that generate creative outputs can be investigated using all of these evaluation scenarios and others. A scenario that is somewhat more specific to computational creativity is where people are asked to evaluate the aesthetic quality of the system's output or to otherwise evaluate whether they believe the system to be creative or humanlike.","","9780262361750","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9394872.pdf&bkn=9394486&pdfType=chapter","","","","","","","","5 Apr 2021","","","MIT Press","MIT Press eBook Chapters"
"5 Creative Algorithms","O. Bown",University of New South Wales,Beyond the Creative Species: Making Machines That Make Art and Music,"","2021","","","157","214","Switching to talking about algorithms involves a sharp break from the previous discussion. We set aside, for the moment, thinking about psychology, social science, art and creativity studies and consider what it is that we can do with computers and how this is being used to achieve the goals of computational creativity. The study of algorithms belongs to what Herbert Simon described as The Sciences of the Artificial,<superscript>2</superscript> where we do not study the world as it is, out there, but study our own built environment and the process of building that environment, which we can do in an interactive, probing manner through the production of new designs and inventions. In Simon's words, “engineering, medicine, business, architecture, and painting are concerned not with the necessary but with the contingent—not with how things are but with how they might be—in short, with design.” I begin this chapter by returning to a theme from chapter 1, the essential distinction between processes of generation and evaluation in creativity, which will help frame the relations between algorithms in the construction of artificially creative systems.","","9780262361750","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9394517.pdf&bkn=9394486&pdfType=chapter","","","","","","","","2 Apr 2021","","","MIT Press","MIT Press eBook Chapters"
"Engineering Data Mesh in Azure Cloud: Implement data mesh using Microsoft Azure's Cloud Adoption Framework","A. Deswandikar",NA,Engineering Data Mesh in Azure Cloud: Implement data mesh using Microsoft Azure's Cloud Adoption Framework,"","2024","","","","","Overcome data mesh adoption challenges using the cloud-scale analytics framework and make your data analytics landscape agile and efficient by using standard architecture patterns for diverse analytical workloads Key FeaturesDelve into core data mesh concepts and apply them to real-world situationsSafely reassess and redesign your framework for seamless data mesh integrationConquer practical challenges, from domain organization to building data contractsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionDecentralizing data and centralizing governance are practical, scalable, and modern approaches to data analytics. However, implementing a data mesh can feel like changing the engine of a moving car. Most organizations struggle to start and get caught up in the concept of data domains, spending months trying to organize domains. This is where Engineering Data Mesh in Azure Cloud can help. The book starts by assessing your existing framework before helping you architect a practical design. As you progress, you’ll focus on the Microsoft Cloud Adoption Framework for Azure and the cloud-scale analytics framework, which will help you quickly set up a landing zone for your data mesh in the cloud. The book also resolves common challenges related to the adoption and implementation of a data mesh faced by real customers. It touches on the concepts of data contracts and helps you build practical data contracts that work for your organization. The last part of the book covers some common architecture patterns used for modern analytics frameworks such as artificial intelligence (AI). By the end of this book, you’ll be able to transform existing analytics frameworks into a streamlined data mesh using Microsoft Azure, thereby navigating challenges and implementing advanced architecture patterns for modern analytics workloads.What you will learnBuild a strategy to implement a data mesh in Azure CloudPlan your data mesh journey to build a collaborative analytics platformAddress challenges in designing, building, and managing data contractsGet to grips with monitoring and governing a data meshUnderstand how to build a self-service portal for analyticsDesign and implement a secure data mesh architectureResolve practical challenges related to data mesh adoptionWho this book is forThis book is for chief data officers and data architects of large and medium-size organizations who are struggling to maintain silos of data and analytics projects. Data architects and data engineers looking to understand data mesh and how it can help their organizations democratize data and analytics will also benefit from this book. Prior knowledge of managing centralized analytical systems, as well as experience with building data lakes, data warehouses, data pipelines, data integrations, and transformations is needed to get the most out of this book.","","9781805128946","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769214.pdf&bkn=10769213&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Machine Learning for Time-Series with Python: Forecast, predict, and detect anomalies with state-of-the-art machine learning methods","B. Auffarth",NA,"Machine Learning for Time-Series with Python: Forecast, predict, and detect anomalies with state-of-the-art machine learning methods","","2021","","","","","Get better insights from time-series data and become proficient in model performance analysisKey FeaturesExplore popular and modern machine learning methods including the latest online and deep learning algorithmsLearn to increase the accuracy of your predictions by matching the right model with the right problemMaster time series via real-world case studies on operations management, digital marketing, finance, and healthcareBook DescriptionThe Python time-series ecosystem is huge and often quite hard to get a good grasp on, especially for time-series since there are so many new libraries and new models. This book aims to deepen your understanding of time series by providing a comprehensive overview of popular Python time-series packages and help you build better predictive systems. Machine Learning for Time-Series with Python starts by re-introducing the basics of time series and then builds your understanding of traditional autoregressive models as well as modern non-parametric models. By observing practical examples and the theory behind them, you will become confident with loading time-series datasets from any source, deep learning models like recurrent neural networks and causal convolutional network models, and gradient boosting with feature engineering. This book will also guide you in matching the right model to the right problem by explaining the theory behind several useful models. You’ll also have a look at real-world case studies covering weather, traffic, biking, and stock market data. By the end of this book, you should feel at home with effectively analyzing and applying machine learning methods to time-series.What you will learnUnderstand the main classes of time series and learn how to detect outliers and patternsChoose the right method to solve time-series problemsCharacterize seasonal and correlation patterns through autocorrelation and statistical techniquesGet to grips with time-series data visualizationUnderstand classical time-series models like ARMA and ARIMAImplement deep learning models, like Gaussian processes, transformers, and state-of-the-art machine learning modelsBecome familiar with many libraries like Prophet, XGboost, and TensorFlowWho this book is forThis book is ideal for data analysts, data scientists, and Python developers who want instantly useful and practical recipes to implement today, and a comprehensive reference book for tomorrow. Basic knowledge of the Python Programming language is a must, while familiarity with statistics will help you get the most out of this book.","","9781801816106","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163198.pdf&bkn=10163197&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Cross-Domain Visual Exploration of Academic Corpora via the Latent Meaning of User-Authored Keywords","A. Benito-Santos; R. Therón Sánchez","Visual Analytics and Information Visualization Group, University of Salamanca, Salamanca, Spain; Visual Analytics and Information Visualization Group, University of Salamanca, Salamanca, Spain",IEEE Access,"1 Aug 2019","2019","7","","98144","98160","Nowadays, scholars dedicate a substantial amount of their work to the querying and browsing of increasingly large collections of research papers on the Internet. In parallel, the recent surge of novel interdisciplinary approaches in science requires scholars to acquire competencies in new fields for which they may lack the necessary vocabulary to formulate adequate queries. This problem, together with the issue of information overload, poses new challenges in the fields of natural language processing (NLP) and visualization design that call for a rapid response from the scientific community. In this respect, we report on a novel visualization scheme that enables the exploration of research paper collections via the analysis of semantic proximity relationships found in author-assigned keywords. Our proposal replaces traditional string queries with a bag-of-words (BoW) extracted from a user-generated auxiliary corpus that captures the intentionality of the research. Continuing along the lines established by other authors in the fields of literature-based discovery (LBD), NLP, and visual analytics (VA), we combine novel advances in the fields of NLP with visual network analysis techniques to offer scholars a perspective of the target corpus that better fits their research interests. To highlight the advantages of our proposal, we conduct two experiments employing a collection of visualization research papers and an auxiliary cross-domain BoW. Here, we showcase how our visualization can be used to maximize the effectiveness of a browsing session by enhancing the language acquisition task, which allows for effectively extracting knowledge that is in line with the users' previous expectations.","2169-3536","","10.1109/ACCESS.2019.2929754","Ministerio de Economía y Competitividad(grant numbers:PCIN-2017-064); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8766090","Academic corpora;digital humanities;document exploration;human-computer interaction;knowledge elicitation;latent semantic analysis;literature-based discovery;visualization","Semantics;Task analysis;Analytical models;Data visualization;Visual analytics;Natural language processing","","13","","64","CCBY","18 Jul 2019","","","IEEE","IEEE Journals"
"Statistical Hypothesis Testing Based on Machine Learning: Large Deviations Analysis","P. Braca; L. M. Millefiori; A. Aubry; S. Marano; A. De Maio; P. Willett","Research Department, Centre for Maritime Research and Experimentation, La Spezia, SP, Italy; Research Department, Centre for Maritime Research and Experimentation, La Spezia, SP, Italy; DIETI, University of Naples “Federico II”, Naples, NA, Italy; DIEM, University of Salerno, Fisciano, SA, Italy; DIETI, University of Naples “Federico II”, Naples, NA, Italy; Department of Electrical and Computer Engineering, University of Connecticut, Storrs, CT, USA",IEEE Open Journal of Signal Processing,"7 Feb 2023","2022","3","","464","495","We study the performance of Machine Learning (ML) classification techniques. Leveraging the theory of large deviations, we provide the mathematical conditions for a ML classifier to exhibit error probabilities that vanish exponentially, say $\exp (-n\,I)$, where $n$ is the number of informative observations available for testing (or another relevant parameter, such as the size of the target in an image) and $I$ is the error rate. Such conditions depend on the Fenchel-Legendre transform of the cumulant-generating function of the Data-Driven Decision Function (D3F, i.e., what is thresholded before the final binary decision is made) learned in the training phase. As such, the D3F and the related error rate $I$ depend on the given training set. The conditions for the exponential convergence can be verified and tested numerically exploiting the available dataset or a synthetic dataset generated according to the underlying statistical model. Coherently with the large deviations theory, we can also establish the convergence of the normalized D3F statistic to a Gaussian distribution. Furthermore, approximate error probability curves $\zeta _{n} \exp (-n\,I)$ are provided, thanks to the refined asymptotic derivation, where $\zeta _{n}$ represents the most representative sub-exponential terms of the error probabilities. Leveraging the refined asymptotic, we are able to compute an accurate analytical approximation of the classification performance for both the regimes of small and large values of $n$. Theoretical findings are corroborated by extensive numerical simulations and by the use of real-world data, acquired by an X-band maritime radar system for surveillance.","2644-1322","","10.1109/OJSP.2022.3232284","NATO Allied Command Transformation; Data Knowledge Operational Effectiveness; NIUVT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10008020","Machine learning;deep learning;large deviations principle;exact asymptotics;statistical hypothesis testing;Fenchel-Legendre transform;extended target detection;radar/sonar detection;X-band maritime radar","Error probability;Training;Artificial intelligence;Convergence;Error analysis;Surveillance;Signal processing","","10","","112","CCBY","6 Jan 2023","","","IEEE","IEEE Journals"
"A Survey on Intelligent Internet of Things: Applications, Security, Privacy, and Future Directions","O. Aouedi; T. -H. Vu; A. Sacco; D. C. Nguyen; K. Piamrat; G. Marchetto; Q. -V. Pham","SIGCOM, SnT, University of Luxembourg, Luxembourg; Department of Electrical, Electronic and Computer Engineering, University of Ulsan, Republic of Korea; DAUIN, Politecnico di Torino, Turin, Italy; Department of Electrical and Computer Engineering, University of Alabama, Huntsville, USA; Nantes University, École Centrale Nantes, CNRS, INRIA, LS2N UMR 6004, France; DAUIN, Politecnico di Torino, Turin, Italy; Trinity College Dublin, School of Computer Science and Statistics, The University of Dublin, Dublin 2, Ireland",IEEE Communications Surveys & Tutorials,"","2024","PP","99","1","1","The rapid advances in the Internet of Things (IoT) have promoted a revolution in communication technology and offered various customer services. Artificial intelligence (AI) techniques have been exploited to facilitate IoT operations and maximize their potential in modern application scenarios. In particular, the convergence of IoT and AI has led to a new networking paradigm called Intelligent IoT (IIoT), which has the potential to significantly transform businesses and industrial domains. This paper presents a comprehensive survey of IIoT by investigating its significant applications in mobile networks, as well as its associated security and privacy issues. Specifically, we explore and discuss the roles of IIoT in a wide range of key application domains, from smart healthcare and smart cities to smart transportation and smart industries. Through such extensive discussions, we investigate important security issues in IIoT networks, where network attacks, confidentiality, integrity, and intrusion are analyzed, along with a discussion of potential countermeasures. Privacy issues in IIoT networks were also surveyed and discussed, including data, location, and model privacy leakage. Finally, we outline several key challenges and highlight potential research directions in this important area.","1553-877X","","10.1109/COMST.2024.3430368","European Union Horizon-CL4-2021 Research and Innovation Program(grant numbers:101070181); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10601684","Internet of Things;artificial intelligence;wireless networks;industrial applications;security;privacy","Industrial Internet of Things;Artificial intelligence;Privacy;Surveys;Security;Biological system modeling;Reviews","","9","","","CCBYNCND","18 Jul 2024","","","IEEE","IEEE Early Access Articles"
"End-Edge-Cloud Collaborative Computing for Deep Learning: A Comprehensive Survey","Y. Wang; C. Yang; S. Lan; L. Zhu; Y. Zhang","School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Economics and Management, University of the Chinese Academy of Sciences, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; Department of Informatics, University of Oslo, Oslo, Norway",IEEE Communications Surveys & Tutorials,"21 Nov 2024","2024","26","4","2647","2683","The booming development of deep learning applications and services heavily relies on large deep learning models and massive data in the cloud. However, cloud-based deep learning encounters challenges in meeting the application requirements of responsiveness, adaptability, and reliability. Edge-based and end-based deep learning enables rapid, near real-time analysis and response, but edge nodes and end devices usually have limited resources to support large models. This necessitates the integration of end, edge, and cloud computing technologies to combine their different advantages. Despite the existence of numerous studies on edge-cloud collaboration, a comprehensive survey for end-edge-cloud computing-enabled deep learning is needed to review the current status and point out future directions. Therefore, this paper: 1) analyzes the collaborative elements within the end-edge-cloud computing system for deep learning, and proposes collaborative training, inference, and updating methods and mechanisms for deep learning models under the end-edge-cloud collaboration framework. 2) provides a systematic investigation of the key enabling technologies for end-edge-cloud collaborative deep learning, including model compression, model partition, and knowledge transfer. 3) highlights six open issues to stimulate continuous research efforts in the field of end-edge-cloud deep learning.","1553-877X","","10.1109/COMST.2024.3393230","National Key Research and Development Program of China(grant numbers:2021YFB1715700); National Natural Science Foundation of China(grant numbers:62103046,72201266,72192843,72192844); Fundamental Research Funds for the Central Universities(grant numbers:E1E40805X2,2023CX01020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10508191","Deep learning;deep neural networks;edge computing;cloud computing;end-edge-cloud collaboration;end-edge-cloud computing","Computational modeling;Artificial intelligence;Collaboration;Surveys;Training;Cloud computing;Deep learning","","9","","252","IEEE","24 Apr 2024","","","IEEE","IEEE Journals"
"Evolutionary Developments of Today’s Remote Sensing Radar Technology—Right From the Telemobiloscope: A review","S. S. Kari; A. A. B. Raj; B. K","Department of Space, Indian Institute of Space Science and Technology (IIST), Thiruvananthapuram, India; Defence Institute of Advanced Technology, Pune, India; Department of Metallurgical of Materials Engineering, Ministry of Defence, Defence Institute of Advanced Technology, Pune, India",IEEE Geoscience and Remote Sensing Magazine,"6 Mar 2024","2024","12","1","67","107","Today, remote sensing systems/technologies are one of the most essential requirements for civil and military sectors for various applications. This review article discusses the evolutionary developments of today’s remote sensing radar/optical/electronic warfare (EW) technologies, right from the telemobiloscope. This review article addresses the fundamentals of radar sensing techniques, top-level radar classifications, and revolutionary developments of antenna technologies for remote sensing applications. The various techniques available for radar waveform design, a characteristics analysis of it using ambiguity functions (AFs), pulse compression/stretch processing, a time-frequency (T-F) analysis, and so on are reviewed. The significant transformations that have happened in radar system engineering since vacuum tube microwave devices are reported. Contemporary societal applications of radar systems, tracking and guidance radar systems, advanced EW systems, photonics EW systems, and photonics signal processing are reviewed and reported. State-of-the-art optical technologies available for today’s remote sensing applications are discussed. In addition to these reviews, a comprehensive comparative study is performed in terms of available remote sensing systems/technologies, their typical operating frequency ranges, potential applications, types of waveforms, and so forth, and the quantitative results are reported.","2168-6831","","10.1109/MGRS.2023.3329928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10324327","","Radar cross-sections;Radar antennas;Radar remote sensing;Spaceborne radar;Remote sensing;Doppler radar;Radar tracking;Time-frequency analysis;Systems engineering and theory;Performance evaluation;Optical signal processing;Electronic warfare","","4","","180","IEEE","21 Nov 2023","","","IEEE","IEEE Magazines"
"Enhancing Anti-Money Laundering Frameworks: An Application of Graph Neural Networks in Cryptocurrency Transaction Classification","S. Ferretti; G. D’Angelo; V. Ghini","Department of Computer Science and Engineering (DISI), University of Bologna, Bologna, Italy; Department of Computer Science and Engineering (DISI), University of Bologna, Bologna, Italy; Department of Computer Science and Engineering (DISI), University of Bologna, Bologna, Italy",IEEE Access,"25 Mar 2025","2025","13","","50201","50215","Cryptocurrency money laundering is a pressing issue, as it not only facilitates and hides criminal activities but also disrupts markets and the overall financial system. To respond this challenge, researchers are trying to develop robust Anti-Money Laundering (AML) frameworks. These efforts play a crucial role in promoting societal welfare by mitigating the impact of criminal activities. This paper explores the application of Graph Neural Networks (GNNs) for classifying Bitcoin transactions. The research specifically employs Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), the Chebyshev spatial convolutional neural networks, and GraphSAGE networks. Based on the dataset analysis, we experiment with different subsets of features. Our findings suggest that the use of Graph Neural Network convolutions, combined with a final linear layer and skip connections, allow for an improvement in the state-of-the-art results, especially when Chebyshev and GATv2 convolutions are used.","2169-3536","","10.1109/ACCESS.2025.3552240","European Union—NextGenerationEU within the framework of Piano Nazionale di Ripresa e Resilienza (PNRR) Mission 4—Component 2—Investment 1.1 under Italian Ministry of University and Research (MUR) Programme “Progetti di Ricerca d’Interesse Nazionale (PRIN) 2022” through SmartShires—CUP(grant numbers:H53D23003570006,2022N2NH42); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10930500","Anti-money laundering;deep learning;graph neural networks;classification","Cryptocurrency;Fraud;Automated machine learning;Chebyshev approximation;Graph convolutional networks;Random forests;Online banking;Machine learning algorithms;Laplace equations;Heuristic algorithms","","","","42","CCBYNCND","17 Mar 2025","","","IEEE","IEEE Journals"
"The Art of Data Visualization","E. Schmitt",NA,Big Data: An Art of Decision Making,"","2020","","","187","217","Data visualization plays a critical role in both understanding data, i.e. in the analyst's interpretation of the results of the computations made on the data, and in conveying that understanding. Historically, the emergence of visualization software has greatly facilitated and spread the use of data visualization. In the fields of human–machine interfaces and ergonomics, which are concerned with data visualization and are derived in particular from cognitive sciences, visualization can be evaluated on a measurable readability criterion. Revealing the epistemic value of data entails both giving meaning to certain signs and discarding others. In the digital context, design is first associated with what is visible on the screen: the graphical user interface. It is thus assimilated to a work of graphic creation (or modification), which consists of making aesthetic an already existing interface created by the developers.","","9781119777007","10.1002/9781119777014.ch6","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9821871.pdf&bkn=9820910&pdfType=chapter","","Data visualization;Graphics;Art;Visualization;Image color analysis;Software;Big Data","","","","","","12 Jul 2022","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"The Computer Self-Efficacy Construct: A History of Application in Information Systems Research","G. M. Marakas; M. Aguirre-Urreta; A. Shoja; E. Kim; S. Wang",NA; NA; NA; NA; NA,The Computer Self-Efficacy Construct: A History of Application in Information Systems Research,"","2022","","","","","Computer self-efficacy (CSE) has captured the interest of researchers from widely diverse knowledge domains for over four decades. During that time, the realm of computer adoption and use has evolved and flourished. Along with this evolution, our understanding of CSE, its utility in behavior modeling and training development, and its relationship to a diverse array of antecedents and precedents has continued to evolve. This monograph provides a comprehensive history of the CSE construct as it has been developed and applied within the field of information systems (IS), and within the broader academic communities that benefit from reference to IS research contributions. The authors present the breadth and depth of the CSE construct and offer a framework of extant knowledge and implications for future research within this knowledge domain. The principal contribution of this work is the assemblage of the bulk of the authors’ understanding and knowledge regarding the CSE construct and its associated streams of research into a single compendium. It is intended to facilitate future researchers to access the current thinking regarding the CSE construct and direct their efforts to the continued advancement of our understanding of computer self-efficacy.","","9781638280811","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9928436.pdf&bkn=9928435&pdfType=book","","","","","","","","25 Oct 2022","","","now","Now Foundations and Trends Books"
"INGR Roadmap Security and Privacy Chapter","A. Dutta; E. Hammad; M. Enright; F. Behmann; A. Chorti; A. Cheema; K. Kadio; J. Urbina-Pineda; K. Alam; A. Limam; F. Chu; J. Lester; J. -G. Park; J. Bio-Ukeme; S. S. Pawar; R. Layton; P. Ramchandran; K. Okonkwo; L. Ong; M. Emmelmann; O. Issa; R. Arul; S. Malik; S. Krishnan; S. Sugumar; T. Lala; B. Chng; B. Rawal; T. Sajid; H. Lai; G. Carvalho; M. Borst; B. Kloza","John's Hopkins University / Applied Physics Lab, Security Working Group Co-Chair; Texas A&M University, Security Working Group Co-Chair; Quantum Dimension, Inc.; IEEE ComSoc North America Regional Board, TelNet Management Consulting, Inc.; ENSEA, CNRS; Shared Services, Canada; Shared Services, Canada; IEEE HKN Member and CyberIIoT CEO; Rogers Communications (Formerly); Higher Institute of Engineering and Technology (ESPRIT); University of California, Los Angeles; Our Lady of Fatima University Valenzuela, Philippines; Seoul National University of Science and Technology; Carleton University; Usha Mittal University of Technology; Aalborg University; Intel; Chevron; Ciena; Fraunhofer FOKUS; Department of National Defence, Canada; Amrita Vishwa Vidyapeetham; T-Mobile; National Library of Medicine; Intel Corporation; ZecureZ Consulting Company; Bawman LLC; Gannon University; Comcast; Haobo Lai Associates; Brock University; IEEE Future Networks Initiative; IEEE Future Networks Initiative",2023 IEEE Future Networks World Forum (FNWF),"13 May 2024","2023","","","6","87","5G/6G technologies provide ubiquitous connectivity while also addressing the demands of both individual consumers and businesses [1]. 5G/6G technologies are expected to provide higher throughput, lower latency, higher density and mobility range without compromising reliability. By virtue of its flexibility and an agile development methodology that uses modular network functions, 5G/6G Networks support various use cases that are both scalable and cost-effective. 6G can support exciting new use cases, including massive IoT, smart transportation, e-Health, smart cities, tactile computing, kinesthetic communication, and holographic interactions. 5G and beyond technologies present a paradigm shift of wireless mobile communication[1].","2770-7679","979-8-3503-2458-7","10.1109/FNWF58287.2023.10520409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10520409","5G Cybersecurity;security;privacy;data protection;reliability;resilience;mMTC;URLLC;SDN / NFV;cyber risk assessment and management;threat scenarios;cyber-attacks;security controls;mitigation;defense","Wireless communication;Privacy;5G mobile communication;Smart cities;Throughput;Smart transportation;Security;Risk management;Reliability;Computer crime","","","","36","IEEE","13 May 2024","","","IEEE","IEEE Conferences"
"Research Fiction and Thought Experiments in Design","M. Blythe; E. Encinas",NA; NA,Research Fiction and Thought Experiments in Design,"","2018","","","","","Any design process involves an imaginative act, a picturing of the world as other than it is. Fiction has long played a part in design research in the form of scenarios, personas, sketches, paper-based prototypes, simulations, prototypes, and speculative design. The term “design fiction” has been recently adopted to describe more elaborate and detailed representations of products and services that do not exist yet. Design fiction is an emerging practice and there are several competing definitions and forms. Research Fiction and Thought Experiments in Design traces design fiction from the Italian radical design of the 1960s through British Art Schools in the late 1990s to contemporary adaptations of the practice by companies like Google, Microsoft and Facebook. Design fiction is now produced regularly by individuals launching Kickstarter campaigns, corporations selling visions of future products and governments imagining new digital services. But there is little agreement about the status of such fictions: what constitutes a good fiction? How does fiction relate to research? In what sense does fiction contribute to existing knowledge? Although fiction can sometimes result in accurate prediction, this is not its main value. It is rather the creation of ambiguous artefacts that help us think carefully about emerging technologies and their potential impact. Fiction may seem to be the antithesis of empirical enquiry but it is often employed in the form of “thought experiments” in Physics, Mathematics, Ethics and Philosophy. Research Fiction and Thought Experiments in Design argues that design fiction can also be considered as a form of thought experiment. Excerpts from a fictional Wikipedia article about Valdis Ozols, a Latvian historian and author writing design fiction in the 1940s, precede each section as think pieces about the nature and value of fiction. The text is illustrated with pages from a fictional design workbook written in an invented language.","","9781680834192","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8384203.pdf&bkn=8384202&pdfType=book","","","","","","","","14 Jun 2018","","","now","Now Foundations and Trends Books"
"Creators of Intelligence: Industry secrets from AI leaders that you can easily apply to advance your data science career","D. A. Antic; J. K. Thompson",NA; NA,Creators of Intelligence: Industry secrets from AI leaders that you can easily apply to advance your data science career,"","2023","","","","","Get your hands on the secret recipe for a rewarding career in data science from 18 AI leaders Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesGain access to insights and expertise from data science leaders shared in one-on-one interviewsGet pragmatic advice on how to become a successful data scientist and data science leaderReceive guidance to overcome common pitfalls and challenges and ensure your projects’ successBook DescriptionA Gartner prediction in 2018 led to numerous articles stating that ""85% of AI and machine learning projects fail to deliver.” Although it's unclear whether a mass extinction event occurred for AI implementations at the end of 2022, the question remains: how can I ensure that my project delivers value and doesn't become a statistic? The demand for data scientists has only grown since 2015, when they were dubbed the new “rock stars” of business. But how can you become a data science rock star? As a new senior data leader, how can you build and manage a productive team? And what is the path to becoming a chief data officer? Creators of Intelligence is a collection of in-depth, one-on-one interviews where Dr. Alex Antic, a recognized data science leader, explores the answers to these questions and more with some of the world's leading data science leaders and CDOs. Interviews with: Cortnie Abercrombie, Edward Santow, Kshira Saagar, Charles Martin, Petar Veličković, Kathleen Maley, Kirk Borne, Nikolaj Van Omme, Jason Tamara Widjaja, Jon Whittle, Althea Davis, Igor Halperin, Christina Stathopoulos, Angshuman Ghosh, Maria Milosavljevic, Dr. Meri Rosich, Dat Tran, and Stephane Doyen.What you will learnFind out where to start with AI ethics and how to evolve from frameworks to practiceDiscover tips on building and managing a data science teamReceive advice for organizations seeking to build or mature a data science capabilityStop beating your head against a brick wall – pick the environment that’ll support your successRead stories from successful data leaders as they reflect on the successes and failures in data strategy developmentUnderstand how business areas can best work with data science teams to drive business valueWho this book is forThis book is for a wide range of audience, from people working in the data science industry through to data science leaders and chief data officers. This book will also cater to senior business leaders interested in learning how data and analytics are used to support decision-making in different domains and sectors. Students contemplating a career in artificial intelligence (AI) and the broader data sector will also find this book useful, along with anyone developing and delivering university-level education, including undergraduate, postgraduate, and executive programs.","","9781804619315","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251358.pdf&bkn=10251357&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Current and Evolving Applications to Natural Language Processing","D. Minoli; B. Occhiogrosso","Stevens Institute of Technology, AT&T, Red Bank, NJ; NA",AI Applications to Communications and Information Technologies: The Role of Ultra Deep Neural Networks,"","2024","","","65","116","Speech synthesis, automatic speech recognition, and basic text‐to‐speech have been around for decades. Now, at the intersection of computer science, artificial intelligence, and computational linguistics, one finds advanced fields that include natural language processing (NLP), natural language understanding (NLU), and natural language generation (NLG), that combine older capabilities with newer content creation techniques. This chapter focuses on NLP technologies. Language processing has existed for some time and has utilized a number of techniques. In recent years, however, these disciplines, and NLG in particular, have seen significant progress because of the application of machine learning and, more specifically, deep learning techniques. NLG is the process or mechanism of generating narratives, descriptions, dialogs, story‐telling in a natural language starting from/with structured data. Neural networks, in particular recurrent neural networks, now comprise the leading approaches to NLP/NLU tasks.","","9781394190027","10.1002/9781394190034.ch2","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10320221.pdf&bkn=10320140&pdfType=chapter","","Natural language processing;Artificial intelligence;Computational modeling;Hidden Markov models;Training;Task analysis;Semantics","","","","","","16 Nov 2023","","","IEEE","Wiley-IEEE Press eBook Chapters"
"Mastering Transformers: Build state-of-the-art models from scratch with advanced natural language processing techniques","S. Yıldırım; M. A. -. Chenaghlu",NA; NA,Mastering Transformers: Build state-of-the-art models from scratch with advanced natural language processing techniques,"","2021","","","","","Take a problem-solving approach to learning all about transformers and get up and running in no time by implementing methodologies that will build the future of NLPKey FeaturesExplore quick prototyping with up-to-date Python libraries to create effective solutions to industrial problemsSolve advanced NLP problems such as named-entity recognition, information extraction, language generation, and conversational AIMonitor your model's performance with the help of BertViz, exBERT, and TensorBoardBook DescriptionTransformer-based language models have dominated natural language processing (NLP) studies and have now become a new paradigm. With this book, you'll learn how to build various transformer-based NLP applications using the Python Transformers library. The book gives you an introduction to Transformers by showing you how to write your first hello-world program. You'll then learn how a tokenizer works and how to train your own tokenizer. As you advance, you'll explore the architecture of autoencoding models, such as BERT, and autoregressive models, such as GPT. You'll see how to train and fine-tune models for a variety of natural language understanding (NLU) and natural language generation (NLG) problems, including text classification, token classification, and text representation. This book also helps you to learn efficient models for challenging problems, such as long-context NLP tasks with limited computational capacity. You'll also work with multilingual and cross-lingual problems, optimize models by monitoring their performance, and discover how to deconstruct these models for interpretability and explainability. Finally, you'll be able to deploy your transformer models in a production environment. By the end of this NLP book, you'll have learned how to use Transformers to solve advanced NLP problems using advanced models.What you will learnExplore state-of-the-art NLP solutions with the Transformers libraryTrain a language model in any language with any transformer architectureFine-tune a pre-trained language model to perform several downstream tasksSelect the right framework for the training, evaluation, and production of an end-to-end solutionGet hands-on experience in using TensorBoard and Weights & BiasesVisualize the internal representation of transformer models for interpretabilityWho this book is forThis book is for deep learning researchers, hands-on NLP practitioners, as well as ML/NLP educators and students who want to start their journey with Transformers. Beginner-level machine learning knowledge and a good command of Python will help you get the best out of this book.","","9781801078894","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163525.pdf&bkn=10163524&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"2 Coming to Terms with Creative Machines","O. Bown",University of New South Wales,Beyond the Creative Species: Making Machines That Make Art and Music,"","2021","","","43","70","While artists like Cohen, Lewis, and Cope were reflexively tinkering with their machines, a great body of research and commentary has risen up on the nature of human creativity. This spans myriad academic areas, from developmental and evolutionary psychology to organization science, management, and philosophy. As a subject of study, creativity became a topic of serious focus in the nineteenth century and one of great social importance in the twentieth. In the twenty-first century, it is a veritable juggernaut of debate and research, reshaping our cities, governments, and corporations.","","9780262361750","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9394896.pdf&bkn=9394486&pdfType=chapter","","","","","","","","5 Apr 2021","","","MIT Press","MIT Press eBook Chapters"
"User Simulation for Evaluating Information Access Systems","K. Balog; C. Zhai",NA; NA,User Simulation for Evaluating Information Access Systems,"","2024","","","","","Information access systems, such as search engines, recommender systems, and conversational assistants, have become integral to our daily lives as they help us satisfy our information needs. However, evaluating the effectiveness of these systems presents a complex scientific challenge, rooted in the difficulty of assessing a system’s overall effectiveness in assisting users to complete tasks through interactive support, and by the substantial variation in user behaviour and preferences. This monograph focuses on providing a thorough understanding of user simulation techniques designed specifically for evaluation purposes. The authors begin with a background of information access system evaluation and explore the diverse applications of user simulation. Subsequently, they systematically review the major research progress in user simulation, covering both general frameworks for designing user simulators, utilizing user simulation for evaluation, and specific models and algorithms for simulating user interactions with search engines, recommender systems, and conversational assistants. Realizing that user simulation is an interdisciplinary research topic, whenever possible, the authors establish connections with related fields, including machine learning, dialogue systems, user modelling, and economics. This monograph provides a systematic review of user simulation techniques aimed at giving designers methods for successfully evaluating and improving modern information access systems.","","9781638283799","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10558724.pdf&bkn=10558723&pdfType=book","","","","","","","","14 Jun 2024","","","now","Now Foundations and Trends Books"
"DuckDB in Action","M. Hunger; M. Needham; M. Simons",Manning Publications; Manning Publications; Manning Publications,DuckDB in Action,"","2024","","","","","Dive into DuckDB and start processing gigabytes of data with ease—all with no data warehouse. DuckDB is a cutting-edge SQL database that makes it incredibly easy to analyze big data sets right from your laptop. In DuckDB in Action you’ll learn everything you need to know to get the most out of this awesome tool, keep your data secure on prem, and save you hundreds on your cloud bill. From data ingestion to advanced data pipelines, you’ll learn everything you need to get the most out of DuckDB—all through hands-on examples. Open up DuckDB in Action and learn how to:  Read and process data from CSV, JSON and Parquet sources both locally and remote Write analytical SQL queries, including aggregations, common table expressions, window functions, special types of joins, and pivot tables Use DuckDB from Python, both with SQL and its ""Relational""-API, interacting with databases but also data frames Prepare, ingest and query large datasets Build cloud data pipelines Extend DuckDB with custom functionality  Pragmatic and comprehensive, DuckDB in Action introduces the DuckDB database and shows you how to use it to solve common data workflow problems. You won’t need to read through pages of documentation—you’ll learn as you work. Get to grips with DuckDB's unique SQL dialect, learning to seamlessly load, prepare, and analyze data using SQL queries. Extend DuckDB with both Python and built-in tools such as MotherDuck, and gain practical insights into building robust and automated data pipelines.","","9781633437258","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10745361.pdf&bkn=10745360&pdfType=book","SQL;CLI;Python;analyze;process;aggregation;prepare;CSV;JSON;ingest;fast;query;cloud;pipelines;relational;custom;frames;Postgres;SQLite;extend;structured;data;Parquet","","","","","","","6 Nov 2024","","","Manning","Manning eBooks"
"Zero to Birth: How the Human Brain Is Built","W. A. Harris",NA,Zero to Birth: How the Human Brain Is Built,"","2022","","","","","A revelatory tale of how the human brain develops, from conception to birth and beyondBy the time a baby is born, its brain is equipped with billions of intricately crafted neurons wired together through trillions of interconnections to form a compact and breathtakingly efficient supercomputer. Zero to Birth takes you on an extraordinary journey to the very edge of creation, from the moment of an egg’s fertilization through each step of a human brain’s development in the womb—and even a little beyond.As pioneering experimental neurobiologist W. A. Harris guides you through the process of how the brain is built, he takes up the biggest questions that scientists have asked about the developing brain, describing many of the thrilling discoveries that were foundational to our current understanding. He weaves in a remarkable evolutionary story that begins billions of years ago in the Proterozoic eon, when multicellular animals first emerged from single-cell organisms, and reveals how the growth of a fetal brain over nine months reflects the brain’s evolution through the ages. Our brains have much in common with those of other animals, and Harris offers an illuminating look at how comparative animal studies have been crucial to understanding what makes a human brain human.An unforgettable chronicle of one of nature’s greatest achievements, Zero to Birth describes how the brain’s incredible feat of orchestrated growth ensures that every brain is unique, and how breakthroughs at the frontiers of science are helping us to decode many traits that only reveal themselves later in life.","","9780691237077","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9782478.pdf&bkn=9782477&pdfType=book","Neuron;Embryo;Growth cone;Synapsis;Motor neuron;Neural tube;Axon;Gene;Neural stem cell;Stem cell;Neural development;Neural crest;Protein;Spinal cord;Hox gene;Morphogen;Gastrulation;FOXP2;Neuroblastoma;Apoptosis;Neuroglia;Critical period;Neural tube defect;Progenitor cell;Ganglion cell;Human brain;Hindbrain;Immortalised cell line;Dendrite;Hebbian theory;Protocadherin;Marian Diamond;Reelin;Blastula;Neural plate;Neuroblast;Reticular theory;Myocyte;Sonic hedgehog;Spindle apparatus;Midbrain;Renshaw cell;Chemical synapse;Broca's area;Hirschsprung's disease;Eric Knudsen;Cerebral atrophy;Petri dish;Neuron doctrine;Lancelot Hogben;Endocrinology;White blood cell;Nematode;Cancer cell;Homeosis;Retinoic acid;Filopodia;Lateralization of brain function;Sydney Brenner;Zygote;Embryology;Cerebral cortex;Synaptic plasticity;Agrin;Cell type;Visual word form area;Vertebrate;Antibody;Hans Spemann;Degenerative disease;Model organism;Muscle;Neuroimaging;Action potential;Nervous tissue;Angiogenesis;Ectoderm;Thrombospondin;Proneural genes;Axon guidance;Down syndrome;Thomas Hunt Morgan;Charles Darwin;Spina bifida;Reeler;Forebrain;John Gurdon;Torsten Wiesel;Ross Granville Harrison;Evolution;Brain asymmetry;Neuroepithelial cell;Transformation (genetics);Astrocyte;Purkinje cell;Twin;Cyclopamine;Organoid;Roel Nusse;Sarcoma","","","","","","","26 May 2022","","","Princeton University Press","Princeton University Press eBooks"
"Integration of Federated Learning and AI-Generated Content: A Survey of Overview, Opportunities, Challenges, and Solutions","Y. Liu; J. Yin; W. Zhang; C. An; Y. Xia; H. Zhang","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; Department of Head and Neck Surgery, National Cancer Center/National Clinical Research Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, P. R. China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China",IEEE Communications Surveys & Tutorials,"","2024","PP","99","1","1","Artificial intelligence generated content (AIGC) relies on advanced AI algorithms supported by extensive datasets and substantial computing power to generate precise and pertinent content. Federated learning (FL) enables the aggregation of large volumes of data and computing resources from various sources, all while safeguarding privacy. As a result, FL has emerged as a critical enabler in the realm of AIGC. This survey paper provides a comprehensive overview of the integration of FL and AIGC, namely federated AIGC models. First, we introduce the fundamental concepts of FL and AIGC. Next, we summarize four typical types of federated AIGC models. Subsequently, We highlight the threats to centralized federated AIGC models regarding data confidentiality, integrity, and availability and discuss the unique advantages of blockchain technology in decentralized federated AIGC models in addressing these issues. Finally, we look at potential emerging application scenarios and explore open issues and future directions for federated AIGC models.","1553-877X","","10.1109/COMST.2024.3523350","the Major Key Project of PCL(grant numbers:PCL2023A07); National Natural Science Foundation of China(grant numbers:62201029); CAMS Innovation Fund for Medical Science(grant numbers:2023-I2M-C&T-B-072); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10816667","Federated learning;AI-generated content;Data Privacy;Blockchain","Data models;Training;Computational modeling;Artificial intelligence;Surveys;Data privacy;Blockchains;Bandwidth;Servers;Security","","","","","IEEE","27 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Data Science for Decision Makers: Enhance your leadership skills with data science and AI expertise","J. Howells",NA,Data Science for Decision Makers: Enhance your leadership skills with data science and AI expertise,"","2024","","","","","Bridge the gap between business and data science by learning how to interpret machine learning and AI models, manage data teams, and achieve impactful results Key FeaturesMaster the concepts of statistics and ML to interpret models and guide decisionsIdentify valuable AI use cases and manage data science projects from start to finishEmpower top data science teams to solve complex problems and build AI productsPurchase of the print Kindle book includes a free PDF eBookBook DescriptionAs data science and artificial intelligence (AI) become prevalent across industries, executives without formal education in statistics and machine learning, as well as data scientists moving into leadership roles, must learn how to make informed decisions about complex models and manage data teams. This book will elevate your leadership skills by guiding you through the core concepts of data science and AI. This comprehensive guide is designed to bridge the gap between business needs and technical solutions, empowering you to make informed decisions and drive measurable value within your organization. Through practical examples and clear explanations, you'll learn how to collect and analyze structured and unstructured data, build a strong foundation in statistics and machine learning, and evaluate models confidently. By recognizing common pitfalls and valuable use cases, you'll plan data science projects effectively, from the ground up to completion. Beyond technical aspects, this book provides tools to recruit top talent, manage high-performing teams, and stay up to date with industry advancements. By the end of this book, you’ll be able to characterize the data within your organization and frame business problems as data science problems.What you will learnDiscover how to interpret common statistical quantities and make data-driven decisionsExplore ML concepts as well as techniques in supervised, unsupervised, and reinforcement learningFind out how to evaluate statistical and machine learning modelsUnderstand the data science lifecycle, from development to monitoring of models in productionKnow when to use ML, statistical modeling, or traditional BI methodsManage data teams and data science projects effectivelyWho this book is forThis book is designed for executives who want to understand and apply data science methods to enhance decision-making. It is also for individuals who work with or manage data scientists and machine learning engineers, such as chief data officers (CDOs), data science managers, and technical project managers. ","","9781837638345","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769356.pdf&bkn=10769355&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Wireless Communication, Sensing, and REM: A Security Perspective","H. M. Furqan; M. S. J. Solaija; H. Türkmen; H. Arslan","Department of Electrical and Electronics Engineering, Istanbul Medipol University, Istanbul, Turkey; Department of Electrical and Electronics Engineering, Istanbul Medipol University, Istanbul, Turkey; Department of Electrical and Electronics Engineering, Istanbul Medipol University, Istanbul, Turkey; Department of Electrical Engineering, University of South Florida, Tampa, FL, USA",IEEE Open Journal of the Communications Society,"19 Feb 2021","2021","2","","287","321","The diverse requirements of next-generation communication systems necessitate awareness, flexibility, and intelligence as essential building blocks of future wireless networks. The awareness can be obtained from the radio signals in the environment using wireless sensing and radio environment mapping (REM) methods. This is, however, accompanied by threats such as eavesdropping, manipulation, and disruption posed by malicious attackers. To this end, this work analyzes the wireless sensing and radio environment awareness mechanisms, highlighting their vulnerabilities and provides solutions for mitigating them. As an example, the different threats to REM and its consequences in a vehicular communication scenario are described. Furthermore, the use of REM for securing communications is discussed and future directions regarding sensing/REM security are highlighted.","2644-125X","","10.1109/OJCOMS.2021.3054066","HISAR Lab at TUBÍTAK BILGEM, Gebze, Turkey; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9336039","5G;6G;cryptography;joint radar and communication (JRC);physical layer security;radio environment mapping (REM);REM security;sensing security;vehicle-to-everything (V2X) communication;wireless sensing;WLAN sensing","Sensors;Communication system security;Wireless sensor networks;Robot sensing systems;Wireless networks;Jamming;Hash functions","","27","","175","CCBY","26 Jan 2021","","","IEEE","IEEE Journals"
"A Survey on Intelligent Gesture Recognition Techniques","J. J. Ojeda-Castelo; M. d. L. M. Capobianco-Uriarte; J. A. Piedra-Fernandez; R. Ayala","Department of Informatics, University of Almeria, Almeria, Spain; Department of Economics and Business, University of Almeria, Almeria, Spain; Department of Informatics, University of Almeria, Almeria, Spain; Department of Informatics, University of Almeria, Almeria, Spain",IEEE Access,"24 Aug 2022","2022","10","","87135","87156","Gesture recognition is an ideal means of interaction because it allows users not to have to make contact with any surface, which is a safe and hygienic means, especially in the pandemic situation that is occurring worldwide. However, gesture recognition is not a new discipline and it has been researched for many years but this type of interaction has not succeeded in replacing the keyboard and mouse. It is very useful to know about the advances that are being made with artificial intelligence in gesture recognition to be able to perform a more robust and reliable gesture recognition with a low response time. As it is, deep learning is being integrated into various areas to increase improvement in performance and one such area is artificial intelligence. In this way, there is the possibility that in the future the recognition of gestures will be a viable option as a means of daily interaction for the user and the main objective of this paper is to contribute to that process. For this reason, this study has analyzed 571 papers related to gesture recognition and artificial intelligence. This analysis has extracted relevant information related to scientific production, such as the most productive authors and journals or the most pertinent articles on the subject. Furthermore, we have developed our own model, which shows the relationship between the types of gesture recognition and the artificial intelligence techniques that have been applied for this task.","2169-3536","","10.1109/ACCESS.2022.3199358","EU ERDF; Andalusian Government (Spain)(grant numbers:P20_00809); https://acg.ual.es/projects/cosmart/; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858153","Artificial intelligence;deep learning;gesture recognition;machine learning","Gesture recognition;Artificial intelligence;Feature extraction;Face recognition;Three-dimensional displays;Solid modeling;Hidden Markov models","","13","","136","CCBY","17 Aug 2022","","","IEEE","IEEE Journals"
"Visual Information Literacy: Definition, Construct Modeling and Assessment","A. Locoro; W. P. Fisher; L. Mari","Università Carlo Cattaneo–LIUC, Castellanza (VA), Italy; BEAR Center, Graduate School of Education, University of California, Berkeley, Berkeley, CA, USA; Università Carlo Cattaneo–LIUC, Castellanza (VA), Italy",IEEE Access,"18 May 2021","2021","9","","71053","71071","A major problem in education and visual information design is that, while tools to measure people's reading and writing ability with texts and numbers are ripe, the ability to properly process information from data graphics - an ability that can be called Visual Information Literacy - is still off the radar, and even less interest is apparently devoted to its evaluation. The purpose of this research is that of presenting an exploration of methods and tools towards the measurement of data graphics effectiveness and efficiency, and of proposing a definition of `Visual Information Literacy', together with the design of a model characterizing it as a developmental skills progression that covers the cognitive abilities activated when dealing with data graphics. A final goal of this paper is to report a first round of results assessing the validity of the model designed, by bringing statistical evidence that data graphics comprehension depends on the matching of users' ability and data graphics difficulty. The contribution of this paper is twofold: comparing the current research on Visual Information Literacy and advancing it by designing a model for its characterization to allow the design of a Visual Information Literacy measurement scale standard.","2169-3536","","10.1109/ACCESS.2021.3078429","Università Carlo Cattaneo - LIUC, funding the 2019 research project with title “Walking the last mile from data to decision-making: the role of data validation and data visualization.”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9427272","Computers and information processing;information and communication technology;information management;visual communication;visualization","Visualization;Data visualization;Graphics;Tools;Standards;Data models;Media","","8","","61","CCBY","10 May 2021","","","IEEE","IEEE Journals"
"A Survey on XAI for 5G and Beyond Security: Technical Aspects, Challenges and Research Directions","T. Senevirathna; V. H. La; S. Marchal; B. Siniarski; M. Liyanage; S. Wang","School of Computer Science, University College Dublin, Ireland; Montimage, France; VTT Technical Research Centre of Finland, Finkand; School of Computer Science, University College Dublin, Ireland; School of Computer Science, University College Dublin, Ireland; School of Computer Science, University College Dublin, Ireland",IEEE Communications Surveys & Tutorials,"","2024","PP","99","1","1","With the advent of 5G commercialization, the need for more reliable, faster, and intelligent telecommunication systems is envisaged for the next generation beyond 5G (B5G) radio access technologies. Artificial Intelligence (AI) and Machine Learning (ML) are immensely popular in service layer applications and have been proposed as essential enablers in many aspects of 5G and beyond networks, from IoT devices and edge computing to cloud-based infrastructures. However, existing 5G ML-based security surveys tend to emphasize AI/ML model performance and accuracy more than the models’ accountability and trustworthiness. In contrast, this paper explores the potential of Explainable AI (XAI) methods, which would allow stakeholders in 5G and beyond to inspect intelligent black-box systems used to secure next-generation networks. The goal of using XAI in the security domain of 5G and beyond is to allow the decision-making processes of ML-based security systems to be transparent and comprehensible to 5G and beyond stakeholders, making the systems accountable for automated actions. In every facet of the forthcoming B5G era, including B5G technologies such as ORAN, zero-touch network management, and end-to-end slicing, this survey emphasizes the role of XAI in them that the general users would ultimately enjoy. Furthermore, we presented the lessons from recent efforts and future research directions on top of the currently conducted projects involving XAI.","1553-877X","","10.1109/COMST.2024.3437248","Science Foundation Ireland under CONNECT phase 2(grant numbers:13/RC/2077_P2); European Union SPATIAL Project(grant numbers:101021808); Academy of Finland in 6Genesis(grant numbers:318927); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10620685","B5G;5G;XAI;AI security;cyber-security;6G mobile communication;Accountability;Trustworthy AI;Explainable security","Artificial intelligence;5G mobile communication;Explainable AI;Security;Surveys;6G mobile communication;Wireless sensor networks","","6","","","IEEE","2 Aug 2024","","","IEEE","IEEE Early Access Articles"
"Human-Machine Duality: What’s Next in Cognitive Aspects of Artificial Intelligence?","A. N. Raikov; M. Pirani","Trapeznikov Institute of Control Sciences, Russian Academy of Sciences, Moscow, Russia; Department of Information Engineering, Marche Polytechnic University, Ancona, Italy",IEEE Access,"1 Jun 2022","2022","10","","56296","56315","The goal of the paper is to find means for the unification of human-machine duality in collective behavior of people and machines, by conciliating approaches that proceed in opposite directions. The first approach proceeds top-down from non-formalizable, cognitive, uncaused, and chaotic human consciousness towards purposeful and sustainable human-machine interaction. The second approach proceeds bottom-up from intelligent machines towards high-end computing and is based on formalizable models leveraging multi-agent architectures. The resulting work reviews the extent, the merging points, and the potential of hybrid artificial intelligence frameworks that accept the idea of strong artificial intelligence. These models concern the pairing of connectionist and cognitive architectures, conscious and unconscious actions, symbolic and conceptual realizations, emergent and brain-based computing, and automata and subjects. The special authors’ convergent methodology is considered, which is based on the integration of inverse problem-solving on topological spaces, cognitive modelling, quantum field theory, category theory methods, and holonic approaches. It aims to a more purposeful and sustainable human-machine interaction in form of algorithms or requirements, rules of strategic conversations or network brainstorming, and cognitive semantics. The paper addresses the reduction of the impact of AI development on ethics violation. The findings delivered are used to provide perspectives on the shaping of societal, ethical, and normative aspects in the symbiosis between humans and machines. Implementations in real practice are represented.","2169-3536","","10.1109/ACCESS.2022.3177657","Russian Science Foundation “Socio-Humanitarian Foundations of Criteria for Evaluating Innovations Using Digital Technology and Artificial Intelligence”(grant numbers:21-18-00184); University Scientific Research (RSA) by Marche Polytechnic University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780404","Cognitive semantics;category theory;human-machine duality;hybrid artificial intelligence;holonic systems;stability in dynamical systems","Artificial intelligence;Ethics;Man-machine systems;Cognition;Symbiosis;Semantics;Correlation","","6","","121","CCBY","23 May 2022","","","IEEE","IEEE Journals"
"An In-Depth Examination of Artificial Intelligence-Enhanced Cybersecurity in Robotics, Autonomous Systems, and Critical Infrastructures","F. Santoso; A. Finn","Defence and Systems Institute, UniSA STEM (Science, Technology, Engineering, and Mathematics), The University of South Australia, Mawson Lakes, SA, Australia; Defence and Systems Institute, UniSA STEM (Science, Technology, Engineering, and Mathematics), The University of South Australia, Mawson Lakes, SA, Australia",IEEE Transactions on Services Computing,"11 Jun 2024","2024","17","3","1293","1310","Recent developments in cutting-edge robotics constantly face increased cyber threats, not only in terms of quantity and frequency of attacks, but also when it comes to quality and severity of the intrusions. This paper provides a systematic overview and critical assessment of state-of-the-art scientific developments in the security aspects of robotics, autonomous systems, and critical infrastructures. Our review highlights open research questions addressing significant research gaps and/or new conceptual frameworks given recent advancements in artificial intelligence (AI) and machine learning. Thus, the contributions of this paper can be summarized as follows. We first compare and contrast the benefits of multiple cutting-edge AI-based learning algorithms (e.g., fuzzy logic and neural networks) relative to traditional model-based systems (e.g., distributed control and filtering). Subsequently, we point out some specific benefits of AI algorithms to quickly learn and adapt the dynamics of non-linear systems in the absence of complex mathematical models. We also present some potential future research directions (open challenges) in the field. Lastly, this review also delivers an open message to encourage collaborations among experts from multiple disciplines. The implementation of multiple AI algorithms to tackle current security issues in robotics will transform and create novel hybrid knowledge for intelligent cybersecurity at the application level.","1939-1374","","10.1109/TSC.2023.3331083","The United States Army Futures Command; The United States Army Combat Capabilities Development Command; International Technology Center Indo-Pacific(grant numbers:FA5209-18-P-0146); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10313082","Cybersecurity;artificial intelligence;machine learning;robotics;autonomous systems;and critical infrastructures","Robots;Computer security;Robot sensing systems;Security;Autonomous systems;Operating systems;Middleware","","3","","136","CCBYNCND","8 Nov 2023","","","IEEE","IEEE Journals"
"Deep Learning for Cybersecurity: A Review","Z. Chen","Department of Information Technology, Shanghai Ocean University, Shanghai, China",2020 International Conference on Computing and Data Science (CDS),"9 Dec 2020","2020","","","7","18","With the development of Internet technology, the scale of Internet has increased considerably, which brings with a large number of cyber-attacks. Traditional protection techniques are confronted with complex, advanced and ongoing evolvement adversarial situations, which have to be more adaptive and responsive in order to handle future security and privacy problems. Deep Learning (DL), as one of the most currently remarkable machine learning techniques, has a great potential in cybersecurity. In this paper, author is committed to analyze current cyber-attacks, to review recent state-of-the-art deep learning algorithms and Figure out pros and cons of them, and to discuss the feasibility of deep learning technology applied to cybersecurity to defend malware attacks, DDoS attacks and spoofing attacks. This article also analyzes some vulnerabilities of deep learning algorithms and potential security problems which might come out when DL is applied to cybersecurity. Finally, paper discusses some challenges a DL-based defense mechanism has to overcome, and status and future directions of DL-based defense technology.","","978-1-7281-7106-7","10.1109/CDS49703.2020.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9275959","cybersecurity;deep learning","Hafnium;Three-dimensional displays;Data science;5G mobile communication","","3","","76","IEEE","9 Dec 2020","","","IEEE","IEEE Conferences"
"A Taxonomy-Based Survey of EM-SCA and Implications for Multi-Robot Systems","Y. M. Ibrahim; S. K. Kermanshahi; K. Kasmarik; J. Hu","School of Systems and Computing, University of New South Wales, Canberra, ACT, Australia; School of Systems and Computing, University of New South Wales, Canberra, ACT, Australia; School of Systems and Computing, University of New South Wales, Canberra, ACT, Australia; School of Systems and Computing, University of New South Wales, Canberra, ACT, Australia",IEEE Open Journal of the Computer Society,"4 Oct 2024","2024","5","","511","529","Electromagnetic Side Channel Analysis (EM-SCA) is a major area of interest within the field of cybersecurity. EM-SCA makes use of the electromagnetic radiation that naturally leaks from any device that runs on electricity. Information about the observed device can be gained by gathering and analysing these electromagnetic traces. Numerous studies have demonstrated the applicability of this side channel in various environments for legal and illegal objectives. On the other hand, multi-robot systems, including swarm robotics, have received considerable attention in recent years due to their ability to conduct complex tasks using simple robots cooperating with each other. Although multi-robot and swarm robot systems are likely to be widely used in practical applications in the near future, security concerns in this context have not yet received enough attention. In particular, to the best of our knowledge, EM-SCA threats and benefits have never been thoroughly examined in this context before. In order to spotlight this matter, this work begins with a thorough introduction to EM-SCA and provides a taxonomic structure. Then, guided by this taxonomy, we present a range of EM-SCA scenarios that need to be considered in multi-robot applications.","2644-1268","","10.1109/OJCS.2024.3461808","ARC Discovery(grant numbers:DP190103660,DP200103207); ARC Linkage(grant numbers:LP180100663); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10681247","Electromagnetic side-channel analysis;electromagnetic side-channel attacks;multi-robot systems","Surveys;Robots;Security;Electromagnetics;Cryptography;Timing;Power demand","","","","128","CCBYNCND","16 Sep 2024","","","IEEE","IEEE Journals"
"Successful Proposal Strategies On-the-Go!","R. Frey",NA,Successful Proposal Strategies On-the-Go!,"","2023","","","","","An invaluable compendium of up-to-date, real-world vignettes, these detailed depictions are crafted from 35 years of thought leadership and hands-on engagement in U.S. Federal Government proposal development for support services contractors. There is meaningful context built around each vignette, allowing readers to see immediately how to apply the lessons learned. The insights presented are applicable to small businesses and mid-tier companies, as well as global Fortune 50 corporations. Written in a highly accessible style and accompanied by the author’s own photographs, On-the-Go! delivers concise, clear-sighted observations and helpful business-driven recommendations. Topics are drawn from actual challenges and situations that organizations and their staff professionals face every proposal. Across the spectrum of vignette topics, attention is paid to multiple dimensions in and around proposal development—human and organizational dynamics, linking business decisions to proposal strengths, building the proposal response, proposal writing, and more. In addition, coverage extends to academic and public-sector grant proposals, as well as international private-sector tenders. All the vignettes are easy to use and integrate into an organization's thinking and best practices because they are streamlined. Importantly, On-the-Go! brings practical value to executive leadership, business developers, capture managers, and proposal developers and managers, along with technical and programmatic subject matter experts and knowledge managers.","","9781685690144","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10352638.pdf&bkn=10352637&pdfType=book","","","","","","","","11 Dec 2023","","","Artech","Artech Books"
"Diffense: Defense Against Backdoor Attacks on Deep Neural Networks With Latent Diffusion","B. Hu; C. -H. Chang","School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore",IEEE Journal on Emerging and Selected Topics in Circuits and Systems,"13 Dec 2024","2024","14","4","729","742","As deep neural network (DNN) models are used in a wide variety of applications, their security has attracted considerable attention. Among the known security vulnerabilities, backdoor attacks have become the most notorious threat to users of pre-trained DNNs and machine learning services. Such attacks manipulate the training data or training process in such a way that the trained model produces a false output to an input that carries a specific trigger, but behaves normally otherwise. In this work, we propose Diffense, a method for detecting such malicious inputs based on the distribution of the latent feature maps to clean input samples of the possibly infected target DNN. By learning the feature map distribution using the diffusion model and sampling from the model under the guidance of the data to be inspected, backdoor attack data can be detected by its distance from the sampled result. Diffense does not require knowledge about the structure, weights, and training data of the target DNN model, nor does it need to be aware of the backdoor attack method. Diffense is non-intrusive. The accuracy of the target model to clean inputs will not be affected by Diffense and the inference service can be run uninterruptedly with Diffense. Extensive experiments were conducted on DNNs trained for MNIST, CIFRA-10, GSTRB, ImageNet-10, LSUN Object and LSUN Scene applications to show that the attack success rates of diverse backdoor attacks, including BadNets, IDBA, WaNet, ISSBA and HTBA, can be significantly suppressed by Diffense. The results generally exceed the performances of existing backdoor mitigation methods, including those that require model modifications or prerequisite knowledge of model weights or attack samples.","2156-3365","","10.1109/JETCAS.2024.3469377","Ministry of Education, Singapore, under its Academic Research Fund (AcRF) Tier 2(grant numbers:MOE-T2EP50220-0003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10697229","Deep neural networks;AI security;backdoor attack;diffusion model","Data models;Training;Artificial neural networks;Diffusion models;Artificial intelligence;Computational modeling;Training data;Security;Prevention and mitigation;Predictive models","","","","57","IEEE","27 Sep 2024","","","IEEE","IEEE Journals"
"Standard-essential Patent Prediction: Discussion of Patent Standardization Time","W. Liu; Z. Bai; K. Gan; Y. Cao","College of Computer Science, Inner Mongolia University, Hohhot, China; College of Computer Science, Inner Mongolia University, Hohhot, China; China National Institute of Standardization, Beijing, China; Institute of Scientific and Technical Information of China, Beijing, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Standard-essential patents are crucial tools in international trade competition, and standard-essential patent prediction is becoming competition point among countries. However, existing standard-essential patent prediction methods cannot adequately use various patent features, and cannot provide verifiable predicted results on whether a valid patent can be a standard-essential patent or not. To address these issues, this paper proposes a dynamic standard-essential patent prediction model. In our model, we partition different time slots for each patent to extract the dynamic features in each time slot of the patent, and the static features of the patent are extracted. Then we dynamically predict whether the patent can be a standard-essential patent in the next few years or not. The experimental results show that our model significantly outperforms the baseline models in the values of accuracy, precision, recall and F1.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890086","National Science Foundation; China Postdoctoral Science Foundation; Science and Technology Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890086","Standard-essential Patents;Standardization Time;Dynamic Model;Patent Data Analysis;Technology Mining","Patents;Analytical models;International trade;Standardization;Predictive models;Signal processing;Feature extraction;Data models;Data mining;Speech processing","","","","20","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"8 Cybersecurity and International Complexities","N. Choucri; D. D. Clark",Massachusetts Institute of Technology; MIT,International Relations in the Cyber Age: The Co-Evolution Dilemma,"","2019","","","209","245","This chapter presents several somewhat countervailing perspectives on threats to cybersecurity—modes, mechanisms, and manifestations. The first is a brief note on perennial predicaments in the cyber domain, some ""state-of-the-art"" views. The second is a look at well-known modes of cyber threat. The third focuses on seventeen cases of cyber conflicts for insights these might yield. We ask: what systematic inferences, if any, can be drawn from a set of cases reported in narrative form? Without foreshadowing the nature of the evidence or the reliability of prevailing assessments, we then turn to the fourth perspective and ask: what is the international community doing, if anything, in response to contentions the cyber domain? Invariably, in each of these lines of inquiry we are constrained by the nature of the evidence, the type of methods used, the diversity of damage—to note only a few of the most salient factors. Our purpose overall is to signal, ""reader, beware, uncertainties abound.""","","9780262349710","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8674412.pdf&bkn=8671655&pdfType=chapter","","","","","","","","28 Mar 2019","","","MIT Press","MIT Press eBook Chapters"
"Combining knowledge graphs and LLMs for hazardous chemical information management and reuse","M. Da Silveira; L. Deladiennee; K. Acem; O. Freudenthal","Luxembourg Institute of Science and Technology, Esch-Sur-Alzette, Luxembourg; Luxembourg Institute of Science and Technology, Esch-Sur-Alzette, Luxembourg; Luxembourg Institute of Science and Technology, Esch-Sur-Alzette, Luxembourg; Luxembourg Institute of Science and Technology, Esch-Sur-Alzette, Luxembourg",2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"10 Jan 2025","2024","","","6766","6773","Human health is increasingly threatened by exposure to hazardous substances, particularly persistent and toxic chemicals. The link between these substances, often encountered in complex mixtures, and various diseases are demonstrated in scientific studies. However, this information is scattered across several sources and hardly accessible by humans and machines. This paper evaluates current practices for publishing/accessing information on hazardous chemicals and proposes a novel platform designed to facilitate retrieval of critical chemical data in urgent situations. The platform aggregates information from multiple sources and organizes it into a structured knowledge graph. Users can access this information through a visual interface such as Neo4J Bloom and dashboards, or via natural language queries using a Chatbot. Our findings demonstrate a significant reduction in the time and effort required to access vital chemical information when datasets follow FAIR principles. Furthermore, we discuss the lessons learned from the development and implementation of this platform and provide recommendations for data owners and publishers to enhance data reuse and interoperability. This work aims to improve the accessibility and usability of chemical information by healthcare professionals, thereby supporting better health outcomes and informed decision-making in the face of patients exposed to chemical intoxication risks.","2156-1133","979-8-3503-8622-6","10.1109/BIBM62325.2024.10821991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10821991","","Visualization;Soft sensors;Aggregates;Natural languages;Decision making;Knowledge graphs;Medical services;Chatbots;Chemical elements;Chemicals","","","","27","IEEE","10 Jan 2025","","","IEEE","IEEE Conferences"
"Log and Event Analysis","A. Basta; N. Basta; W. Anwar; M. I. Essar","NA; Reinhardt University, GA, USA; NA; NA","Open-Source Security Operations Center (SOC): A Complete Guide to Establishing, Managing, and Maintaining a Modern SOC","","2025","","","67","98","Summary <p>Log and event analysis form a crucial pillar of modern security operations center (SOC) functions. By centralized collection and monitoring of log data from endpoints, networks, and applications, SOCs gain visibility into potential security issues across the enterprise infrastructure. Effective log collection, storage, and management are crucial capabilities for SOCs to enable comprehensive visibility, efficient investigations, and compliance. Beyond basic alerting and reporting, leading SOCs extract richer insights from expanding log quantities using advanced analytical techniques. Machine learning and statistical analysis empower the detection of both known and unknown threats from log‐extracted events. Dimensionality reduction represents data relationships in fewer variables for clustering similar events. Anomaly scores evaluate events for deviation from expected baselines. As critical infrastructure and core evidence, robust controls safeguard log data credibility. A diligent defense‐in‐depth methodology addresses persistent availability and integrity risks. Common challenges include data variability, identity context gaps, and restrictive extraction application programming interfaces.</p>","","9781394201617","10.1002/9781394201631.ch4","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10897093.pdf&bkn=10896967&pdfType=chapter","","Data visualization;Virtual private networks;Cloud computing;Web servers;Throughput;Threat assessment;Testing;Statistical analysis;Security;Search problems","","","","","","20 Feb 2025","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"Data Governance Handbook: A practical approach to building trust in data","W. S. Batchelder",NA,Data Governance Handbook: A practical approach to building trust in data,"","2024","","","","","Build an actionable, business value driven case for data governance to obtain executive support and implement with excellenceKey FeaturesDevelop a solid foundation in data governance and increase your confidence in data solutionsAlign data governance solutions with measurable business results and apply practical knowledge from real-world projectsLearn from a three-time chief data officer who has worked in leading Fortune 500 companiesPurchase of the print or Kindle book includes a free PDF eBookBook Description2.5 quintillion bytes! This is the amount of data being generated every single day across the globe. As this number continues to grow, understanding and managing data becomes more complex. Data professionals know that it’s their responsibility to navigate this complexity and ensure effective governance, empowering businesses with the right data, at the right time, and with the right controls. If you are a data professional, this book will equip you with valuable guidance to conquer data governance complexities with ease. Written by a three-time chief data officer in global Fortune 500 companies, the Data Governance Handbook is an exhaustive guide to understanding data governance, its key components, and how to successfully position solutions in a way that translates into tangible business outcomes. By the end, you’ll be able to successfully pitch and gain support for your data governance program, demonstrating tangible outcomes that resonate with key stakeholders. What you will learnComprehend data governance from ideation to delivery and beyondPosition data governance to obtain executive buy-inLaunch a governance program at scale with a measurable impactUnderstand real-world use cases to drive swift and effective actionObtain support for data governance-led digital transformationLaunch your data governance program with confidenceWho this book is forChief data officers, data governance leaders, data stewards, and engineers who want to understand the business value of their work, and IT professionals seeking further understanding of data management, will find this book useful. You need a basic understanding of working with data, business needs, and how to meet those needs with data solutions. Prior coding experience or skills in selling data solutions to executives are not required.","","9781803242477","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769338.pdf&bkn=10769337&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Federated Edge Learning for 6G: Foundations, Methodologies, and Applications","M. Tao; Y. Zhou; Y. Shi; J. Lu; S. Cui; J. Lu; K. B. Letaief","Department of Electronic Engineering and Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Huawei Technologies Company Ltd, Shenzhen, China; School of Science and Engineering (SSE), Shenzhen Future Network of Intelligence Institute (FNii-Shenzhen), and Guangdong Provincial Key Laboratory of Future Networks of Intelligence, The Chinese University of Hong Kong, Shenzhen, China; Department of Electronic Engineering and Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology (HKUST), Hong Kong, Hong Kong",Proceedings of the IEEE,"","2024","PP","99","1","39","Artificial intelligence (AI) is envisioned to be natively integrated into the sixth-generation (6G) mobile networks to support a diverse range of intelligent applications. Federated edge learning (FEEL) emerges as a vital enabler of this vision by leveraging the sensing, communication, and computation capabilities of geographically dispersed edge devices to collaboratively train AI models without sharing raw data. This article explores the pivotal role of FEEL in advancing both the “wireless for AI” and “AI for wireless” paradigms, thereby facilitating the realization of scalable, adaptive, and intelligent 6G networks. We begin with a comprehensive overview of learning architectures, models, and algorithms that form the foundations of FEEL. We, then, establish a novel task-oriented communication principle to examine key methodologies for deploying FEEL in dynamic and resource-constrained wireless environments, focusing on device scheduling, model compression, model aggregation, and resource allocation. Furthermore, we investigate the domain-specific optimizations of FEEL to facilitate its promising applications, ranging from wireless air-interface technologies to mobile and the Internet of Things (IoT) services. Finally, we highlight key future research directions for enhancing the design and impact of FEEL in 6G.","1558-2256","","10.1109/JPROC.2024.3509739","National Natural Science Foundation of China (NSFC)(grant numbers:62125108); Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China(grant numbers:U20A20159); Natural Science Foundation of Shanghai(grant numbers:23ZR1442800); National Natural Science Foundation of China(grant numbers:62271318); Shanghai Rising-Star Program(grant numbers:22QA1406100); Shenzhen Outstanding Talents Training Fund(grant numbers:202002); Guangdong Research Project(grant numbers:2017ZT07X152); National Key Research and Development Program of China(grant numbers:2018YFA0701601); Research Grants Council through the Areas of Excellence Scheme(grant numbers:AoE,E-601/22-R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10799091","Domain-specific optimization;federated edge learning (FEEL);integrated sensing–communication–computation;sixth-generation (6G);task-oriented communications","Wireless sensor networks;6G mobile communication;Artificial intelligence;Solid modeling;Sensors;Data models;Wireless networks;Computational modeling;Training;Optimization","","","","","IEEE","13 Dec 2024","","","IEEE","IEEE Early Access Articles"
"IC SEM Reverse Engineering Tutorial using Artificial Intelligence","O. P. Dizon-Paradis; D. S. Koblah; R. Wilson; D. Forte; D. L. Woodard",NA; NA; NA; NA; NA,IEEE Design & Test,"","2025","PP","99","1","1","In this tutorial, we will leverage artificial intelligence (AI) techniques to facilitate the reverse engineering of an integrated circuit based on scanning electron microspcopy, contributing to hardware assurance. This work encompasses various subjects, including image processing, computer vision, and machine learning, providing a comprehensive learning experience in these specialized domains. We encourage you to actively participate in this tutorial to execute your own project. The skills and knowledge acquired in this activity can bolster your resume, enhancing your prospects when seeking employment, pursuing further education, or advancing in your current professional role.","2168-2364","","10.1109/MDAT.2025.3543464","National Science Foundation (NSF)(grant numbers:2131480); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10891859","artificial intelligence;integrated circuits;reverse engineering;scanning electron microscopy;segmentation;tutorial","Artificial intelligence;Tutorials;Scanning electron microscopy;Image segmentation;Feature extraction;Electrons;Reverse engineering;Training;Stakeholders;Machine learning","","","","","IEEE","18 Feb 2025","","","IEEE","IEEE Early Access Articles"
"6 Putting Computational Creativity to Work","O. Bown",University of New South Wales,Beyond the Creative Species: Making Machines That Make Art and Music,"","2021","","","215","266","While there are many algorithmic methods to choose from to create computationally creative systems, they each have specific limitations and affordances that need to be worked with to get anything done. There are no magic bullets or general purpose solutions, no general intelligence systems. For the moment, AI solutions are highly domain-specific tailored tools with quirks and constraints that influence how they might be used. They may be operated by professional programmers who run custom scripts and output the processes of their generation for others to access, or through one-button-interfaces, simple navigation interfaces, or text-based search bars that an unskilled user can explore. Since machine learning is simultaneously so successful and so complicated, some researchers and companies have set out to make general-purpose creative machine learning toolkits that can be more easily used by end-users. The Runway toolkit is described as “an interface and framework that orchestrates the training, use and deployment of artificial intelligence models in design and creative platforms,”<superscript>2</superscript> taking an ecosystemic approach where different contributors can provide models and templates that can be used by others.","","9780262361750","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9394893.pdf&bkn=9394486&pdfType=chapter","","","","","","","","5 Apr 2021","","","MIT Press","MIT Press eBook Chapters"
"Modes of Uncertainty in HCI","R. Soden; L. Devendorf; R. Wong; Y. Akama; A. Light",NA; NA; NA; NA; NA,Modes of Uncertainty in HCI,"","2022","","","","","Uncertainty is a prevalent characteristic of contemporary life and a central challenge of HCI. As humans, researchers, and designers we encounter uncertainty in a multitude of forms and a variety of settings. The growing attention to uncertainty in HCI is due to the ever increasing expansion of the field and questions and contexts to which we seek to apply HCI research and practice. It is also due to events in the world that force us to engage more directly with questions related to uncertainty. Consequently, society is turning more than ever to data as a means to enable or mediate our understanding of phenomena such as climate change, political turmoil, increased economic upheaval, and a global pandemic. This monograph examines how HCI conceptualizes, situates, and responds to uncertainty - particularly arguing that our ability to respond to such uncertainties is governed to a great extent by the concepts we use to enframe a single, encompassing, overburdened and slippery idea. The authors propose four distinct ""modes of uncertainty"" to begin to draw together the varied strands of work in HCI that address uncertainty in its many forms. Rather than focusing on uncertainty as a discrete phenomenon in the world to be studied, they look to how research goals, methods, and theoretical frames used in HCI research influence the various ways in which we encounter it. By switching from uncertainty (a noun) to modes of engaging uncertainty (a verb), they foreground uncertainty as a relational concept and show that it is an active and ongoing condition that designers and researchers make present in different fashions depending upon their priorities and the context in which they are working. The authors show that adding modes of uncertainty to our conceptual toolbox facilitates conversation between domains and lets us draw new connections between disparate areas of research including visualization studies, critical design, feminist epistemologies, and sustainability.","","9781638280552","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9870674.pdf&bkn=9870673&pdfType=book","","","","","","","","2 Sep 2022","","","now","Now Foundations and Trends Books"
"Everyday Data Visualization: Design effective charts and dashboards","D. Abbott",Manning Publications,Everyday Data Visualization: Design effective charts and dashboards,"","2024","","","","","Radically improve the quality of your data visualizations by employing core principles of color, typography, chart types, data storytelling, and more. Everyday Data Visualization is a field guide for design techniques that will improve the charts, reports, and data dashboards you build every day. Everything you learn is tool-agnostic, with universal principles you can apply to any data stack. In Everyday Data Visualization you’ll learn important design principles for the most common data visualizations:  Harness the power of perception to guide a user’s attention Bring data to life with color and typography Choose the best chart types for your data story Design for interactive visualizations Keep the user’s needs first throughout your projects   This book gives you the tools you need to bring your data to life with clarity, precision, and flair. You’ll learn how human brains perceive and process information, wield modern accessibility standards, get the basics of color theory and typography, and more.","","9781633438408","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10745353.pdf&bkn=10745352&pdfType=book","charts;tables;dashboards;infographics;color;typography;tool-agnostic;positioning;design fundamentals;interactive;live data flow;storytelling;presentation","","","","","","","6 Nov 2024","","","Manning","Manning eBooks"
"Robust Query Processing: A Survey","J. R. Haritsa",NA,Robust Query Processing: A Survey,"","2024","","","","","The primary function of a database system is to efficiently compute correct answers to user queries. Therefore, robust query processing (RQP), where strong numerical guarantees are provided on query performance, has been a long-standing core objective in the design of industrial-strength database engines. Unfortunately, however, RQP has proved to be a largely intractable and elusive challenge, despite sustained efforts spanning several decades. In this monograph, a holistic coverage of the RQP innovations is provided, and strengths and limitations are highlighted. Further, open technical problems that remain to be solved to make RQP a contemporary reality are also enumerated. In this monograph, representative techniques along these various dimensions are covered. After the introduction, a background to RQP is given. In the chapters thereafter, the authors cover Robust Operators, Plans, and Execution, and then Structural Bounds, Cost Models and Machine Learning Techniques are surveyed. The monograph concludes with a chapter on Holistic Robustness, and Future Directions. The target audience for this monograph includes researchers, developers and students with an interest in the internals of database engines.","","9781638284277","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10807149.pdf&bkn=10807148&pdfType=book","","","","","","","","19 Dec 2024","","","now","Now Foundations and Trends Books"
"Matryoshka: Exploiting the Over-Parametrization of Deep Learning Models for Covert Data Transmission","X. Pan; M. Zhang; Y. Yan; S. Zhang; M. Yang","School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China",IEEE Transactions on Pattern Analysis and Machine Intelligence,"9 Jan 2025","2025","47","2","663","678","High-quality private machine learning (ML) data stored in local data centers becomes a key competitive factor for AI corporations. In this paper, we present a novel insider attack called Matryoshka to reveal the possibility of breaking the privacy of ML data even with no exposed interface. Our attack employs a scheduled-to-publish DNN model as a carrier model for covert transmission of secret models which memorize the information of private ML data that otherwise has no interface to the outsider. At the core of our attack, we present a novel parameter sharing approach which exploits the learning capacity of the carrier model for information hiding. Our approach simultaneously achieves: (i) High Capacity – With almost no utility loss of the carrier model, Matryoshka can transmit over 10,000 real-world data samples within a carrier model which has $220\times$220× less parameters than the total size of the stolen data, and simultaneously transmit multiple heterogeneous datasets or models within a single carrier model under a trivial distortion rate, neither of which can be done with existing steganography techniques; (ii) Decoding Efficiency – once downloading the published carrier model, an outside colluder can exclusively decode the hidden models from the carrier model with only several integer secrets and the knowledge of the hidden model architecture; (iii) Effectiveness – Moreover, almost all the recovered models either have similar performance as if it is trained independently on the private data, or can be further used to extract memorized raw training data with low error; (iv) Robustness – Information redundancy is naturally implemented to achieve resilience against common post-processing techniques on the carrier before its publishing; (v) Covertness – A model inspector with different levels of prior knowledge could hardly differentiate a carrier model from a normal model.","1939-3539","","10.1109/TPAMI.2024.3434417","National Key Research and Development Program of China(grant numbers:2021YFB3101200); National Natural Science Foundation of China(grant numbers:62402114,62472096,62172104,62172105,62102093,62102091,62302101,62402116,62202106); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10612241","Training data privacy;deep learning privacy;steganography;covert transmission;AI security","Data models;Training;Predictive models;Training data;Computational modeling;Task analysis;Data privacy","","","","89","IEEE","26 Jul 2024","","","IEEE","IEEE Journals"
"ICCSE 2019 Book of Abstracts","",,2019 14th International Conference on Computer Science & Education (ICCSE),"23 Sep 2019","2019","","","54","94","Presents abstracts for the articles comprising the conference proceedings.","2473-9464","978-1-7281-1846-8","10.1109/ICCSE.2019.8845477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8845477","","","","","","","IEEE","23 Sep 2019","","","IEEE","IEEE Conferences"
"Hookworm Detection in Wireless Capsule Endoscopy Images With Deep Learning","J. -Y. He; X. Wu; Y. -G. Jiang; Q. Peng; R. Jain","School of Information Science and Technology, Xipu Campus, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Xipu Campus, Southwest Jiaotong University, Chengdu, China; School of Computer Science, Fudan University, Shanghai, China; School of Information Science and Technology, Xipu Campus, Southwest Jiaotong University, Chengdu, China; School of Information and Computer Science, University of California at Irvine, Irvine, CA, USA",IEEE Transactions on Image Processing,"21 Feb 2018","2018","27","5","2379","2392","As one of the most common human helminths, hookworm is a leading cause of maternal and child morbidity, which seriously threatens human health. Recently, wireless capsule endoscopy (WCE) has been applied to automatic hookworm detection. Unfortunately, it remains a challenging task. In recent years, deep convolutional neural network (CNN) has demonstrated impressive performance in various image and video analysis tasks. In this paper, a novel deep hookworm detection framework is proposed for WCE images, which simultaneously models visual appearances and tubular patterns of hookworms. This is the first deep learning framework specifically designed for hookworm detection in WCE images. Two CNN networks, namely edge extraction network and hookworm classification network, are seamlessly integrated in the proposed framework, which avoid the edge feature caching and speed up the classification. Two edge pooling layers are introduced to integrate the tubular regions induced from edge extraction network and the feature maps from hookworm classification network, leading to enhanced feature maps emphasizing the tubular regions. Experiments have been conducted on one of the largest WCE datasets with 440K WCE images, which demonstrate the effectiveness of the proposed hookworm detection framework. It significantly outperforms the state-of-the-art approaches. The high sensitivity and accuracy of the proposed method in detecting hookworms shows its potential for clinical application.","1941-0042","","10.1109/TIP.2018.2801119","National Natural Science Foundation of China(grant numbers:61772436,61373121,61272290); Sichuan Science and Technology Innovation Seedling Fund(grant numbers:2017RZ0015,2017018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8279473","Hookworm detection;deep learning;convolutional neural network;computer-aided detection;wireless capsule endoscopy","Image edge detection;Feature extraction;Machine learning;Task analysis;Wireless sensor networks;Wireless communication;Endoscopes","Adolescent;Adult;Aged;Capsule Endoscopy;Deep Learning;Hookworm Infections;Humans;Image Interpretation, Computer-Assisted;Middle Aged;Support Vector Machine;Young Adult","117","","42","IEEE","2 Feb 2018","","","IEEE","IEEE Journals"
"Security and Trust in the 6G Era","V. Ziegler; P. Schneider; H. Viswanathan; M. Montag; S. Kanugovi; A. Rezaki","Nokia Bell Labs, Munich, Germany; Nokia Bell Labs, Munich, Germany; Nokia Bell Labs, Murray Hill, NJ, USA; Nokia Bell Labs, Munich, Germany; Nokia Standards, Bengaluru, India; Nokia Standards, Munich, Germany",IEEE Access,"26 Oct 2021","2021","9","","142314","142327","A comprehensive set of security technology enablers will be critically required for communication systems for the 6G era of the 2030s. Trustworthiness must be assured across IoT, heterogenous cloud and networks, devices, sub-networks, and applications. The 6G threat vector will be defined by 6G architectural disaggregation, open interfaces and an environment with multiple stakeholders. Broadly decomposed into domains of cyber-resilience, privacy and trust and their respective intersection, we explore relevant security technology enablers including automated software creation and automated closed-loop security operation, privacy preserving technologies, hardware and cloud embedded anchors of trust, quantum-safe security, jamming protection and physical layer security as well as distributed ledger technologies. Artificial intelligence and machine learning (AI/ML) as a key technology enabler will be pervasive and of pivotal relevance across the security technology stack and architecture. A novel vision for a trustworthy Secure Telecom Operation Map is developed as part of the automated closed loop operations paradigm.","2169-3536","","10.1109/ACCESS.2021.3120143","Nokia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9570274","6G;security;cyber-resilience;privacy;trustworthiness;sub-networks;wireless networks","Security;6G mobile communication;5G mobile communication;Privacy;Cloud computing;Authentication;3GPP","","53","","44","CCBY","14 Oct 2021","","","IEEE","IEEE Journals"
"A Review of Small UAV Navigation System Based on Multisource Sensor Fusion","X. Ye; F. Song; Z. Zhang; Q. Zeng","School of Aeronautics and Astronautics, Sun Yat-sen University, Shenzhen, China; School of Aeronautics and Astronautics, Sun Yat-sen University, Shenzhen, China; School of Aeronautics and Astronautics, Sun Yat-sen University, Shenzhen, China; School of Aeronautics and Astronautics, Sun Yat-sen University, Shenzhen, China",IEEE Sensors Journal,"30 Aug 2023","2023","23","17","18926","18948","In recent years, unmanned aircraft systems (UASs) have played an increasingly significant role in the military and civil fields. The flight control system, as the “hub” of an unmanned aerial vehicle (UAV), is responsible for the key function of autonomous flight, while a reliable and stable navigation system provides important information such as position status for flight control and represents the “sensory” function of the UAV. A highly autonomous and credible UAV requires a navigation system that meets specific requirements for accuracy, integrity, and continuity, resulting in a multitude of sensors on-board the UAV that are heterogeneous, redundant, and multisource, creating a highly complex navigation system. In this article, we review multisensor fusion (MSF) technology for small UAVs over the last 20 years and provide an overview of three typical multisource fusion architectures based on filtering, factor graph optimization, and data-driven, focusing on inductive identification of key technologies for multisource information fusion state estimation systems, including calibration techniques to improve data quality, observability analysis to provide theoretical support, additional model constraint correction using aircraft, and resilient fusion management techniques across all sources. Finally, we propose future directions for UAS navigation systems to address the limitations of the existing systems.","1558-1748","","10.1109/JSEN.2023.3292427","National Natural Science Foundation of China(grant numbers:61174120); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179240","Factor graph optimization (FGO);Kalman filter (KF);multisensor fusion (MSF);resilient;unmanned aircraft systems (UASs)","Sensors;Intelligent sensors;Navigation;Sensor systems;Autonomous aerial vehicles;Aerospace control;Sensor phenomena and characterization","","23","","170","IEEE","11 Jul 2023","","","IEEE","IEEE Journals"
"Linear Functionality Equivalence Attack Against Deep Neural Network Watermarks and a Defense Method by Neuron Mapping","F. -Q. Li; S. -L. Wang; A. W. -C. Liew","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Information and Communication Technology, Griffith University, Gold Coast, QLD, Australia",IEEE Transactions on Information Forensics and Security,"3 Apr 2023","2023","18","","1963","1977","As an ownership verification technique for deep neural networks, the white-box neural network watermark is being challenged by the functionality equivalence attack. By leveraging the structural symmetry within a deep neural network and manipulating the parameters accordingly, an adversary can invalidate almost all white-box watermarks without affecting the network’s performance. This paper introduces the linear functionality equivalence attack, which can adapt to different network architectures without requiring knowledge of either the watermark or data. We also propose NeuronMap, a framework that can efficiently neutralize linear functionality equivalence attacks and can be easily combined with existing white-box watermarks to enhance their robustness. Experiments conducted on several deep neural networks and state-of-the-art white-box watermarking schemes have demonstrated not only the destructive power of linear functionality equivalence attacks but also the defense capability of NeuronMap. Our result shows that the threat of basic linear functionality equivalence attacks against deep neural network watermarks can be effectively solved using NeuronMap.","1556-6021","","10.1109/TIFS.2023.3259881","National Natural Science Foundation of China(grant numbers:62271307,61771310); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10077406","Artificial intelligence security;deep neural network watermarking;functionality equivalence attack","Watermarking;Glass box;Neurons;Feature extraction;Deep learning;Closed box;Biological neural networks","","1","","63","IEEE","20 Mar 2023","","","IEEE","IEEE Journals"
"Robust Hardware Trojan Detection Method by Unsupervised Learning of Electromagnetic Signals","D. Lee; J. Lee; Y. Jung; J. Kauh; T. Song","Department of Cybersecurity, Korea University, Seoul, South Korea; Department of Cybersecurity, Korea University, Seoul, South Korea; YM-Naeultech, Incheon, South Korea; Agency for Defense Development (ADD), Seoul, South Korea; School of Electronic and Electrical Engineering, Kyungpook National University (KNU), Daegu, South Korea",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,"11 Dec 2024","2024","32","12","2327","2340","This article explores the threat posed by Hardware Trojans (HTs), malicious circuits clandestinely embedded in hardware akin to software backdoors. Activation by attackers renders these Trojans capable of inducing malfunctions or leaking confidential information by manipulating the hardware’s normal operation. Despite robust software security, detecting and ensuring normal hardware operation becomes challenging in the presence of malicious circuits. This issue is particularly acute in weapon systems, where HTs can present a significant threat, potentially leading to immediate disablement in adversary countries. Given the severe risks associated with HTs, detection becomes imperative. The study focuses on demonstrating the efficacy of deep learning-based HT detection by comparing and analyzing methods using deep learning with existing approaches. This article proposes utilizing the deep support vector data description (Deep SVDD) model for HT detection. The proposed method outperforms existing methods when detecting untrained HTs. It achieves 92.87% of accuracy on average, which is higher than that of an existing method, 50.00%. This finding contributes valuable insights to the field of hardware security and lays the foundation for practical applications of Deep SVDD in real-world scenarios.","1557-9999","","10.1109/TVLSI.2024.3458892","Agency for Defense Development by the Korean Government(grant numbers:UG233015TD); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10689630","Electromagnetic (EM) signals;hardware Trojan (HT);machine learning (ML);neural network algorithm;side channel;Trojan detection","Hardware;Circuits;Trojan horses;Silicon;Unsupervised learning;Software;Foundries","","1","","58","CCBYNCND","24 Sep 2024","","","IEEE","IEEE Journals"
"INGR Roadmap Security and Privacy Chapter","A. Dutta; E. Hammad; M. Enright; F. Behmann; A. Chorti; A. Cheema; K. Kadio; J. Urbina-Pineda; K. Alam; A. Limam; F. Chu; J. Lester; J. -G. Park; J. Bio-Ukeme; S. S. Pawar; R. Layton; P. Ramchandran; K. Okonkwo; L. Ong; M. Emmelmann; O. Issa; R. Arul; S. Malik; S. Krishnan; S. Sugumar; T. Lala; M. Borst; B. Kloza; G. K. Kurt","Applied Physics Lab, Security Working Group Co-Chair, John's Hopkins University; Security Working Group Co-Chair, Texas A&M University - RELLIS; Quantum Dimension, Inc.; IEEE ComSoc North America Regional Board, TelNet Management Consulting, Inc.; ENSEA, CNRS; Shared Services Canada; Shared Services Canada; IEEE HKN Member and CyberIIoT CEO; Rogers Communications (Formerly); Higher Institute of Engineering and Technology (ESPRIT); University of California, Los Angeles; Our Lady of Fatima University, Valenzuela, Philippines; Seoul National University of Science and Technology; Carleton University; Usha Mittal University of Technology; Aalborg University; Intel; Chevron; Ciena; Fraunhofer FOKUS; Department of National Defence, Canada; Amrita Vishwa Vidyapeetham; T-Mobile; National Library of Medicine; Intel Corporation; ZecureZ Consulting Company; IEEE Future Networks Initiative; IEEE Future Networks Initiative; NA",2022 IEEE Future Networks World Forum (FNWF),"8 Mar 2023","2022","","","1","71","The digital transformation brought on by 5G is redefining current models of end-to-end (E2E) connectivity and service reliability to include security-by-design principles necessary to enable 5G to achieve its promise. 5G trustworthiness highlights the importance of embedding security capabilities from the very beginning while the 5G architecture is being defined and standardized. Security requirements need to overlay and permeate through the different layers of 5G systems (physical, network, and application) as well as different parts of an E2E 5G architecture within a risk-management framework that takes into account the evolving security-threats landscape. 5G presents a typical use-case of wireless communication and computer networking convergence, where 5G fundamental building blocks include components such as Software Defined Networks (SDN), Network Functions Virtualization (NFV) and the edge cloud. This convergence extends many of the security challenges and opportunities applicable to SDN/NFV and cloud to 5G networks. Thus, 5G security needs to consider additional security requirements (compared to previous generations) such as SDN controller security, hypervisor security, orchestrator security, cloud security, edge security, etc. At the same time, 5G networks offer security improvement opportunities that should be considered. Here, 5G architectural flexibility, programmability and complexity can be harnessed to improve resilience and reliability. The working group scope fundamentally addresses the following: •5G security considerations need to overlay and permeate through the different layers of the 5G systems (physical, network, and application) as well as different parts of an E2E 5G architecture including a risk management framework that takes into account the evolving security threats landscape. •5G exemplifies a use-case of heterogeneous access and computer networking convergence, which extends a unique set of security challenges and opportunities (e.g., related to SDN/NFV and edge cloud, etc.) to 5G networks. Similarly, 5G networks by design offer potential security benefits and opportunities through harnessing the architecture flexibility, programmability and complexity to improve its resilience and reliability. •The IEEE FNI security WG's roadmap framework follows a taxonomic structure, differentiating the 5G functional pillars and corresponding cybersecurity risks. As part of cross collaboration, the security working group will also look into the security issues associated with other roadmap working groups within the IEEE Future Network Initiative.","2770-7679","978-1-6654-6250-1","10.1109/FNWF55208.2022.00131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10056734","5G Cybersecurity;security;privacy;data protection;reliability;resilience;mMTC;URLLC;SDN/NFV;cyber risk assessment and management;threat scenarios;cyber-attacks;security controls;mitigation;defense","5G mobile communication;Computer network reliability;Computer architecture;Network function virtualization;Complexity theory;Security;Risk management;Software defined networking;Convergence;Resilience","","","","24","IEEE","8 Mar 2023","","","IEEE","IEEE Conferences"
"The Kaggle Book: Data analysis and machine learning for competitive data science","K. Banachewicz; L. Massaron; A. Goldbloom",NA; NA; NA,The Kaggle Book: Data analysis and machine learning for competitive data science,"","2022","","","","","Get a step ahead of your competitors with insights from over 30 Kaggle Masters and Grandmasters. Discover tips, tricks, and best practices for competing effectively on Kaggle and becoming a better data scientist. Purchase of the print or Kindle book includes a free eBook in the PDF format.Key FeaturesLearn how Kaggle works and how to make the most of competitions from over 30 expert KagglersSharpen your modeling skills with ensembling, feature engineering, adversarial validation and AutoMLA concise collection of smart data handling techniques for modeling and parameter tuningBook DescriptionMillions of data enthusiasts from around the world compete on Kaggle, the most famous data science competition platform of them all. Participating in Kaggle competitions is a surefire way to improve your data analysis skills, network with an amazing community of data scientists, and gain valuable experience to help grow your career. The first book of its kind, The Kaggle Book assembles in one place the techniques and skills you’ll need for success in competitions, data science projects, and beyond. Two Kaggle Grandmasters walk you through modeling strategies you won’t easily find elsewhere, and the knowledge they’ve accumulated along the way. As well as Kaggle-specific tips, you’ll learn more general techniques for approaching tasks based on image, tabular, textual data, and reinforcement learning. You’ll design better validation schemes and work more comfortably with different evaluation metrics. Whether you want to climb the ranks of Kaggle, build some more data science skills, or improve the accuracy of your existing models, this book is for you. Plus, join our Discord Community to learn along with more than 1,000 members and meet like-minded people!What you will learnGet acquainted with Kaggle as a competition platformMake the most of Kaggle Notebooks, Datasets, and Discussion forumsCreate a portfolio of projects and ideas to get further in your careerDesign k-fold and probabilistic validation schemesGet to grips with common and never-before-seen evaluation metricsUnderstand binary and multi-class classification and object detectionApproach NLP and time series tasks more effectivelyHandle simulation and optimization competitions on KaggleWho this book is forThis book is suitable for anyone new to Kaggle, veteran users, and anyone in between. Data analysts/scientists who are trying to do better in Kaggle competitions and secure jobs with tech giants will find this book useful. A basic understanding of machine learning concepts will help you make the most of this book.","","9781801812214","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162223.pdf&bkn=10162222&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Ethical Challenges of AI: Nurturing Life-Care Perspectives in ECSIT Students","M. C. Castillo-González; A. M. Del Pilar Castillo-González","Humanities and Education Department, Tecnológico de Monterrey, Querétaro, México; Economic Administrative Science Department, Tecnológico Nacional de México, Querétaro, México",2024 IEEE International Symposium on Technology and Society (ISTAS),"29 Oct 2024","2024","","","1","8","We present an exploratory research aimed at understanding the perceptions that students and professors associated with the Engineering in Computer Science and Information Technology (ECSIT) program have regarding the ethical challenges of Artificial Intelligence, based on a theoretical framework that engages with ecofeminist, care, and ecological ethics. To achieve the research objective, a qualitative methodology was designed, and a comparative study was conducted between two ECSIT university programs in Mexico, one public and one private. Eight professors were interviewed, and two focus groups were conducted with the participation of 10 students. In both cases, the participation of women was encouraged, and professors teaching engineering courses and ethics courses for engineers were included. Findings reveal that the type of academic program is more relevant than the type of university, as well as the gender of professors and their interdisciplinary training.","2158-3412","979-8-3315-4070-8","10.1109/ISTAS61960.2024.10732170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10732170","ethics of care;ecological ethics;ecofeminism;artificial intelligence;sustainability education;ethics and artificial intelligence","Training;Economics;Computer science;Ethics;Production systems;Organizations;Reflection;Artificial intelligence;Interviews;Information technology","","","","14","IEEE","29 Oct 2024","","","IEEE","IEEE Conferences"
"RBEDH: A Decentralized Role Based Event Driven Hybrid Framework for Smart Contracts","E. Sivakumar; K. J. Singh; P. Chawla; G. Ganesan","Assistant Professor, Department of ECE, Jain University, Bengaluru, India; Associate Professor, Department of ECE, Chandigarh University, Punjab, India; Amity School of Engineering and Technology (ASET), Professor & Director, Amity Institute of Information Technology (AIIT), Amity University, Haryana, India; Professor and Director, Department of Computer Science Engineering, Jain (Deemed-to-be) University, Bengaluru, India",IEEE Access,"","2025","PP","99","1","1","The complex nature of smart contracts necessities the development of a novel adaptable framework. As blockchain technology continues to expand into diverse fields, the demand for secure, efficient, and transparent systems becomes increasingly critical. RBEDH is the integration of Role Based Access Control (RBAC), Event Driven Architecture (EDA) and Hybrid Functionalities. The framework is applied to the scholarly publishing sector, where academic integrity, transparency, and reliability are crucial for ensuring the credibility and trustworthiness of published research. The smart contracts are executed between authors, reviewers and publishers. Functionality test was conducted using Ganache, Ropsten, and Ethereum Mainnet. These evaluations confirmed the consistency and effectiveness of contract deployment. Vulnerabilities such as re-entrancy, integer overflow/underflow, unauthorized access were tested using Securify, Mythril, Smartcheck and Oynete. It is found that the system is secure and it not susceptible to any of the above vulnerability. Further timestamp dependency was tested through Manticore, Slither and Echidna and the test results indicate the absence of vulnerability. Performance analysis results proves that the proposed framework is better on the basis of average energy consumption, latency and memory requirement when compared with the existing literature.","2169-3536","","10.1109/ACCESS.2025.3554630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10938587","Blockchain;Smart Contract;Design Pattern;Role Based;Access Control;Event Driven Architecture;Hybrid Functionality","","","","","","CCBY","25 Mar 2025","","","IEEE","IEEE Early Access Articles"
"PID-Gen: Towards an Algorithm for the Generation of Random P&IDs","S. Merkelbach; T. Heuwinkel; R. Dumitrescu","Advanced Systems Engineering, Fraunhofer IEM, Paderborn, Germany; Advanced Systems Engineering, Fraunhofer IEM, Paderborn, Germany; Advanced Systems Engineering, Paderborn University, Paderborn, Germany",2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA),"16 Oct 2024","2024","","","1","4","Piping and Instrumentation Diagrams (P&IDs) are used in the process industry to visualize the elements present in a process plant and the connections between them. In reality, P&IDs are often only available as image, PDF, or as hard-copy. Methods to digitize P &IDs exist, but there are only few publicly available P &IDs that can be utilized for the development of new, especially data-driven, methods. We address this lack by proposing an algorithm that is able to create a random structure of P&IDs. Different parameters, such as the number and type of components, inflows, and outflows, can be defined. Our goal is to create a large, labeled, random dataset for the training and validation of models for the digitization of P &IDs. The algorithm consists of the two parts: graph creation and visualization. In this work, we present the graph creation part.","1946-0759","979-8-3503-6123-0","10.1109/ETFA61755.2024.10710643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10710643","P&ID;graph generation;benchmark","Training;Industries;Visualization;Instruments;Manufacturing automation","","","","21","IEEE","16 Oct 2024","","","IEEE","IEEE Conferences"
"Fuzzing Against the Machine: Automate vulnerability research with emulated IoT devices on QEMU","A. Nappa; E. Blázquez; N. Bassen; D. J. López-gómez",NA; NA; NA; NA,Fuzzing Against the Machine: Automate vulnerability research with emulated IoT devices on QEMU,"","2023","","","","","Find security flaws in any architecture effectively through emulation and fuzzing with QEMU and AFL Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesUnderstand the vulnerability landscape and useful tools such as QEMU and AFLExplore use cases to find vulnerabilities and execute unknown firmwareCreate your own firmware emulation and fuzzing environment to discover vulnerabilitiesBook DescriptionEmulation and fuzzing are among the many techniques that can be used to improve cybersecurity; however, utilizing these efficiently can be tricky. Fuzzing Against the Machine is your hands-on guide to understanding how these powerful tools and techniques work. Using a variety of real-world use cases and practical examples, this book helps you grasp the fundamental concepts of fuzzing and emulation along with advanced vulnerability research, providing you with the tools and skills needed to find security flaws in your software. The book begins by introducing you to two open source fuzzer engines: QEMU, which allows you to run software for whatever architecture you can think of, and American fuzzy lop (AFL) and its improved version AFL++. You’ll learn to combine these powerful tools to create your own emulation and fuzzing environment and then use it to discover vulnerabilities in various systems, such as iOS, Android, and Samsung's Mobile Baseband software, Shannon. After reading the introductions and setting up your environment, you’ll be able to dive into whichever chapter you want, although the topics gradually become more advanced as the book progresses. By the end of this book, you’ll have gained the skills, knowledge, and practice required to find flaws in any firmware by emulating and fuzzing it with QEMU and several fuzzing engines.What you will learnUnderstand the difference between emulation and virtualizationDiscover the importance of emulation and fuzzing in cybersecurityGet to grips with fuzzing an entire operating systemDiscover how to inject a fuzzer into proprietary firmwareKnow the difference between static and dynamic fuzzingLook into combining QEMU with AFL and AFL++Explore Fuzz peripherals such as modemsFind out how to identify vulnerabilities in OpenWrtWho this book is forThis book is for security researchers, security professionals, embedded firmware engineers, and embedded software professionals. Learners interested in emulation, as well as software engineers interested in vulnerability research and exploitation, software testing, and embedded software development will also find it useful. The book assumes basic knowledge of programming (C and Python); operating systems (Linux and macOS); and the use of Linux shell, compilation, and debugging.","","9781804614228","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251324.pdf&bkn=10251323&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Leveraging Machine Learning and Big Data for Smart Buildings: A Comprehensive Survey","B. Qolomany; A. Al-Fuqaha; A. Gupta; D. Benhaddou; S. Alwajidi; J. Qadir; A. C. Fong","Department of Cyber Systems, University of Nebraska at Kearney (UNK), Kearney, NE, USA; Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA; Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA; Engineering Technology Department, University of Houston, Houston, TX, USA; Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA; Information Technology University, Lahore, Pakistan; Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA",IEEE Access,"22 Jul 2019","2019","7","","90316","90356","Future buildings will offer new convenience, comfort, and efficiency possibilities to their residents. Changes will occur to the way people live as technology involves people's lives and information processing is fully integrated into their daily living activities and objects. The future expectation of smart buildings includes making the residents' experience as easy and comfortable as possible. The massive streaming data generated and captured by smart building appliances and devices contain valuable information that needs to be mined to facilitate timely actions and better decision making. Machine learning and big data analytics will undoubtedly play a critical role to enable the delivery of such smart services. In this paper, we survey the area of smart building with a special focus on the role of techniques from machine learning and big data analytics. This survey also reviews the current trends and challenges faced in the development of smart building services.","2169-3536","","10.1109/ACCESS.2019.2926642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754678","Smart buildings;smart homes;the Internet of Things (IoT);big data analytics;machine learning (ML)","Smart homes;Big Data;Machine learning;Internet of Things;Robot sensing systems;Smart buildings","","138","","307","CCBY","3 Jul 2019","","","IEEE","IEEE Journals"
"Deep Learning towards Mobile Applications","J. Wang; B. Cao; P. Yu; L. Sun; W. Bao; X. Zhu","College of Systems Engineering, National University of Defense Technology, Changsha, Hunan, P. R. China; College of Systems Engineering, National University of Defense Technology, Changsha, Hunan, P. R. China; Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA; Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA; College of Systems Engineering, National University of Defense Technology, Changsha, Hunan, P. R. China; College of Systems Engineering, National University of Defense Technology, Changsha, Hunan, P. R. China",2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS),"23 Jul 2018","2018","","","1385","1393","Recent years have witnessed an explosive growth of mobile devices. Mobile devices are permeating every aspect of our daily lives. With the increasing usage of mobile devices and intelligent applications, there is a soaring demand for mobile applications with machine learning services. Inspired by the tremendous success achieved by deep learning in many machine learning tasks, it becomes a natural trend to push deep learning towards mobile applications. However, there exist many challenges to realize deep learning in mobile applications, including the contradiction between the miniature nature of mobile devices and the resource requirement of deep neural networks, the privacy and security concerns about individuals' data, and so on. To resolve these challenges, during the past few years, great leaps have been made in this area. In this paper, we provide an overview of the current challenges and representative achievements about pushing deep learning on mobile devices from three aspects: training with mobile data, efficient inference on mobile devices, and applications of mobile deep learning. The former two aspects cover the primary tasks of deep learning. Then, we go through our two recent applications that apply the data collected by mobile devices to inferring mood disturbance and user identification. Finally, we conclude this paper with the discussion of the future of this area.","2575-8411","978-1-5386-6871-9","10.1109/ICDCS.2018.00139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416402","Mobile Device;Deep Learning;Federated Learning;Mobile Cloud;Mobile Application","Mobile handsets;Training;Machine learning;Servers;Data models;Cloud computing;Distributed databases","","74","","47","IEEE","23 Jul 2018","","","IEEE","IEEE Conferences"
"Multi-Level Security in Healthcare by Integrating Lattice-Based Access Control and Blockchain- Based Smart Contracts System","T. Haritha; A. Anitha","School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, India; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, India",IEEE Access,"23 Oct 2023","2023","11","","114322","114340","Access control to patient information has become increasingly important in healthcare systems. It is vital to enhance the security of healthcare systems to avoid data loss despite the various security policies imposed by healthcare management. The issue needs to be resolved with a comprehensive secure framework, which allows users to access data according to their level of confidentiality. This article presents a solution by imposing multi-level security in e-health systems by integrating the Lattice-Based Access Control (LBAC) model and blockchain-based smart contract mechanisms. These mechanisms provide security levels in compliance with data access restrictions among users and resources while maintaining compliance security levels. By using LBAC, you can provide multilevel protection for access control restrictions, whereas smart contracts are used to ensure the transaction process in a decentralized system via an agreement between the parties. A smart contract validates every user and performs the authentication process in the envisioned model, which uses the Ethereum Virtual Machine (EVM). In the blockchain network, the patient’s e-health details are accessed and stored as immutable blocks. Comparing the proposed scheme with existing benchmarking methods reveals that the proposed scheme preserves privacy, maintains transparency, provides an authentication process, maintains data integrity, and provides multilevel access control security. The proposed model performs better than other existing models. As a result, lattice-based access control enhances the security of e-health records.","2169-3536","","10.1109/ACCESS.2023.3324740","School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10285844","Blockchain;e-health records;multi-level security;lattice-based access control;smart contracts","Blockchains;Security;Medical services;Access control;Data privacy;Cloud computing;Smart contracts;Electronic medical records","","11","","59","CCBYNCND","16 Oct 2023","","","IEEE","IEEE Journals"
"Modeling and Training of Neural Processing Systems","E. Kusmenko; S. Nickels; S. Pavlitskaya; B. Rumpe; T. Timmermanns","Chair of Software Engineering, RWTH Aachen University, Aachen, Germany; Chair of Software Engineering, RWTH Aachen University, Aachen, Germany; Chair of Software Engineering, RWTH Aachen University, Aachen, Germany; Chair of Software Engineering, RWTH Aachen University, Aachen, Germany; Chair of Software Engineering, RWTH Aachen University, Aachen, Germany",2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems (MODELS),"21 Nov 2019","2019","","","283","293","The field of deep learning has become more and more pervasive in the last years as we have seen varieties of problems being solved using neural processing techniques. Image analysis and detection, control, speech recognition, translation are only a few prominent examples tackled successfully by neural networks. Thereby, the discipline imposes a completely new problem solving paradigm requiring a rethinking of classical software development methods. The high demand for deep learning technology has led to a large amount of competing frameworks mostly having a Python interface - a quasi standard in the community. Although, existing tools often provide great flexibility and high performance, they still lack to deliver a completely domain oriented problem view. Furthermore, using neural networks as reusable building blocks with clear interfaces in productive systems is still a challenge. In this work we propose a domain specific modeling methodology tackling design, training, and integration of deep neural networks. Thereby, we distinguish between three main modeling concerns: architecture, training, and data. We integrate our methodology in a component-based modeling toolchain allowing one to employ and reuse neural networks in large software architectures.","","978-1-7281-2536-7","10.1109/MODELS.2019.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8906978","deep learning, neural networks, model-driven software engineering","","","11","","43","IEEE","21 Nov 2019","","","IEEE","IEEE Conferences"
"The Social Implications of XR: Promises, Perils, and Potential","K. Michael; R. Abbas; S. Papagiannidis","Arizona State University, Tempe, AZ, USA; University of Wollongong, Wollongong, NSW, Australia; Newcastle University, Newcastle upon Tyne, U.K",IEEE Technology and Society Magazine,"11 Apr 2024","2024","43","1","91","108","In 1992, Stephenson [1] described a type of metaverse in his science fiction novel Snow Crash. Through his imagination, Stephenson is said to have directly influenced the makers of Google Earth, and, more recently, Silicon Valley’s “Metaverse” [2]. In addition, there was a spate of literature published 15–20 years ago, related to metaverse concepts with the introduction of Second Life in June 2003 by Linden Labs. Many users experienced virtual spaces, shopped with virtual currency (Linden dollar, L ${\$}$ ), and even frequented virtual storefronts [3].","1937-416X","","10.1109/MTS.2024.3370023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496903","","Metaverse;Online banking;Second Life;Computer crashes;Social implications of technology;X reality;Stakeholders;Ethics;Data privacy;Internet;Middleware;Data processing","","4","","67","IEEE","11 Apr 2024","","","IEEE","IEEE Magazines"
"Enhancing Security in Cloud Computing Using Artificial Intelligence (AI)","D. Stutz; J. T. Assis; A. A. Laghari; A. A. Khan; N. Andreopoulos; A. Terziev; A. Deshpande; D. Kulkarni; E. G. H. Grata","Centro Federal de Educa&#xe7;&#xe3;o Tecnol&#xf3;gica Celso Suckow da Fonseca (CEFET) at Nova Friburgo, Nova Friburgo, RJ, Brazil; Instituto Politecnico do Rio de Janeiro, Nova Friburgo, RJ, Brazil; Sindh Madresstul Islam University, Karachi, Sindh, Pakistan; Benazir Bhutto Shaheed University, Research Lab of Artificial Intelligence and Information Security, Faculty of Computing, Science and Information Technology, Karachi, Sindh, Pakistan; Computer Science Department, Technological Institute of Iceland, Reykjav&#xed;k, Iceland; TerziA, Sofia, Bulgaria; Electronics and Communication Engineering, Angadi Institute of Technology and Management, Belagavi, India; Department of Computer Science and Engineering, Angadi Institute of Technology and Management, Belagavi, India; Department of Telecommunications, Federal Fluminense University (UFF), Niter&#xf3;i, RJ, Brazil",Applying Artificial Intelligence in Cybersecurity Analytics and Cyber Threat Detection,"","2024","","","179","220","Cloud computing (CC) technologies (viz artificial intelligence (AI), data science, blockchain, “big data” (BD), etc.) are progressively widespread and practically applied. Researchers face the biggest challenges regarding efficient data access and acquisition. The CPS's efficiency will help researchers advance in CC, cyber‐attacks (CAs), “cyber threats” (CTs), and “CT intelligence” (CTI). Alongside optimally storing the data, one major caveat is security. The best possible protection can enhance data storage/handling to protect end systems from illegal attacks and thefts. Smart environments (SEs) are collections of sensors, actuators, and numerous computing pieces that ease human life. With the booming SEs, data have notably increased, thus demanding smart, optimal management. Sometimes, data processing is inadequate due to risks and resource costs. Some characteristic security difficulties are “advanced persistent threats” (APTs), denial‐of‐service (DoS), eclipse, double‐spending attacks, and other malware. Thus, advanced AI‐reliant anomaly discovery and mitigation tactics can address and disentangle the issues above. Cybersecurity (CS) in SEs, sub‐CPss, and CPSs relates to working well with AI‐based anomaly detection techniques. This chapter investigates challenges, trends, and prospective design pathways for securely taming networks and CPSs via AI‐centered, robust, real‐time, and CC‐reliant data storage to inhibit unauthorized access and innumerable CAs (e.g., ransomware, spyware, and phishing. This taxonomy bestows readers with an ample AI potential overview to improve CS in different contexts. The chapter inspects future opportunities in emerging CS applications, advanced AI tactics, data representation, and new infrastructures' development for successfully adopting AI‐based CS (AICS) in today's digital transformation era and syndemic circumstances.","","9781394196456","10.1002/9781394196470.ch11","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10494607.pdf&bkn=10494576&pdfType=chapter","","Cloud computing;Artificial intelligence;Software as a service;Virtual machining;Artificial neural networks;Natural language processing;Web services","","2","","","","8 Apr 2024","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"Contemporary Generation: Artificial Intelligence Contribution to Manufacturing","T. Osmëni; M. Ali","Risk Advisory, Deloitte, Tirana, Albania; Electronics and Telecommunications Engineering Department, Canadian Institute of Technology, Tirana, Albania","2022 International Conference on Computing, Networking, Telecommunications & Engineering Sciences Applications (CoNTESA)","13 Jan 2023","2022","","","31","35","The manufacturing industries’ main purpose is transforming raw materials into finished goods, usually on a large industrial scale. The creation and trading of quality manufacturing items could expose challenges that could put a great deal of strain on companies. Delivering a product that needs to meet global high-quality standards, requires: navigating through often a confusing logistical supply chain; customer driven time-to-market deadlines and maybe, if it is a new product, companies will have to cope effectively in terms of cost, design and time required to fulfil this unfamiliar project. This paper highlights the positive influence AI (Artificial Intelligence) has on the manufacturing processes. Contemporary manufacturing industries are characterized by a comparatively low level of IT spending and tend to be at the forefront in the use of IoT (Internet of Things). Predicting and reducing failures can yield significant cost savings in contemporary generation of goods. This paper presents the research on predictive maintenance in AI with the use of Unsupervised Learning (UL) algorithm, related to manufacturing industries. This is done to show how the implementation of innovative technologies is beneficial in terms of reduction in human error and the need for human availability andassistance. All of which leads to the lowering of costs by automating specific parts of the industrial processes and tasks, in this case, predictive maintenance. Innovative technology is common in the present generation, but what is still lacking is the right use when it comes to businesses and specific needs.","","979-8-3503-9801-4","10.1109/CoNTESA57046.2022.10011233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011233","Artificial Intelligence;balancing maintenance;manufacturing;predictive maintenance;product;time-to-market;transforming;supply chain;Unsupervised Learning","Manufacturing industries;Training;Solid modeling;Costs;Companies;Manufacturing;Unsupervised learning","","1","","12","IEEE","13 Jan 2023","","","IEEE","IEEE Conferences"
"IEEE Approved Draft Recommended Practice for Improving Generalizability of Artificial Intelligence for Medical Imaging","",,"IEEE P3350/D3, November 2024","5 Dec 2024","2025","","","1","38","This recommended practice delineates an architecture and offers suggestions for enhancing the generalizability of artificial intelligence (AI) models in medical imaging.","","979-8-8557-1557-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10780997","Generalizability;dataset;modeling;training;evaluation","IEEE Standards;Data models;Training;Biomedical imaging;Artificial intelligence;Performance evaluation;Medical services","","","","","","5 Dec 2024","","","IEEE","IEEE Standards"
"Intelligent Radio Concepts","D. Chew; A. L. Adams; J. Uher",NA; NA; NA,"Wireless Coexistence: Standards, Challenges, and Intelligent Solutions","","2021","","","155","203","This chapter introduces intelligent radio concepts, and aims to establish a set of basic definitions describing the various radio technologies associated with DSA. It presents the desired capabilities of an intelligent communications system in the context of wireless coexistence. The chapter discusses the cognitive cycle at length, but this generally involves the process of observe‐orient‐decide‐act, and focuses on intelligent‐radio rather than cognitive‐radio. The implementation of cognitive capability is very much an active area of research. The cognitive resource manager (CRM) framework is capable of supporting both centralized and decentralized network configurations. The CRM toolbox is a library of algorithms used by the CRM core for data pre‐processing, modeling, learning, and decision‐making. The chapter discusses several learning algorithms and their recent application to intelligent radio. Advances in automation, machine learning, and artificial intelligence should only increase our expectations regarding intelligent radio capabilities.","","9781119584223","10.1002/9781119584230.ch6","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9655986.pdf&bkn=9655983&pdfType=chapter","","Telephone sets;Software;Wireless sensor networks;Machine learning;Long Term Evolution;Standards;Cognitive radio","","","","","","20 Dec 2021","","","IEEE","Wiley-IEEE Press eBook Chapters"
"Search as Learning","K. Urgo; J. Arguello",NA; NA,Search as Learning,"","2025","","","","","Search systems are often designed to support simple look-up tasks, such as fact-finding and navigation tasks. However, people increasingly use search engines to complete tasks that require deeper learning. In recent years, the search as learning (SAL) research community has argued that search systems should also be designed to support information seeking tasks that involve complex learning as an important outcome. This monograph provides a comprehensive review of prior research in search as learning and related areas. Searching to learn can be characterized by specific learning objectives, strategies, and context. Therefore, the monograph begins with a review of research in education that has aimed at characterizing learning objectives, strategies, and context. Then, review methods used in prior studies to measure learning during a search session are covered. Two important recommendations for future work are studied: (1) measuring learning retention and (2) measuring a learner’s ability to transfer their new knowledge to a novel scenario. Following this, studies that have focused on understanding factors that influence learning during search and search behaviors that are predictive of earning are discussed. Also, tools that have been developed to support learning during search are surveyed. Searching for the purpose of learning is often a solitary activity. Research in self-regulated learning (SRL) aims to understand how people monitor and control their own learning. Therefore, existing models of SRL are researched, as well as methods to measure engagement with specific SRL processes, and tools to support effective SRL. The monograph concludes with suggesting potential areas for future research.","","9781638285373","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10924684.pdf&bkn=10924683&pdfType=book","","","","","","","","13 Mar 2025","","","now","Now Foundations and Trends Books"
"Lightweight privacy-preserving federated deep intrusion detection for industrial cyber-physical system","I. A. Soomro; H. ur Rehman Khan; S. J. Hussain; Z. Ashraf; M. M. Alnfiai; N. N. Alotaibi","Sir Syed CASE Institute of Technology, Islamabad, Pakistan; Sir Syed CASE Institute of Technology, Islamabad, Pakistan; Sir Syed CASE Institute of Technology, Islamabad, Pakistan; Department of Computing and Information Technology, IISAT, Gujranwala, Pakistan; Department of Information Technology, College of Computers and Information Technology, Taif University, Taif P.O. Box 11099, Taif, 21944, Saudi Arabia; Department of Special Education, College of Education, Najran University, Saudi Arabia",Journal of Communications and Networks,"8 Jan 2025","2024","26","6","632","649","The emergence of Industry 4.0 entails extensive reliance on industrial cyber-physical systems (ICPS). ICPS promises to revolutionize industries by fusing physical systems with computational functionality. However, this potential increase in ICPS makes them prone to cyber threats, necessitating effective intrusion detection systems (IDS) systems. Privacy provision, system complexity, and system scalability are major challenges in IDS research. We present FedSecureIDS, a novel lightweight federated deep intrusion detection system that combines CNNs, LSTMs, MLPs, and federated learning (FL) to overcome these challenges. FedSecureIDS solves major security issues, namely eavesdropping and man-in-the-middle attacks, by employing a simple protocol for symmetric session key exchange and mutual authentication. Our Experimental results demonstrate that the proposed method is effective with an accuracy of 98.68%, precision of 98.78%, recall of 98.64%, and an F1-score of 99.05% with different edge devices. The model is similarly performed in conventional centralized IDS models. We also carry out formal security evaluations to confirm the resistance of the proposed framework to known attacks and provisioning of high data privacy and security.","1976-5541","","10.23919/JCN.2024.000054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10834496","Federated learning;industrial cyber-physical systems;Internet of things;intrusion detection system;symmetric key","Security;Data models;Training;Intrusion detection;Cyber-physical systems;Sensors;Control systems;Real-time systems;Machinery;Denial-of-service attack","","","","","","8 Jan 2025","","","KICS","KICS Journals"
"Backdoor Attacks and Countermeasures in Natural Language Processing Models: A Comprehensive Security Review","P. Cheng; Z. Wu; W. Du; H. Zhao; W. Lu; G. Liu","Department of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; StatNLP Research Group, Singapore University of Technology and Design, Singapore, Singapore; Department of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China",IEEE Transactions on Neural Networks and Learning Systems,"","2025","PP","99","1","21","Language models (LMs) are becoming increasingly popular in real-world applications. Outsourcing model training and data hosting to third-party platforms has become a standard method for reducing costs. In such a situation, the attacker can manipulate the training process or data to inject a backdoor into models. Backdoor attacks are a serious threat where malicious behavior is activated when triggers are present; otherwise, the model operates normally. However, there is still no systematic and comprehensive review of LMs from the attacker’s capabilities and purposes on different backdoor attack surfaces. Moreover, there is a shortage of analysis and comparison of the diverse emerging backdoor countermeasures. Therefore, this work aims to provide the natural language processing (NLP) community with a timely review of backdoor attacks and countermeasures. According to the attackers’ capability and affected stage of the LMs, the attack surfaces are formalized into four categorizations: attacking the pretrained model with fine-tuning (APMF) or parameter-efficient fine-tuning (PEFT), attacking the final model with training (AFMT), and attacking large language model (ALLM). Thus, attacks under each categorization are combed. The countermeasures are categorized into two general classes: sample inspection and model inspection. Thus, we review countermeasures and analyze their advantages and disadvantages. Also, we summarize the benchmark datasets and provide comparable evaluations for representative attacks and defenses. Drawing the insights from the review, we point out the crucial areas for future research on the backdoor, especially soliciting more efficient and practical countermeasures.","2162-2388","","10.1109/TNNLS.2025.3540303","Joint Funds of the National Natural Science Foundation of China(grant numbers:U21B2020); National Natural Science Foundation of China(grant numbers:62406188); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10905032","Artificial intelligence (AI) security;backdoor attacks;backdoor countermeasures;natural language processing (NLP)","Training;Reviews;Inspection;Data models;Natural language processing;Benchmark testing;Security;Large language models;Text categorization;Outsourcing","","","","","IEEE","26 Feb 2025","","","IEEE","IEEE Early Access Articles"
"12 CONCLUSIONS","V. Lehdonvirta",Oxford University,Cloud Empires: How Digital Platforms Are Overtaking the State and How We Can Regain Control,"","2022","","","205","236","The Internet was supposed to change the structure of society. It was supposed to get rid of gatekeepers and middlemen.<superscript>1</superscript> It was supposed to empower individuals and communities, create a ""level playing field,"" and give ""everyone access to the same information.""<superscript>2</superscript> It was supposed to obsolete centralized authorities that set up artificial boundaries and compile dossiers on us.<superscript>3</superscript> It was supposed to be governed by ""ethics"" instead of ""systems erected to impose order.""<superscript>4</superscript> It was supposed to topple autocrats and promote individual liberty over top-down control.<superscript>5</superscript> This is what Internet visionaries and technologists promised us. But they delivered something very different. They created some of the most powerful gatekeepers in history. They carved up the Internet into walled domains and registered us into their databases. Instead of making state authority obsolete, they rivaled it. Why did they do this? What does it mean for our societies and economies? And what is to be done about it?","","9780262371094","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9855629.pdf&bkn=9850417&pdfType=chapter","","","","","","","","16 Aug 2022","","","MIT Press","MIT Press eBook Chapters"
"Distributed Artificial Intelligence Empowered by End-Edge-Cloud Computing: A Survey","S. Duan; D. Wang; J. Ren; F. Lyu; Y. Zhang; H. Wu; X. Shen","School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China; School of Computer Science and Engineering, Central South University, Changsha, China; Computer School, Beijing Information Science and Technology University, Beijing, China; Department of Electrical and Software Engineering, University of Calgary, Calgary, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada",IEEE Communications Surveys & Tutorials,"23 Feb 2023","2023","25","1","591","624","As the computing paradigm shifts from cloud computing to end-edge-cloud computing, it also supports artificial intelligence evolving from a centralized manner to a distributed one. In this paper, we provide a comprehensive survey on the distributed artificial intelligence (DAI) empowered by end-edge-cloud computing (EECC), where the heterogeneous capabilities of on-device computing, edge computing, and cloud computing are orchestrated to satisfy the diverse requirements raised by resource-intensive and distributed AI computation. Particularly, we first introduce several mainstream computing paradigms and the benefits of the EECC paradigm in supporting distributed AI, as well as the fundamental technologies for distributed AI. We then derive a holistic taxonomy for the state-of-the-art optimization technologies that are empowered by EECC to boost distributed training and inference, respectively. After that, we point out security and privacy threats in DAI-EECC architecture and review the benefits and shortcomings of each enabling defense technology in accordance with the threats. Finally, we present some promising applications enabled by DAI-EECC and highlight several research challenges and open issues toward immersive performance acquisition.","1553-877X","","10.1109/COMST.2022.3218527","National Key Research and Development Program of China(grant numbers:2022YFF0604502); National Natural Science Foundation of China(grant numbers:62122095,62002389,62072472,U19A2067); Grant from the Guoqiang Institute, Tsinghua University; Natural Science Foundation of Hunan Province, China(grant numbers:2020JJ2050,2021JJ20079); Young Elite Scientist Sponsorship Program by CAST(grant numbers:YESS20200238); Young Talents Plan of Hunan Province of China(grant numbers:2021RC3004); Higher Education Discipline Innovation Project(grant numbers:B18059); Central South University Innovation-Driven Research Programme(grant numbers:2023CXQD029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933792","Distributed artificial intelligence;end-edge-cloud computing;network computing;federated learning","Artificial intelligence;Cloud computing;Privacy;Security;Training;Edge computing;Servers","","116","","318","IEEE","1 Nov 2022","","","IEEE","IEEE Journals"
"Efficient Acceleration of Deep Learning Inference on Resource-Constrained Edge Devices: A Review","M. M. H. Shuvo; S. K. Islam; J. Cheng; B. I. Morshed","Department of Electrical Engineering and Computer Science, Analog/Mixed Signal VLSI and Devices Laboratory (AVDL), University of Missouri, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, Analog/Mixed Signal VLSI and Devices Laboratory (AVDL), University of Missouri, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, Bioinformatics and Machine Learning Laboratory (BML), University of Missouri, Columbia, MO, USA; Department of Computer Science, Cyber Physical Systems (CPS) Laboratory, Texas Tech University, Lubbock, TX, USA",Proceedings of the IEEE,"11 Jan 2023","2023","111","1","42","91","Successful integration of deep neural networks (DNNs) or deep learning (DL) has resulted in breakthroughs in many areas. However, deploying these highly accurate models for data-driven, learned, automatic, and practical machine learning (ML) solutions to end-user applications remains challenging. DL algorithms are often computationally expensive, power-hungry, and require large memory to process complex and iterative operations of millions of parameters. Hence, training and inference of DL models are typically performed on high-performance computing (HPC) clusters in the cloud. Data transmission to the cloud results in high latency, round-trip delay, security and privacy concerns, and the inability of real-time decisions. Thus, processing on edge devices can significantly reduce cloud transmission cost. Edge devices are end devices closest to the user, such as mobile phones, cyber–physical systems (CPSs), wearables, the Internet of Things (IoT), embedded and autonomous systems, and intelligent sensors. These devices have limited memory, computing resources, and power-handling capability. Therefore, optimization techniques at both the hardware and software levels have been developed to handle the DL deployment efficiently on the edge. Understanding the existing research, challenges, and opportunities is fundamental to leveraging the next generation of edge devices with artificial intelligence (AI) capability. Mainly, four research directions have been pursued for efficient DL inference on edge devices: 1) novel DL architecture and algorithm design; 2) optimization of existing DL methods; 3) development of algorithm–hardware codesign; and 4) efficient accelerator design for DL deployment. This article focuses on surveying each of the four research directions, providing a comprehensive review of the state-of-the-art tools and techniques for efficient edge inference.","1558-2256","","10.1109/JPROC.2022.3226481","College of Engineering, University of Missouri, Columbia, MO, USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985008","Algorithm–hardware codesign;artificial intelligence (AI);artificial intelligence on edge (edge-AI);deep learning (DL);model compression;neural accelerator","Edge computing;Image edge detection;Real-time systems;Cloud computing;Artificial intelligence;Optimization;Computer architecture;Neural networks;Deep learning","","76","","424","CCBY","14 Dec 2022","","","IEEE","IEEE Journals"
"A Survey on Signal Processing Based Pathological Voice Detection Techniques","R. Islam; M. Tarique; E. Abdel-Raheem","Department of Electrical and Computer Engineering, University of Windsor, Windsor, Canada; Department of Electrical Engineering, University of Science and Technology of Fujairah, Fujairah, UAE; Department of Electrical and Computer Engineering, University of Windsor, Windsor, Canada",IEEE Access,"17 Apr 2020","2020","8","","66749","66776","Voice disability is a barrier to effective communication. Around 1.2% of the World's population is facing some form of voice disability. Surgical procedures namely laryngoscopy, laryngeal electromyography, and stroboscopy are used for voice disability diagnosis. Researchers and practitioners have been working to find alternatives to these surgical procedures. Voice sample based diagnosis is one of them. The major steps followed by these works are (a) to extract voice features from voice samples and (b) to discriminate pathological voices from normal voices by using a classifier algorithm. However, there is no consensus about the voice feature and the classifier algorithm that can provide the best accuracy in screening voice disability. Moreover, some of the works use multiple voice features and multiple classifiers to ensure high reliability. In this paper, we address these issues. The motivation of the work is to address the need for non-invasive signal processing techniques to detect voice disability in the general population. This paper conducts a survey related to voice disability detection methods. The paper contains two main parts. In the first part, we present background information including causes of voice disability, current procedures and practices, voice features, and classifiers. In the second part, we present a comprehensive survey work on voice disability detection algorithms. The issues and challenges related to the selection of voice feature and classifier algorithms have been addressed at the end of this paper.","2169-3536","","10.1109/ACCESS.2020.2985280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9055386","Algorithms;issues and challenges;signal processing;surgical methods;survey;voice disability;voice features","Cancer;Alzheimer's disease;Signal processing;Pathology;Signal processing algorithms;Lung","","62","","118","CCBY","2 Apr 2020","","","IEEE","IEEE Journals"
"Employing Remote Sensing, Data Communication Networks, AI, and Optimization Methodologies in Seismology","M. S. Abdalzaher; H. A. Elsayed; M. M. Fouda","Department of Seismology, National Research Institute of Astronomy and Geophysics, Cairo, Egypt; Department of Electronics and Electrical Communications Engineering, Ain Shams University, Cairo, Egypt; Department of Electrical and Computer Engineering, Idaho State University, Pocatello, ID, USA",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"11 Nov 2022","2022","15","","9417","9438","Seismology is among the intrinsic sciences that strictly affect human lives. Many research efforts are presented in the literature aiming at achieving risk mitigation and disaster management. More particularly, modern technologies have been employed in such a pivot. However, the day-to-day challenges and complexities of such natural science that face the stack holders still need more reliable and intelligent solutions. The solution can depend on a partial or integrated system of modern technologies. In this article, we extensively survey the corelated modern technologies aiming to gather the major efforts exerted in this regard. It also outlines the desirability of seismology to modern technologies. Then, we present a detailed analysis of remote sensing and data communication networks (DCNs), which are considered the backend of seismic networks. Furthermore, for seismology, we depict both classical and nonclassical approaches based on DCN principles, such as optical fiber-based acoustic sensors, social media, and the Internet of things (IoT). Following that, a comprehensive description of the various optimization techniques utilized for seismic wave analysis and for prolonging network lifetime is offered. A description of the important functions that artificial intelligence (AI) can play in different fields of seismology is also included. Finally, we present some recommendations for stack holders to prevent natural calamities and preserve human lives.","2151-1535","","10.1109/JSTARS.2022.3216998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9928324","Data communication networks (DCNs);deep learning;Internet-of-things (IoT);machine learning;optimization techniques;seismology;social media","Seismology;Artificial intelligence;Seismic waves;Optimization;Remote sensing;Earth;Internet of Things","","26","","220","CCBYNCND","25 Oct 2022","","","IEEE","IEEE Journals"
"The Knowledge Structure and Development Trend in Artificial Intelligence Based on Latent Feature Topic Model","Y. Liu; M. Chen","School of Cultural Heritage and Information Management, Shanghai University, Shanghai, China; School of Business, Wenzhou University, Wenzhou, China",IEEE Transactions on Engineering Management,"7 Aug 2024","2024","71","","12593","12604","Currently, with the rapid development of science and technology, the field of artificial intelligence presents characteristics such as a wide crossover of disciplines and fast update, and the field of artificial intelligence has become a new focus of international competition. As an interdisciplinary field, the field of artificial intelligence has rich knowledge and strategic management significance. This article conducts an in-depth study on the knowledge structure and evolution trends in the field of AI, and the main work is as follows. First, a new potential feature topic model New-LDA is proposed for the study of topic recognition, which enhances the feature learning ability of the traditional LDA model, and makes up for the deficiency of the traditional LDA model in the ability of recognizing topics in complex environments. Second, the knowledge structure in the field of AI is analyzed from two aspects: topic recognition and coword analysis. The time series model is introduced to establish the topic evolution network, and the high-frequency words in three periods are compared and analyzed to find the evolution regular of knowledge structure in the AI domain. Finally, taking the cross-discipline of AI as an example, the thematic evolution of the field and its cross-discipline is analyzed to determine the future development direction and evolutionary trend of the field of AI.","1558-0040","","10.1109/TEM.2022.3232178","Joint Laboratory Project Between Institute of Scientific and Technical Information of China; Elsevier in 2022; Multidimensional Evaluation Paradigm of Scientific Literature and Evaluation System of Scientific and Technological Talents(grant numbers:21wsk169); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10017444","Artificial intelligence;development trend;knowledge structures;LDA models","Artificial intelligence;Market research;Analytical models;Patents;Semantics;Collaboration;Computational modeling","","25","","43","IEEE","16 Jan 2023","","","IEEE","IEEE Journals"
"Location-Based Social Network Data Generation Based on Patterns of Life","J. -S. Kim; H. Jin; H. Kavak; O. C. Rouly; A. Crooks; D. Pfoser; C. Wenk; A. Züfle","Geography and Geoinformation Science, George Mason University; Geography and Geoinformation Science, George Mason University; Computational and Data Science, George Mason University; Computer Science, Tulane University; Computational and Data Science, George Mason University; Geography and Geoinformation Science, George Mason University; Computer Science, Tulane University; Geography and Geoinformation Science, George Mason University",2020 21st IEEE International Conference on Mobile Data Management (MDM),"7 Aug 2020","2020","","","158","167","Location-based social networks (LBSNs) have been studied extensively in recent years. However, utilizing real-world LBSN data sets yields several weaknesses: sparse and small data sets, privacy concerns, and a lack of authoritative ground-truth. To overcome these weaknesses, we leverage a large-scale LBSN simulation to create a framework to simulate human behavior and to create synthetic but realistic LBSN data based on human patterns of life. Such data not only captures the location of users over time but also their interactions via social networks. Patterns of life are simulated by giving agents (i.e., people) an array of “needs” that they aim to satisfy, e.g., agents go home when they are tired, to restaurants when they are hungry, to work to cover their financial needs, and to recreational sites to meet friends and satisfy their social needs. While existing real-world LBSN data sets are trivially small, the proposed framework provides a source for massive LBSN benchmark data that closely mimics the real-world. As such, it allows us to capture 100% of the (simulated) population without any data uncertainty, privacy-related concerns, or incompleteness. It allows researchers to see the (simulated) world through the lens of an omniscient entity having perfect data. Our framework is made available to the community. In addition, we provide a series of simulated benchmark LBSN data sets using different synthetic towns and real-world urban environments obtained from OpenStreetMap. The simulation software and data sets, which comprise gigabytes of spatio-temporal and temporal social network data, are made available to the research community.","2375-0324","978-1-7281-4663-8","10.1109/MDM48529.2020.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162189","Data Generation;Location-Based Social Networks;Temporal Social Network Data;Social Simulation;Patterns of Life;Trajectory Data Generation;Social Network Data Generation","Social network services;Data models;Data privacy;Query processing;Urban areas;Data science;Benchmark testing","","16","","107","IEEE","7 Aug 2020","","","IEEE","IEEE Conferences"
"Improved Domain Generation Algorithm To Detect Cyber-Attack With Deep Learning Techniques","C. C; P. K. Pareek; V. H. Costa de Albuquerque; A. Khanna; D. Gupta","Department of CSE, BMS Institute of Technology and Management; Department of CSE, Nitte Meenakshi Institute of Technology, Bengaluru and Post Doctoral Fellow, Federal University of Ceara, Brazil; Department of Teleinformatics Engineering, Federal University of Ceará, Fortaleza, Brazil; Dept of CSE, Maharaja Agrasen Institute of Technology GGSIPU; Dept of CSE, Maharaja Agrasen Institute of Technology GGSIPU",2022 IEEE 2nd Mysore Sub Section International Conference (MysuruCon),"13 Dec 2022","2022","","","1","8","Deep learning is a subfield of machine learning (ML) that focuses on the development of artificial intelligence. It is also often referred to by its acronym, DL (AI). This technique lays an emphasis on the use of big capacity, scalable models that are able to construct distributed representations depending on the input data set. This proposed illustrates the generalizability of these methods and the usage of them in a broad range of cyber security investigations that are peculiar to their environment. The neural network models have been continuously refined and extended during the whole of this research in order to achieve greater adaptability. The following is a list of the important contributions that this proposed makes, in order from most significant to least significant: Work is currently being done to create a comprehensive database for the identification of domain names that have been generated by a domain generation algorithm (DGA), as well as a one-of-a-kind architecture that will increase the overall effectiveness of DGA domain name detection. Both of these will help increase overall efficiency. The creation of a hybrid intrusion detection warning system that is founded on a deep neural network (DNN) and that has the capability to monitor network and host-level activities inside an Ethernet local area network (LAN) (LAN). The examination of information gathered from social media platforms, electronic mail (email), and uniform resource locators in order to design a unified, DL-based framework for the detection of spam and phishing (URL). The creation of a technique based on DL for the study of secure shell (SSH) traffic, the categorization of application network traffic, the classification of malicious traffic, and the detection of harmful traffic is being worked on. The name of the new framework that has been suggested, which is called ScaleMalNet, reflects how hybrid and scalable it is. In the first stage, the executables file is classified as malware or genuine by using static and dynamic analysis. In the second stage, the malicious executables _le are grouped into corresponding malware families. This is a two-step process. For the aim of conducting investigations into Android ransomware and malware, an analogous hybrid DL framework is now in the process of being developed. This framework is better in its capacity to detect dangerous software and ransomware on Android when compared to the typical ML-based techniques that are presently in use. These approaches are already in widespread usage. The development of a framework for DNS-based botnet detection and DL-based network intrusion detection is now being worked on in the context of the Internet of things (IoT) and smart cities","","978-1-6654-9790-9","10.1109/MysuruCon55714.2022.9972526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9972526","Local Area Network;Secure Shell;Deep Learning;Artificial Intelligence","Deep learning;Uniform resource locators;Training;Neural networks;Feature extraction;Data models;Ransomware","","15","","22","IEEE","13 Dec 2022","","","IEEE","IEEE Conferences"
"Beyond Von Neumann in the Computing Continuum: Architectures, Applications, and Future Directions","D. Kimovski; N. Saurabh; M. Jansen; A. Aral; A. Al-Dulaimy; A. B. Bondi; A. Galletta; A. V. Papadopoulos; A. Iosup; R. Prodan","University of Klagenfurt, Klagenfurt, Austria; Utrecht University, Utrecht, The Netherlands; Vrije Universiteit Amsterdam, Amsterdam, The Netherlands; Umeå University, Umeå, Sweden; Mälardalen University, Västerås, Sweden; Software Performance and Scalability Consulting LLC, Red Bank, NJ, USA; University of Messina, Messina, Italy; Mälardalen University, Västerås, Sweden; Vrije Universiteit Amsterdam, Amsterdam, The Netherlands; University of Klagenfurt, Klagenfurt, Austria",IEEE Internet Computing,"3 Jun 2024","2024","28","3","6","16","The article discusses emerging non-von Neumann computer architectures and their integration in the computing continuum for supporting modern distributed applications, including artificial intelligence, big data, and scientific computing. It provides a detailed summary of available and emerging non-von Neumann architectures, which range from power-efficient single-board accelerators to quantum and neuromorphic computers. Furthermore, it explores their potential benefits for revolutionizing data processing and analysis in various societal, science, and industry fields. The article provides a detailed analysis of the most widely used class of distributed applications and discusses the difficulties in their execution over the computing continuum, including communication, interoperability, orchestration, and sustainability issues.","1941-0131","","10.1109/MIC.2023.3301010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10207712","","Computer architecture;Quantum computing;Computational modeling;Internet;Artificial intelligence;Neurons;Distributed databases","","10","","20","CCBY","3 Aug 2023","","","IEEE","IEEE Magazines"
"OPUPO: Defending Against Membership Inference Attacks With Order-Preserving and Utility-Preserving Obfuscation","Y. Liu; H. Li; G. Huang; W. Hua","School of Information Engineering, Hangzhou Medical College, Hangzhou, Zhejiang, China; Zhejiang Lab, Hangzhou, Zhejiang, China; Zhejiang Lab, Hangzhou, Zhejiang, China; Zhejiang Lab, Hangzhou, Zhejiang, China",IEEE Transactions on Dependable and Secure Computing,"10 Nov 2023","2023","20","6","4690","4701","In this work, we present OPUPO to protect machine learning classifiers against black-box membership inference attacks by alleviating the prediction difference between training and non-training samples. Specifically, we apply order-preserving and utility-preserving obfuscation to prediction vectors. The order-preserving constraint strictly maintains the order of confidence scores in the prediction vectors, guaranteeing that the model's classification accuracy is not affected. The utility-preserving constraint, on the other hand, enables adaptive distortions to the prediction vectors in order to protect their utility. Moreover, OPUPO is proved to be adversary resistant that even well-informed defense-aware adversaries cannot restore the original prediction vectors to bypass the defense. We evaluate OPUPO on machine learning and deep learning classifiers trained with four popular datasets. Experiments verify that OPUPO can effectively defend against state-of-the-art attack techniques with negligible computation overhead. In specific, the inference accuracy could be reduced from as high as 87.66% to around 50%, i.e., random guess, and the prediction time will increase by only 0.44% on average. The experiments also show that OPUPO could achieve better privacy-utility trade-off than existing defenses.","1941-0018","","10.1109/TDSC.2022.3232111","National Natural Science Foundation of China(grant numbers:52007173,U19B2042); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LQ20E070002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999284","Membership inference attack;machine learning;privacy;defense;obfuscation","Privacy;Predictive models;Closed box;Training;Analytical models;Distortion;Micromechanical devices","","1","","61","IEEE","26 Dec 2022","","","IEEE","IEEE Journals"
"David and Goliath: An Empirical Evaluation of Attacks and Defenses for QNNs at the Deep Edge","M. Costa; S. Pinto","ALGORITMI Research Centre, Universidade do Minho, Portugal; ALGORITMI Research Centre, Universidade do Minho, Portugal",2024 IEEE 9th European Symposium on Security and Privacy (EuroS&P),"22 Aug 2024","2024","","","524","541","Machine learning (ML) is shifting from the cloud to the edge. Edge computing reduces the surface exposing private data and enables reliable throughput guarantees in real-time applications. Of the panoply of devices deployed at the edge, resource-constrained microcontrollers (MCUs), e.g., Arm Cortex-M, are more prevalent, orders of magnitude cheaper, and less power-hungry than application processors (APUs) or graphical processing units (GPUs). Thus, enabling intelligence at the so-called deep/extreme edge is the zeitgeist, with researchers focusing on unveiling novel approaches to deploy artificial neural networks (ANN) on these constrained devices. Quantization is a well-established technique that has proved effective, i.e., negligible impact on accuracy, in enabling the deployment of neural networks on MCUs; however, it is still an open question to understand the robustness of quantized neural networks (QNNs) in the face of well-known adversarial examples. To fill this gap, we empirically evaluate the effectiveness of attacks and defenses from (full-precision) ANNs on (constrained) QNNs. Our evaluation suite includes three QNNs targeting TinyML applications, ten attacks, and six defenses. With this study, we draw a set of interesting findings. First, quantization increases the point distance to the decision boundary and leads the gradient estimated by some attacks to explode or vanish. Second, quantization can act as a noise attenuator or amplifier, depending on the noise magnitude, and causes gradient misalignment. Regarding adversarial defenses, we conclude that input pre-processing defenses show impressive results on small perturbations; however, they fall short as the perturbation increases. At the same time, train-based defenses increase adversarial robustness by increasing the average point distance to the decision boundary, which holds even after quantization. However, we argue that train-based defenses still need to smooth the quantization-shift and gradient misalignment phenomenons to counteract adversarial example transferability to QNNs. All artifacts are open-sourced to enable independent validation of results and encourage further exploration of the robustness of QNNs.","2995-1356","979-8-3503-5425-6","10.1109/EuroSP60621.2024.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10629007","","Quantization (signal);Tiny machine learning;Perturbation methods;Noise;Artificial neural networks;Throughput;Robustness","","","","50","IEEE","22 Aug 2024","","","IEEE","IEEE Conferences"
"Atlassian DevOps Toolchain Cookbook: Recipes for building, automating, and managing applications with Jira, Bitbucket Pipelines, and more","R. Wen; A. Ortiz; E. Gaile; R. Nissen",NA; NA; NA; NA,"Atlassian DevOps Toolchain Cookbook: Recipes for building, automating, and managing applications with Jira, Bitbucket Pipelines, and more","","2024","","","","","Seamlessly integrate Atlassian Open DevOps tools such as Jira, Bitbucket Pipelines, Compass, Confluence, and Opsgenie with other automated testing, monitoring, and security tools such as SonarQube and Snyk for a powerful and agile DevSecOps deployment processKey FeaturesStreamline development progress visibility by connecting Jira with other DevOps tools through Open DevOpsEnhance observability by integrating development tools like Bitbucket Pipelines with CompassLearn best practices for DevSecOps integrations and processes using real-world examplesPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionImplementing DevOps practices and toolchains for automated testing and deployment can accelerate product development with minimal errors in the production environment. However, creating DevOps toolchains by integrating tools from various vendors presents challenges for both administrators and developers. Written by four well-known experts from the Atlassian community, this book addresses the complexities of DevOps toolchain creation and integration by leveraging Atlassian’s Open DevOps solution. Starting with a holistic overview of the DevOps and Atlassian Open DevOps solution, you’ll learn to integrate Jira with other tools. You’ll then find out how to create and integrate a CI/CD pipeline in Bitbucket for automated testing and deployment to Docker containers. With step-by-step guidance, you’ll connect Jira and Bitbucket with other tools, such as Snyk for security and SonarQube for testing, to form an extensive toolchain. You’ll also learn how Compass uses CheckOps for observability and how to use Confluence for documentation and reporting. Finally, you’ll leverage Opsgenie’s ChatOps functionality to enhance collaboration between developers and operations teams. By the end of this book, you’ll be able to establish your DevOps toolchain by integrating Atlassian tools to automate and optimize the software development lifecycle and beyond.What you will learnExtend reporting capabilities in Jira using Open DevOpsIntegrate Jira with popular tools for tracking the build and deployment statusTrack the progress of product ideas with Jira Product DiscoveryDocument and report projects using ConfluenceCreate and deploy CI/CD pipelines in Bitbucket and perform testing in SonarQubeIntegrate security scanning into your CI/CD pipeline using SnykCreate an observability portal in CompassUse Opsgenie to collaborate with other teams when incidents occurWho this book is forThis book is a valuable resource for DevOps engineers, platform engineers, SREs, software developers, and Atlassian tool administrators who want to automate testing, integration, and deployment processes using the Atlassian DevOps toolchain. A basic understanding of DevOps processes will be beneficial. While prior knowledge of administering Atlassian tools may be helpful, it is not necessary, as the recipes cover the administrative tasks needed to implement the DevOps toolchain and practices.","","9781835469644","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10803967.pdf&bkn=10803966&pdfType=book","","","","","","","","16 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"NOTES","T. S. Mullaney",Stanford University,The Chinese Computer: A Global History of the Information Age,"","2024","","","247","311","","","9780262372428","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10497717.pdf&bkn=10492997&pdfType=chapter","","","","","","","","12 Apr 2024","","","MIT Press","MIT Press eBook Chapters"
"The Benacerraf Problem as a Challenge for Ontic Structural Realism","M. D. Beni","Department of Management, Science, and Technology, Amirkabir University of Technology, Tehran, Iran; Department of History, Philosophy, and Religious Studies, Nazarbayev University, Nur-Sultan City, Kazakhstan; majid.beni@nu.edu.kz",Philosophia Mathematica,"8 Jun 2020","2019","28","1","35","59","Benacerraf has presented two problems for the philosophy of mathematics. These are the problem of identification and the problem of representation. This paper aims to reconstruct the latter problem and to unpack its undermining bearing on the version of Ontic Structural Realism that frames scientific representations in terms of abstract structures. I argue that the dichotomy between mathematical structures and physical ones cannot be used to address the Benacerraf problem but strengthens it. I conclude by arguing that versions of OSR that do not rely on mathematical frameworks for representational purposes need not be vulnerable to Benacerraf's second problem.","1744-6406","","10.1093/philmat/nkz022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9110532","","","","","","","","8 Jun 2020","","","OUP","OUP Journals"
"Seeing is Not Always Believing: An Empirical Analysis of Fake Evidence Generators","Z. Hu; J. Ye; Y. Zhang; X. Wang","University of Central Florida, Orlando, USA; University of Central Florida, Orlando, USA; Indiana University Bloomington, Bloomington, USA; University of Central Florida, Orlando, USA",2024 IEEE 9th European Symposium on Security and Privacy (EuroS&P),"22 Aug 2024","2024","","","560","579","Online scams pose a growing threat to the cyberspace, with cybercriminals frequently using fake evidence, such as identification and financial documents, to illicitly elevate their credibility in online activities. This deceptive trend is fueled by an emerging set of fake evidence generators (FEGens). These FeGensreplicate the output of authoritative sources, such as official bank applications, to automatically generate large quantities of authentic-looking fake evidence. To the best of our knowledge, FeGens,as effective tools for cybercriminals, have not been systematically analyzed in terms of their supply chain, including development, promotion, and delivery, as well as the risks and impacts they pose to end users. In this paper, we present the first systematic empirical analysis of FegEnsand related fake evidence. Our findings shed light on the FegEn ecosystem, particularly the tactics employed by FegEndevelopers and retailers to mimic authoritative sources and promote the use of FeGens. We also evaluate the effectiveness of FeGensand associated risks in cybercrime.","2995-1356","979-8-3503-5425-6","10.1109/EuroSP60621.2024.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628523","","Systematics;Supply chains;Ecosystems;Cyberspace;Market research;Generators;Computer crime","","","","106","IEEE","22 Aug 2024","","","IEEE","IEEE Conferences"
"A Holistic Review of the TinyML Stack for Predictive Maintenance","E. Njor; M. A. Hasanpour; J. Madsen; X. Fafoutis","Department of Applied Mathematics and Computer Science, Technical University of Denmark, Kongens, Lyngby, Denmark; Department of Applied Mathematics and Computer Science, Technical University of Denmark, Kongens, Lyngby, Denmark; Department of Applied Mathematics and Computer Science, Technical University of Denmark, Kongens, Lyngby, Denmark; Department of Applied Mathematics and Computer Science, Technical University of Denmark, Kongens, Lyngby, Denmark",IEEE Access,"13 Dec 2024","2024","12","","184861","184882","Downtime caused by failing equipment can be extremely costly for organizations. Predictive Maintenance (PdM), which uses data to predict when maintenance should be conducted, is an essential tool for increasing safety, maximizing uptime and minimizing costs. Contempoary PdM systems primarily have sensors collect information about the equipment under observation. This information is afterwards transmitted off the device for processing at a high-performance computer system. While this can allow high-quality predictions, it also imposes barriers that keep some organisations from adopting PdM. For example, some applications prevent data transmission off sensor devices due to regulatory or infrastructure limitations. Being able to process the collected information right at the sensor device is, therefore, desirable in many sectors - something that recent progress in the field of TinyML promises to deliver. This paper investigates the intersection between PdM and TinyML and explores how TinyML can enable many new PdM applications. We consider a holistic view of TinyML-based PdM, focusing on the full stack of Machine Learning (ML) models, hardware, toolchains, data and PdM applications. Our main findings are that each part of the TinyML stack has received varying degrees of attention. In particular, ML models and their optimisations have seen a lot of attention, while data optimisations and TinyML datasets lack contributions. Furthermore, most TinyML research focuses on image and audio classification, with little attention paid to other application areas such as PdM. Based on our observations, we suggest promising avenues of future research to scale and improve the application of TinyML to PdM.","2169-3536","","10.1109/ACCESS.2024.3512860","Innovation Fund Denmark for the Project Digital Research Centre Denmark (DIREC)(grant numbers:9142-00001B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10781407","Embedded AI;embedded machine learning;optimization;predictive maintenance;resource-constrained systems;TinyML","Tiny machine learning;Surveys;Program processors;Costs;Organizations;Maintenance;Reviews;Microcontrollers;Energy efficiency;Optimization","","","","137","CCBY","9 Dec 2024","","","IEEE","IEEE Journals"
"The Technological Emergence of AutoML: A Survey of Performant Software and Applications in the Context of Industry","A. Scriven; D. J. Kedziora; K. Musial; B. Gabrys",NA; NA; NA; NA,The Technological Emergence of AutoML: A Survey of Performant Software and Applications in the Context of Industry,"","2024","","","","","The Technological Emergence of AutoML presents a comprehensive snapshot of how AutoML has permeated into mainstream use within the early 2020s. This work surveys both their implementation and application in the context of industry. It also defines what a ‘performant’ AutoML system is – HCI support is valued highly here – and assesses how the current crop of available packages and services lives up to expectations. To do so in a systematic manner, this survey is structured as follows. Section 2 begins by elaborating on the notion of an ML workflow, conceptually framing AutoML in terms of the high-level operations required to develop, deploy and maintain an ML model. Section 3 uses this workflow to support the introduction of industry-related stakeholders and their interests/obligations. These requirements are unified into a comprehensive set of criteria, supported by methods of assessment, that determine whether an AutoML system can be considered performant. Section 4 launches the survey in earnest, assessing the nature and capabilities of existing AutoML technology beginning with an examination of open-source AutoML packages. The section additionally investigates AutoML systems that are designed for specific domains, as well as commercial products. Subsequently, Section 5 assesses where AutoML technology has been used and how it has fared. Academic work focusing on real-world applications is surveyed, as are vendor-based case studies. All key findings and assessments are then synthesized in Section 6, with commentary around how mature AutoML technology is, as well as whether there are obstacles and opportunities for future uptake. Finally, Section 7 provides a concluding overview on the technological emergence of AutoML.","","9781638283232","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10410242.pdf&bkn=10410241&pdfType=book","","","","","","","","22 Jan 2024","","","now","Now Foundations and Trends Books"
"RISE with SAP towards a Sustainable Enterprise: Become a value-driven, sustainable, and resilient enterprise using RISE with SAP","A. Zafar; D. Alturi; S. Taur; M. R. Gor",NA; NA; NA; NA,"RISE with SAP towards a Sustainable Enterprise: Become a value-driven, sustainable, and resilient enterprise using RISE with SAP","","2023","","","","","Get insights and guidance on various challenges within the industry and what business levers you can consider to effortlessly lead your business transformation through RISE with SAP Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesGain actionable insights into end-to-end process performance with process analyticsScale performance and reliability to accelerate your journey to the cloud and beyondGet a clear overview of the enabling tools and services you can leverage for the transformationBook DescriptionIf you’re unsure whether adopting SAP S/4HANA is the right move for your enterprise, then this book is for you. This practical and comprehensive guide will help you determine your next steps toward building a business case, while preparing you for all the possible scenarios and enabling you to make informed decisions during implementation. RISEwith SAP toward a Sustainable Enterprise is packed with clear and detailed advice, including a run-through of what it takes to design the landscape using RISE with SAP. As you go through the chapters, you’ll get a solid understanding of precisely what services are available (such as Process Discovery, data migration, the fit-to-standard approach), and which scope items on RISE with SAP should be considered, allowing you to make the most of RISE with the SAP-based model. Finally, you’ll get an overview of different industry-based use cases and how they can be brought to reality with the platform that’s set up on the RISE with SAP offering. By the end of this book, you’ll be able to build a detailed business case to determine if RISE with SAP is the right transformation engine for you, along with a clear idea of optimized landscape design on RISE with SAP that addresses the pain points for your implementation and support activities. What you will learnUnderstand the challenges faced by organizations and CXOs with the emerging market trendsKnow what to consider when creating a business case for RISE with SAPExplore deployment options within RISE with SAP and other functional and non-functional servicesUnderstand optimized landscape design on RISE with SAP along with effective implementation and supportTake the optimum approach in adopting S/4HANA with levers like Process Discovery, testing, and automationDiscover possibilities when dealing with SAP, the vendor ecosystem, and cloud products driven by industriesWho this book is forThis book is for CXOs and solutions and enterprise architects who’ve been working in the SAP ecosystem and want practical and concise advice on how to get up and running with the adoption of S/4HANA by leveraging RISE with SAP as the enabling engine. This book is also for professionals working toward creating a business case and trying to identify all possible best practices around the adoption of RISE with SAP and associated industry use cases. Prior experience with either SAP or a different ERP will help you get the most out of this book.","","9781801819817","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162586.pdf&bkn=10162585&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"A Survey on Security of Automatic Dependent Surveillance -Broadcast (ADS-B) Protocol: Challenges, Potential Solutions and Future Directions","H. A. Khan; H. Khan; S. Ghafoor; M. A. Khan","College of Aeronautical Engineering, National University of Sciences and Technology, Islamabad, Pakistan; College of Aeronautical Engineering, National University of Sciences and Technology, Islamabad, Pakistan; SEECS, National University of Sciences and Technology (NUST), Pakistan; College of Aeronautical Engineering, National University of Sciences and Technology, Islamabad, Pakistan",IEEE Communications Surveys & Tutorials,"","2024","PP","99","1","1","This work delves into critical examination of the broadcast data safety of Automatic Dependent Surveillance-Broadcast (ADS-B) system, an essential protocol for aircraft identification and navigation. Globally mandated by civil aviation regulatory bodies, ADS-B plays a pivotal role in shaping the future of Air Traffic Management initiatives. This study thoroughly investigates the vulnerabilities inherent in the open and un-encrypted nature of ADS-B data transmission. Given the widespread availability of Software-Defined Radios (SDRs), these security threats pose significant risks to Air Traffic Services and passenger safety. In light of these challenges, the paper scrutinises existing research and industry documents to comprehensively understand ADS-B vulnerabilities and assess threat levels and potential attacks. We also review recent developments and analyze proposed countermeasures aimed at enhancing the security of ADS-B data, possibly through protocol modifications or infrastructure enhancements.","1553-877X","","10.1109/COMST.2024.3513213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10783027","ADS-B;Air Traffic Control;Authentication;Aviation;broadcast;data security;encryption;privacy;Private Key;transponders;wireless","Security;Surveys;Jamming;Aircraft;Air traffic control;Protocols;Tutorials;Authentication;Surveillance;Kalman filters","","","","","IEEE","9 Dec 2024","","","IEEE","IEEE Early Access Articles"
"IEEE Draft Recommended Practice for the Quality Management of Datasets for Medical Artificial Intelligence","",,"IEEE P2801/D3.0, November 2021","1 Feb 2022","2022","","","1","36","Promoted in this recommended practice are quality management activities for datasets used for artificial intelligence medical devices (AIMD). The document highlights quality objectives for organizations responsible for datasets. The document describes control of records during the lifecycle of datasets, including but not limited to data collection, annotation, transfer, utilization, storage, maintenance, updates, retirement, and other activities. The document emphasizes special consideration for the dataset quality management system, including but not limited to responsibility management, resource management, dataset realization, and quality control.","","978-1-5044-8401-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9700986","AIMD;artificial intelligence medical device;datasets;IEEE 2801;quality management","IEEE Standards;Medical devices;Artificial intelligence;Quality of service;Data models","","","","","","1 Feb 2022","","","IEEE","IEEE Standards"
"RAM-Based Firmware Attestation for IoT Security: A Representation Learning Framework","A. Iqbal; U. Zia; M. N. Aman; B. Sikdar","Department of Electrical and Computer Engineering, National University of Singapore, Cluny Road, Singapore; Department of Computer Systems Engineering, University of Engineering and Technology Peshawar, Peshawar, Pakistan; School of Computing, University of Nebraska–Lincoln, Lincoln, NE, USA; Department of Electrical and Computer Engineering, National University of Singapore, Cluny Road, Singapore",IEEE Internet of Things Journal,"25 Oct 2024","2024","11","21","35124","35140","With the proliferation of 4G and 5G mobile networks in smart cities, the adoption of Internet of Things (IoT) devices has surged, emphasizing the critical need for robust security measures. Existing firmware attestation techniques often require high computational budget or access to the device’s authentic firmware, posing challenges due to resource and proprietary constraints. To counter these two fundamental challenges, this article introduces a novel software-based attestation framework utilizing RAM traces from IoT devices for remote verification. In the proposed framework, the need for an authentic firmware copy is eliminated, and the most computationally intensive task is assigned to the gateway node of the IoT ecosystem. This approach yields a robust and highly accurate device attestation strategy, while imposing minimal computational demands on the verification device itself. Employing deep learning models trained in a representation learning paradigm, our framework enables the remote verifier to authenticate the internal state of IoT devices. Leveraging data collected from real-world prototype devices, under eight different applications, our approach achieves a remarkable 100% accuracy in detecting critical attacks on IoT devices with a false positive rate of  $10^{-3}$ . Notably, our framework preserves device availability and maintains low authentication latency, underscoring its efficacy and practicality for securing IoT ecosystems.","2327-4662","","10.1109/JIOT.2024.3436057","National Research Foundation, Singapore, and Infocomm Media Development Authority under its Future Communications Research Development Programme(grant numbers:FCP-NUS-RG-2022-019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10616208","Device attestation;firmware;Internet of Things (IoT);RAM trace;variational autoencoder (VAE)","Internet of Things;Microprogramming;Random access memory;Security;Task analysis;Hardware;Logic gates","","","","42","IEEE","31 Jul 2024","","","IEEE","IEEE Journals"
"Computation System and Information Technology for Sustainable Solutions","",,2023 7th International Conference on Computation System and Information Technology for Sustainable Solutions (CSITSS),"7 Dec 2023","2023","","","1","184","Computation System and Information Technology for Sustainable Solutions.","","979-8-3503-4314-4","10.1109/CSITSS60515.2023.10334182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10334182","","Information technology;Transportation;Task analysis;Service robots;Renewable energy sources;Medical services;Manufacturing","","","","","IEEE","7 Dec 2023","","","IEEE","IEEE Conferences"
"14 Conversations with Computer Artists","M. A. Boden; E. A. Edmonds",University of Sussex; De Montfort University,From Fingers to Digits: An Artificial Aesthetic,"","2019","","","265","364","This chapter presents a set of conversations with artists who use computer-based systems in various ways, primarily via writing of computer programs. Following short introductions, the conversations are presented in full for readers to develop their own understandings from the point of view of the artists' voices.","","9780262352093","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8751708.pdf&bkn=8746070&pdfType=chapter","","","","","","","","1 Jul 2019","","","MIT Press","MIT Press eBook Chapters"
"FedMark: Privacy-Preserving Federated Learning-Based Watermarking for Large-Scale Image Datasets","H. K. Singh; K. N. Singh; A. K. Singh","Department of CSE, Government Engineering College Siwan, Siwan, Bihar, India; Department of CSE, Jaypee institute of information technology, Noida, UP, India; Department of CSE, National Institute of Technology, Patna, Bihar, India",IEEE Transactions on Consumer Electronics,"","2024","PP","99","1","1","With the accelerated advancement of consumer devices and multimedia editing software, the manipulation and sharing of digital images have become ubiquitous. While these functions enhance user convenience in image editing, they also face more threats, such as data leakage and information theft. Deep learning-based watermarking provides a unique method of digital-image protection. However, it is challenging for existing approaches to provide an effective solution for privacy, generalisation, and scalability at the same time. This study proposes a federated learning-based watermarking framework, called FedMark, to improve the robustness and imperceptibility of watermarks in large-scale image datasets. It enables collaborative model training across distributed consumer devices while maintaining data privacy and model generalisation and scalability across diverse datasets. Empirical validation across multiple datasets shows that FedMark consistently outperforms existing methods with significantly improvement of 36.8 % in terms of robustness and 48.2 % in terms of imperceptibility while ensuring reversibility and maintaining stringent security standards. With its combination of federated learning and advanced watermarking techniques, FedMark is a promising step towards a secure, privacy-preserving future for digital-image watermarking.","1558-4127","","10.1109/TCE.2024.3481043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10716791","Privacy;Digital watermarking;Federated Learning;Robustness;Consumers electronics","Watermarking;Training;Feature extraction;Data models;Computational modeling;Robustness;Servers;Consumer electronics;Data privacy;Scalability","","","","","IEEE","15 Oct 2024","","","IEEE","IEEE Early Access Articles"
"Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding","S. Abdelnabi; M. Fritz",CISPA Helmholtz Center for Information Security; CISPA Helmholtz Center for Information Security,2021 IEEE Symposium on Security and Privacy (SP),"26 Aug 2021","2021","","","121","140","Recent advances in natural language generation have introduced powerful language models with high-quality output text. However, this raises concerns about the potential misuse of such models for malicious purposes. In this paper, we study natural language watermarking as a defense to help better mark and trace the provenance of text. We introduce the Adversarial Watermarking Transformer (AWT) with a jointly trained encoder-decoder and adversarial training that, given an input text and a binary message, generates an output text that is unobtrusively encoded with the given message. We further study different training and inference strategies to achieve minimal changes to the semantics and correctness of the input text.AWT is the first end-to-end model to hide data in text by automatically learning -without ground truth- word substitutions along with their locations in order to encode the message. We empirically show that our model is effective in largely preserving text utility and decoding the watermark while hiding its presence against adversaries. Additionally, we demonstrate that our method is robust against a range of attacks.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519400","","Training;Data privacy;Smoothing methods;Natural languages;Semantics;Watermarking;Robustness","","29","","93","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"Unleashing the Potential of Conversational AI: Amplifying Chat-GPT’s Capabilities and Tackling Technical Hurdles","V. Hassija; A. Chakrabarti; A. Singh; V. Chamola; B. Sikdar","School of Computing, National University of Singapore, Queenstown, Singapore; School of Computer Science, KIIT, Bhubaneshwar, India; School of Computer Science, KIIT, Bhubaneshwar, India; Department of Electrical and Electronics Engineering, BITS Pilani, Pilani Campus, Pilani, India; Department of Electrical and Computer Engineering, National University of Singapore, Queenstown, Singapore",IEEE Access,"25 Dec 2023","2023","11","","143657","143682","Conversational AI has seen a growing interest among government, researchers, and industrialists. This comprehensive survey paper provides an in-depth analysis of large language models, specifically focusing on ChatGPT. This paper discusses the architecture, training process, and challenges associated with large language models, including bias, interpretability, and ethics. It explores various applications of ChatGPT and examines future research trends, such as improving model generalization, addressing data scarcity, and integrating multimodal capabilities. This survey also serves as a roadmap for researchers, practitioners, and policymakers, offering valuable insights into the current state and future potential of large language models and ChatGPT.","2169-3536","","10.1109/ACCESS.2023.3339553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10343095","Large language models;ChatGPT;natural language processing;deep learning;neural networks;transformer models;pre-training and fine-tuning;language generation;text completion;model interpretability;bias in language models;ethics in AI;data scarcity;multimodal models;generalization;conversational AI;language understanding;text classification;sentiment analysis;dialogue systems","Chatbots;Transformers;Artificial intelligence;Context modeling;Usability;Surveys;Analytical models;Natural language processing;Deep learning;Neural networks;Sentiment analysis","","19","","191","CCBYNCND","5 Dec 2023","","","IEEE","IEEE Journals"
"Semantics-Aware Privacy Risk Assessment Using Self-Learning Weight Assignment for Mobile Apps","J. Chen; C. Wang; K. He; Z. Zhao; M. Chen; R. Du; G. -J. Ahn","School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; Rochester Institute of Technology, Rochester, NY, USA; Huazhong University of Science and Technology, Wuhan, Hubei, China; School of Cyber Science and Engineering, and the Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, Hubei, China; Arizona State University, Tempe, AZ, USA",IEEE Transactions on Dependable and Secure Computing,"7 Jan 2021","2021","18","1","15","29","Most of the existing mobile application (app) vetting mechanisms only estimate risks at a coarse-grained level by analyzing app syntax but not semantics. We propose a semantics-aware privacy risk assessment framework (SPRisk), which considers the sensitivity discrepancy of privacy-related factors at semantic level. Our framework can provide qualitative (i.e., risk level) and quantitative (i.e., risk score) assessment results, both of which help users make decisions to install an app or not. Furthermore, to find the reasonable weight distribution of each factor automatically, we exploit a self-learning weight assignment method, which is based on fuzzy clustering and knowledge dependency theory. We implement a prototype system and evaluate the effectiveness of SPRisk with 192,445 normal apps and 7,111 malicious apps. A measurement study further reveals some interesting findings, such as the privacy risk distribution of Google Play Store, the diversity of official and unofficial marketplaces, which provide insights into understanding the seriousness of privacy threat in the Android ecosystem.","1941-0018","","10.1109/TDSC.2018.2871682","National Natural Science Foundation of China(grant numbers:U1836202,61772383,61572380,61702379); Joint Foundation of Ministry of Education(grant numbers:6141A02033341); Foundation of Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:JCYJ20170303170108208); Foundation of Collaborative Innovation Center of Geospatial Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8470120","Android;semantics-aware;self-learning weight assignment;privacy risk assessment","Privacy;Risk management;Sensitivity;Androids;Humanoid robots;Data privacy;Semantics","","11","","42","IEEE","23 Sep 2018","","","IEEE","IEEE Journals"
"The Ethics of AI in Games","D. Melhart; J. Togelius; B. Mikkelsen; C. Holmgård; G. N. Yannakakis","modl.ai, Copenhagen, Denmark; modl.ai, Copenhagen, Denmark; modl.ai, Copenhagen, Denmark; modl.ai, Copenhagen, Denmark; modl.ai, Copenhagen, Denmark",IEEE Transactions on Affective Computing,"29 Feb 2024","2024","15","1","79","92","Video games are one of the richest and most popular forms of human-computer interaction and, hence, their role is critical for our understanding of human behaviour and affect at a large scale. As artificial intelligence (AI) tools are gradually adopted by the game industry a series of ethical concerns arise. Such concerns, however, have so far not been extensively discussed in a video game context. Motivated by the lack of a comprehensive review on the ethics of AI as applied to games, we survey the current state of the art in this area and discuss ethical considerations of these systems from the holistic perspective of the affective loop. Through the components of this loop, we study the ethical challenges that AI faces in video game development. Elicitation highlights the ethical boundaries of artificially induced emotions; sensing showcases the trade-off between privacy and safe gaming spaces; and detection, as utilised during in-game adaptation, poses challenges to transparency and ownership. This paper calls for an open dialogue and action for the games of today and the virtual spaces of the future. By setting an appropriate framework we aim to protect users and to guide developers towards safer and better experiences for their customers.","1949-3045","","10.1109/TAFFC.2023.3276425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10125072","Artificial intelligence;ethics;video games;affective computing","Artificial intelligence;Games;Ethics;Computational modeling;Industries;Data models;Video games","","9","","119","IEEE","16 May 2023","","","IEEE","IEEE Journals"
"Jointly Composite Feature Learning and Autism Spectrum Disorder Classification Using Deep Multi-Output Takagi-Sugeno-Kang Fuzzy Inference Systems","Z. Lu; J. Wang; R. Mao; M. Lu; J. Shi","Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Joint International Research Laboratory of Specialty Fiber Optics and Advanced Communication, Shanghai Institute for Advanced Communication and Data Science, School of Communication and Information Engineering, Shanghai University, Shanghai, China; Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Joint International Research Laboratory of Specialty Fiber Optics and Advanced Communication, Shanghai Institute for Advanced Communication and Data Science, School of Communication and Information Engineering, Shanghai University, Shanghai, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, Guangdong, China; Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Joint International Research Laboratory of Specialty Fiber Optics and Advanced Communication, Shanghai Institute for Advanced Communication and Data Science, School of Communication and Information Engineering, Shanghai University, Shanghai, China",IEEE/ACM Transactions on Computational Biology and Bioinformatics,"3 Feb 2023","2023","20","1","476","488","Autism spectrum disorder (ASD) is characterized by poor social communication abilities and repetitive behaviors or restrictive interests, which has brought a heavy burden to families and society. In many attempts to understand ASD neurobiology, resting-state functional magnetic resonance imaging (rs-fMRI) has been an effective tool. However, current ASD diagnosis methods based on rs-fMRI have two major defects. First, the instability of rs-fMRI leads to functional connectivity (FC) uncertainty, affecting the performance of ASD diagnosis. Second, many FCs are involved in brain activity, making it difficult to determine effective features in ASD classification. In this study, we propose an interpretable ASD classifier DeepTSK, which combines a multi-output Takagi-Sugeno-Kang (MO-TSK) fuzzy inference system (FIS) for composite feature learning and a deep belief network (DBN) for ASD classification in a unified network. To avoid the suboptimal solution of DeepTSK, a joint optimization procedure is employed to simultaneously learn the parameters of MO-TSK and DBN. The proposed DeepTSK was evaluated on datasets collected from three sites of the Autism Brain Imaging Data Exchange (ABIDE) database. The experimental results showed the effectiveness of the proposed method, and the discriminant FCs are presented by analyzing the consequent parameters of Deep MO-TSK.","1557-9964","","10.1109/TCBB.2022.3163140","Science and Technology Commission of Shanghai Municipality(grant numbers:20ZR1419900); National Natural Science Foundation of China(grant numbers:62072311); Higher Education Discipline Innovation Project(grant numbers:D20031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9744509","Autism spectrum disorder;Resting-state functional magnetic resonance imaging;multi-output TSK fuzzy inference system;deep belief network","Feature extraction;Functional magnetic resonance imaging;Uncertainty;Representation learning;Correlation;Takagi-Sugeno model;Optimization","Humans;Autism Spectrum Disorder;Brain;Brain Mapping;Autistic Disorder;Magnetic Resonance Imaging","7","","53","IEEE","29 Mar 2022","","","IEEE","IEEE Journals"
"Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in Deep Neural Networks","S. Lee; W. Song; S. Jana; M. Cha; S. Son","School of Computing, KAIST, Daejeon, South Korea; School of Computing, KAIST, Daejeon, South Korea; Department of Computer Science, Columbia University, New York, NY, USA; School of Computing, KAIST, Daejeon, South Korea; School of Computing, KAIST, Daejeon, South Korea",IEEE Transactions on Dependable and Secure Computing,"10 Jul 2023","2023","20","4","3434","3448","Trigger set-based watermarking schemes have gained emerging attention as they provide a means to prove ownership for deep neural network model owners. In this paper, we argue that state-of-the-art trigger set-based watermarking algorithms do not achieve their designed goal of proving ownership. We posit that this impaired capability stems from two common experimental flaws that the existing research practice has committed when evaluating the robustness of watermarking algorithms: (1) incomplete adversarial evaluation and (2) overlooked adaptive attacks. We conduct a comprehensive adversarial evaluation of 11 representative watermarking schemes against six of the existing attacks and demonstrate that each of these watermarking schemes lacks robustness against at least two non-adaptive attacks. We also propose novel adaptive attacks that harness the adversary's knowledge of the underlying watermarking algorithm of a target model. We demonstrate that the proposed attacks effectively break all of the 11 watermarking schemes, consequently allowing adversaries to obscure the ownership of any watermarked model. We encourage follow-up studies to consider our guidelines when evaluating the robustness of their watermarking schemes via conducting comprehensive adversarial evaluation that includes our adaptive attacks to demonstrate a meaningful upper bound of watermark robustness.","1941-0018","","10.1109/TDSC.2022.3196790","Institute of Information & Communications Technology Planning & Evaluation; Korea government (MSIT)(grant numbers:2020-0-00153); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851498","Deep neural networks;watermark removal attacks;backdoor attacks;watermark robustness;trigger set-based watermarks","Watermarking;Robustness;Adaptation models;Training;Computational modeling;Neural networks;Upper bound","","7","","59","IEEE","5 Aug 2022","","","IEEE","IEEE Journals"
"A Cross-Age Face Recognition Approach Using Fog Computing Architecture for User Authentication on Mobile Devices","P. Wang; W. -H. Lin; B. -H. Wu; K. -M. Chao; C. -C. Lo","Department of Information Management, Kun Shan University, Tainan, Taiwan; Department of Information Management, Kun Shan University, Tainan, Taiwan; Department of Information Management, Kun Shan University, Tainan, Taiwan; Professor of Engineering and Computing, Coventry University, UK; Institute of Information Management, National Chiao Tung University, Hsinchu, Taiwan",2018 IEEE 15th International Conference on e-Business Engineering (ICEBE),"30 Dec 2018","2018","","","86","93","Mobile commerce security needs quality services where mobile devices respond in real-time and traditional cloud computing approach uses a centralized architecture does not support these systems with such time dependency well. Thus, this study proposes a cross-age face-recognition model with MobileNets to determine the identity of a user in the situation of aging appearance change on a fog computing architecture. Especially, this study uses a face-recognition technique based on a distributed concept. In this concept, MobileNet-based face recognition system effectively performs cross-age face recognition for identity authentication with the enhanced facial features of the users locally. Overall, the MobileNet model reaches acceptable prediction accuracy with lower latency in face recognition process compared with that of the existing CNNs schemes.","","978-1-5386-7992-0","10.1109/ICEBE.2018.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8592634","cross-age face recognition, user authentication, fog computing architecture, MobileNets, convolutional neuron networks","Face recognition;Mobile handsets;Edge computing;Authentication;Computational modeling;Cloud computing;Facial features","","4","","17","IEEE","30 Dec 2018","","","IEEE","IEEE Conferences"
"Berkeley Open Extended Reality Recordings 2023 (BOXRR-23): 4.7 Million Motion Capture Recordings from 105,000 XR Users","V. Nair; W. Guo; R. Wang; J. F. O'Brien; L. Rosenberg; D. Song","UC Berkeley, US; Purdue University, US; Carnegie Mellon, US; UC Berkeley, US; Unanimous AI, US; UC Berkeley, US",IEEE Transactions on Visualization and Computer Graphics,"19 Apr 2024","2024","30","5","2239","2246","Extended reality (XR) devices such as the Meta Quest and Apple Vision Pro have seen a recent surge in attention, with motion tracking “telemetry” data lying at the core of nearly all XR and metaverse experiences. Researchers are just beginning to understand the implications of this data for security, privacy, usability, and more, but currently lack large-scale human motion datasets to study. The BOXRR-23 dataset contains 4,717,215 motion capture recordings, voluntarily submitted by 105,852 XR device users from over 50 countries. BOXRR-23 is over 200 times larger than the largest existing motion capture research dataset and uses a new, highly efficient and purpose-built XR Open Recording (XROR) file format.","1941-0506","","10.1109/TVCG.2024.3372087","Minderoo foundation; National Science Foundation; National Physical Science Consortium; Fannie and John Hertz Foundation; Berkeley Center for Responsible, Decentralized Intelligence (RDI); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10458406","Dataset;virtual reality;extended reality;motion capture;big data","Recording;Motion capture;X reality;Brushes;Tracking;Games;Internet","","4","","58","IEEE","4 Mar 2024","","","IEEE","IEEE Journals"
"An Imperceptible Data Augmentation Based Blackbox Clean-Label Backdoor Attack on Deep Neural Networks","C. Xu; W. Liu; Y. Zheng; S. Wang; C. -H. Chang","School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore",IEEE Transactions on Circuits and Systems I: Regular Papers,"18 Dec 2023","2023","70","12","5011","5024","Deep neural networks (DNNs) have permeated into many diverse application domains, making them attractive targets of malicious attacks. DNNs are particularly susceptible to data poisoning attacks. Such attacks can be made more venomous and harder to detect by poisoning the training samples without changing their ground-truth labels. Despite its pragmatism, the clean-label requirement imposes a stiff restriction and strong conflict in simultaneous optimization of attack stealth, success rate, and utility of the poisoned model. Attempts to circumvent the pitfalls often lead to a high injection rate, ineffective embedded backdoors, unnatural triggers, low transferability, and/or poor robustness. In this paper, we overcome these constraints by amalgamating different data augmentation techniques for the backdoor trigger. The spatial intensities of the augmentation methods are iteratively adjusted by interpolating the clean sample and its augmented version according to their tolerance to perceptual loss and augmented feature saliency to target class activation. Our proposed attack is comprehensively evaluated on different network models and datasets. Compared with state-of-the-art clean-label backdoor attacks, it has lower injection rate, stealthier poisoned samples, higher attack success rate, and greater backdoor mitigation resistance while preserving high benign accuracy. Similar attack success rates are also demonstrated on the Intel Neural Compute Stick 2 edge AI device implementation of the poisoned model after weight-pruning and quantization.","1558-0806","","10.1109/TCSI.2023.3298802","National Research Foundation Singapore; Cyber Security Agency of Singapore under its National Cybersecurity Research and Development Program/Cyber-Hardware Forensic and Assurance Evaluation Research and Development Program(grant numbers:NRF2018NCR-NCR009-0001,CHFA-GC1-AW01); Ministry of Education, Singapore, through the Academic Research Fund Tier 2(grant numbers:MOE-T2EP50220-0003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10208211","Clean-label backdoor attack;data augmentation;data poisoning;deep neural networks;edge AI","Training;Neurons;Closed box;Artificial neural networks;Perturbation methods;Edge computing;Data augmentation;Data augmentation;Artificial neural networks","","2","","56","IEEE","4 Aug 2023","","","IEEE","IEEE Journals"
"A Framework for Realistic Simulation of Daily Human Activity","I. Idrees; S. Singh; K. Xu; D. F. Glas","Dept. of Computer Science, Brown University, Providence, RI, USA; Amazon Lab126, 1100 Enterprise Way, Sunnyvale, CA, USA; Amazon Lab126, 1100 Enterprise Way, Sunnyvale, CA, USA; Amazon Lab126, 1100 Enterprise Way, Sunnyvale, CA, USA",2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),"13 Nov 2023","2023","","","30","37","For social robots like Astro which interact with and adapt to the daily movements of users within the home, realistic simulation of human activity is needed for feature development and testing. This paper presents a framework for simulating daily human activity patterns in home environments at scale, supporting manual configurability of different personas or activity patterns, variation of activity timings, and testing on multiple home layouts. We introduce a method for specifying day-to-day variation in schedules and present a bidirectional constraint propagation algorithm for generating schedules from templates. We validate the expressive power of our framework through a use case scenario analysis and demonstrate that our method can be used to generate data closely resembling human behavior from three public datasets and a self-collected dataset. Our contribution supports systematic testing of social robot behaviors at scale, enables procedural generation of synthetic datasets of human movement in different households, and can help minimize bias in training data, leading to more robust and effective robots for home environments.","1944-9437","979-8-3503-3670-2","10.1109/RO-MAN57019.2023.10309457","Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10309457","","Schedules;Systematics;Social robots;Training data;Manuals;Behavioral sciences;Timing","","1","","19","IEEE","13 Nov 2023","","","IEEE","IEEE Conferences"
"Edge-Intelligence-Based Seismic Event Detection Using a Hardware-Efficient Neural Network With Field-Programmable Gate Array","Y. Zhu; S. Zhao; F. Zhang; W. Wei; F. Zhao","School of Information Engineering, Beijing Institute of Petrochemical Technology, Beijing, China; School of Information Engineering, Beijing Institute of Petrochemical Technology, Beijing, China; College of Instrumentation and Electrical Engineering, Jilin University, Changchun, China; School of Information Engineering, Beijing Institute of Petrochemical Technology, Beijing, China; College of Instrumentation and Electrical Engineering, Jilin University, Changchun, China",IEEE Internet of Things Journal,"8 Mar 2024","2024","11","6","9432","9443","This article presents a neural network model based on edge intelligence for seismic event detection. We implemented the model in hardware using a field-programmable gate array (FPGA) to achieve in-situ detection of seismic events at acquisition nodes or edge nodes. We designed and implemented the model, focusing on its suitability for hardware implementation on FPGA, employing an encoder—decoder structure. The encoder incorporates reparameterization and depthwise separable convolutions. During training, a multibranch structure was employed, which was then converted to an equivalent single-branch structure during inference to reduce model complexity and parameters. The features extracted by the encoder were further learned by the bi-directional long short-term memory (Bi-LSTM) network and then fed into the decoder for classification. We evaluated the model using the stanford earthquake data set (STEAD) and observed a 70% reduction in parameters while achieving comparable detection performance to EQTransformer. Furthermore, the model structure is well-suited for hardware implementation on FPGA. Applying this model to edge devices for seismic event detection can effectively minimize redundant data transmission and enable in-situ quality control.","2327-4662","","10.1109/JIOT.2023.3323331","National Natural Science Foundation of China(grant numbers:42104175); Research and Development Program of Beijing Municipal Education Commission(grant numbers:KM202210017006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275085","Edge intelligence;field-programmable gate array (FPGA);hardware efficient;neural network;seismic event detection","Computational modeling;Image edge detection;Feature extraction;Data models;Training;Hardware;Field programmable gate arrays","","1","","41","IEEE","10 Oct 2023","","","IEEE","IEEE Journals"
"Efficient Hardware Implementation for Hyperspectral Anomaly and Target Detection","J. Lei; W. Xie; J. Li; K. Wang; K. Liu; Y. Li","State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an, China",Advances in Hyperspectral Image Processing Techniques,"","2023","","","67","105","Hyperspectral anomaly and target detection are two key solutions for detecting objects of interest in hyperspectral imagery with abundant spectral information, according to whether or not a priori knowledge of the object is provided, respectively. The detection has important applications in rare mineral exploration, military target reconnaissance, unknown detection in deep space or deep sea, and so on. The importance of real‐time detection has recently been realized and recognized in many fields, especially for military. Because moving targets such as tanks, missiles, and unmanned aerial vehicles pose a real threat to ground troops. In this chapter, we retrospect some traditional methods and designed novel detections for real‐time processing, such as deep pipelined background statistics (DPBS)‐constrained energy minimization (CEM), deep brief network (DBN)‐Reed‐Xiaoli anomaly detector (RXD), Fast‐automatic target‐generation process (ATGP), and Fast‐morphological reconstruction and the simplified guided filtering detector (MGD). In experiments, our proposed methods are conducted on six data sets including TE1, HyMap, ABU, Cuprite, San Diego, and HYDICE. And these detection algorithms are deployed on the FPGA, and more importantly, model‐based design is considered for hardware implementation. With the validation by the different metrics (AUC scores, LUTs, FFs, time, and so on), our proposed methods accomplish a trade‐off between detection accuracy and inference speed.","","9781119687757","10.1002/9781119687788.ch3","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9951649.pdf&bkn=9951409&pdfType=chapter","","Object detection;Hyperspectral imaging;Anomaly detection;Field programmable gate arrays;Real-time systems;Principal component analysis;Logistics","","","","","","15 Nov 2022","","","IEEE","Wiley-IEEE Press eBook Chapters"
"Leveraging Deep Learning to Strengthen the Cyber-Resilience of Renewable Energy Supply Chains: A Survey","M. N. Halgamuge","Department of Information Systems and Business Analytics, RMIT University, Melbourne, VIC, Australia",IEEE Communications Surveys & Tutorials,"22 Aug 2024","2024","26","3","2146","2175","Deep learning shows immense potential for strengthening the cyber-resilience of renewable energy supply chains. However, research gaps in comprehensive benchmarks, real-world model evaluations, and data generation tailored to the renewable domain persist. This study explores applying state-of-the-art deep learning techniques to secure renewable supply chains, drawing insights from over 300 publications. We aim to provide an updated, rigorous analysis of deep learning applications in this field to guide future research. We systematically review literature spanning 2020–2023, retrieving relevant articles from major databases. We examine deep learning’s role in intrusion/anomaly detection, supply chain cyberattack detection frameworks, security standards, historical attack analysis, data management strategies, model architectures, and supply chain cyber datasets. Our analysis demonstrates deep learning enables renewable supply chain anomaly detection by processing massively distributed data. We highlight crucial model design factors, including accuracy, adaptation capability, communication security, and resilience to adversarial threats. Comparing 18 major historical attacks informs risk analysis. We also showcase potential deep learning architectures, evaluating their relative strengths and limitations in security applications. Moreover, our review emphasizes best practices for renewable data curation, considering quality, labeling, access efficiency, and governance. Effective deep learning integration necessitates tailored benchmarks, model tuning guidance, and renewable energy data generation. Our multi-dimensional analysis motivates focused efforts on enhancing detection explanations, securing communications, continually retraining models, and establishing standardized assessment protocols. Overall, we provide a comprehensive roadmap to progress renewable supply chain cyber-resilience leveraging deep learning’s immense potential.","1553-877X","","10.1109/COMST.2024.3365076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10433003","Renewable energy system;supply chain;cyber security;deep learning;wind energy;solar energy;hydropower","Renewable energy sources;Supply chains;Deep learning;Security;Surveys;Computer security;Internet of Things","","","","326","IEEE","12 Feb 2024","","","IEEE","IEEE Journals"
"4 What's Your Cognitive Strategy?","T. H. Davenport; P. Michelman",Babson College; Massachusetts Institute of Technology,The AI Advantage: How to Put the Artificial Intelligence Revolution to Work,"","2018","","","61","98","Artificial intelligence or cognitive technologies are burgeoning in the business world, but many companies are not yet getting strategic value from their projects and investments.1 Their initiatives are not targeted at important business problems or opportunities. They may lack critical resources needed to achieve substantial projects. As one AI startup CEO put it, “AI has enormous promise but also a 1% problem. Less than 10 companies in the world are achieving the full potential of AI and the rest are really struggling.”2 This may be slightly exaggerated, but there is definitely truth to it.","","9780262350631","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8555411.pdf&bkn=8555410&pdfType=chapter","","Artificial intelligence;Companies;IEEE Sections;Automation;Investment","","","","","","13 Dec 2018","","","MIT Press","MIT Press eBook Chapters"
"Navigating the Soundscape of Deception: A Comprehensive Survey on Audio Deepfake Generation, Detection, and Future Horizons","T. M. Wani; S. A. A. Qadri; F. A. Wani; I. Amerini",NA; NA; NA; NA,"Navigating the Soundscape of Deception: A Comprehensive Survey on Audio Deepfake Generation, Detection, and Future Horizons","","2024","","","","","The rise of audio deepfakes presents a significant security threat that undermines trust in digital communications and media. These synthetic audio technologies can convincingly mimic a person’s voice, enabling malicious activities like impersonation, fraud, and misinformation. Addressing this growing threat requires robust detection systems to ensure the authenticity of digital content. In this monograph, a comprehensive analysis of the state-of-the-art techniques in audio deepfake generation and detection is provided. Various methods used to generate audio deepfakes are examined, including Text-to-Speech (TTS) and Voice Conversion (VC) technologies, and their capabilities in producing highly realistic synthetic audio are discussed. On the detection front, a wide range of approaches are explored, encompassing traditional machine learning and deep learning models for feature extraction and classification. The importance of publicly available datasets for training and evaluating these models is emphasized, showcasing their role in advancing detection capabilities. Additionally, the integration of audio and video deepfake detection systems is discussed, providing a comprehensive defense against sophisticated attacks. This monograph critically assesses existing methods and datasets, highlighting challenges like the high realism of deepfakes, limited data diversity, and the need for models that generalize well. It aims to guide future research in enhancing detection and safeguarding digital media integrity.","","9781638284932","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10771667.pdf&bkn=10771666&pdfType=book","Privacy and Security","","","","","","","29 Nov 2024","","","now","Now Foundations and Trends Books"
"5 Intromission","C. Therrien",Université de Montréal,The Media Snatcher: PC/CORE/TURBO/ENGINE/GRAFX/16/CDROM2/SUPER/DUO/ARCADE/RX,"","2019","","","117","145","Since those who feel moe toward a character tend to buy its related goods excessively, the success of a project for the producers of such goods is directly determined not by the quality of the work itself but by its ability to evoke the moe desire through character design and illustrations. This tendency goes back to the 1970s, but its significance decisively increased in the context of the 1990s multimedia trend.","","9780262354868","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8849172.pdf&bkn=8844569&pdfType=chapter","","","","","","","","26 Sep 2019","","","MIT Press","MIT Press eBook Chapters"
"1 A ""Kind of"" Prosumer","I. Harbison","Goldsmiths College, London",Performing Image,"","2019","","","11","33","Standing at the rostrum of a museum auditorium in 2007, British artist Mark Leckey announces: This presentation is an attempt, by me, to try and grasp a particular experience that I have with certain things in the world, things that I mistake for images or pictures but that somehow impose on me their actual weight, density and volume—their being in the world […] How does an image find that presence? And—in turn—channel its effects through my body?1","","9780262350792","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8674406.pdf&bkn=8671660&pdfType=chapter","","","","","","","","28 Mar 2019","","","MIT Press","MIT Press eBook Chapters"
"Neuropedia: A Brief Compendium of Brain Phenomena","E. H. Chudler; K. Chudler",NA; NA,Neuropedia: A Brief Compendium of Brain Phenomena,"","2022","","","","","A fun and fact-filled A–Z treasury for anyone with a head on their shouldersNeuropedia journeys into the mysteries and marvels of the three pounds of tissue between your ears—the brain. Eric Chudler takes you on a breathtaking tour of the nervous system with dozens of entries that explore the structure and function of the brain and cover topics such as the spinal cord and nerve cells, the methods of neuroscientific research, and the visionary scientists who have dedicated their lives to understanding what makes each of us who we are.The brain has fascinated and puzzled researchers, physicians, and philosophers for thousands of years and captivated us with each new discovery. This compendium of neuroscientific wonders is brimming with facts and insights, helping us to make sense of our current understanding of the nervous system while identifying the frontiers in our knowledge that remain unexplored. Chudler guides readers through a variety of rare and common neurological disorders such as alien hand disorder, Capgras syndrome, Alzheimer’s disease, Parkinson’s disease, and stroke, and discusses the latest brain-imaging methods used to diagnose them. He discusses neurochemicals, neurotoxins, and lifesaving drugs, and offers bold perspectives on human consciousness that enable us to better appreciate our place in nature.With marvelous illustrations by Kelly Chudler, Neuropedia is an informative and entertaining trip into the inner world of the brain.","","9780691242187","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9969963.pdf&bkn=9969962&pdfType=book","Neuron;Neurotransmitter;Action potential;Spinal cord;Symptom;Serotonin;Cerebral cortex;Dopamine;Neurological disorder;Cerebellum;Electroencephalography;Syndrome;Schizophrenia;Acetylcholine;Headache;Catalepsy;Temporal lobe;Axon;Muscle;Rapid eye movement sleep;Dementia with Lewy bodies;Blood vessel;Addiction;Attention deficit hyperactivity disorder;Princeton University;Cerebrospinal fluid;Nicotinic acetylcholine receptor;Human brain;Amnesia;Brain injury;Dendrite;Rosemary Kennedy;Parietal lobe;Nerve agent;Tourette syndrome;Chemical synapse;Alzheimer's disease;Wernicke's encephalopathy;Blood–brain barrier;Peripheral nervous system;Cocaine;CT scan;Psychology;Sodium;Spina bifida;Scientist;Parkinson's disease;Dura mater;Epilepsy;Local anesthetic;Acetylcholine receptor;Brain damage;Stimulant;Axon terminal;Visual Objects;Narcolepsy;Neuroscientist;Amphibian;Sleep disorder;Lewy body;Riluzole;Panaeolus;Anxiolytic;MDMA;Ion channel;Arachnoid mater;Gamma-Aminobutyric acid;Astrocyte;Cranial nerves;On Intelligence;Eye movement;Covid-19;Pathology;L-DOPA;Agenesis of the corpus callosum;Oval window;James W. Watts;Posterior cerebral artery;Spatial memory;Ventricular system;Receptor (biochemistry);Node of Ranvier;The Canon of Medicine;Dextroamphetamine;Neural development;Short-term memory;Anterior cerebral artery;Electrode;Prosopagnosia;Meningitis;Alien hand syndrome;Radio frequency;Coronavirus;Hormone;Insecticide;Delicacy;Toothpaste;Vomiting;Hypothalamus;Anger","","","","","","","5 Dec 2022","","","Princeton University Press","Princeton University Press eBooks"
"Survey on Ethical Challenges of Implementing AI in Healthcare","N. Lalwani; M. A. B. Ur Rahim; A. Anand; S. P. Shaik; G. S. Moghe; D. Datta","Dept. of Computer Engineering and Computer Science, California State University, Long Beach, Long Beach, United States; Dept. of Computer Engineering and Computer Science, California State University, Long Beach, Long Beach, United States; Dept. of Computer Engineering and Computer Science, California State University, Long Beach, Long Beach, United States; Dept. of Computer Engineering and Computer Science, California State University, Long Beach, Long Beach, United States; Dept. of Computer Engineering and Computer Science, California State University, Long Beach, Long Beach, United States; Dept. of Computer Engineering and Computer Science, California State University, Long Beach, Long Beach, United States",2024 7th International Conference on Pattern Recognition and Artificial Intelligence (PRAI),"14 Jan 2025","2024","","","824","834","In today's world, Artificial Intelligence (AI) and its usage concerning human tasks have become an integral part of our daily lives. Humans depend upon AI to provide faster and more efficient solutions. In the world of medicine, where every decision made by a doctor, physician, or consultant directly impacts the lives of those being treated or diagnosed, AI is making its way to help efficiently provide such results as the intelligence of a human being. Healthcare systems around the world are now making use of AI to help provide humane solutions using algorithms and Machine Learning (ML). Fallacies or inaccuracies in the arena of healthcare can have horrendous outcomes for the patient on whom the procedure is being carried out. Sometimes, certain ethics could be traded off by the system, such as confidentiality, autonomy, justice, and biased decisions leading to false results and victim blaming, which directs the masses into not trusting AI-related healthcare solutions. This situation can lead to mishaps like “AI winter,” where the public could start disbelieving the results of AI solutions. This survey review makes an effort to analyze and address these pertinent ethical issues underlying the requirement for algorithmic translucency, concealment, and safeguarding the interests of all those involved.","","979-8-3503-5089-0","10.1109/PRAI62207.2024.10827739","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10827739","Artificial intelligence;Ethics;Healthcare;Health policies;Machine Learning;Medical ethics","Training;Surveys;Ethics;Collaboration;Medical services;Regulation;Stakeholders;Artificial intelligence;Medical diagnostic imaging;Standards","","","","36","IEEE","14 Jan 2025","","","IEEE","IEEE Conferences"
"Understanding Fear Responses and Coping Mechanisms in VR Horror Gaming: Insights From Semistructured Interviews","H. Zhang; X. Li; X. Fu; C. Qiu; J. Zhang; J. M. Carroll","Future Laboratory, Tsinghua University, Beijing, China; Future Laboratory, and Academy of Arts & Design, Tsinghua University, Beijing, China; Future Laboratory, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Courant Institute of Mathematical Sciences, New York University, New York, NY, USA; College of Information Sciences and Technology, Pennsylvania State University, State College, PA, USA",IEEE Transactions on Games,"17 Dec 2024","2024","16","4","868","881","Virtual reality (VR) has the capacity to offer unparalleled immersive experiences, particularly in the domain of horror gaming. However, creating both practical and enjoyable VR applications necessitates a nuanced understanding of user emotions and behaviors. To fill this gap, we conducted semistructured interviews with 25 participants who engaged in VR horror games, specifically exploring their emotional responses to fear-inducing stimuli and their coping strategies. The results revealed the motivations behind users' behaviors in response to different types of fear, highlighting the need to understand and manage negative emotions in VR environments, and provides insights into user needs and design methodologies in VR horror games.","2475-1510","","10.1109/TG.2024.3403768","Ministry of Education of the People's Republic of China's Humanities; Social Science Fund for Youth Projects(grant numbers:23YJCZH049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10535527","Coping strategy;fear;qualitative analysis;virtual reality (VR) game","Games;Interviews;Virtual environments;Anxiety disorders;Three-dimensional displays;Physiology;Media","","","","100","IEEE","21 May 2024","","","IEEE","IEEE Journals"
"NLP4ReF: Requirements Classification and Forecasting: From Model-Based Design to Large Language Models","J. Peer; Y. Mordecai; Y. Reich","Faculty of Engineering, Tel Aviv University, Tel-Aviv, Israel; Faculty of Engineering, Tel Aviv University, Tel-Aviv, Israel; Faculty of Engineering, Tel Aviv University, Tel-Aviv, Israel",2024 IEEE Aerospace Conference,"13 May 2024","2024","","","1","16","We introduce Natural Language Processing for Requirement Forecasting (NLP4ReF), a model-based machine learning and natural language processing solution for enhancing the Requirements Engineering (RE) process. RE continues to face significant challenges and demands innovative approaches for process efficiency. Traditional RE methods relying on natural language struggle with incomplete, hidden, forgotten, and evolving requirements during and after the critical design review, risking project failures and setbacks. NLP4ReF tackles several key challenges: a) distinguishing between functional and non-functional requirements, b) classification of requirements by their respective system classes, and c) generation of unanticipated requirements to enhance project success. NLP4ReF employs a common natural language toolkit (NLTK) package and the recently-trending Chat-GPT. We tested NLP4ReF on PROMISE_exp, a pre-existing dataset with 1000 software requirements, and PROMISE_IoT, an enhanced dataset with 2000 software and IoT requirements. We validated NLP4ReF on a genuine IoT project. NLP4ReF swiftly generated dozens of new requirements, verified by a team of systems engineers, of which over 70% were crucial for project success. We found that GPT is superior in authentic requirement generation, while NLTK excels at requirement classification. NLP4ReF offers significant time saving, effort reduction, and improved future-proofing. Our model-based design approach provides a foundation for enhanced RE practices and future research in this domain.","1095-323X","979-8-3503-0462-6","10.1109/AERO58975.2024.10521022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10521022","Natural Language Processing;Requirements Engineering Requirement Forecasting;Internet of Things;Machine Learning;Model-Based Systems Engineering","Software algorithms;Training data;Software;Natural language processing;Data models;Classification algorithms;Requirements engineering","","","","38","IEEE","13 May 2024","","","IEEE","IEEE Conferences"
"Spatial-Temporal Coherence in Extreme Video Retargeting for Consumer Screening Devices","H. Imani; M. B. Islam","Florida Gulf Coast University, Fort Myers, FL, USA; Florida Gulf Coast University, Fort Myers, FL, USA",IEEE Transactions on Consumer Electronics,"","2024","PP","99","1","1","The accessibility of diverse display devices and their aspect ratios has drawn much research attention to video retargeting. Non-consistent video retargeting can significantly affect a video’s spatial and temporal quality, particularly in extreme retargeting cases. Since there are no perfectly annotated datasets for video retargeting, deep learning-based techniques are rarely utilized. This paper proposes a method that learns to retarget videos by detecting the salient areas and shifting them to the appropriate location. First, we segment the salient objects using a unified Transformer model. Using convolutional layers and a shifting strategy, we shift and warp objects to the appropriate size and location in the frame. We use 1D convolution to move the salient items in the scene. Additionally, we employ a frame interpolation technique to preserve temporal information. To train the network, we feed the retargeted frames to a variational auto-encoder network to map the retargeted frames back to the input frames. Furthermore, we design perceptual and wavelet-based loss functions to train our model. Thus, we train the network unsupervised. Extensive qualitative and quantitative experiments on the DAVIS dataset show the superiority of the proposed method over existing image and video-based methods.","1558-4127","","10.1109/TCE.2024.3502422","T?rkiye Bilimsel ve Teknolojik Ara?t?rma Kurumu(grant numbers:118C301); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10757345","Salient objects;Spatial and temporal coherence;Segmentation;Video retargeting","Distortion;Image segmentation;Visualization;Consumer electronics;Coherence;Transformers;Strips;Shape;Object detection;Media","","","","","IEEE","19 Nov 2024","","","IEEE","IEEE Early Access Articles"
"Data Stewardship in Action: A roadmap to data value realization and measurable business outcomes","P. S. Lee; D. T. Charm",NA; NA,Data Stewardship in Action: A roadmap to data value realization and measurable business outcomes,"","2024","","","","","Take your organization's data maturity to the next level by operationalizing data governanceKey FeaturesDevelop the mindset and skills essential for successful data stewardshipApply practical advice and industry best practices, spanning data governance, quality management, and compliance, to enhance data stewardshipFollow a step-by-step program to develop a data operating model and implement data stewardship effectivelyPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionIn the competitive data-centric world, mastering data stewardship is not just a requirement—it's the key to organizational success. Unlock strategic excellence with Data Stewardship in Action, your guide to exploring the intricacies of data stewardship and its implementation for maximum efficiency. From business strategy to data strategy, and then to data stewardship, this book shows you how to strategically deploy your workforce, processes, and technology for efficient data processing. You’ll gain mastery over the fundamentals of data stewardship, from understanding the different roles and responsibilities to implementing best practices for data governance. You’ll elevate your data management skills by exploring the technologies and tools for effective data handling. As you progress through the chapters, you’ll realize that this book not only helps you develop the foundational skills to become a successful data steward but also introduces innovative approaches, including leveraging AI and GPT, for enhanced data stewardship. By the end of this book, you’ll be able to build a robust data governance framework by developing policies and procedures, establishing a dedicated data governance team, and creating a data governance roadmap that ensures your organization thrives in the dynamic landscape of data management.What you will learnEnhance your job prospects by understanding the data stewardship field, roles, and responsibilitiesDiscover how to develop a data strategy and translate it into a functional data operating modelDevelop an effective and efficient data stewardship programGain practical experience of establishing a data stewardship initiativeImplement purposeful governance with measurable ROIPrioritize data use cases with the value and effort matrixWho this book is forThis book is for professionals working in the field of data management, including business analysts, data scientists, and data engineers looking to gain a deeper understanding of the data steward role. Senior executives who want to (re)establish the data governance body in their organizations will find this resource invaluable. While accessible to both beginners and professionals, basic knowledge of data management concepts, such as data modeling, data warehousing, and data quality, is a must to get started.","","9781837638123","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10460889.pdf&bkn=10460888&pdfType=book","","","","","","","","6 Mar 2024","","","Packt Publishing","Packt Publishing eBooks"
"Cross-Attention watermarking of Large Language Models","F. B. Baldassini; H. H. Nguyen; C. -C. Chang; I. Echizen","Sorbonne University, France; National Institute of Informatics, Japan; National Institute of Informatics, Japan; National Institute of Informatics, Japan","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","4625","4629","A new approach to linguistic watermarking of language models is presented in which information is imperceptibly inserted into the output text while preserving its readability and original meaning. A cross-attention mechanism is used to embed watermarks in the text during inference. Two methods using cross-attention are presented that minimize the effect of watermarking on the performance of a pretrained model. Exploration of different training strategies for optimizing the watermarking and of the challenges and implications of applying this approach in real-world scenarios clarified the tradeoff between watermark robustness and text quality. Watermark selection substantially affects the generated output for high entropy sentences. This proactive watermarking approach has potential application in future model development.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10446397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446397","Large Language Models;Linguistic Watermarking;Cross Attention;Steganography","Training;Shape;Watermarking;Linguistics;Signal processing;Robustness;Vectors","","","","29","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications","W. Samek; G. Montavon; S. Lapuschkin; C. J. Anders; K. -R. Müller","Department of Artificial Intelligence, Fraunhofer Heinrich Hertz Institute, Berlin, Germany; BIFOLD✔Berlin Institute for the Foundations of Learning and Data, Berlin, Germany; Department of Artificial Intelligence, Fraunhofer Heinrich Hertz Institute, Berlin, Germany; BIFOLD✔Berlin Institute for the Foundations of Learning and Data, Berlin, Germany; BIFOLD✔Berlin Institute for the Foundations of Learning and Data, Berlin, Germany",Proceedings of the IEEE,"4 Mar 2021","2021","109","3","247","278","With the broader and highly successful usage of machine learning (ML) in industry and the sciences, there has been a growing demand for explainable artificial intelligence (XAI). Interpretability and explanation methods for gaining a better understanding of the problem-solving abilities and strategies of nonlinear ML, in particular, deep neural networks, are, therefore, receiving increased attention. In this work, we aim to: 1) provide a timely overview of this active emerging field, with a focus on “post hoc” explanations, and explain its theoretical foundations; 2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations; 3) outline best practice aspects, i.e., how to best include interpretation methods into the standard usage of ML; and 4) demonstrate successful usage of XAI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of ML.","1558-2256","","10.1109/JPROC.2021.3060483","Institute of Information & Communications Technology Planning & Evaluation (IITP) grants funded by the Korea Government(grant numbers:2017-0-00451,2019-0-00079); German Ministry for Education and Research (BMBF)(grant numbers:01IS14013A-E,01GQ1115,01GQ0850,01IS18025A,01IS18037A); German Research Foundation (DFG)(grant numbers:Math+,EXC 2046/1,390685689); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369420","Black-box models;deep learning;explainable artificial intelligence (XAI);Interpretability;model transparency;neural networks","Deep learning;Systematics;Neural networks;Artificial intelligence;Machine learning;Unsupervised learning;Problem-solving;Best practices","","621","","79","CCBY","4 Mar 2021","","","IEEE","IEEE Journals"
"A Decade Survey of Transfer Learning (2010–2020)","S. Niu; Y. Liu; J. Wang; H. Song","Department of Electrical Engineering and Computer Science, Security and Optimization for Networked Globe Laboratory (SONG Lab), Embry-Riddle Aeronautical University, Daytona Beach, FL, USA; Department of Electrical Engineering and Computer Science, Security and Optimization for Networked Globe Laboratory (SONG Lab), Embry-Riddle Aeronautical University, Daytona Beach, FL, USA; Department of Electrical Engineering and Computer Science, Security and Optimization for Networked Globe Laboratory (SONG Lab), Embry-Riddle Aeronautical University, Daytona Beach, FL, USA; Department of Electrical Engineering and Computer Science, Security and Optimization for Networked Globe Laboratory (SONG Lab), Embry-Riddle Aeronautical University, Daytona Beach, FL, USA",IEEE Transactions on Artificial Intelligence,"3 Mar 2021","2020","1","2","151","166","Transfer learning (TL) has been successfully applied to many real-world problems that traditional machine learning (ML) cannot handle, such as image processing, speech recognition, and natural language processing (NLP). Commonly, TL tends to address three main problems of traditional machine learning: (1) insufficient labeled data, (2) incompatible computation power, and (3) distribution mismatch. In general, TL can be organized into four categories: transductive learning, inductive learning, unsupervised learning, and negative learning. Furthermore, each category can be organized into four learning types: learning on instances, learning on features, learning on parameters, and learning on relations. This article presents a comprehensive survey on TL. In addition, this article presents the state of the art, current trends, applications, and open challenges.","2691-4581","","10.1109/TAI.2021.3054609","National Science Foundation(grant numbers:1956193); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9336290","Transfer learning;machine learning;domain adaptation;distant domain transfer;cross-modality transfer learning","Task analysis;Training;Artificial intelligence;Market research;Deep learning;Adaptation models;Training data","","335","","84","IEEE","26 Jan 2021","","","IEEE","IEEE Journals"
"A Survey on Deep Visual Place Recognition","C. Masone; B. Caputo","Visual and Multimodal Applied Learning Team, Istituto Italiano di Tecnologia, Torino, Italy; Visual and Multimodal Applied Learning Team, Istituto Italiano di Tecnologia, Torino, Italy",IEEE Access,"3 Feb 2021","2021","9","","19516","19547","In recent years visual place recognition (VPR), i.e., the problem of recognizing the location of images, has received considerable attention from multiple research communities, spanning from computer vision to robotics and even machine learning. This interest is fueled on one hand by the relevance that visual place recognition holds for many applications and on the other hand by the unsolved challenge of making these methods perform reliably in different conditions and environments. This paper presents a survey of the state-of-the-art of research on visual place recognition, focusing on how it has been shaped by the recent advances in deep learning. We start discussing the image representations used in this task and how they have evolved from using hand-crafted to deep-learned features. We further review how metric learning techniques are used to get more discriminative representations, as well as techniques for dealing with occlusions, distractors, and shifts in the visual domain of the images. The survey also provides an overview of the specific solutions that have been proposed for applications in robotics and with aerial imagery. Finally the survey provides a summary of datasets that are used in visual place recognition, highlighting their different characteristics.","2169-3536","","10.1109/ACCESS.2021.3054937","H2020 European Research Council(grant numbers:637076 (project RoboExNovo)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9336674","Visual place recognition;image representation learning;deep learning","Visualization;Task analysis;Robots;Image recognition;Databases;Deep learning;Image retrieval","","114","","266","CCBY","27 Jan 2021","","","IEEE","IEEE Journals"
"Advances and Opportunities in Remote Sensing Image Geometric Registration: A systematic review of state-of-the-art approaches and future research directions","R. Feng; H. Shen; J. Bai; X. Li","School of Geography and Tourism, Shaanxi Normal University, Xi’an, China; School of Resource and Environment Science and the Collaborative Innovation Center for Geospatial Technology, Wuhan University, Wuhan, China; School of Geography and Tourism, Shaanxi Normal University, Xi’an, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China",IEEE Geoscience and Remote Sensing Magazine,"10 Feb 2022","2021","9","4","120","142","Geometric registration is often an accuracy assurance for most remote sensing image processing and analysis, such as image mosaicking, image fusion, and time-series analysis. In recent decades, geometric registration has attracted considerable attention in the remote sensing community, leading to a large amount of research on the subject. However, few studies have systematically reviewed its current status and deeply investigated its development trends. Moreover, new approaches are constantly emerging, and some issues still need to be solved. Thus, this article presents a survey of state-of-the-art approaches for remote sensing image registration in terms of intensity-based, feature-based, and combination techniques. Optical flow estimation and deep learning-based methods are summarized, and software-operated registration and registration evaluation are introduced. Building on recent advances, promising opportunities are explored.","2168-6831","","10.1109/MGRS.2021.3081763","National Natural Science Foundation of China(grant numbers:41971303,41701394); Key Research and Development Program of Shaanxi Province(grant numbers:2020NY-166); Fundamental Research Funds for the Central Universities(grant numbers:GK202103143); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468875","","Remote sensing;Image registration;Feature extraction;Optical flow;Measurement;Learning systems;Time series analysis;Market research;Image fusion","","65","","188","IEEE","30 Jun 2021","","","IEEE","IEEE Magazines"
"Backdoor Attacks Against Transfer Learning With Pre-Trained Deep Learning Models","S. Wang; S. Nepal; C. Rudolph; M. Grobler; S. Chen; T. Chen","Faculty of Information Technology, Monash University, Clayton, VIC, Australia; CSIRO's Data61, Melbourne, VIC, Australia; Faculty of Information Technology, Monash University, Melbourne, VIC, Australia; CSIRO's Data61, Melbourne, VIC, Australia; University of Melbourne, Melbourne, VIC, Australia; Faculty of Information Technology, Monash University, Melbourne, VIC, Australia",IEEE Transactions on Services Computing,"17 Jun 2022","2022","15","3","1526","1539","Transfer learning provides an effective solution for feasibly and fast customize accurate Student models, by transferring the learned knowledge of pre-trained Teacher models over large datasets via fine-tuning. Many pre-trained Teacher models used in transfer learning are publicly available and maintained by public platforms, increasing their vulnerability to backdoor attacks. In this article, we demonstrate a backdoor threat to transfer learning tasks on both image and time-series data leveraging the knowledge of publicly accessible Teacher models, aimed at defeating three commonly adopted defenses: pruning-based, retraining-based and input pre-processing-based defenses. Specifically, ($\mathcal {A}$A) ranking-based selection mechanism to speed up the backdoor trigger generation and perturbation process while defeating pruning-based and/or retraining-based defenses. ($\mathcal {B}$B) autoencoder-powered trigger generation is proposed to produce a robust trigger that can defeat the input pre-processing-based defense, while guaranteeing that selected neuron(s) can be significantly activated. ($\mathcal {C}$C) defense-aware retraining to generate the manipulated model using reverse-engineered model inputs. We launch effective misclassification attacks on Student models over real-world images, brain Magnetic Resonance Imaging (MRI) data and Electrocardiography (ECG) learning systems. The experiments reveal that our enhanced attack can maintain the 98.4 and 97.2 percent classification accuracy as the genuine model on clean image and time series inputs while improving $27.9\%-100\%$27.9%-100% and $27.1\%-56.1\%$27.1%-56.1% attack success rate on trojaned image and time series inputs respectively in the presence of pruning-based and/or retraining-based defenses.","1939-1374","","10.1109/TSC.2020.3000900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9112322","Web service;deep neural network;backdoor attack;transfer learning;pre-trained model","Biological system modeling;Task analysis;Learning systems;Training;Data models;Electrocardiography","","57","","36","IEEE","9 Jun 2020","","","IEEE","IEEE Journals"
"Graph Signal Processing, Graph Neural Network and Graph Learning on Biological Data: A Systematic Review","R. Li; X. Yuan; M. Radfar; P. Marendy; W. Ni; T. J. O’Brien; P. M. Casillas-Espinosa","Department of Neuroscience, Central Clinical School, Monash University, Melbourne, VIC, Australia; Cybernetics Group, Cyber-Physical Systems Program, Data61, CSIRO, Marsfield, NSW, Australia; Cybernetics Group, Cyber-Physical Systems Program, Data61, CSIRO, Marsfield, NSW, Australia; Cybernetics Group, Cyber-Physical Systems Program, Data61, CSIRO, Marsfield, NSW, Australia; Cybernetics Group, Cyber-Physical Systems Program, Data61, CSIRO, Marsfield, NSW, Australia; Department of Neuroscience, Central Clinical School, Monash University, Melbourne, VIC, Australia; Department of Neuroscience, Central Clinical School, Monash University, Melbourne, VIC, Australia",IEEE Reviews in Biomedical Engineering,"5 Jan 2023","2023","16","","109","135","Graph networks can model data observed across different levels of biological systems that span from population graphs (with patients as network nodes) to molecular graphs that involve omics data. Graph-based approaches have shed light on decoding biological processes modulated by complex interactions. This paper systematically reviews graph-based analysis methods of Graph Signal Processing (GSP), Graph Neural Networks (GNNs) and graph topology inference, and their applications to biological data. This work focuses on the algorithms of graph-based approaches and the constructions of graph-based frameworks that are adapted to a broad range of biological data. We cover the Graph Fourier Transform and the graph filter developed in GSP, which provides tools to investigate biological signals in the graph domain that can potentially benefit from the underlying graph structures. We also review the node, graph, and interaction oriented applications of GNNs with inductive and transductive learning manners for various biological targets. As a key component of graph analysis, we provide a review of graph topology inference methods that incorporate assumptions for specific biological objectives. Finally, we discuss the biological application of graph analysis methods within this exhaustive literature collection, potentially providing insights for future research in biological sciences.","1941-1189","","10.1109/RBME.2021.3122522","National Health and Medical Research Council(grant numbers:APP1087172); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585532","Graph signal processing;graph neural network;graph convolutional network;graph learning;biological data","Biology;Laplace equations;Signal processing;Matrix decomposition;Topology;Network topology;Fourier transforms","Humans;Neural Networks, Computer;Algorithms;Signal Processing, Computer-Assisted","48","","162","IEEE","26 Oct 2021","","","IEEE","IEEE Journals"
"A Survey of Deep Learning-Based Object Detection Methods and Datasets for Overhead Imagery","J. Kang; S. Tariq; H. Oh; S. S. Woo","Department of Artificial Intelligence, Sungkyunkwan University, Suwon, South Korea; Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea; National Satellite Operation and Application Center, Korea Aerospace Research Institute (KARI), Daejeon, South Korea; Department of Artificial Intelligence, Sungkyunkwan University, Suwon, South Korea",IEEE Access,"28 Feb 2022","2022","10","","20118","20134","Significant advancements and progress made in recent computer vision research enable more effective processing of various objects in high-resolution overhead imagery obtained by various sources from drones, airplanes, and satellites. In particular, overhead images combined with computer vision allow many real-world uses for economic, commercial, and humanitarian purposes, including assessing economic impact from access crop yields, financial supply chain prediction for company’s revenue management, and rapid disaster surveillance system (wildfire alarms, rising sea levels, weather forecast). Likewise, object detection in overhead images provides insight for use in many real-world applications yet is still challenging because of substantial image volumes, inconsistent image resolution, small-sized objects, highly complex backgrounds, and nonuniform object classes. Although extensive studies in deep learning-based object detection have achieved remarkable performance and success, they are still ineffective yielding a low detection performance, due to the underlying difficulties in overhead images. Thus, high-performing object detection in overhead images is an active research field to overcome such difficulties. This survey paper provides a comprehensive overview and comparative reviews on the most up-to-date deep learning-based object detection in overhead images. Especially, our work can shed light on capturing the most recent advancements of object detection methods in overhead images and the introduction of overhead datasets that have not been comprehensively surveyed before.","2169-3536","","10.1109/ACCESS.2022.3149052","Institute of Information and Communications Technology Planning and Evaluation (IITP); Korea Government (MSIT), AI Graduate School Support Program, Sungkyunkwan University(grant numbers:2019-0-00421); Regional strategic industry convergence security core talent training business(grant numbers:2019-0-01343); Basic Science Research Program through the National Research Foundation of Korea (NRF); Korea Government (MSIT)(grant numbers:2020R1C1C1006004,2020M1A3B2A02084969); IITP; Korea Government (MSIT), Original Technology Development of Artificial Intelligence Industry(grant numbers:2021-0-02157,2021-0-00017); Institute of Information and Communications Technology Planning and Evaluation (IITP); Korea Government (MSIT) (Artificial Intelligence Innovation Hub)(grant numbers:2021-0-02068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9703336","Object detection;satellites;synthetic aperture radar;unmanned aerial vehicles","Object detection;Head;Detectors;Transformers;Satellites;Feature extraction;Remote sensing","","36","","119","CCBY","4 Feb 2022","","","IEEE","IEEE Journals"
"Student Perceptions of ChatGPT Use in a College Essay Assignment: Implications for Learning, Grading, and Trust in Artificial Intelligence","C. C. Tossell; N. L. Tenhundfeld; A. Momen; K. Cooley; E. J. de Visser","Department of Behavioral Sciences and Leadership, United States Air Force Academy, Colorado Springs, CO, USA; Department of Psychology, University of Alabama, Huntsville, AL, USA; Department of Behavioral Sciences and Leadership, United States Air Force Academy, Colorado Springs, CO, USA; Department of Behavioral Sciences and Leadership, United States Air Force Academy, Colorado Springs, CO, USA; Department of Behavioral Sciences and Leadership, United States Air Force Academy, Colorado Springs, CO, USA",IEEE Transactions on Learning Technologies,"5 Feb 2024","2024","17","","1069","1081","This article examined student experiences before and after an essay writing assignment that required the use of ChatGPT within an undergraduate engineering course. Utilizing a pre–post study design, we gathered data from 24 participants to evaluate ChatGPT's support for both completing and grading an essay assignment, exploring its educational value and impact on the learning process. Our quantitative and thematic analyses uncovered that ChatGPT did not simplify the writing process. Instead, the tool transformed the student learning experience yielding mixed responses. Participants reported finding ChatGPT valuable for learning, and their comfort with its ethical and benevolent aspects increased postuse. Concerns with ChatGPT included poor accuracy and limited feedback on the confidence of its output. Students preferred instructors to use ChatGPT to help grade their assignments, with appropriate oversight. They did not trust ChatGPT to grade by itself. Student views of ChatGPT evolved from a perceived “cheating tool” to a collaborative resource that requires human oversight and calibrated trust. Implications for writing, education, and trust in artificial intelligence are discussed.","1939-1382","","10.1109/TLT.2024.3355015","Air Force Office of Scientific Research(grant numbers:21USCOR004); Defense Advanced Research Projects Agency(grant numbers:FA8650-23-C-7318); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10400910","Artificial intelligence (AI);ChatGPT;education;student perceptions;writing","Chatbots;Writing;Education;Artificial intelligence;Human factors;Ethics;Task analysis","","34","","87","CCBY","16 Jan 2024","","","IEEE","IEEE Journals"
"Automatic Audio Chord Recognition With MIDI-Trained Deep Feature and BLSTM-CRF Sequence Decoding Model","Y. Wu; W. Li","School of Informatics, Kyoto University, Kyoto, Japan; Shanghai Key Laboratory of Intelligent Information Processing, and the School of Computer Science and Technology, Fudan University, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","29 Nov 2018","2019","27","2","355","366","With the advances of machine learning technologies, data-driven feature extraction and sequence modeling approaches are being widely explored for automatic chord recognition tasks. Currently, there is a bottleneck in the amount of enough annotated data for training robust acoustic models, as hand-annotating time-synchronized chord labels requires professional musical skills and considerable labor. To cope with this limitation, in this paper, we propose a convolutional neural network (CNN) based deep feature extractor, which is trained on a large set of time, synchronized musical instrument digital interface audio data pairs and can robustly estimate pitch class activations of real-world music audio recordings. The CNN feature extractor plus a bidirectional long short-term memory conditional random field decoding model forms the proposed hybrid system for automatic chord recognition. Experiments show that the proposed model is compatible for both regular major/minor triad chord classification and larger vocabulary chord recognition, and outperforms other state-of-the-art chord recognition systems.","2329-9304","","10.1109/TASLP.2018.2879399","National Natural Science Foundation of China(grant numbers:61671156); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8523662","Automatic chord recognition;bidirectional long short-term memory (BLSTM);conditional random fields (CRF)","Feature extraction;Hidden Markov models;Music;Decoding;Training;Harmonic analysis","","29","","46","IEEE","4 Nov 2018","","","IEEE","IEEE Journals"
"DVMark: A Deep Multiscale Framework for Video Watermarking","X. Luo; Y. Li; H. Chang; C. Liu; P. Milanfar; F. Yang","Google Research - Mountain View, California, United States; Google Research - Mountain View, California, United States; Google Research - Mountain View, California, United States; Google Research - Mountain View, California, United States; Google Research - Mountain View, California, United States; Google Research - Mountain View, California, United States",IEEE Transactions on Image Processing,"","2023","PP","99","1","1","Video watermarking embeds a message into a cover video in an imperceptible manner, which can be retrieved even if the video undergoes certain modifications or distortions. Traditional watermarking methods are often manually designed for particular types of distortions and thus cannot simultaneously handle a broad spectrum of distortions. To this end, we propose a robust deep learning-based solution for video watermarking that is end-to-end trainable. Our model consists of a novel multiscale design where the watermarks are distributed across multiple spatial-temporal scales. Extensive evaluations on a wide variety of distortions show that our method outperforms traditional video watermarking methods as well as deep image watermarking models by a large margin. We further demonstrate the practicality of our method on a realistic video-editing application.","1941-0042","","10.1109/TIP.2023.3251737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10086041","","Watermarking;Distortion;Transforms;Decoding;Deep learning;Robustness;Training","","20","","","IEEE","28 Mar 2023","","","IEEE","IEEE Early Access Articles"
"A Survey of Intrusion Detection Techniques","D. H. Lakshminarayana; J. Philips; N. Tabrizi","Department of Computer Science, East Carolina University, Greenville, NC, USA; Department of Computer Science, East Carolina University, Greenville, NC, USA; Department of Computer Science, East Carolina University, Greenville, NC, USA",2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),"17 Feb 2020","2019","","","1122","1129","With the growing rate of cyber attacks, there is a significant need for intrusion detection systems (IDS) in networked environments. As intrusion tactics become more sophisticated and more challenging to detect, this necessitates improved intrusion detection technology to retain user trust and preserve network security. Over the last decade, several detection methodologies have been designed to provide users with reliability, privacy, and information security. This paper reviews three intrusion detection techniques: blockchain technologies, machine learning, and deep learning. This survey overviews various machine learning and deep learning algorithms, summarizes blockchain technology, and discusses different blockchain methods used for intrusion detection and cybersecurity. We provide insight into their applications, drawbacks, and challenges.","","978-1-7281-4550-1","10.1109/ICMLA.2019.00187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999075","Intrusion detection, cyber security, collaborative system, blockchain, machine learning, deep learning, decentralized network, algorithm, smart contracts, security","Intrusion detection;Machine learning;Machine learning algorithms;Contracts","","18","","58","IEEE","17 Feb 2020","","","IEEE","IEEE Conferences"
"Perceptual Grouping in Contrastive Vision-Language Models","K. Ranasinghe; B. McKinzie; S. Ravi; Y. Yang; A. Toshev; J. Shlens",Apple; Apple; Apple; Apple; Apple; Apple,2023 IEEE/CVF International Conference on Computer Vision (ICCV),"15 Jan 2024","2023","","","5548","5561","Recent advances in zero-shot image recognition suggest that vision-language models learn generic visual representations with a high degree of semantic information that may be arbitrarily probed with natural language phrases. Understanding an image, however, is not just about understanding what content resides within an image, but importantly, where that content resides. In this work we examine how well vision-language models are able to understand where objects reside within an image and group together visually related parts of the imagery. We demonstrate how contemporary vision and language representation learning models based on contrastive losses and large web-based data capture limited object localization information. We propose a minimal set of modifications that results in models that uniquely learn both semantic and spatial information. We measure this performance in terms of zero-shot image recognition, unsupervised bottom-up and top-down semantic segmentations, as well as robustness analyses. We find that the resulting model achieves state-of-the-art results in terms of unsupervised segmentation, and demonstrate that the learned representations are uniquely robust to spurious correlations in datasets designed to probe the causal behavior of vision models.","2380-7504","979-8-3503-0718-4","10.1109/ICCV51070.2023.00513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10377228","","Representation learning;Location awareness;Visualization;Image recognition;Computational modeling;Semantic segmentation;Semantics","","17","","126","IEEE","15 Jan 2024","","","IEEE","IEEE Conferences"
"Aesthetic Image Captioning From Weakly-Labelled Photographs","K. Ghosal; A. Rana; A. Smolic","V-SENSE, School of Computer Science and Statistics, Trinity College Dublin, Ireland.; V-SENSE, School of Computer Science and Statistics, Trinity College Dublin, Ireland.; V-SENSE, School of Computer Science and Statistics, Trinity College Dublin, Ireland.",2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),"5 Mar 2020","2019","","","4550","4560","Aesthetic image captioning (AIC) refers to the multi-modal task of generating critical textual feedbacks for photographs. While in natural image captioning (NIC), deep models are trained in an end-to-end manner using large curated datasets such as MS-COCO, no such large-scale, clean dataset exists for AIC. Towards this goal, we propose an automatic cleaning strategy to create a benchmarking AIC dataset, by exploiting the images and noisy comments easily available from photography websites. We propose a probabilistic caption-filtering method for cleaning the noisy web-data, and compile a large-scale, clean dataset 'AVA-Captions', (~ 230, 000 images with ~ 5 captions per image). Additionally, by exploiting the latent associations between aesthetic attributes, we propose a strategy for training the convolutional neural network (CNN) based visual feature extractor, the first component of the AIC framework. The strategy is weakly supervised and can be effectively used to learn rich aesthetic representations, without requiring expensive ground-truth annotations. We finally show-case a thorough analysis of the proposed contributions using automatic metrics and subjective evaluations.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022573","Image Aesthetics;Captioning;Weakly Supervised Learning;Noisy Data","Task analysis;Noise measurement;Image color analysis;Visualization;Feature extraction;Standards;Training","","17","","85","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Blind Visual Motif Removal From a Single Image","A. Hertz; S. Fogel; R. Hanocka; R. Giryes; D. Cohen-Or",Tel Aviv University; Tel Aviv University; Tel Aviv University; Tel Aviv University; Tel Aviv University,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"9 Jan 2020","2019","","","6851","6860","Many images shared over the web include overlaid objects, or visual motifs, such as text, symbols or drawings, which add a description or decoration to the image. For example, decorative text that specifies where the image was taken, repeatedly appears across a variety of different images. Often, the reoccurring visual motif, is semantically similar, yet, differs in location, style and content (e.g. text placement, font and letters). This work proposes a deep learning based technique for blind removal of such objects. In the blind setting, the location and exact geometry of the motif are unknown. Our approach simultaneously estimates which pixels contain the visual motif, and synthesizes the underlying latent image. It is applied to a single input image, without any user assistance in specifying the location of the motif, achieving state-of-the-art results for blind removal of both opaque and semi-transparent visual motifs.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00702","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953189","Vision + Graphics;Deep Learning","","","16","","36","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Neuro-Symbolic Program Corrector for Introductory Programming Assignments","S. Bhatia; P. Kohli; R. Singh","Netaji Subhas Institute of Technology, New Delhi, Delhi, IN; Google Deepmind, London, UK; Microsoft Research, Redmond, WA, US",2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),"2 Sep 2018","2018","","","60","70","Automatic correction of programs is a challenging problem with numerous real world applications in security, verification, and education. One application that is becoming increasingly important is the correction of student submissions in online courses for providing feedback. Most existing program repair techniques analyze Abstract Syntax Trees (ASTs) of programs, which are unfortunately unavailable for programs with syntax errors. In this paper, we propose a novel Neuro-symbolic approach that combines neural networks with constraint-based reasoning. Specifically, our method first uses a Recurrent Neural Network (RNN) to perform syntax repairs for the buggy programs; subsequently, the resulting syntactically-fixed programs are repaired using constraint-based techniques to ensure functional correctness. The RNNs are trained using a corpus of syntactically correct submissions for a given programming assignment, and are then queried to fix syntax errors in an incorrect programming submission by replacing or inserting the predicted tokens at the error location. We evaluate our technique on a dataset comprising of over 14,500 student submissions with syntax errors. Our method is able to repair syntax errors in 60% (8689) of submissions, and finds functionally correct repairs for 23.8% (3455) submissions.","1558-1225","978-1-4503-5638-1","10.1145/3180155.3180219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453063","Neural Program Correction;Automated Feedback Generation;Neural guided search","Syntactics;Maintenance engineering;Programming;Recurrent neural networks;Prediction algorithms;Semantics","","16","","","","2 Sep 2018","","","IEEE","IEEE Conferences"
"MOE/RF: A Novel Phishing Detection Model Based on Revised Multiobjective Evolution Optimization Algorithm and Random Forest","E. Zhu; Z. Chen; J. Cui; H. Zhong","School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China",IEEE Transactions on Network and Service Management,"1 Feb 2023","2022","19","4","4461","4478","To effectively boost computer usage, machine learning models are used in several phishing detection systems to classify enormous phishing datasets. Based on phishing patterns, researchers prefer to extract a considerable number of features to improve phishing detection performance. However, redundant and useless features in the feature set degrade the performance of the underlying classification models. In addition, several existing phishing detection models mainly focus on detection accuracy and overlook recall rates. However, in phishing detection, it is more harmful to falsely detect a phishing website as a legitimate website than it is to detect a legitimate website as a phishing website. This study proposes a novel phishing detection model, multi-objective evolution/random forest (MOE/RF), which is based on the revised multi-objective evolution optimization algorithm (MOE) and random forest (RF). The MOE/RF model uses accuracy as the detection target and minimizes the probability of false detection of phishing sites. In addition, two new strategies, the symmetric uncertainty-based population initialization and the population state-based adaptive environmental selection, are proposed to improve the performance of the MOE. Experimental results on testing five different phishing datasets demonstrated that the MOE/RF performs superior to several existing methods.","1932-4537","","10.1109/TNSM.2022.3162885","Natural Science Foundation of Anhui Province (P. R. China)(grant numbers:2008085MF188); University Natural Science Research Project of Anhui Province (P.R. China)(grant numbers:KJ2021A0041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9744099","Phishing detection;multi-objective optimization;evolution computation;random forest","Phishing;Statistics;Sociology;Feature extraction;Blocklists;Visualization;Optimization","","12","","59","IEEE","29 Mar 2022","","","IEEE","IEEE Journals"
"An End-to-End Deep Learning Framework for Real-Time Denoising of Heart Sounds for Cardiac Disease Detection in Unseen Noise","S. N. Ali; S. B. Shuvo; M. I. S. Al-Manzo; A. Hasan; T. Hasan","Department of Biomedical Engineering, MHealth Laboratory, Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh; Department of Biomedical Engineering, MHealth Laboratory, Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh; Department of Cardiac Surgery, National Heart Foundation Hospital and Research Institute, Mirpur, Dhaka, Bangladesh; Department of Mechanical and Industrial Engineering, Qatar University, Doha, Qatar; Department of Biomedical Engineering, MHealth Laboratory, Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh",IEEE Access,"22 Aug 2023","2023","11","","87887","87901","The heart sound signals captured via a digital stethoscope are often distorted by environmental and physiological noise, altering their salient and critical properties. The problem is exacerbated in crowded low-resource hospital settings with high noise levels which degrades the diagnostic performance. In this study, we present a novel deep encoder-decoder-based denoising architecture (LU-Net) to suppress ambient and internal lung sound noises. Training is done using a large benchmark PCG dataset mixed with physiological noise, i.e., breathing sounds. Two different noisy datasets were prepared for experimental evaluation by mixing unseen lung sounds and hospital ambient noises with the clean heart sound recordings. We also used the inherently noisy portion of the PASCAL heart sound dataset for evaluation. The proposed framework showed effective suppression of background noises in both unseen real-world data and synthetically generated noisy heart sound recordings, improving the signal-to-noise ratio (SNR) level by 5.575 dB on an average using only 1.32 M parameters. The proposed model outperforms the current state-of-the-art U-Net model with an average SNR improvement of 5.613 dB and 5.537 dB in the presence of lung sound and unseen hospital noise, respectively. LU-Net also outperformed the state-of-the-art Fully Convolutional Network (FCN) by 1.750 dB and 1.748 dB for lung sound and unseen hospital noise conditions, respectively. In addition, the proposed denoising method model improves classification accuracy by 38.93% in the noisy portion of the PASCAL heart sound dataset. The results presented in the paper indicate that our proposed architecture demonstrated a robust denoising performance on different datasets with diverse levels and characteristics of noise. The proposed deep learning-based PCG denoising approach is a pioneering study that can significantly improve the accuracy of computer-aided auscultation systems for detecting cardiac diseases in noisy, low-resource hospitals and underserved communities.","2169-3536","","10.1109/ACCESS.2023.3292551","Qatar National Library; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173517","Heart sound;real time denoising;deep learning;denoising autoencoder;cardiovascular diseases;cardiac disease detection","Noise reduction;Heart;Phonocardiography;Noise measurement;Hospitals;Lungs;Electrocardiography;Deep learning;Cardiovascular diseases;Cardiac disease","","12","","62","CCBY","5 Jul 2023","","","IEEE","IEEE Journals"
"What Users Tweet on NFTs: Mining Twitter to Understand NFT-Related Concerns Using a Topic Modeling Approach","S. C. Meyns; F. Dalipi","Department of Informatics, Linnaeus University, Växjö, Sweden; Department of Informatics, Linnaeus University, Växjö, Sweden",IEEE Access,"15 Nov 2022","2022","10","","117658","117680","Non-fungible token (NFT) trade has grown drastically over recent years. While scholarship on the technical aspects and potential applications of NFTs has been steadily increasing, less attention has been directed to the human perception of or attitudes toward this new type of digital asset. The aim of this research is to investigate what concerns are expressed in relation to non-fungible tokens by those who engage with NFTs on the social media platform Twitter. In this study, data was gathered through online social media data mining of NFT-related posts on Twitter. Two datasets (with 18,373 and 36,354 individual tweet records, respectively) were obtained. Topic modeling was used as a method of data analysis. Our results reveal 19 overall themes of concerns around NFTs as expressed on Twitter, which broadly fall into two categories: concerns about attacks and threats by third parties; and concerns about trading and the role of marketplaces. Overall, this study offers a better understanding of the expressions of concern, uncertainty, and the perception of possible barriers related to NFT trading. These findings contribute to theoretical insight and can, moreover, function as a basis for developing practical design and policy interventions.","2169-3536","","10.1109/ACCESS.2022.3219495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9938986","Concerns;digital asset;non-fungible tokens (NFTs);social media;topic modeling;Twitter","Social networking (online);Blogs;Nonfungible tokens;Media;Data mining;Data models;Soft sensors","","11","","94","CCBY","4 Nov 2022","","","IEEE","IEEE Journals"
"Landslide Mapping Using Multilevel-Feature-Enhancement Change Detection Network","L. Wang; M. Zhang; X. Shen; W. Shi","School of Environment and Spatial Informatics, China University of Mining and Technology, Xuzhou, China; Smart Cities Research Institute and the Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hong Kong; School of Environment and Spatial Informatics, China University of Mining and Technology, Xuzhou, China; Smart Cities Research Institute and the Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hong Kong",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"14 Apr 2023","2023","16","","3599","3610","Landslide mapping (LM) from bitemporal remote sensing images is essential for disaster prevention and mitigation. Although bitemporal change detection technology has been applied for LM, there remains room for improvement in its accuracy and automation. In this article, a multilevel feature enhancement network (MFENet) is proposed for LM based on modules built in convolutional neural networks (CNNs) like CNN-Attention. MFENet mainly consists of three modules: the postevent feature enhancement module (PFEM), the bifeature difference enhancement module (BFDEM), and the flow direction calibration module (FDCM). Specifically, the main role of PFEM is to selectively fuse postevent multilayer features to provide discriminative postevent features. BFDEM fuses the multilayer differences of both pre-event and postevent features to generate high-quality change detection features, which are sufficiently powerful to distinguish foreground from background. FDCM uses a digital elevation model to calibrate the flow direction of each pixel of the landslide detection results to complete the LM task. Experiments were conducted to test the effectiveness of MFENet on two real-world regions, Lantau Island and Sharp Peak, Hong Kong, where landslides occurred after rainstorms. Compared with other state-of-the-art general change detection methods and landslide-specific change detection methods, the proposed method outperforms all metrics, with its intersection over union reaching 87.23%. The availability of additional features and the generalization performance of MFENet are demonstrated experimentally. It is anticipated that the proposed network will further contribute to disaster prevention and mitigation.","2151-1535","","10.1109/JSTARS.2023.3245062","Otto Poon Charitable Foundation Smart Cities Research Institute; Hong Kong Polytechnic University(grant numbers:CD03); Ministry of Science and Technology; People's Republic of China(grant numbers:2019YFB2103102); Beijing Key Laboratory of Urban Spatial Information Engineering(grant numbers:2020101); Hong Kong Polytechnic University(grant numbers:ZVU1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10044973","Change detection;convolutional neural network (CNN);flow direction;landslide mapping (LM);remote sensing images","Feature extraction;Terrain factors;Remote sensing;Convolutional neural networks;Neural networks;Fuses;Image segmentation","","10","","44","CCBY","15 Feb 2023","","","IEEE","IEEE Journals"
"Decentralized Machine Learning Governance: Overview, Opportunities, and Challenges","D. Alsagheer; L. Xu; W. Shi","Department of Computer Science, University of Houston, Houston, TX, USA; Department of Computer Science, Kent State University, Kent, OH, USA; Department of Computer Science, University of Houston, Houston, TX, USA",IEEE Access,"11 Sep 2023","2023","11","","96718","96732","Researchers have started to recognize the necessity for a well-defined ML governance framework based on the principle of decentralization and comprehensively defining its scope of research and practice due to the growth of machine learning (ML) research and applications in the real world and the success of blockchain-based technology. In this paper, we study decentralized ML governance, which includes ML value chain management, decentralized identity for the ML community, decentralized ownership and rights management of ML assets, community-based decision-making for the ML process, decentralized ML finance, and risk management.","2169-3536","","10.1109/ACCESS.2023.3311713","University of Houston; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10238468","Blockchain;DAO;decentralization;governance;MLOps","Blockchains;Machine learning;Data models;Security;Smart contracts;Training;Unified modeling language","","8","","92","CCBYNCND","4 Sep 2023","","","IEEE","IEEE Journals"
"Propagating Configuration Decisions with Modal Implication Graphs","S. Krieter; T. Thüm; S. Schulze; R. Schröter; G. Saake","Otto von Guericke Universitat Magdeburg, Magdeburg, Sachsen-Anhalt, DE; Technische Universitat Braunschweig, Braunschweig, Niedersachsen, DE; Otto von Guericke Universitat Magdeburg, Magdeburg, Sachsen-Anhalt, DE; Otto von Guericke Universitat Magdeburg, Magdeburg, Sachsen-Anhalt, DE; Otto von Guericke Universitat Magdeburg, Magdeburg, Sachsen-Anhalt, DE",2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),"2 Sep 2018","2018","","","898","909","Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.","1558-1225","978-1-4503-5638-1","10.1145/3180155.3180159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453168","Software product line;Configuration;Decision Propagation","Servers;Frequency modulation;Hafnium;Operating systems;Monitoring;Task analysis","","7","","","","2 Sep 2018","","","IEEE","IEEE Conferences"
"Advances in Skeleton-Based Fall Detection in RGB Videos: From Handcrafted to Deep Learning Approaches","V. -H. Hoang; J. W. Lee; M. J. Piran; C. -S. Park","Department of Software, Sejong University, Seoul, South Korea; Department of Software, Sejong University, Seoul, South Korea; Department of Computer Science and Engineering, Sejong University, Seoul, South Korea; Department of Computer Education, Sungkyunkwan University, Seoul, South Korea",IEEE Access,"1 Sep 2023","2023","11","","92322","92352","In the elderly population, falls are one of the leading causes of fatal and non-fatal injuries. Fall detection and early alarms play an important role in mitigating the negative effects of falls, especially given the growing proportion of the elderly population. Due to their non-intrusive nature, data availability, and low deployment costs, RGB videos have been used in many previous studies to detect falls. The RGB data, however, can be affected by background environment changes, resulting in non-recognition. To overcome these challenges, many researchers propose extracting skeleton data from RGB videos and using it for fall detection. Although there have been multiple surveys on fall detection, most of them focus on assessing fall detection systems using different kinds of sensors, and a comprehensive evaluation of skeleton-based fall detection in RGB videos is lacking. In this paper, we examine the most recent advances in skeleton-based fall detection in RGB videos, from handcrafted feature-based methods to advanced deep learning algorithms. Further, we present several skeleton-based fall detection techniques and their performance results on various benchmark datasets, along with challenges and future directions in this field.","2169-3536","","10.1109/ACCESS.2023.3307138","Technology Development Program through the Korean Ministry of Small and Medium Enterprises (SMEs) and Startups(grant numbers:S3147433); Ministry of Science and ICT (MSIT), South Korea, under the Information Technology Research Center (ITRC) Support Program supervised by the Institute for Information & Communications Technology Planning & Evaluation (IITP)(grant numbers:IITP-2022-RS-2022-00156354); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10225507","Deep Learning;fall detection;pose estimation;RGB video;skeleton sequence;skeleton-based fall detection","Deep learning;Fall detection;Pose estimation;Skeleton;Aging;Feature detection","","6","","192","CCBY","21 Aug 2023","","","IEEE","IEEE Journals"
"Are anonymity-seekers just like everybody else? An analysis of contributions to Wikipedia from Tor","C. Tran; K. Champion; A. Forte; B. M. Hill; R. Greenstadt","Department of Computer Science & Engineering, New York University, New York, USA; Department of Communication, University of Washington, Seatle, USA; College of Computing & Informatics, Drexel University, Philadelphia, USA; Department of Communication, University of Washington, Seatle, USA; Department of Computer Science & Engineering, New York University, New York, USA",2020 IEEE Symposium on Security and Privacy (SP),"30 Jul 2020","2020","","","186","202","User-generated content sites routinely block contributions from users of privacy-enhancing proxies like Tor because of a perception that proxies are a source of vandalism, spam, and abuse. Although these blocks might be effective, collateral damage in the form of unrealized valuable contributions from anonymity seekers is invisible. One of the largest and most important user-generated content sites, Wikipedia, has attempted to block contributions from Tor users since as early as 2005. We demonstrate that these blocks have been imperfect and that thousands of attempts to edit on Wikipedia through Tor have been successful. We draw upon several data sources and analytical techniques to measure and describe the history of Tor editing on Wikipedia over time and to compare contributions from Tor users to those from other groups of Wikipedia users. Our analysis suggests that although Tor users who slip through Wikipedia's ban contribute content that is more likely to be reverted and to revert others, their contributions are otherwise similar in quality to those from other unregistered participants and to the initial contributions of registered users.","2375-1207","978-1-7281-3497-0","10.1109/SP40000.2020.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152664","","Internet;Encyclopedias;Electronic publishing;IP networks;Relays;Peer-to-peer computing","","5","","39","IEEE","30 Jul 2020","","","IEEE","IEEE Conferences"
"AI auditing: The Broken Bus on the Road to AI Accountability","A. Birhane; R. Steed; V. Ojewale; B. Vecchione; I. D. Raji","Mozilla Foundation and Sch. of Computer Science and Statistics, Trinity College Dublin, Dublin, Ireland; Heinz College & Machine Learning Dept., Carnegie Mellon University, Pittsburgh, USA; Ctr. for Technological Responsibility, Reimagination, & Redesign, Brown University, Providence, USA; Data & Society, New York City, USA; Mozilla Foundation and University of California, Berkeley, Berkeley, USA",2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML),"10 May 2024","2024","","","612","643","One of the most concrete measures towards meaningful AI accountability is to consequentially assess and report the systems’ performance and impact. However, the practical nature of the ""AI audit"" ecosystem is muddled and imprecise, making it difficult to work through various concepts, practices, and involved (as well as ignored) stakeholders. First, we taxonomize current AI audit practices as completed by regulators, law firms, civil society, journalism, academia, and consulting agencies. Next, we assess the impact of audits done by stakeholders within each domain. We find that only a subset of AI audit studies translate to desired accountability outcomes. We thus assess and isolate practices necessary for effective AI audit results, articulating the observed connections between AI audit design, methodology and institutional context on its effectiveness as a meaningful mechanism for accountability.","","979-8-3503-4950-4","10.1109/SaTML59370.2024.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516659","Index Terms—Evaluation;auditing;accountability;transparency;artificial intelligence;society;law;machine learning;data science","Regulators;Roads;Design methodology;Ecosystems;Machine learning;Journalism;Regulation","","4","","197","IEEE","10 May 2024","","","IEEE","IEEE Conferences"
"Federated Learning for Urban Sensing Systems: A Comprehensive Survey on Attacks, Defences, Incentive Mechanisms, and Applications","A. Kapoor; D. Kumar","Department of Electronics and Communication Engineering, Indian Institute of Technology (IIT) Roorkee, Uttarakhand, India; Department of Electronics and Communication Engineering, Indian Institute of Technology (IIT) Roorkee, Uttarakhand, India",IEEE Communications Surveys & Tutorials,"","2024","PP","99","1","1","In recent years, advancements in Artificial Intelligence (AI), the Internet of Things (IoT) and wireless technologies have propelled the evolution of smart cities. Urban sensing systems collect real-time data from urban areas for various applications, such as environmental monitoring, healthcare, and intelligent transportation, that contribute to the growth of smart cities. In urban sensing, the active participation of users gives rise to participatory sensing, where individuals contribute real-time data through their smartphones or IoT devices, but it encounters bottlenecks in communication, network latency, and user privacy with an exponential rise in data. A prominent characteristic of urban sensing applications is the highly individualized and personal nature of the data, e.g., location and time. Hence, adequate privacy and security provisions are required for these applications to succeed on a high scale. Conventional centralised machine learning approaches expose participants to potential vulnerabilities from malicious tasking servers or inference based on anonymized data. Federated learning (FL) has been proposed as the most viable alternative that leverages the advances in modern-day smartphones’ computation and communication capabilities by allowing participants to train local models on their devices. These models are aggregated by the application server to form a global model without the need for users to share their private data. However, large-scale FL-based urban sensing systems are still not practical due to various challenges associated with their real-life implementation. This paper presents a comprehensive survey addressing practical challenges in implementing FL-based urban sensing applications, e.g., inference attacks, poisoning attacks, and fair incentivization to participants while preserving privacy. We then provide an extensive survey on the use of FL in various urban sensing applications, highlighting that current applications do not simultaneously address all three aforementioned challenges. We conclude this survey by highlighting the research challenges to form a practical FL-based urban sensing system and future research directions.","1553-877X","","10.1109/COMST.2024.3434510","Ministry of Education, India(grant numbers:PM-31-22-732-414,Prime minister's Research Fellowship (PMRF)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10612842","Federated learning;urban sensing;smart city;inference attack;poisoning attack;incentive mechanism","Sensors;Surveys;Data models;Smart cities;Internet of Things;Servers;Security","","3","","","IEEE","29 Jul 2024","","","IEEE","IEEE Early Access Articles"
"Evaluating Data Attribution for Text-to-Image Models","S. -Y. Wang; A. A. Efros; J. -Y. Zhu; R. Zhang",Carnegie Mellon University; UC Berkeley; Carnegie Mellon University; Adobe Research,2023 IEEE/CVF International Conference on Computer Vision (ICCV),"15 Jan 2024","2023","","","7158","7169","While large text-to-image models are able to synthesize ""novel"" images, these images are necessarily a reflection of the training data. The problem of data attribution in such models – which of the images in the training set are most responsible for the appearance of a given generated image – is a difficult yet important one. As an initial step toward this problem, we evaluate attribution through ""customization"" methods, which tune an existing large-scale model toward a given exemplar object or style. Our key insight is that this allow us to efficiently create synthetic images that are computationally influenced by the exemplar by construction. With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces. Furthermore, by training on our dataset, we can tune standard models, such as DINO, CLIP, and ViT, toward the attribution problem. Even though the procedure is tuned towards small exemplar sets, we show generalization to larger sets. Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images.","2380-7504","979-8-3503-0718-4","10.1109/ICCV51070.2023.00661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10377133","","Training;Computer vision;Uncertainty;Computational modeling;Training data;Data models;Reflection","","2","","71","IEEE","15 Jan 2024","","","IEEE","IEEE Conferences"
"Commonsense Injection in Conversational Systems: An Adaptable Framework for Query Expansion","G. Rocchietti; O. Frieder; C. I. Muntean; F. M. Nardini; R. Perego","ISTI-CNR, University of Pisa, Pisa, Italy; Georgetown University, Washington, DC, USA; ISTI-CNR, Pisa, Italy; ISTI-CNR, Pisa, Italy; ISTI-CNR, Pisa, Italy",2023 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),"19 Dec 2023","2023","","","48","55","Recent advancements in conversational agents are leading a paradigm shift in how people search for their information needs, from text queries to entire spoken conversations. This paradigm shift poses a new challenge: a single question may lack the context driven by the entire conversation. We propose and evaluate a framework to deal with multi-turn conversations with the injection of commonsense knowledge. Specifically, we propose a novel approach for conversational search that uses pre- trained large language models and commonsense knowledge bases to enrich queries with relevant concepts. Our framework comprises a generator of candidate concepts related to the context of the conversation and a selector for deciding which candidate concept to add to the current utterance to improve retrieval effectiveness. We use the TREC CAsT datasets and ConceptNet to show that our framework improves retrieval performance by up to 82% in terms of Recall@200 and up to 154% in terms of NDCG@3 as compared to the performance achieved by the original utterances in the conversations.","","979-8-3503-0918-8","10.1109/WI-IAT59888.2023.00013","European Union (EU); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350091","conversational systems;query expansion;commonsense knowledge;KBs;information retrieval","Oral communication;Generators;Intelligent agents;Commonsense reasoning","","2","","28","IEEE","19 Dec 2023","","","IEEE","IEEE Conferences"
"Multi-modal visual tracking: Review and experimental comparison","P. Zhang; D. Wang; H. Lu","Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian 116024, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian 116024, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian 116024, China",Computational Visual Media,"20 Feb 2025","2024","10","2","193","214","Visual object tracking has been drawing increasing attention in recent years, as a fundamental task in computer vision. To extend the range of tracking applications, researchers have been introducing information from multiple modalities to handle specific scenes, with promising research prospects for emerging methods and benchmarks. To provide a thorough review of multi-modal tracking, different aspects of multi-modal tracking algorithms are summarized under a unified taxonomy, with specific focus on visible-depth (RGB-D) and visible-thermal (RGB-T) tracking. Subsequently, a detailed description of the related benchmarks and challenges is provided. Extensive experiments were conducted to analyze the effectiveness of trackers on five datasets: PTB, VOT19-RGBD, GTOT, RGBT234, and VOT19-RGBT. Finally, various future directions, including model design and dataset construction, are discussed from different perspectives for further research.","2096-0662","","10.1007/s41095-023-0345-5","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897644","visual tracking;object tracking;multi-modal fusion;RGB-T tracking;RGB-D tracking","Target tracking;Radar tracking;Surveys;Feature extraction;Visualization;Reviews;Object tracking;Accuracy;Cameras;Three-dimensional displays","","2","","","","20 Feb 2025","","","TUP","TUP Journals"
"Foundation models meet visualizations: Challenges and opportunities","W. Yang; M. Liu; Z. Wang; S. Liu","School of Software, Tsinghua University, Beijing 100084, China; Microsoft, Redmond 98052, USA; School of Software, Tsinghua University, Beijing 100084, China; School of Software, Tsinghua University, Beijing 100084, China",Computational Visual Media,"20 Feb 2025","2024","10","3","399","424","Recent studies have indicated that foundation models, such as BERT and GPT, excel at adapting to various downstream tasks. This adaptability has made them a dominant force in building artificial intelligence (AI) systems. Moreover, a new research paradigm has emerged as visualization techniques are incorporated into these models. This study divides these intersections into two research areas: visualization for foundation model (VIS4FM) and foundation model for visualization (FM4VIS). In terms of VIS4FM, we explore the primary role of visualizations in understanding, refining, and evaluating these intricate foundation models. VIS4FM addresses the pressing need for transparency, explainability, fairness, and robustness. Conversely, in terms of FM4VIS, we highlight how foundation models can be used to advance the visualization field itself. The intersection of foundation models with visualizations is promising but also introduces a set of challenges. By highlighting these challenges and promising opportunities, this study aims to provide a starting point for the continued exploration of this research avenue.","2096-0662","","10.1007/s41095-023-0393-x","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897629","visualization;artificial intelligence (AI);machine learning;foundation models;visualization for foundation model (VIS4FM);foundation model for visualization (FM4VIS)","Foundation models;Data visualization;Adaptation models;Data models;Analytical models;Visualization;Training;Artificial intelligence;Machine learning;Predictive models","","1","","","","20 Feb 2025","","","TUP","TUP Journals"
"Foundation Data Space Models: Bridging the Artificial Intelligence and Data Ecosystems (Vision Paper)","E. Curry; T. Zaarour; Y. Yang; M. Timilsina; M. Al-Qatf; R. Haque","Insight Centre for Data Analytics, University of Galway, Ireland; Insight Centre for Data Analytics, University of Galway, Ireland; Insight Centre for Data Analytics, University of Galway, Ireland; Insight Centre for Data Analytics, University of Galway, Ireland; Insight Centre for Data Analytics, University of Galway, Ireland; Insight Centre for Data Analytics, University of Galway, Ireland",2023 IEEE International Conference on Big Data (BigData),"22 Jan 2024","2023","","","190","195","Two major trends significantly changed the global Artificial Intelligence (AI) and Data landscape. Recent AI and Machine Learning developments are driving a paradigm shift to creating large task-agnostic foundation models pre-trained using web-scale data. Foundation models are then adapted to different downstream tasks via techniques such as fine-tuning. At the same time, we see a movement to the creation of large-scale data-sharing infrastructures. Data Spaces are an emerging approach to data management and sharing at the core of the European Data Strategy to provide access to high-quality data for AI. This paper brings together work on foundation models and data spaces into a holistic vision for Foundation Data Space Models. The paper highlights the data management requirements challenges for data spaces and details a high-level approach for foundation data space models together with a unified lifecycle for data spaces and foundation models. Finally, it sets out a research agenda.","","979-8-3503-2445-7","10.1109/BigData59044.2023.10386161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386161","foundation models;data spaces;lifecycle;data management;artificial intelligence","Symbiosis;Adaptation models;Biological system modeling;Ecosystems;Europe;Distributed databases;Machine learning","","1","","12","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"Exploring Public Response to ChatGPT With Sentiment Analysis and Knowledge Mapping","J. Zhou; Z. Liang; Y. Fang; Z. Zhou","College of Art, Zhejiang Normal University, Jinhua, China; School of Arts and Media, Guangzhou Vocational and Technical University of Science and Technology, Guangzhou, China; Faulty of Social Science and Humanities, Universiti Teknologi Malaysia, Johor Bahru, Malaysia; College of Engineering, Northern Arizona University, Flagstaff, AZ, USA",IEEE Access,"15 Apr 2024","2024","12","","50504","50516","Information about artificial intelligence generated content (AIGC) has attracted great attention from the public. ChatGPT has gained attention from many countries due to its powerful functionality and efficiency. However, there are still many countries, such as China, that refuse to introduce ChatGPT into their own countries. This article aims to study the attention that ChatGPT has attracted in China, further investigate the reasons for the obstruction of ChatGPT’s dissemination, and provide prospects and suggestions for the global development of artificial intelligence. First, we apply the Latent Dirichlet Allocation (LDA) topic modeling algorithm to analyze the 28,122 web tweets and comments from China regarding ChatGPT. The results show that there are more negative emotions compared to positive and neutral emotions. The attention on ChatGPT rapidly increased twice between March and May 2023. We find the audience interest and concern words:“improvement”, “company” and “technology”. The analysis reveals three main themes: social, technological, and educational. Additionally, we conduct knowledge mapping to analyze the publication time, research hotspots, and subject distribution of Chinese scholars’ literature. In conclusion, this paper highlights several key issues that need to be addressed for the further advancement of AIGC, including the evolution of job roles, the changing technological landscape, the pursuit of artificial general intelligence, and ethical concerns.","2169-3536","","10.1109/ACCESS.2024.3386362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10494726","LDA modeling;artificial intelligence;sentiment analysis;ChatGPT","Chatbots;Blogs;Sentiment analysis;Analytical models;Tag clouds;Recurrent neural networks;Artificial intelligence;Information analysis;Ethics;Social networking (online);Knowledge management;User experience","","1","","71","CCBYNCND","8 Apr 2024","","","IEEE","IEEE Journals"
"WFormer: A Transformer-Based Soft Fusion Model for Robust Image Watermarking","T. Luo; J. Wu; Z. He; H. Xu; G. Jiang; C. -C. Chang","College of Science and Technology, Ningbo University, Ningbo, China; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China; College of Science and Technology, Ningbo University, Ningbo, China; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China; Department of Information Engineering and Computer Science, Feng Chia University, Taichung, Taiwan",IEEE Transactions on Emerging Topics in Computational Intelligence,"22 Nov 2024","2024","8","6","4179","4196","Most deep neural network (DNN) based image watermarking models often employ the encoder-noise-decoder structure, in which watermark is simply duplicated for expansion and then directly fused with image features to produce the encoded image. However, simple duplication will generate watermark over-redundancies, and the communication between the cover image and watermark in different domains is lacking in image feature extraction and direction fusion, which degrades the watermarking performance. To solve those drawbacks, this paper proposes a Transformer-based soft fusion model for robust image watermarking, namely WFormer. Specifically, to expand watermark effectively, a watermark preprocess module (WPM) is designed with Transformers to extract valid and expanded watermark features by computing its self-attention. Then, to replace direct fusion, a soft fusion module (SFM) is deployed to integrate Transformers into image fusion with watermark by mining their long-range correlations. Precisely, self-attention is computed to extract their own latent features, and meanwhile, cross-attention is learned for bridging their gap to embed watermark effectively. In addition, a feature enhancement module (FEM) builds communication between the cover image and watermark by capturing their cross-feature dependencies, which tunes image features in accordance with watermark features for better fusion. Experimental results show that the proposed WFormer outperforms the existing state-of-the-art watermarking models in terms of invisibility, robustness, and embedding capacity. Furthermore, ablation results prove the effectiveness of the WPM, the FEM, and the SFM.","2471-285X","","10.1109/TETCI.2024.3386916","National Natural Science Foundation of China(grant numbers:61971247,61501270,62171243); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LY22F020020,LQ23F010011); Natural Science Foundation of Ningbo(grant numbers:2021J134,2022J136); Zhejiang Provincial Postdoctoral Research Excellence Foundation(grant numbers:ZJ2022130); Science and Technology Innovation 2025 Major Project of Ningbo Municipality(grant numbers:2021Z069); “Leading Goose” R&D Program of Zhejiang Province(grant numbers:2024C01107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10505734","Watermarking;transformer;soft fusion;cross-attention","Watermarking;Feature extraction;Transformers;Decoding;Convolutional neural networks;Noise measurement;Robustness;Artificial neural networks","","1","","50","IEEE","19 Apr 2024","","","IEEE","IEEE Journals"
"Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat","S. Feng; H. Lu; J. Jiang; T. Xiong; L. Huang; Y. Liang; X. Li; Y. Deng; A. Aleti","Monash University, Melbourne, Australia; Tencent Inc., Guangzhou, China; Tencent Inc., Guangzhou, China; Tencent Inc., Guangzhou, China; Tencent Inc., Guangzhou, China; Tencent Inc., Guangzhou, China; Tencent Inc., Guangzhou, China; Tencent Inc., Guangzhou, China; Monash University, Melbourne, Australia",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","1973","1978","UI automation tests play a crucial role in ensuring the quality of mobile applications. Despite the growing popularity of machine learning techniques to generate these tests, they still face several challenges, such as the mismatch of UI elements. The recent advances in Large Language Models (LLMs) have addressed these issues by leveraging their semantic understanding capabilities. However, a significant gap remains in applying these models to industrial-level app testing, particularly in terms of cost optimization and knowledge limitation. To address this, we introduce CAT to create cost-effective UI automation tests for industry apps by combining machine learning and LLMs with best practices. Given the task description, CAT employs Retrieval Augmented Generation (RAG) to source examples of industrial app usage as the few-shot learning context, assisting LLMs in generating the specific sequence of actions. CAT then employs machine learning techniques, with LLMs serving as a complementary optimizer, to map the target element on the UI screen. Our evaluations on the WeChat testing dataset demonstrate the CAT’s performance and cost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming the state-of-the-art. We have also integrated our approach into the real-world WeChat testing platform, demonstrating its usefulness in detecting 141 bugs and enhancing the developers’ testing process.CCS CONCEPTS• Software and its engineering → Software testing and debugging.","2643-1572","979-8-4007-1248-7","","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765004","UI automation test;large language model;retrieval-augmented generation;cost optimization","Software testing;Automation;Costs;Social networking (online);Computer bugs;Message services;Mobile applications;Optimization;Testing;Software engineering","","1","","37","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Image resizing by reconstruction from deep features","D. Danon; M. Arar; D. Cohen-Or; A. Shamir","Tel Aviv University, Tel Aviv, 69978, Israel; Tel Aviv University, Tel Aviv, 69978, Israel; Tel Aviv University, Tel Aviv, 69978, Israel; The Interdisciplinary Center Herzliya, Herzliya, 4610101, Israel",Computational Visual Media,"20 Feb 2025","2021","7","4","453","466","Traditional image resizing methods usually work in pixel space and use various saliency measures. The challenge is to adjust the image shape while trying to preserve important content. In this paper we perform image resizing in feature space using the deep layers of a neural network containing rich important semantic information. We directly adjust the image feature maps, extracted from a pre-trained classification network, and reconstruct the resized image using neural-network based optimization. This novel approach leverages the hierarchical encoding of the network, and in particular, the high-level discriminative power of its deeper layers, that can recognize semantic regions and objects, thereby allowing maintenance of their aspect ratios. Our use of reconstruction from deep features results in less noticeable artifacts than use of image-space resizing operators. We evaluate our method on benchmarks, compare it to alternative approaches, and demonstrate its strengths on challenging images.","2096-0662","","10.1007/s41095-021-0216-x","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897522","image retargeting;reconstruction;deep seam carving;image resizing","Image reconstruction;Feature extraction;Semantics;Optimization;Media;Deformation;Deep learning;Visualization;Shape;Extraterrestrial measurements","","1","","","","20 Feb 2025","","","TUP","TUP Journals"
"Phishing Webpage Detection: Unveiling the Threat Landscape and Investigating Detection Techniques","A. Kulkarni; V. Balachandran; T. Das","Indian Institute of Technology (IIT) Dharwad, India; Singapore Institute of Technology, Singapore; Indian Institute of Technology (IIT) Dharwad, India",IEEE Communications Surveys & Tutorials,"","2024","PP","99","1","1","In the realm of cybersecurity, phishing stands as a prevalent cyber attack, where attackers employ various tactics to deceive users into gathering their sensitive information, potentially leading to identity theft or financial gain. Researchers have been actively working on advancing phishing webpage detection approaches to detect new phishing URLs, bolstering user protection. Nonetheless, the ever-evolving strategies employed by attackers, aimed at circumventing existing detection approaches and tools, present an ongoing challenge to the research community. This survey presents a systematic categorization of diverse phishing webpage detection approaches, encompassing URL-based, webpage content-based, third-party-based and visual techniques. Through a comprehensive review of these approaches and an in-depth analysis of existing literature, our study underscores current research gaps in phishing webpage detection. Furthermore, we suggest potential solutions to address some of these gaps, contributing valuable insights to the ongoing efforts to combat phishing attacks.","1553-877X","","10.1109/COMST.2024.3441752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10633701","Cybersecurity;Phishing Webpage Detection;Machine Learning;Deep Learning","Phishing;Feature extraction;Surveys;Computer security;Reviews;Electronic mail;Uniform resource locators","","1","","","IEEE","12 Aug 2024","","","IEEE","IEEE Early Access Articles"
"Case Studies","C. Parraman; M. V. Ortiz Segovia",NA; NA,2.5D Printing: Bridging the Gap Between 2D and 3D Applications,"","2018","","","127","237","This chapter explores a miscellany of subjects and provides a short historical context, contemporary methods that are now being used, visual cross‐overs and comparisons and suggestions for further reading. The case studies demonstrate a survey of printing and reproduction methods used in the past and how many have evolved over time. The chapter also explores the ideas, traditional processes and materials that may have been modified or recycled in response to emerging technologies and trends. The reason for including a case study on wallcovering, is that, like the traditional print, it demonstrates several centuries of provenance, and through print archives and picture books, highlights 2.5dimensional (2.5D) printings as applied design and art form. Advances in 2.5D and three‐dimensional (3D) printing also allow the production of accurate three‐dimensional physical models. The research also looks to develop accompanying software and data representation to produce a cohesive workflow toward future 2.5D printing technology and its creative applications.","","9781118967331","10.1002/9781118967317.ch5","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8527615.pdf&bkn=8527584&pdfType=chapter","","Printing;Image color analysis;Surface treatment;Absorption;Pigments;Visualization;Turning","","1","","","","12 Nov 2018","","","Wiley","Wiley Telecom eBook Chapters"
"Protecting the Distribution of Color Images via Inverse Colorization, Visible-Imperceptible Watermarking and Reversible Data Hiding","E. Fragoso-Navarro; F. Garcia-Ugalde; M. Cedillo-Hernandez","Facultad de Ingenieria, Universidad Nacional Autónoma de México (UNAM), Coyoacán, Mexico City, Mexico; Facultad de Ingenieria, Universidad Nacional Autónoma de México (UNAM), Coyoacán, Mexico City, Mexico; Escuela Superior de Ingenieria Mecanica y Electrica, Instituto Politécnico Nacional (IPN), Coyoacán, Mexico City, Mexico",IEEE Access,"23 Jun 2023","2023","11","","61025","61048","Invertible color-to-gray algorithms are good alternatives for protecting color images in storage, transmission, or limited-access environments. These approaches hide color information in a gray-scale version of an image. Thus, illegal recovery of the original color becomes a challenging task, and only authorized users can restore the colorized image. However, although the color is protected, the original structure of the image remains vulnerable to illegal usage. In this paper, we present a reversible distortion of the protected gray-scale image to make illegal reconstruction more difficult. In addition, we preserve the general visibility of the original content. Furthermore, a visible imperceptible watermark is embedded to protect ownership of the colorized image in public access. The watermark remains imperceptible in the colorized image and reveals the logo of the owner when the image is protected. We propose different levels of distortions obtaining mean PSNR qualities between 14.74 dB and 32.92 dB with respect to a reference gray-scale image. Furthermore, the mean PSNR quality of the colorized images remains between 38.87 dB and 41.55 dB.","2169-3536","","10.1109/ACCESS.2023.3286865","Universidad Nacional Autónoma de México (UNAM) under the Dirección General de Asuntos del Personal Académico (DGAPA) Postdoctoral Scholarship Program and Research Project(grant numbers:PAPIIT-IT100123); Instituto Politécnico Nacional (IPN); Consejo Nacional de Ciencia y Tecnología (CONACYT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10154034","Colorization;contrast enhancement;image distribution;image protection;invertible color-to-gray;reversible data hiding;visible-imperceptible watermark","Image color analysis;Gray-scale;Indexes;Watermarking;Color;Distortion;Visualization","","1","","47","CCBYNCND","16 Jun 2023","","","IEEE","IEEE Journals"
"Light field salient object detection: A review and benchmark","K. Fu; Y. Jiang; G. -P. Ji; T. Zhou; Q. Zhao; D. -P. Fan","College of Computer Science, Sichuan University, and National Key Laboratory of Fundamental Science on Synthetic Vision, Sichuan University, Chengdu 610065, China; College of Computer Science, Sichuan University, and National Key Laboratory of Fundamental Science on Synthetic Vision, Sichuan University, Chengdu 610065, China; School of Computer Science, Wuhan University, Wuhan 430072, China; PCA Lab, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China; College of Computer Science, Sichuan University, and National Key Laboratory of Fundamental Science on Synthetic Vision, Sichuan University, Chengdu 610065, China; Computer Vision Lab, ETH Zürich, Zürich, Switzerland",Computational Visual Media,"20 Feb 2025","2022","8","4","509","534","Salient object detection (SOD) is a longstanding research topic in computer vision with increasing interest in the past decade. Since light fields record comprehensive information of natural scenes that benefit SOD in a number of ways, using light field inputs to improve saliency detection over conventional RGB inputs is an emerging trend. This paper provides the first comprehensive review and a benchmark for light field SOD, which has long been lacking in the saliency community. Firstly, we introduce light fields, including theory and data forms, and then review existing studies on light field SOD, covering ten traditional models, seven deep learning-based models, a comparative study, and a brief review. Existing datasets for light field SOD are also summarized. Secondly, we benchmark nine representative light field SOD models together with several cutting-edge RGB-D SOD models on four widely used light field datasets, providing insightful discussions and analyses, including a comparison between light field SOD and RGB-D SOD models. Due to the inconsistency of current datasets, we further generate complete data and supplement focal stacks, depth maps, and multi-view images for them, making them consistent and uniform. Our supplemental data make a universal benchmark possible. Lastly, light field SOD is a specialised problem, because of its diverse data representations and high dependency on acquisition hardware, so it differs greatly from other saliency detection tasks. We provide nine observations on challenges and future directions, and outline several open issues. All the materials including models, datasets, benchmarking results, and supplemented light field datasets are publicly available at https://github.com/kerenfu/LFSOD-Survey.","2096-0662","","10.1007/s41095-021-0256-2","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897577","light field;salient object detection (SOD);deep learning;benchmarking","Reviews;Benchmark testing;Data models;Object detection;Cameras;Lenses;Computational modeling;Three-dimensional displays;Computer vision;Arrays","","1","","","","20 Feb 2025","","","TUP","TUP Journals"
"Coordinated Robotic Exploration of Dynamic Open Ocean Phenomena","J. Pinto; M. Costa; R. Mendes; K. Lima; P. Dias; J. Pereira; M. Ribeiro; R. Campos; M. P. Tomasino; C. Magalhães; F. L. Castejón; J. Gilabert; A. M. S. Ferreira; J. C. B. da Silva; P. Relvas; T. Lukaczyk; K. A. Skarpnes; E. Davies; A. Chekalyuk; B. Loureiro; I. G. Brosnan; J. Li; J. B. de Sousa; K. Rajan","Laboratório de Sistemas e Tecnologia Subaquática (LSTS), Faculdade de Engenharia da Universidade do Porto, Portugal; OceanScan Marine Systems & Technology, Matosinhos, Portugal; Laboratório de Sistemas e Tecnologia Subaquática (LSTS), Faculdade de Engenharia da Universidade do Porto, Portugal; Laboratório de Sistemas e Tecnologia Subaquática (LSTS), Faculdade de Engenharia da Universidade do Porto, Portugal; Collaborative Laboratory +Atlantic, Porto, Portugal; Laboratório de Sistemas e Tecnologia Subaquática (LSTS), Faculdade de Engenharia da Universidade do Porto, Portugal; Laboratório de Sistemas e Tecnologia Subaquática (LSTS), Faculdade de Engenharia da Universidade do Porto, Portugal; Laboratório de Sistemas e Tecnologia Subaquática (LSTS), Faculdade de Engenharia da Universidade do Porto, Portugal; Laboratório de Sistemas e Tecnologia Subaquática (LSTS), Faculdade de Engenharia da Universidade do Porto, Portugal; Laboratório de Sistemas e Tecnologia Subaquática (LSTS), Faculdade de Engenharia da Universidade do Porto, Portugal; OceanScan Marine Systems & Technology, Matosinhos, Portugal; Interdisciplinary Centre of Marine and Environmental Research, Univ. of Porto, Matosinhos, Portugal; Interdisciplinary Centre of Marine and Environmental Research, Univ. of Porto, Matosinhos, Portugal; Technical University of Cartagena (UPCT), Spain; Technical University of Cartagena (UPCT), Spain; Laboratório de Sistemas e Tecnologia Subaquática (LSTS), Faculdade de Engenharia da Universidade do Porto, Portugal; Interdisciplinary Centre of Marine and Environmental Research, Univ. of Porto, Matosinhos, Portugal; Department of Geosciences, Environment and Spatial Planning, Univ. of Porto, Porto, Portugal; Department of Geosciences, Environment and Spatial Planning, Univ. of Porto, Porto, Portugal; Institute of Earth Sciences, University of Porto, Porto, Portugal; CCMAR and Univ. of Algarve, Faro, Portugal; FlightWave Aerospace Systems, Santa Monica, California, United States; Department of Marine Technology, Norwegian Univ. of Science & Technology, Trondheim, Norway; Department of Environment and New Resources, SINTEF Ocean, Trondheim, Norway; Columbia University, New York, USA; CIBIO - Centro de Investigação em Biodiversidade e Recursos Genéticos, Vairão, Portugal; NASA Ames Research Center, Moffett Field, California, United States; Laboratório de Sistemas e Tecnologia Subaquática (LSTS), Faculdade de Engenharia da Universidade do Porto, Portugal; Interdisciplinary Centre of Marine and Environmental Research, Univ. of Porto, Matosinhos, Portugal; Laboratório de Sistemas e Tecnologia Subaquática (LSTS), Faculdade de Engenharia da Universidade do Porto, Portugal; Laboratório de Sistemas e Tecnologia Subaquática (LSTS), Faculdade de Engenharia da Universidade do Porto, Portugal; SIFT LLC, Minneapolis, Minnesota, United States",Field Robotics,"25 Feb 2025","2022","2","","843","871","The study of dynamic features of the ocean, in which complex physical, chemical, and biological interactions evolve on multiple time scales, poses significant sampling challenges because the required spatial and temporal resolutions are not possible by ship or satellite studies alone. Satellite remote sensing captures only surface effects while expensive research vessels can only make discrete observations in finite periods of time. Our work with networked marine robotics in the aerial, surface, and underwater domains is at the vanguard of a new approach to scientific exploration and observation, which brings together several technologies to enable oceanographic vessels and robots to work in tandem, thus expanding the observational footprint of these vessels. We describe a scientific cruise in the Spring of 2018 in the open waters of the Pacific where we deployed a fleet of autonomous robots to demonstrate this approach for the synoptic observation of mesoscale and sub-mesoscale features of a frontal zone. We articulate the elements and methods to multi-vehicle coordination and challenges that lie ahead in ocean observation.","2771-3989","","10.55417/fr.2022028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10876018","Multi-domain Inter-disciplinary Ocean Exploration;Networked Marine Robotics;Multi-vehicle operations;Mixed-initiative control","Robots;Robot kinematics;Sea measurements;Sea surface;Vehicle dynamics;Satellites;Ocean temperature;Remote sensing;Robot sensing systems;Meteorology","","","","","CCBY","25 Feb 2025","","","FRPS","FRPS Journals"
"Foveated rendering: A state-of-the-art survey","L. Wang; X. Shi; Y. Liu","State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing 100000, China; Peng Cheng Laboratory, Shengzhen 518000, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing 100000, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing 100000, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing 100000, China",Computational Visual Media,"20 Feb 2025","2023","9","2","195","228","Recently, virtual reality (VR) technology has been widely used in medical, military, manufacturing, entertainment, and other fields. These applications must simulate different complex material surfaces, various dynamic objects, and complex physical phenomena, increasing the complexity of VR scenes. Current computing devices cannot efficiently render these complex scenes in real time, and delayed rendering makes the content observed by the user inconsistent with the user's interaction, causing discomfort. Foveated rendering is a promising technique that can accelerate rendering. It takes advantage of human eyes' inherent features and renders different regions with different qualities without sacrificing perceived visual quality. Foveated rendering research has a history of 31 years and is mainly focused on solving the following three problems. The first is to apply perceptual models of the human visual system into foveated rendering. The second is to render the image with different qualities according to foveation principles. The third is to integrate foveated rendering into existing rendering paradigms to improve rendering performance. In this survey, we review foveated rendering research from 1990 to 2021. We first revisit the visual perceptual models related to foveated rendering. Subsequently, we propose a new foveated rendering taxonomy and then classify and review the research on this basis. Finally, we discuss potential opportunities and open questions in the foveated rendering field. We anticipate that this survey will provide new researchers with a high-level overview of the state-of-the-art in this field, furnish experts with up-to-date information, and offer ideas alongside a framework to VR display software and hardware designers and engineers.","2096-0662","","10.1007/s41095-022-0306-4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897656","foveated rendering;virtual reality (VR);real-time rendering","Rendering (computer graphics);Visualization;Sensitivity;Surveys;Spatial resolution;Taxonomy;Reviews;Solid modeling;Computational modeling;Virtual reality","","","","","","20 Feb 2025","","","TUP","TUP Journals"
"Why is AI not a Panacea for Data Workers? An Interview Study on Human-AI Collaboration in Data Storytelling","H. Li; Y. Wang; Q. V. Liao; H. Qu","Hong Kong University of Science and Technology, Hong Kong; Microsoft Research Asia, China; Microsoft Research Montréal, Canada; Hong Kong University of Science and Technology, Hong Kong",IEEE Transactions on Visualization and Computer Graphics,"","2025","PP","99","1","16","This paper explores the potential for human-AI collaboration in the context of data storytelling for data workers. Data storytelling communicates insights and knowledge from data analysis. It plays a vital role in data workers' daily jobs since it boosts team collaboration and public communication. However, to make an appealing data story, data workers need to spend tremendous effort on various tasks, including outlining and styling the story. Recently, a growing research trend has been exploring how to assist data storytelling with advanced artificial intelligence (AI). However, existing studies focus more on individual tasks in the workflow of data storytelling and do not reveal a complete picture of humans' preference for collaborating with AI. To address this gap, we conducted an interview study with 18 data workers to explore their preferences for AI collaboration in the planning, implementation, and communication stages of their workflow. We propose a framework for expected AI collaborators' roles, categorize people's expectations for the level of automation for different tasks, and delve into the reasons behind them. Our research provides insights and suggestions for the design of future AI-powered data storytelling tools.","1941-0506","","10.1109/TVCG.2025.3552017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10930704","Data storytelling;Human-AI collaboration;Interview study","Artificial intelligence;Collaboration;Interviews;Data science;Data visualization;Automated machine learning;Data models;Industries;Image color analysis;Visual analytics","","","","","IEEE","17 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Mastering iOS 18 Development: Take your iOS development experience to the next level with iOS, Xcode, Swift, and SwiftUI","A. Tsadok",NA,"Mastering iOS 18 Development: Take your iOS development experience to the next level with iOS, Xcode, Swift, and SwiftUI","","2024","","","","","Elevate your iOS development skills with this comprehensive iOS 18 guide covering the latest changes and improvements in iOS SDK and Swift, while mastering practical techniques to become an exceptional iOS developerKey FeaturesStay up to date with the latest changes and improvements in iOS SDK and Swift programming languageLearn how you can improve user experience by focusing on customizing components and animationsGet to grips with advanced topics such as SwiftData and high-efficiency applications through an in-depth discussionPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionEmbark on a comprehensive iOS 18 development journey with Avi Tsadok, a veteran iOS developer and author of 4 books and over 40 tutorials and articles. A recognized public speaker, Avi has a knack for demystifying complex concepts and brings unparalleled expertise to the forefront of iOS 18 development education. This guide focuses on iOS 18 advancements, equipping developers with tools to maximize its potential. This book covers essential topics for seasoned developers, including Swift, SwiftUI, Xcode foundations, and the latest iOS SDK updates. You’ll get to grips with optimizing performance and understanding advanced architectural paradigms. By implementing the newest iOS updates, you’ll also explore intricate animation methods and harness a new framework, SwiftData that replaces Core Data for having persistent storage. The book builds your proficiency in advanced networking with URLSession and shows you how to conjure stunning visuals and adopt sophisticated testing techniques. You'll explore the world of machine learning with Apple’s Core ML diving into built-in frameworks like NLP, vision, and sound analysis to train and integrate your own models into iOS apps. By the end of the book, you'll possess skills to build exceptional apps, excel in advanced roles, and confidently tackle iOS development challenges.What you will learnDevelop functional iOS applications on the iOS platformBuild intricate custom animations and UI elementsMaster data handling and persistence in iOS appsUtilize Combine for efficient data managementHarness the power of the neural engine through CoreMLExplore architectures and streamline programming with Swift MacrosImprove engagement by adding Widgets and App IntentsWho this book is forIf you are an experienced iOS developer looking to enhance your mobile development skills, create exceptional applications, and excel in advanced positions, this book is designed for you. To derive maximum benefit from this book and ensure a strong understanding of the advanced content, it is recommended that you have a solid foundation in Swift, SwiftUI, and Xcode.","","9781835463277","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10763463.pdf&bkn=10763462&pdfType=book","","","","","","","","22 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Applied Machine Learning and High-Performance Computing on AWS: Accelerate the development of machine learning applications following architectural best practices","M. Khanuja; F. Sabir; S. Subramanian; T. Potgieter",NA; NA; NA; NA,Applied Machine Learning and High-Performance Computing on AWS: Accelerate the development of machine learning applications following architectural best practices,"","2022","","","","","Build, train, and deploy large machine learning models at scale in various domains such as computational fluid dynamics, genomics, autonomous vehicles, and numerical optimization using Amazon SageMakerKey FeaturesUnderstand the need for high-performance computing (HPC)Build, train, and deploy large ML models with billions of parameters using Amazon SageMakerLearn best practices and architectures for implementing ML at scale using HPCBook DescriptionMachine learning (ML) and high-performance computing (HPC) on AWS run compute-intensive workloads across industries and emerging applications. Its use cases can be linked to various verticals, such as computational fluid dynamics (CFD), genomics, and autonomous vehicles. This book provides end-to-end guidance, starting with HPC concepts for storage and networking. It then progresses to working examples on how to process large datasets using SageMaker Studio and EMR. Next, you’ll learn how to build, train, and deploy large models using distributed training. Later chapters also guide you through deploying models to edge devices using SageMaker and IoT Greengrass, and performance optimization of ML models, for low latency use cases. By the end of this book, you’ll be able to build, train, and deploy your own large-scale ML application, using HPC on AWS, following industry best practices and addressing the key pain points encountered in the application life cycle.What you will learnExplore data management, storage, and fast networking for HPC applicationsFocus on the analysis and visualization of a large volume of data using SparkTrain visual transformer models using SageMaker distributed trainingDeploy and manage ML models at scale on the cloud and at the edgeGet to grips with performance optimization of ML models for low latency workloadsApply HPC to industry domains such as CFD, genomics, AV, and optimizationWho this book is forThe book begins with HPC concepts, however, it expects you to have prior machine learning knowledge. This book is for ML engineers and data scientists interested in learning advanced topics on using large datasets for training large models using distributed training concepts on AWS, deploying models at scale, and performance optimization for low latency use cases. Practitioners in fields such as numerical optimization, computation fluid dynamics, autonomous vehicles, and genomics, who require HPC for applying ML models to applications at scale will also find the book useful.","","9781803244440","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162249.pdf&bkn=10162248&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Beyond Correlation: Evaluating Multimedia Quality Models with the Constrained Concordance Index","A. Ragano; H. B. Martinez; A. Hines","Insight Centre for Data Analytics and the School of Computer Science, University College Dublin, Dublin, Ireland; Insight Centre for Data Analytics and the School of Computer Science, University College Dublin, Dublin, Ireland; Insight Centre for Data Analytics and the School of Computer Science, University College Dublin, Dublin, Ireland",IEEE Transactions on Multimedia,"","2025","PP","99","1","13","This study investigates the evaluation of multimedia quality models, focusing on the inherent uncertainties in subjective Mean Opinion Score (MOS) ratings due to factors like rater inconsistency and bias. Traditional statistical measures such as Pearson's Correlation Coefficient (PCC), Spearman's Rank Correlation Coefficient (SRCC), and Kendall's Tau (KTAU) often fail to account for these uncertainties, leading to inaccuracies in model performance assessment. We introduce the Constrained Concordance Index (CCI), a novel metric designed to overcome the limitations of existing metrics by considering the statistical significance of MOS differences and excluding comparisons where MOS confidence intervals overlap. Through comprehensive experiments across various domains including speech and image quality assessment, we demonstrate that CCI provides a more robust and accurate evaluation of instrumental quality models, especially in scenarios of low sample sizes, rater group variability, and restriction of range. Our findings suggest that incorporating rater subjectivity and focusing on statistically significant pairs can significantly enhance the evaluation framework for multimedia quality prediction models. This work not only sheds light on the overlooked aspects of subjective rating uncertainties but also proposes a methodological advancement for more reliable and accurate quality model evaluation.","1941-0077","","10.1109/TMM.2025.3542991","Science Foundation Ireland(grant numbers:12/RC/2289_P2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10891584","performance evaluation;speech quality assessment;image quality assessment subjective quality evaluation;objective quality metrics","Measurement;Instruments;Uncertainty;Databases;Robustness;Predictive models;Correlation coefficient;Standards;Signal to noise ratio;Indexes","","","","","CCBY","17 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Deep Learning for Multimedia Forensics","I. Amerini; A. Anagnostopoulos; L. Maiano; L. Ricciardi Celsi",NA; NA; NA; NA,Deep Learning for Multimedia Forensics,"","2021","","","","","Over the past few decades, we have witnessed a huge increase in the use of multimedia content on the internet, for multiple applications ranging from innocuous to critical ones. This development has led to threats posed when content can be manipulated/used for malicious purposes. For example, fake media can be used to drive personal opinions, or for criminal activities such as terrorist propaganda and cyberbullying. This research and practice activity gave rise to the creation of the multimedia forensics field. In this survey, the latest trends and deep-learning-based techniques for multimedia forensics are introduced, in both architectural and data-processing. Firstly, different techniques used to manipulate content are presented, followed by image and video forgery techniques. Thereafter, deep learning methods for source identification and recent solutions for deepfake detection are covered. Datasets and evaluation metrics are included, and conclusions are presented. The publication is intended for researchers, students and professionals active in the fields of Deep Learning and Multimedia Forensics.","","9781680838558","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9527195.pdf&bkn=9527194&pdfType=book","","","","","","","","2 Sep 2021","","","now","Now Foundations and Trends Books"
"Toward Open-World Multimedia Forensics Through Media Signature Encoding","D. Baracchi; G. Boato; F. De Natale; M. Iuliani; A. Montibeller; C. Pasquini; A. Piva; D. Shullani","Dipartimento di Ingegneria dell’Informazione (DINFO), University of Florence, Florence, Italy; Dipartimento di Ingegneria e Scienza dell’Informazione (DISI), University of Trento, Trento, Italy; Dipartimento di Ingegneria e Scienza dell’Informazione (DISI), University of Trento, Trento, Italy; Dipartimento di Ingegneria dell’Informazione (DINFO), University of Florence, Florence, Italy; Dipartimento di Ingegneria e Scienza dell’Informazione (DISI), University of Trento, Trento, Italy; Center for Cybersecurity, Fondazione Bruno Kessler, Trento, Italy; Dipartimento di Ingegneria dell’Informazione (DINFO), University of Florence, Florence, Italy; Dipartimento di Ingegneria dell’Informazione (DINFO), University of Florence, Florence, Italy",IEEE Access,"2 May 2024","2024","12","","59930","59952","Countering image and video manipulations is getting more and more relevant in several fields such as investigation, intelligence and forensics. Multimedia forensics researchers keep developing new tools and updating available detectors to discriminate the processing the media has been subjected to. While these tools can be utilized efficiently in controlled settings, they are generally unreliable in open-world scenarios where the investigated material may have been subjected to several unknown manipulations. In this paper, we present a novel framework to discriminate different toolchains of media manipulation and processing. We introduce the concept of media signature encoding to map image and video contents to latent spaces where media produced by similar processing toolchains cluster together. We demonstrate that this property still holds for toolchain that are not known when building the encoder, expanding the range of applications for our framework to open-world contexts where forensic analysts may face both familiar and unfamiliar manipulation techniques. A significant advantage of this approach lies in its ability to create, in principle, media signatures from any kind of forensic features. We evaluated the effectiveness of the proposed framework in two different experimental setups involving digital images and videos. Results show that encoded signatures are capable of determining whether: (i) a media under analysis belongs to a known life cycle or an entirely novel processing toolchain; (ii) a subset of media items share the same history. This framework can be considered a first step towards the use of forensic features to characterize media life cycles in open-world settings.","2169-3536","","10.1109/ACCESS.2024.3391809","Italian Ministry of Universities and Research (MUR)(grant numbers:2017Z595XS); Defense Advanced Research Projects Agency (DARPA)(grant numbers:HR00112090136); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506491","Multimedia forensics;media signature;feature fusion;autoencoders","Media;Forensics;History;Encoding;Detectors;Training;Feature extraction;Encoding","","","","41","CCBY","22 Apr 2024","","","IEEE","IEEE Journals"
"Deep Learning for Genomics: Data-driven approaches for genomics applications in life sciences and biotechnology","U. K. Devisetty",NA,Deep Learning for Genomics: Data-driven approaches for genomics applications in life sciences and biotechnology,"","2022","","","","","Learn concepts, methodologies, and applications of deep learning for building predictive models from complex genomics data sets to overcome challenges in the life sciences and biotechnology industriesKey FeaturesApply deep learning algorithms to solve real-world problems in the field of genomicsExtract biological insights from deep learning models built from genomic datasetsTrain, tune, evaluate, deploy, and monitor deep learning models for enabling predictions in genomicsBook DescriptionDeep learning has shown remarkable promise in the field of genomics; however, there is a lack of a skilled deep learning workforce in this discipline. This book will help researchers and data scientists to stand out from the rest of the crowd and solve real-world problems in genomics by developing the necessary skill set. Starting with an introduction to the essential concepts, this book highlights the power of deep learning in handling big data in genomics. First, you’ll learn about conventional genomics analysis, then transition to state-of-the-art machine learning-based genomics applications, and finally dive into deep learning approaches for genomics. The book covers all of the important deep learning algorithms commonly used by the research community and goes into the details of what they are, how they work, and their practical applications in genomics. The book dedicates an entire section to operationalizing deep learning models, which will provide the necessary hands-on tutorials for researchers and any deep learning practitioners to build, tune, interpret, deploy, evaluate, and monitor deep learning models from genomics big data sets. By the end of this book, you’ll have learned about the challenges, best practices, and pitfalls of deep learning for genomics.What you will learnDiscover the machine learning applications for genomicsExplore deep learning concepts and methodologies for genomics applicationsUnderstand supervised deep learning algorithms for genomics applicationsGet to grips with unsupervised deep learning with autoencodersImprove deep learning models using generative modelsOperationalize deep learning models from genomics datasetsVisualize and interpret deep learning modelsUnderstand deep learning challenges, pitfalls, and best practicesWho this book is forThis deep learning book is for machine learning engineers, data scientists, and academicians practicing in the field of genomics. It assumes that readers have intermediate Python programming knowledge, basic knowledge of Python libraries such as NumPy and Pandas to manipulate and parse data, Matplotlib, and Seaborn for visualizing data, along with a base in genomics and genomic analysis concepts.","","9781804613016","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163557.pdf&bkn=10163556&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Cross-Modal Adaptive Prototype Learning for Continuous Sign Language Recognition","D. Wei; X. -H. Yang; Y. Weng; X. Lin; H. Hu; S. Liu","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Continuous sign language recognition technology enables effective communication for hearing-impaired individuals by recognizing and interpreting sign language. However, existing research has not fully addressed the large amount of temporal and spatial redundancy in sign language videos, which limits recognition accuracy. Additionally, the scarcity of frame-level annotated data hinders the widerspread utilization of gloss-level features from sign language videos by existing weakly supervised learning methods, thereby impedes the model from sufficient training. To address the above challenges, we propose a novel Cross-Modal Adaptive Prototype Learning model for Continuous Sign Language Recognition (CAP-SLR), which leverages prototype learning to fuse features across different modalities and improves recognition accuracy. Initially, we propose a lightweight Keyframe Extractor and a Multi-Scale Dilated Convolutional Attention to alleviate data redundancy and bolster the visual representation of sign language based on spatial-temporal information. Subsequently, we employ the Contextual Position Encoding-assisted (CoPE) transformer to learn the semantic of sign language, ameliorating the issue of cross-modal semantic prior bias inherent in pre-trained models. Finally, we design a Cross-Modal Adaptive Prototype Updating mechanism (CAP), which adaptively fuse visual features, gloss prototypes, and textual features, and subsequently update the gloss prototypes through the classifier-aware feature states, effectively mitigating the problem of easily introducing erroneous features in traditional momentum update methods. Extensive experiments on the PHOENIX14, PHOENIX14-T, and CSL-Daily datasets demonstrate that the proposed CAP-SLR method can effectively align cross-modal features at the gloss level, and achieves competitive recognition performance.","1558-2205","","10.1109/TCSVT.2025.3544212","Natural Science Foundation of Zhejiang Province(grant numbers:LZ24F030011); National Natural Science Foundation of China(grant numbers:62176236); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10896751","Cross-modal;Prototype learning;Adaptive updating;Continuous sign language recognition","Feature extraction;Sign language;Prototypes;Visualization;Adaptation models;Semantics;Training;Hidden Markov models;Data mining;Attention mechanisms","","","","","IEEE","20 Feb 2025","","","IEEE","IEEE Early Access Articles"
"The AI Product Manager's Handbook: Develop a product that takes advantage of machine learning to solve AI problems","I. Bratsis",NA,The AI Product Manager's Handbook: Develop a product that takes advantage of machine learning to solve AI problems,"","2023","","","","","Master the skills required to become an AI product manager and drive the successful development and deployment of AI products to deliver value to your organization. Purchase of the print or Kindle book includes a free PDF eBook.Key FeaturesBuild products that leverage AI for the common good and commercial successTake macro data and use it to show your customers you’re a source of truthBest practices and common pitfalls that impact companies while developing AI productBook DescriptionProduct managers working with artificial intelligence will be able to put their knowledge to work with this practical guide to applied AI. This book covers everything you need to know to drive product development and growth in the AI industry. From understanding AI and machine learning to developing and launching AI products, it provides the strategies, techniques, and tools you need to succeed. The first part of the book focuses on establishing a foundation of the concepts most relevant to maintaining AI pipelines. The next part focuses on building an AI-native product, and the final part guides you in integrating AI into existing products. You’ll learn about the types of AI, how to integrate AI into a product or business, and the infrastructure to support the exhaustive and ambitious endeavor of creating AI products or integrating AI into existing products. You’ll gain practical knowledge of managing AI product development processes, evaluating and optimizing AI models, and navigating complex ethical and legal considerations associated with AI products. With the help of real-world examples and case studies, you’ll stay ahead of the curve in the rapidly evolving field of AI and ML. By the end of this book, you’ll have understood how to navigate the world of AI from a product perspective.What you will learnBuild AI products for the future using minimal resourcesIdentify opportunities where AI can be leveraged to meet business needsCollaborate with cross-functional teams to develop and deploy AI productsAnalyze the benefits and costs of developing products using ML and DLExplore the role of ethics and responsibility in dealing with sensitive dataUnderstand performance and efficacy across verticalsWho this book is forThis book is for product managers and other professionals interested in incorporating AI into their products. Foundational knowledge of AI is expected. If you understand the importance of AI as the rising fourth industrial revolution, this book will help you surf the tidal wave of digital transformation and change across industries.","","9781804617335","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162789.pdf&bkn=10162788&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Python Feature Engineering Cookbook: A complete guide to crafting powerful features for your machine learning models","S. Galli; C. Molnar",NA; NA,Python Feature Engineering Cookbook: A complete guide to crafting powerful features for your machine learning models,"","2024","","","","","Leverage the power of Python to build real-world feature engineering and machine learning pipelines ready to be deployed to productionKey FeaturesCraft powerful features from tabular, transactional, and time-series dataDevelop efficient and reproducible real-world feature engineering pipelinesOptimize data transformation and save valuable timePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionStreamline data preprocessing and feature engineering in your machine learning project with this third edition of the Python Feature Engineering Cookbook to make your data preparation more efficient. This guide addresses common challenges, such as imputing missing values and encoding categorical variables using practical solutions and open source Python libraries. You’ll learn advanced techniques for transforming numerical variables, discretizing variables, and dealing with outliers. Each chapter offers step-by-step instructions and real-world examples, helping you understand when and how to apply various transformations for well-prepared data. The book explores feature extraction from complex data types such as dates, times, and text. You’ll see how to create new features through mathematical operations and decision trees and use advanced tools like Featuretools and tsfresh to extract features from relational data and time series. By the end, you’ll be ready to build reproducible feature engineering pipelines that can be easily deployed into production, optimizing data preprocessing workflows and enhancing machine learning model performance.What you will learnDiscover multiple methods to impute missing data effectivelyEncode categorical variables while tackling high cardinalityFind out how to properly transform, discretize, and scale your variablesAutomate feature extraction from date and time dataCombine variables strategically to create new and powerful featuresExtract features from transactional data and time seriesLearn methods to extract meaningful features from text dataWho this book is forIf you're a machine learning or data science enthusiast who wants to learn more about feature engineering, data preprocessing, and how to optimize these tasks, this book is for you. If you already know the basics of feature engineering and are looking to learn more advanced methods to craft powerful features, this book will help you. You should have basic knowledge of Python programming and machine learning to get started.","","9781835883594","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769397.pdf&bkn=10769396&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Deep Learning for Dialogue Systems: Chit-Chat and Beyond","R. Yan; J. Li; Z. Yu",NA; NA; NA,Deep Learning for Dialogue Systems: Chit-Chat and Beyond,"","2022","","","","","With the rapid progress of deep neural models and the explosion of data resources, dialogue systems that supports extensive topics and chit-chat conversations are emerging in natural language processing (NLP), information retrieval (IR), and machine learning (ML). To facilitate the development of both retrieval-based chit-chat systems and IR tasks supported by them, the authors survey chit-chat systems from two perspectives: (1) techniques to build chit-chat systems, and (2) chit-chat components in completing IR tasks. The main contributions of this survey are: surveying the deep neural models; connecting the recently resurgent chit-chat systems and task-oriented system; introducing various solutions for challenges from different perspectives, including dataside and model-side solutions and utilization of extra resources; presenting data resources and evaluation methods for building retrieval-based and generation-based chit-chat systems. The authors also analyze the main challenges, possible new exploration directions and rising trends, which will shed light on building human-like systems. This survey is intended to bridge the researchers of IR and the NLP community to move chit-chat systems forward and support more IR tasks. It will be of interest to IR or NLP researchers who want to study chit-chat from different perspectives, IR researchers who need to complete their tasks with the assistance of chit-chat systems, engineers with hands-on experience in building these systems to leverage advanced chit-chat modeling techniques, or anyone who wants keep up with the frontier of chit-chat systems or learn how to build them with deep neural architectures.","","9781638280231","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9800172.pdf&bkn=9800171&pdfType=book","","","","","","","","20 Jun 2022","","","now","Now Foundations and Trends Books"
"Mastering Azure Machine Learning: Execute large-scale end-to-end machine learning with Azure","C. Körner; M. Alsdorf",NA; NA,Mastering Azure Machine Learning: Execute large-scale end-to-end machine learning with Azure,"","2022","","","","","Supercharge and automate your deployments to Azure Machine Learning clusters and Azure Kubernetes Service using Azure Machine Learning servicesKey FeaturesImplement end-to-end machine learning pipelines on AzureTrain deep learning models using Azure compute infrastructureDeploy machine learning models using MLOpsBook DescriptionAzure Machine Learning is a cloud service for accelerating and managing the machine learning (ML) project life cycle that ML professionals, data scientists, and engineers can use in their day-to-day workflows. This book covers the end-to-end ML process using Microsoft Azure Machine Learning, including data preparation, performing and logging ML training runs, designing training and deployment pipelines, and managing these pipelines via MLOps. The first section shows you how to set up an Azure Machine Learning workspace; ingest and version datasets; as well as preprocess, label, and enrich these datasets for training. In the next two sections, you'll discover how to enrich and train ML models for embedding, classification, and regression. You'll explore advanced NLP techniques, traditional ML models such as boosted trees, modern deep neural networks, recommendation systems, reinforcement learning, and complex distributed ML training techniques - all using Azure Machine Learning. The last section will teach you how to deploy the trained models as a batch pipeline or real-time scoring service using Docker, Azure Machine Learning clusters, Azure Kubernetes Services, and alternative deployment targets. By the end of this book, you’ll be able to combine all the steps you’ve learned by building an MLOps pipeline.What you will learnUnderstand the end-to-end ML pipelineGet to grips with the Azure Machine Learning workspaceIngest, analyze, and preprocess datasets for ML using the Azure cloudTrain traditional and modern ML techniques efficiently using Azure MLDeploy ML models for batch and real-time scoringUnderstand model interoperability with ONNXDeploy ML models to FPGAs and Azure IoT EdgeBuild an automated MLOps pipeline using Azure DevOpsWho this book is forThis book is for machine learning engineers, data scientists, and machine learning developers who want to use the Microsoft Azure cloud to manage their datasets and machine learning experiments and build an enterprise-grade ML architecture using MLOps. This book will also help anyone interested in machine learning to explore important steps of the ML process and use Azure Machine Learning to support them, along with building powerful ML cloud applications. A basic understanding of Python and knowledge of machine learning are recommended.","","9781803246796","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162434.pdf&bkn=10162433&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Notes","T. J. Sejnowski",Salk Institute for Biological Studies,ChatGPT and the Future of AI: The Deep Language Revolution,"","2024","","","225","245","","","9780262379854","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10744842.pdf&bkn=10735175&pdfType=chapter","","","","","","","","5 Nov 2024","","","MIT Press","MIT Press eBook Chapters"
"A Tutorial on Non-Terrestrial Networks: Towards Global and Ubiquitous 6G Connectivity","M. A. Jamshed; A. Kaushik; S. Manzoor; M. Z. Shakir; J. Seong; M. Toka; W. Shin; M. Schellmann",NA; NA; NA; NA; NA; NA; NA; NA,A Tutorial on Non-Terrestrial Networks: Towards Global and Ubiquitous 6G Connectivity,"","2025","","","","","The International Mobile Telecommunications (IMT)-2030 framework recently adopted by the International Telecommunication Union Radio Communication Sector (ITU-R) envisions 6G networks to deliver intelligent, seamless connectivity that supports reliable, sustainable, and resilient communications. To achieve this vision, Non-Terrestrial Networks (NTN) represent a significant advancement by extending connectivity beyond the Earth’s surface. These networks integrate advanced communication technologies that go beyond conventional terrestrial infrastructure, enabling comprehensive global connectivity across domains such as the Internet, Internet of Things (IoT), navigation, disaster recovery, remote access, Earth observation, and even scientific initiatives like interplanetary communication.    Recent developments in the 3rd Generation Partnership Project (3GPP) highlight the critical role NTN is set to play in the evolution of 6G standards. The integration of advanced signal processing, edge and cloud computing, and Deep Reinforcement Learning (DRL) for Low Earth Orbit (LEO) satellites and aerial platforms, such as Uncrewed Aerial Vehicles (UAV) and high-, medium-, and low-altitude platform stations, has revolutionized the convergence of space, aerial, and Terrestrial Networks (TN). Artificial Intelligence (AI)-powered deployments for NTN and NTN-IoT, combined with Next Generation Multiple Access (NGMA) technologies, have dramatically reshaped global connectivity.   This monograph provides a comprehensive exploration of emerging NTN-based 6G wireless networks, covering vision, alignment with 5G-Advanced and 6G standards, key principles, trends, challenges, real-world applications, and novel problem solving frameworks. It examines essential enabling technologies like AI for NTN (LEO satellites and aerial platforms), DRL, edge computing for NTN, AI for NTN trajectory optimization, Reconfigurable Intelligent Surfaces (RIS)-enhanced NTN, and robust Multiple-Input-Multiple-Output (MIMO) beamforming. Furthermore, it addresses interference management through NGMA, including Rate-Splitting Multiple Access (RSMA) for NTN, and the use of aerial platforms for access, relay, and fronthaul/backhaul connectivity.","","9781638285151","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10892018.pdf&bkn=10892017&pdfType=book","","","","","","","","19 Feb 2025","","","now","Now Foundations and Trends Books"
"Acing the CCNA Exam, Volume 2: Advanced Networking and Security","J. McDowell",Manning Publications,"Acing the CCNA Exam, Volume 2: Advanced Networking and Security","","2024","","","","","SPECIAL PRICING! Save 20% when you get Volume 1 and Volume 2 together! Add Volume 1 print or eBook to your cart! Master the most challenging elements of the CCNA exam to pass on your very first try! The CCNA goes deep on networking and security. Acing the CCNA Exam, Volume 2 gives you exactly what you need to navigate the most challenging parts of the exam. Author Jeremy McDowell’s CCNA courses have helped hundreds of thousands of students pass their exams. This book distills that expertise into an easy-to-follow guide. In Acing the CCNA Exam, Volume 2—Advanced Networking and Security you’ll dig into tough topics like:  Security concepts and common threats Ethernet and wireless LANs (Wi-Fi) and network automation Essential network services like DHCP and DNS WAN, LAN, and wireless architectures  The Cisco Certified Network Associate is the gold-standard credential for network administrators. It demands an in-depth knowledge of complex network internals, including security, wireless architectures, and more. Acing the CCNA Exam, Volume 2—Advanced Networking and Security builds on the basics you learn in Volume 1 to help you study and prepare for the most challenging parts of the test.","","9781633435780","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10745315.pdf&bkn=10745314&pdfType=book","","","","","","","","6 Nov 2024","","","Manning","Manning eBooks"
"Generalization Bounds: Perspectives from Information Theory and PAC-Bayes","F. Hellström; G. Durisi; B. Guedj; M. Raginsky",NA; NA; NA; NA,Generalization Bounds: Perspectives from Information Theory and PAC-Bayes,"","2025","","","","","Artificial intelligence and machine learning have emerged as driving forces behind transformative advancements in various fields, and have become increasingly pervasive in many industries and daily life. As these technologies continue to gain momentum, so does the need to develop a deeper understanding of their underlying principles, capabilities, and limitations. In this monograph, the authors focus on the theory of machine learning and statistical learning theory, with a particular focus on the generalization capabilities of learning algorithms. Part I covers the foundations of information-theoretic and PAC-Bayesian generalization bounds for standard supervised learning. Part II explores the applications of generalization bounds, as well as extensions to settings beyond standard supervised learning. Several important areas of application include neural networks, federated learning and reinforcement learning. The monograph concludes with a broader discussion of information-theoretic and PAC-Bayesian generalization bounds as a whole. This monograph will be of interest to students and researchers working in generalization and theoretical machine learning. It provides a comprehensive introduction to information-theoretic generalization bounds and their connection to PAC-Bayes, serving as a foundation from which the most recent developments are accessible.","","9781638284215","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10852613.pdf&bkn=10852612&pdfType=book","","","","","","","","27 Jan 2025","","","now","Now Foundations and Trends Books"
"MS-Loc: Toward Pervasive Indoor Localization Utilizing Mobile Single-Site","W. Nie; X. Wang; Z. Liu; Y. Duan; K. -Y. Lam; K. Liu; J. K. Y. Ng; C. J. Xue","Ministry of Education, China; the School of Automation/School of Industrial Internet, Key Laboratory of Industrial Internet of Things and Networked Control, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Automation/School of Industrial Internet, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Automation/School of Industrial Internet, Chongqing University of Posts and Telecommunications, Chongqing, China; Ministry of Education, China; the School of Automation/School of Industrial Internet, Key Laboratory of Industrial Internet of Things and Networked Control, Chongqing University of Posts and Telecommunications, Chongqing, China; Computer Science Department, City University of Hong Kong, Hong Kong; College of Computer Science, Chongqing University, Chongqing, China; Computer Science Department, Hong Kong Baptist University, Hong Kong; Department of Computer Science, Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE",IEEE Internet of Things Journal,"","2025","PP","99","1","1","Leveraging the widespread deployment of existing WiFi sites, WiFi-based techniques offer substantial potential for achieving pervasive indoor localization among various indoor localization techniques. Conventional WiFi-based indoor localization techniques primarily focus on providing fine-grained accuracy. However, previous techniques are not pervasive due to the following constraints: 1) they can hardly be implemented in environments with limited resources of WiFi sites; and 2) they are constrained by high hardware requirements, such as the need for multiple antennas. In this paper, we propose a novel technique called Mobile Single-site Localization (MS-Loc), which leverages a mobile single-site to perform indoor localization. Specifically, MS-Loc utilizes existing hardware at off-the-shelf mobile WiFi sites to achieve pervasive localization rather than relying on multiple sites or multiple antennas. Moreover, in MS-Loc, a tailor-designed path planning algorithm guides the movement of the mobile single-site to locate targets quickly and accurately. We conducted extensive experiments using a real-world testbed. The experimental results demonstrate that MS-Loc presents a competitive localization accuracy compared to previous techniques but is pervasive.","2327-4662","","10.1109/JIOT.2024.3525026","National Natural Science Foundation of China(grant numbers:61902043,62102055); Science and Technology Research Program of Chongqing Municipal Education Commission(grant numbers:KJQN202100621); National Key Research and Development Program of China(grant numbers:2022YFE0204500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10829564","Indoor Localization;Mobile Single-site;Pervasive Localization;Wireless Sensing","Location awareness;Wireless fidelity;Fingerprint recognition;Accuracy;Hardware;MIMO;Indoor environment;Computer science;Line-of-sight propagation;Internet of Things","","","","","IEEE","6 Jan 2025","","","IEEE","IEEE Early Access Articles"
"ReMark: Reversible Lexical Substitution-Based Text Watermarking","Z. Jiang; H. Wang","School of Cyber Science and Engineering, Sichuan University, Chengdu, PR China; School of Cyber Science and Engineering, Sichuan University, Chengdu, PR China","2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20 Jan 2025","2024","","","2581","2586","Neural-based natural language watermarking (NLW) shows promise for generating context-aware lexical substitutions, minimizing semantic loss in watermarked text. However, existing works confront two primary challenges: 1) the reliance and sensitivity on textual context during substitutes generation hinders text reversibility, and 2) strict synchronization constraints on the generation order of substitutes from both original and watermarked text blocks out some suitable substitutes, limiting watermark capacity. This paper puts forward a reversible neural NLW approach with improved capacity and text quality. Specifically, we construct a novel lexical substitution system (LSS), utilizing prompt learning for candidates generation and comprehensive assessment features for candidates ranking. A reversible watermarking scheme is then presented by ingeniously screening recoverable positions and enabling multi-bit substitutions via the proposed LSS. Experiments validate that our method achieves complete reversibility while enhancing watermark payload and text fidelity compared to prior arts.","","978-1-6654-1020-5","10.1109/SMC54092.2024.10831525","National Natural Science Foundation of China(grant numbers:62272331); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10831525","natural language watermarking;lexical sub-stitution;reversible text watermarking;prompt learning","Sensitivity;Limiting;Natural languages;Semantics;Optical character recognition;Watermarking;Manuals;Synchronization;Tuning;Payloads","","","","19","IEEE","20 Jan 2025","","","IEEE","IEEE Conferences"
"WiDoor: Wi-Fi Based Contactless Close-range Identity Recognition","P. Duan; T. Fang; C. Wu; Y. Cao","School of Software, Zhengzhou University, Zhengzhou, China; School of Software, Zhengzhou University, Zhengzhou, China; Meta-Networking Research Center, The University of Electro-Communications, Tokyo, Japan; School of Software, Zhengzhou University, Zhengzhou, China",IEEE Internet of Things Journal,"","2024","PP","99","1","1","In the fields of intelligent security and human-computer interaction, the rapid development of non-contact identity recognition technology based on Wi-Fi signals has shown promising application potential. To address the significant decrease in recognition accuracy in close-range scenarios, an close-range non-contact identity recognition method named WiDoor is proposed. During the data collection phase, the Fresnel propagation model is utilized by WiDoor to optimize the deployment layout of the receiving antennas. Gait information is reconstructed from the multiple antennas to enable the acquisition of more rich gait features. In the identity recognition stage, WiDoor employs a lightweight model that combines self-attention mechanisms with multi-scale convolutional neural networks. This combination effectively enhances the model’s capability to capture key features while significantly reducing computational complexity and maintaining a high recognition accuracy. Experimental results show that WiDoor achieves a recognition accuracy of up to 99.3% on an expanded dataset that includes 10 participants, with a distance of 1 meter between the receiving and transmitting ends, and the parameter quantity of the built-in model is only 2% of the compared model with the same accuracy, offering a significant advantages over similar methods. Additionally, the model can achieve a high-precision recognition across different distances between the transmitter and the receiver using a limited number of samples, showing strong robustness of the model.","2327-4662","","10.1109/JIOT.2024.3501340","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10756566","close-range identity recognition;Wi-Fi;channel state information;Fresnel zone","Wireless fidelity;Accuracy;Feature extraction;Data models;Deep learning;Fingerprint recognition;Data mining;Computational modeling;Neural networks;Transmitters","","","","","IEEE","18 Nov 2024","","","IEEE","IEEE Early Access Articles"
"Cache Optimization Models and Algorithms","G. Paschos; G. Iosifidis; G. Caire",NA; NA; NA,Cache Optimization Models and Algorithms,"","2020","","","","","Caching refers to the act of replicating information at a faster (or closer) medium with the purpose of improving performance. This deceptively simple idea has given rise to some of the hardest optimization problems in the fields of computer systems, networking, and the Internet; many of which remain unsolved several years after their conception. While a wealth of research contributions exists from the topics of memory systems, data centers, Internet traffic, CDNs, and recently wireless networks, the literature is dispersed and overlapping at times. In this monograph, the authors focus on the fundamental underlying mathematical models, into a powerful framework for performing optimization of caching systems. In doing so they the present the reader with a solid background for the anticipated explosion in caching research, and provide a didactic view into how engineers have managed to infuse mathematical models into the study of caching over the last 40 years. Written by leading researchers from academia and industry, this monograph provides students, researchers and practicing engineers with a concise introduction to challenges and solutions for implementing caching in modern computing systems.","","9781680837032","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9173649.pdf&bkn=9173648&pdfType=book","","","","","","","","24 Aug 2020","","","now","Now Foundations and Trends Books"
"SCSE_2023 Conference Proceedings","",,2023 International Research Conference on Smart Computing and Systems Engineering (SCSE),"17 Aug 2023","2023","6","","i","lxxxviii","Summary form only. Contains summaries of all papers presented at the SCSE_2023 Conference Proceedings.","2613-8662","979-8-3503-4145-4","10.1109/SCSE59836.2023.10215006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10215006","","Systems engineering and theory;Recording","","","","","IEEE","17 Aug 2023","","","IEEE","IEEE Conferences"
"Towards Better User Studies in Computer Graphics and Vision","Z. Bylinskii; L. Herman; A. Hertzmann; S. Hutka; Y. Zhang",NA; NA; NA; NA; NA,Towards Better User Studies in Computer Graphics and Vision,"","2023","","","","","Most research in computer graphics and image synthesis produces outputs for human consumption. In many cases, these algorithms operate largely automatically; in other cases, interactive tools allow professionals or everyday users to author or edit images, video, textures, geometry, or animation. Online crowdsourcing platforms have made it increasingly easy to perform evaluations of algorithm outputs with survey questions like “which image is better, A or B?”, leading to their proliferation in vision and graphics research papers. Results of these studies are often used as quantitative evidence in support of a paper’s contributions. When conducted hastily as an afterthought, such studies can lead to an increase of uninformative, and, potentially, misleading conclusions. On the other hand, in these same communities, user research is underutilized in driving project direction and forecasting user needs and reception. Increased attention is needed in both the design and reporting of user studies in computer vision and graphics papers towards (1) improved replicability and (2) improved project direction. This monograph focusses on these aspects, and an overview of methodologies from user experience research (UXR), human-computer interaction (HCI), and applied perception to increase exposure to the available methodologies and best practices are also presented. Foundational user research methods are included, (e.g., need finding) that are presently underutilized in computer vision and graphics research, but can provide valuable project direction. Also, further pointers to the literature for readers interested in exploring other UXR methodologies are given, and broader open issues and recommendations for the research community are described.","","9781638281733","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10122494.pdf&bkn=10122493&pdfType=book","","","","","","","","10 May 2023","","","now","Now Foundations and Trends Books"
"SAFe® for DevOps Practitioners: Implement robust, secure, and scaled Agile solutions with the Continuous Delivery Pipeline","R. Wen; H. Koehnemann",NA; NA,"SAFe® for DevOps Practitioners: Implement robust, secure, and scaled Agile solutions with the Continuous Delivery Pipeline","","2022","","","","","Discover how the DevOps approach with Scaled Agile Framework helps you develop and deliver high-quality, secured solutions with a reduced risk of production failures with this step-by-step guideKey FeaturesExplore the five elements of the CALMR approach to avoid product development challengesUse value stream management to introduce systems thinking and flow for product developmentDemonstrate how the CD pipeline combines practices and technology to optimize your value streamPurchase of the print or Kindle book includes a free eBook in the PDF formatBook DescriptionProduct development and release faces overlapping challenges due to the combined pressure of delivering high-quality products in shorter time-to-market cycles, along with maintaining proper operation and ensuring security in a complex high-tech environment. This calls for new ways of overcoming these challenges from design to development, to release, and beyond. SAFe® for DevOps Practitioners helps you use a DevOps approach with the Scaled Agile Framework and details how value streams help you resolve these challenges using examples and use cases. The book begins by explaining how the CALMR approach makes DevOps effective in resolving product development roadblocks. Next, you’ll learn to apply value stream management to establish a value stream that enables product development flow, measure its effectiveness through appropriate feedback loops, and find ways of improving it. Finally, you’ll get to grips with implementing a continuous delivery pipeline that optimizes the value stream through four phases during release on demand. This book complements the latest SAFe DevOps courses, and you’ll find it useful while studying for the SAFe DevOps Practitioner (SDP) certification. By the end of this DevOps book, you’ll have gained a clear understanding of how to achieve continuous execution and release on demand using DevOps and SAFe.What you will learnUnderstand the important elements of the CALMR approachDiscover how to organize around value using value stream mappingMeasure your value stream using value stream metricsImprove your value stream with continuous learningUse continuous exploration to design high-quality and secure featuresPrevent rework and build in quality using continuous integrationAutomate delivery with continuous deploymentMeasure successful outcomes with Release on DemandWho this book is forThis book is for IT professionals such as DevOps and DevSecOps practitioners, SREs, and managers who are interested in implementing DevOps practices using the Scaled Agile Framework (SAFe) approach. Basic knowledge of DevOps and agile software development lifecycle and methodology will be helpful.","","9781803237435","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162829.pdf&bkn=10162828&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Knowledge-Enriched Recommendations: Bridging the Gap in Alloy Material Selection with Large Language Models","T. Wu; S. Du; Y. Zhang; H. Li","Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Ningbo Key Laboratory of Special Energy Materials and Chemistry, Ningbo Institute of Materials Technology and Engineering, Zhejiang Key Laboratory of Data-Driven High-Safety Energy Materials and Ap-plications, Chinese Academy of Sciences, Ningbo, China; Ningbo Key Laboratory of Special Energy Materials and Chemistry, Ningbo Institute of Materials Technology and Engineering, Zhejiang Key Laboratory of Data-Driven High-Safety Energy Materials and Ap-plications, Chinese Academy of Sciences, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China",IEEE Access,"","2025","PP","99","1","1","Navigating materials performance databases efficiently is a persistent challenge in materials science and engineering, particularly in the selection of alloy materials. While recommendation systems ad-dress information overload, traditional approaches relying on historical user data face limitations such as data sparsity and cold-start issues. This study presents a novel recommendation model that integrates domain-specific knowledge graphs with large language models (LLMs) to enhance recommendation accuracy in alloy material selection. A knowledge graph for alloys is developed, encapsulating technical material data and relationships to improve retrieval and recommendation outcomes. LLMs are employed for label clustering and natural language-based instruction-following to craft user profiles and enhance data representation. Two graph enhancement strategies, integrated with attention mechanisms, effectively capture user preferences. Experimental results on a ferroalloy dataset demonstrate the model's superior performance compared to base-line methods, significantly addressing data sparsity while offering personalized, accurate recommendations. This research bridges the gap between knowledge graphs and LLMs in recommendation systems, contributing a flexible, intelligent solution to streamline material selection processes.","2169-3536","","10.1109/ACCESS.2025.3554125","National Natural Science Foundation of China(grant numbers:11604346,21671195,21707147,21875271,51872302); The Key R & D Projects of Zhejiang Province(grant numbers:2022C01236); Ningbo Major Research and Development Plan Project(grant numbers:2024Z284); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10937740","Recommender systems;Knowledge graph;Large Language Models;Data Augmentation","","","","","","CCBY","25 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Line Drawings from 3D Models: A Tutorial","P. Bénard; A. Hertzmann",NA; NA,Line Drawings from 3D Models: A Tutorial,"","2019","","","","","Drawing is the starting point for many kinds of tasks, for everyone from children making pictures to professional architects sketching ideas. Drawing seems to be fundamentally connected to how we represent the world visually. Most computer graphics focuses on realistic visual simulation, but over the past few decades, line drawing algorithms have matured, providing the ability to automatically create reasonable line drawings from 3D geometry. This tutorial provides a detailed guide to the mathematical theory and computer algorithms for line drawing of 3D objects. It focuses on the curves known as contours as they are the most important curves for line drawing of 3D surfaces. The authors describe the different algorithms required to compute and render these curves, before going on to explain boundary curves and surface-surface intersection curves. The tutorial concludes with other topics in 3D non-photorealistic rendering including: other types of curves, stroke rendering, and non-photorealistic shading. Line Drawings from 3D Models: A Tutorial is a concise, yet comprehensive, introduction to an increasingly important topic in computer graphics. The extensive bibliography is invaluable for readers wishing to further their own research in the area.","","9781680835915","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8836429.pdf&bkn=8836428&pdfType=book","Animation;3D reconstruction and image-based modeling","","","","","","","16 Sep 2019","","","now","Now Foundations and Trends Books"
"Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art","J. Janai; F. Güney; A. Behl; A. Geiger",NA; NA; NA; NA,"Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art","","2020","","","","","Recent years have witnessed enormous progress in AI-related fields such as computer vision, machine learning, and autonomous vehicles. As with any rapidly growing field, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several survey papers on particular sub-problems have appeared, no comprehensive survey on problems, datasets, and methods in computer vision for autonomous vehicles has been published. This monograph attempts to narrow this gap by providing a survey on the state-of-the-art datasets and techniques. Our survey includes both the historically most relevant literature as well as the current state of the art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding, and end-to-end learning for autonomous driving. Towards this goal, we analyze the performance of the state of the art on several challenging benchmarking datasets, including KITTI, MOT, and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we also provide a website that allows navigating topics as well as methods and provides additional information.","","9781680836899","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9134693.pdf&bkn=9134692&pdfType=book","Animation;3D reconstruction and image-based modeling","","","","","","","8 Jul 2020","","","now","Now Foundations and Trends Books"
"A Comprehensive Survey of Fake Text Detection on Misinformation and LM-Generated Texts","S. Kwon; B. Jang","Graduate School of Information, Yonsei University, Seoul, South Korea; Graduate School of Information, Yonsei University, Seoul, South Korea",IEEE Access,"11 Feb 2025","2025","13","","25301","25324","This paper presents a pioneering and comprehensive analysis of fake text, a pressing issue in the digital age, by categorizing it into two main types: Misinformation and LM-generated texts. It is the first study to systematically dissect and examine the intricate challenges and nuances in distinguishing between genuine and artificial text. Through a meticulous review of various methodologies and technologies in fake text detection, the paper provides an in-depth evaluation of their effectiveness across diverse scenarios. Furthermore, this research delves into the significant societal impacts of both misinformation and LM-generated texts, underlining the urgent need for precise and effective detection mechanisms in our increasingly information-saturated world. This extensive survey not only offers a unique perspective on the current landscape of fake text detection, but also paves the way for future research, highlighting critical areas where further innovation and exploration are essential.","2169-3536","","10.1109/ACCESS.2025.3538805","National Research Foundation of Korea funded by Korean Government(grant numbers:RS-2023-00273751); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10870239","Nature language processing;large language model;fake text detection;deep learning","Fake news;Text detection;Surveys;Reviews;Market research;Threat modeling;Social networking (online);Deep learning;Chatbots;Big Data","","","","196","CCBY","4 Feb 2025","","","IEEE","IEEE Journals"
"Radio Frequency Machine Learning: A Practical Deep Learning Perspective","S. Kuzdeba",NA,Radio Frequency Machine Learning: A Practical Deep Learning Perspective,"","2025","","","","","Radio Frequency Machine Learning: A Practical Deep Learning Perspective goes beyond general introductions to deep learning, offering a focused exploration of how modern deep learning techniques can be applied directly to radio frequency (RF) challenges. It covers a wide range of applications, including classification tasks where deep learning is used to label and categorize signals based on a labeled training dataset, as well as clustering tasks that group similar signals together without labels. Additionally, it expands into deep learning (generative AI) for waveform synthesis and how reinforcement learning can be used within the domain. This book also investigates advanced topics like RF sensor control, feedback mechanisms, and real-time system operations, offering a comprehensive understanding of how deep learning can be integrated into dynamic RF environments. This resource addresses the practical concerns of deploying machine learning in operational RF systems. It goes beyond applications and techniques, covering how to ensure the robustness of solutions, with insights into data sources, augmentation techniques, and strategies for integrating ML with existing RF infrastructure. The full development process is examined, from data collection to deployment, along with numerous case studies throughout. Looking to the future, the book explores emerging trends like edge computing and federated learning, offering a forward-looking perspective on the continued evolution of RF machine learning. Whether the reader is just beginning the journey into RF machine learning or is looking to refine skills, this book provides an essential resource for understanding the intersection of deep learning and RF technology. This is a must-have resource for anyone interested in the cutting edge of wireless technologies and their potential to shape the future of communication.","","9781685690342","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10937335.pdf&bkn=10937334&pdfType=book","","","","","","","","24 Mar 2025","","","Artech","Artech Books"
"The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes","M. Ko; F. Kang; W. Shi; M. Jin; Z. Yu; R. Jia",Virginia Tech; Virginia Tech; Columbia University; Virginia Tech; Columbia University; Virginia Tech,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","26276","26285","Large-scale black-box models have become ubiquitous across numerous applications. Understanding the influence of individual training data sources on predictions made by these models is crucial for improving their trustworthiness. Current influence estimation techniques involve computing gradients for every training point or repeated training on different subsets. These approaches face obvious computational challenges when scaled up to large datasets and models. In this paper, we introduce and explore the Mirrored Influence Hypothesis, highlighting a reciprocal nature of influence between training and test data. Specifically, it sug-gests that evaluating the influence of training data on test predictions can be reformulated as an equivalent, yet inverse problem: assessing how the predictions for training samples would be altered if the model were trained on specific test samples. Through both empirical and theoretical validations, we demonstrate the wide applicability of our hypothesis. Inspired by this, we introduce a new method for estimating the influence of training data, which requires calculating gradients for specific test samples, paired with a forward pass for each training point. This approach can capitalize on the common asymmetry in scenarios where the number of test samples under concurrent examination is much smaller than the scale of the training dataset, thus gaining a significant improvement in efficiency compared to existing approaches. We demonstrate the applicability of our method across a range of scenarios, including data attribution in diffusion models, data leakage detection, analy-sis of memorization, mislabeled data detection, and tracing behavior in language models.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.02483","National Science Foundation(grant numbers:IIS-2312794,IIS-2313130,OAC-2239622); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10655634","","Training;Inverse problems;Computational modeling;Training data;Estimation;Predictive models;Diffusion models","","","","41","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"Unity Artificial Intelligence Programming: Add powerful, believable, and fun AI entities in your game with the power of Unity","D. Davide Aversa",NA,"Unity Artificial Intelligence Programming: Add powerful, believable, and fun AI entities in your game with the power of Unity","","2022","","","","","Learn and implement game AI in Unity to build smart environments and enemies with A* pathfinding, finite state machines, behavior trees, and the NavMeshKey FeaturesExplore the latest Unity features to make AI implementation in your game easierBuild richer and more dynamic games using AI concepts such as behavior trees and navigation meshesImplement character behaviors and simulations using the Unity Machine Learning toolkitBook DescriptionDeveloping artificial intelligence (AI) for game characters in Unity has never been easier. Unity provides game and app developers with a variety of tools to implement AI, from basic techniques to cutting-edge machine learning-powered agents. Leveraging these tools via Unity's API or built-in features allows limitless possibilities when it comes to creating game worlds and characters. The updated fifth edition of Unity Artificial Intelligence Programming starts by breaking down AI into simple concepts. Using a variety of examples, the book then takes those concepts and walks you through actual implementations designed to highlight key concepts and features related to game AI in Unity. As you progress, you’ll learn how to implement a finite state machine (FSM) to determine how your AI behaves, apply probability and randomness to make games less predictable, and implement a basic sensory system. Later, you’ll understand how to set up a game map with a navigation mesh, incorporate movement through techniques such as A* pathfinding, and provide characters with decision-making abilities using behavior trees. By the end of this Unity book, you’ll have the skills you need to bring together all the concepts and practical lessons you’ve learned to build an impressive vehicle battle game.What you will learnUnderstand the basics of AI in game designCreate smarter game worlds and characters with C# programmingApply automated character movement using pathfinding algorithm behaviorsImplement character decision-making algorithms using behavior treesBuild believable and highly efficient artificial flocks and crowdsCreate sensory systems for your AI worldBecome well-versed with the basics of procedural content generationExplore the application of machine learning in UnityWho this book is forThis Unity artificial intelligence book is for Unity developers with a basic understanding of C# and the Unity Editor who want to expand their knowledge of AI Unity game development.","","9781803245218","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162656.pdf&bkn=10162655&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Spectral Learning on Matrices and Tensors","M. Janzamin; R. Ge; J. Kossaifi; A. Anandkumar",NA; NA; NA; NA,Spectral Learning on Matrices and Tensors,"","2019","","","","","The authors of this monograph survey recent progress in using spectral methods including matrix and tensor decomposition techniques to learn many popular latent variable models. With careful implementation, tensor-based methods can run efficiently in practice, and in many cases they are the only algorithms with provable guarantees on running time and sample complexity. The focus is on a special type of tensor decomposition called CP decomposition, and the authors cover a wide range of algorithms to find the components of such tensor decomposition. They also discuss the usefulness of this decomposition by reviewing several probabilistic models that can be learned using such tensor methods. The second half of the monograph looks at practical applications. This includes using Tensorly, an efficient tensor algebra software package, which has a simple python interface for expressing tensor operations. It also has a flexible back-end system supporting NumPy, PyTorch, TensorFlow, and MXNet. Spectral Learning on Matrices and Tensors provides a theoretical and practical introduction to designing and deploying spectral learning on both matrices and tensors. It is of interest for all students, researchers and practitioners working on modern day machine learning problems.","","9781680836417","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8918129.pdf&bkn=8918128&pdfType=book","","","","","","","","2 Dec 2019","","","now","Now Foundations and Trends Books"
"IEEE Draft Recommended Practice for Ethically Aligned Design of Artificial Intelligence (AI) in Adaptive Instructional Systems","",,"IEEE P2247.4/D1, January 2025","6 Feb 2025","2025","","","1","36","This Recommended Practice for Ethically Aligned Design of AI in Adaptive Instructional Systems describes ethical considerations and recommended best practices in the design of artificial intelligence as used by adaptive instructional systems. This IP is directly related to P2247.1, P2247.2, and P2247.3.","","979-8-8557-1829-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10877733","adaptive instructional systems;artificial intelligence;ethics;learning;elearning;recommended practice","IEEE Standards;Artificial intelligence;Electronic learning;Learning systems;Adaptive learning","","","","","","6 Feb 2025","","","IEEE","IEEE Standards"
"Azure Data and AI Architect Handbook: Adopt a structured approach to designing data and AI solutions at scale on Microsoft Azure","O. Mertens; B. V. Baelen",NA; NA,Azure Data and AI Architect Handbook: Adopt a structured approach to designing data and AI solutions at scale on Microsoft Azure,"","2023","","","","","Master core data architecture design concepts and Azure Data & AI services to gain a cloud data and AI architect’s perspective to developing end-to-end solutions Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesTranslate and implement conceptual architectures with the right Azure servicesInject artificial intelligence into data solutions for advanced analyticsLeverage cloud computing and frameworks to drive data science workloadsBook DescriptionWith data’s growing importance in businesses, the need for cloud data and AI architects has never been higher. The Azure Data and AI Architect Handbook is designed to assist any data professional or academic looking to advance their cloud data platform designing skills. This book will help you understand all the individual components of an end-to-end data architecture and how to piece them together into a scalable and robust solution. You’ll begin by getting to grips with core data architecture design concepts and Azure Data & AI services, before exploring cloud landing zones and best practices for building up an enterprise-scale data platform from scratch. Next, you’ll take a deep dive into various data domains such as data engineering, business intelligence, data science, and data governance. As you advance, you’ll cover topics ranging from learning different methods of ingesting data into the cloud to designing the right data warehousing solution, managing large-scale data transformations, extracting valuable insights, and learning how to leverage cloud computing to drive advanced analytical workloads. Finally, you’ll discover how to add data governance, compliance, and security to solutions. By the end of this book, you’ll have gained the expertise needed to become a well-rounded Azure Data & AI architect.What you will learnDesign scalable and cost-effective cloud data platforms on Microsoft AzureExplore architectural design patterns with various use casesDetermine the right data stores and data warehouse solutionsDiscover best practices for data orchestration and transformationHelp end users to visualize data using interactive dashboardingLeverage OpenAI and custom ML models for advanced analyticsManage security, compliance, and governance for the data estateWho this book is forThis book is for anyone looking to elevate their skill set to the level of an architect. Data engineers, data scientists, business intelligence developers, and database administrators who want to learn how to design end-to-end data solutions and get a bird’s-eye view of the entire data platform will find this book useful. Although not required, basic knowledge of databases and data engineering workloads is recommended.","","9781803230733","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251228.pdf&bkn=10251227&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Deep residual learning for denoising Monte Carlo renderings","K. -M. Wong; T. -T. Wong","Artixels, Hong Kong S.A.R., China; Department of Computer Science and Engineering, the Chinese University of Hong Kong, Hong Kong S.A.R., China",Computational Visual Media,"20 Feb 2025","2019","5","3","239","255","Learning-based techniques have recently been shown to be effective for denoising Monte Carlo rendering methods. However, there remains a quality gap to state-of-the-art handcrafted denoisers. In this paper, we propose a deep residual learning based method that outperforms both state-of-the-art handcrafted denoisers and learning-based denoisers. Unlike the indirect nature of existing learning-based methods (which e.g., estimate the parameters and kernel weights of an explicit feature based filter), we directly map the noisy input pixels to the smoothed output. Using this direct mapping formulation, we demonstrate that even a simple-and-standard ResNet and three common auxiliary features (depth, normal, and albedo) are sufficient to achieve high-quality denoising. This minimal requirement on auxiliary data simplifies both training and integration of our method into most production rendering pipelines. We have evaluated our method on unseen images created by a different renderer. Consistently superior quality denoising is obtained in all cases.","2096-0662","","10.1007/s41095-019-0142-3","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897421","Monte Carlo rendering;denoising;deep learning;deep residual learning;filter-free denoising","Noise reduction;Rendering (computer graphics);Noise measurement;Monte Carlo methods;Image color analysis;Training;Supervised learning;Kernel;Convolutional neural networks;Reviews","","","","","","20 Feb 2025","","","TUP","TUP Journals"
"A Multifaceted Survey on Federated Learning: Fundamentals, Paradigm Shifts, Practical Issues, Recent Developments, Partnerships, Trade-Offs, Trustworthiness, and Ways Forward","A. Majeed; S. O. Hwang","Department of Computer Engineering, Gachon University, Seongnam, South Korea; Department of Computer Engineering, Gachon University, Seongnam, South Korea",IEEE Access,"20 Jun 2024","2024","12","","84643","84679","Federated learning (FL) is considered a de facto standard for privacy preservation in AI environments because it does not require data to be aggregated in some central place to train an AI model. Preserving data on the client side and sharing only the model’s parameters with a central server preserves privacy while training an AI model of higher generalizability. Unfortunately, sharing the model’s parameters with the server can create privacy leaks, and therefore, FL is unable to meet privacy requirements in many situations. Furthermore, FL is prone to other technical issues, such as data poisoning, model poisoning, fairness, client dropout, and convergence issues, to name just a few. In this work, we provide a multifaceted survey on FL, including its fundamentals, paradigm shifts, technical issues, recent developments, and future prospects. First, we discuss the fundamental concepts of FL (workflow, categorization, the differences between centralized learning and FL, and applications of FL in diverse fields), and we then discuss the paradigm shifts brought on by FL from a broader perspective (e.g., data use, AI model development, resource sharing, etc.). Later, we pinpoint ten practical issues currently hindering the viability of the FL landscape, and we discuss developments made under each issue by summarizing state-of-the-art (SOTA) literature. We highlight FL partnerships with two or more technologies that either improve practical aspects/issues in FL or extend its adoption to new areas/domains. We pinpoint various trade-offs that exist in an FL ecosystem, and the corresponding SOTA developments to mitigate them. We also discuss the latest studies that have been proposed to make FL trustworthy and beneficial for the community. Lastly, we suggest valuable research directions towards enhancing technical efficacy by guiding researchers to less explored topics in FL.","2169-3536","","10.1109/ACCESS.2024.3413069","National Research Foundation of Korea; Korea Government (Ministry of Science and ICT)(grant numbers:RS-2024-00340882); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555253","Federated learning;AI models;poisoning attacks;privacy preservation;training data","Servers;Surveys;Data models;Artificial intelligence;Data privacy;Training;Federated learning;Data privacy;Resource management","","","","492","CCBYNCND","12 Jun 2024","","","IEEE","IEEE Journals"
"System for Multi-Robotic Exploration of Underground Environments CTU-CRAS-NORLAB in the DARPA Subterranean Challenge","T. Rouček; M. Pecka; P. Čížek; T. Petříček; J. Bayer; V. Šalanský; T. Azayev; D. Heřt; M. Petrlík; T. Báča; V. Spurný; V. Krátký; P. Petráček; D. Baril; M. Vaidis; V. Kubelka; F. Pomerleau; J. Faigl; K. Zimmermann; M. Saska; T. Svoboda; T. Krajník","Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Université Laval, Canada; Université Laval, Canada; Université Laval, Canada; Université Laval, Canada; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University",Field Robotics,"25 Feb 2025","2022","2","","1779","1818","We present a field report of the CTU-CRAS-NORLAB team from the Subterranean Challenge (SubT) organized by the Defense Advanced Research Projects Agency (DARPA). The contest seeks to advance technologies that would improve the safety and efficiency of search-and-rescue operations in GPS-denied environments. During the contest rounds, teams of mobile robots have to find specific objects while operating in environments with limited radio communication, e.g., mining tunnels, underground stations or natural caverns. We present a heterogeneous exploration robotic system of the CTU-CRAS-NORLAB team, which achieved the third rank at the SubT Tunnel and Urban Circuit rounds and surpassed the performance of all other non-DARPA-funded teams. The field report describes the team's hardware, sensors, algorithms and strategies, and discusses the lessons learned by participating at the DARPA SubT contest.","2771-3989","","10.55417/fr.2022055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10876069","subterranean robotics;GPS-denied operation;robot teaming;emergency response","Robots;Robot sensing systems;Robot kinematics;Accuracy;Three-dimensional displays;Robustness;Location awareness;Radio links;Hardware;Visualization","","","","","CCBY","25 Feb 2025","","","FRPS","FRPS Journals"
"The Machine Learning Solutions Architect Handbook: Create machine learning platforms to run solutions in an enterprise setting","D. Ping",NA,The Machine Learning Solutions Architect Handbook: Create machine learning platforms to run solutions in an enterprise setting,"","2022","","","","","Build highly secure and scalable machine learning platforms to support the fast-paced adoption of machine learning solutionsKey FeaturesExplore different ML tools and frameworks to solve large-scale machine learning challenges in the cloudBuild an efficient data science environment for data exploration, model building, and model trainingLearn how to implement bias detection, privacy, and explainability in ML model developmentBook DescriptionWhen equipped with a highly scalable machine learning (ML) platform, organizations can quickly scale the delivery of ML products for faster business value realization. There is a huge demand for skilled ML solutions architects in different industries, and this handbook will help you master the design patterns, architectural considerations, and the latest technology insights you’ll need to become one. You’ll start by understanding ML fundamentals and how ML can be applied to solve real-world business problems. Once you've explored a few leading problem-solving ML algorithms, this book will help you tackle data management and get the most out of ML libraries such as TensorFlow and PyTorch. Using open source technology such as Kubernetes/Kubeflow to build a data science environment and ML pipelines will be covered next, before moving on to building an enterprise ML architecture using Amazon Web Services (AWS). You’ll also learn about security and governance considerations, advanced ML engineering techniques, and how to apply bias detection, explainability, and privacy in ML model development. And finally, you'll get acquainted with AWS AI services and their applications in real-world use cases. By the end of this book, you’ll be able to design and build an ML platform to support common use cases and architecture patterns like a true professional. What you will learnApply ML methodologies to solve business problemsDesign a practical enterprise ML platform architectureImplement MLOps for ML workflow automationBuild an end-to-end data management architecture using AWSTrain large-scale ML models and optimize model inference latencyCreate a business application using an AI service and a custom ML modelUse AWS services to detect data and model bias and explain modelsWho this book is forThis book is for data scientists, data engineers, cloud architects, and machine learning enthusiasts who want to become machine learning solutions architects. You’ll need basic knowledge of the Python programming language, AWS, linear algebra, probability, and networking concepts before you get started with this handbook. ","","9781801070416","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163605.pdf&bkn=10163604&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Power BI Machine Learning and OpenAI: Explore data through business intelligence, predictive analytics, and text generation","G. Beaumont",NA,"Power BI Machine Learning and OpenAI: Explore data through business intelligence, predictive analytics, and text generation","","2023","","","","","Unleash the full potential of Power BI with the integration of AI and machine learning techniques using OpenAI Purchase of the print or Kindle book includes a free PDF eBook Key FeaturesTake flight with Power BI machine learning and OpenAI using hands-on examples from the FAA airline dataUnlock the full potential of Power BI for advanced analytics using OpenAIDesign stunning data presentations, seamless integration of machine learning tools and technologies with OpenAIBook DescriptionMicrosoft Power BI is the ultimate solution for businesses looking to make data-driven decisions and unlock the full potential of their data. Unleashing Your Data with Power BI Machine Learning and OpenAI is designed for data scientists and BI professionals seeking to improve their existing solutions and workloads using AI. The book explains the intricacies of the subject by using a workshop-style data story for data ingestion, data modeling, analytics, and predictive analytics with Power BI machine learning. Along the way, you’ll learn about AI features, AI visuals, R/Python integration, and OpenAI integration. The workshop-style content allows you to practice all your learnings in real-life challenges and gain hands-on experience. Additionally, you’ll gain an understanding of AI/ML, step by step, with replicable examples and references. From enhancing data visualizations to building SaaS Power BI ML models, and integrating Azure OpenAI, this book will help you unlock new capabilities in Power BI. By the end of this book, you’ll be well-equipped to build ML models in Power BI, plan projects for both BI and ML, understand R/Python visuals with Power BI, and introduce OpenAI to enhance your analytics solutions.What you will learnDiscover best practices for implementing AI and ML capabilities in Power BI using OpenAIUnderstand how to integrate OpenAI and cognitive services into Power BIExplore how to build a SaaS auto ML model within Power BIGain an understanding of R/Python integration with Power BIEnhance data visualizations for ML feature discoveryDiscover how to improve existing solutions and workloads using AI and ML capabilities in Power BI with OpenAIAcquire tips and tricks for successfully using AI and ML capabilities in Power BI using OpenAIWho this book is forThis book is for data science and BI professionals looking to expand their skill sets into Power BI machine learning and OpenAI. This book is also useful for data scientists, data analysts, and IT professionals who want to learn how to incorporate OpenAI into Power BI for advanced experience.","","9781837634330","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251338.pdf&bkn=10251337&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Interactive and Explainable Robot Learning: A Comprehensive Review","E. Seraj; K. M. Lee; Z. Zaidi; Q. Xiao; Z. Li; A. Nascimento; S. van Waveren; P. Tambwekar; R. Paleja; D. Das; M. Gombolay",NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA,Interactive and Explainable Robot Learning: A Comprehensive Review,"","2024","","","","","This monograph embarks on a comprehensive exploration of approaches, evaluation methods, and ethical considerations in explainable and interactive systems for robotic applications, distinctly focusing on intelligent systems that are specifically designed for learning automated agents. Given the increasing integration of robots in daily life, it is crucial to focus on intelligent systems that not only can learn and adapt, but also can offer clarity and comprehension for their actions. The interactive component of these systems is thoroughly examined, evaluating the algorithms, the modalities used in interaction, and the significance of mixed-initiative and shared autonomy. Adaptive and adaptable methods, emphasizing the centrality of user-inspired research and personalized approaches in interactive robotics are highlighted. Also included is a rigorous examination of safety and ethical considerations of these intelligent systems, including aspects of transparency, privacy, accountability, biases, and psychological well-being. The monograph evaluates existing metrics and benchmarking standards for such systems and explores their practical applications across domains such as healthcare, domestic tasks, and industrial automation. The monograph concludes with key insights and directions for future research, and design guidelines and points of consensus for each subject are included in order to equip readers with a nuanced understanding of current trends and tools in explainable and interactive robotic systems, paving the way for informed research and application in this dynamic field.","","9781638283775","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10551738.pdf&bkn=10551737&pdfType=book","","","","","","","","7 Jun 2024","","","now","Now Foundations and Trends Books"
"IEEE Draft Standard for DevOps: Building Reliable and Secure Systems Including Application Build, Package and Deployment","",,"IEEE P2675/D1, June 2020","12 Aug 2020","2020","","","1","95","Technical principles and processes to build, package, and deploy systems and applications in a reliable and secure way are specified. Establishing effective compliance and information technology (IT) controls is the focus. DevOps principles presented include mission first, customer focus, left-shift, continuous everything, and systems thinking. How stakeholders, including developers and operations staff, can collaborate and communicate effectively is described. The process outcomes and activities herein are aligned with the process model specified in ISO/IEC/IEEE 12207:2017 and ISO/IEC/IEEE 15288:2015.","","978-1-5044-6918-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165985","agile;continuous delivery;continuous deployment;continuous integration;DevOps;IEEE 2675;left-shift","IEEE Standards;IEC Standards;ISO Standards;Systems thinking;Security;Software engineering;Systems engineering and theory;Product life cycle development","","","","","","12 Aug 2020","","","IEEE","IEEE Standards"
"Comet for Data Science: Enhance your ability to manage and optimize the life cycle of your data science project","A. Lo Duca; G. Mendels",NA; NA,Comet for Data Science: Enhance your ability to manage and optimize the life cycle of your data science project,"","2022","","","","","Gain the key knowledge and skills required to manage data science projects using CometKey FeaturesDiscover techniques to build, monitor, and optimize your data science projectsMove from prototyping to production using Comet and DevOps toolsGet to grips with the Comet experimentation platformBook DescriptionThis book provides concepts and practical use cases which can be used to quickly build, monitor, and optimize data science projects. Using Comet, you will learn how to manage almost every step of the data science process from data collection through to creating, deploying, and monitoring a machine learning model. The book starts by explaining the features of Comet, along with exploratory data analysis and model evaluation in Comet. You’ll see how Comet gives you the freedom to choose from a selection of programming languages, depending on which is best suited to your needs. Next, you will focus on workspaces, projects, experiments, and models. You will also learn how to build a narrative from your data, using the features provided by Comet. Later, you will review the basic concepts behind DevOps and how to extend the GitLab DevOps platform with Comet, further enhancing your ability to deploy your data science projects. Finally, you will cover various use cases of Comet in machine learning, NLP, deep learning, and time series analysis, gaining hands-on experience with some of the most interesting and valuable data science techniques available. By the end of this book, you will be able to confidently build data science pipelines according to bespoke specifications and manage them through Comet.What you will learnPrepare for your project with the right dataUnderstand the purposes of different machine learning algorithmsGet up and running with Comet to manage and monitor your pipelinesUnderstand how Comet works and how to get the most out of itSee how you can use Comet for machine learningDiscover how to integrate Comet with GitLabWork with Comet for NLP, deep learning, and time series analysisWho this book is forThis book is for anyone who has programming experience, and wants to learn how to manage and optimize a complete data science lifecycle using Comet and other DevOps platforms. Although an understanding of basic data science concepts and programming concepts is needed, no prior knowledge of Comet and DevOps is required.","","9781801814355","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163569.pdf&bkn=10163568&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Machine Learning for Automated Theorem Proving: Learning to Solve SAT and QSAT","S. B. Holden",NA,Machine Learning for Automated Theorem Proving: Learning to Solve SAT and QSAT,"","2021","","","","","Automated theorem proving represents a significant and long-standing area of research in computer science, with numerous applications. A large proportion of the methods developed to date for the implementation of automated theorem provers (ATPs) have been algorithmic, sharing a great deal in common with the wider study of heuristic search algorithms. However, in recent years researchers have begun to incorporate machine learning (ML) methods into ATPs in an effort to extract better performance. Propositional satisfiability (SAT) solving and machine learning are both large and longstanding areas of research, and each has a correspondingly large literature. In this book, the author presents the results of his thorough and systematic review of the research at the intersection of these two apparently rather unrelated fields. It focusses on the research that has appeared to date on incorporating ML methods into solvers for propositional satisfiability SAT problems, and also solvers for its immediate variants such as and quantified SAT (QSAT). The comprehensiveness of the coverage means that ML researchers gain an understanding of state-of-the-art SAT and QSAT solvers that is sufficient to make new opportunities for applying their own ML research to this domain clearly visible, while ATP researchers gain a clear appreciation of how state-of-the-art machine learning might help them to design better solvers. In presenting the material, the author concentrates on the learning methods used and the way in which they have been incorporated into solvers. This enables researchers and students in both Automated Theorem Proving and Machine Learning to a) know what has been tried and b) understand the often complex interaction between ATP and ML that is needed for success in these undeniably challenging applications.","","9781680838992","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9624177.pdf&bkn=9624176&pdfType=book","","","","","","","","23 Nov 2021","","","now","Now Foundations and Trends Books"
"Deep Reinforcement Learning Hands-On: A practical and easy-to-follow guide to RL from Q-learning and DQNs to PPO and RLHF","M. Lapan",NA,Deep Reinforcement Learning Hands-On: A practical and easy-to-follow guide to RL from Q-learning and DQNs to PPO and RLHF,"","2024","","","","","Maxim Lapan delivers intuitive explanations and insights into complex reinforcement learning (RL) concepts, starting from the basics of RL on simple environments and tasks to modern, state-of-the-art methods Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn with concise explanations, modern libraries, and diverse applications from games to stock trading and web navigationDevelop deep RL models, improve their stability, and efficiently solve complex environmentsNew content on RL from human feedback (RLHF), MuZero, and transformersBook DescriptionStart your journey into reinforcement learning (RL) and reward yourself with the third edition of Deep Reinforcement Learning Hands-On. This book takes you through the basics of RL to more advanced concepts with the help of various applications, including game playing, discrete optimization, stock trading, and web browser navigation. By walking you through landmark research papers in the fi eld, this deep RL book will equip you with practical knowledge of RL and the theoretical foundation to understand and implement most modern RL papers. The book retains its approach of providing concise and easy-to-follow explanations from the previous editions. You'll work through practical and diverse examples, from grid environments and games to stock trading and RL agents in web environments, to give you a well-rounded understanding of RL, its capabilities, and its use cases. You'll learn about key topics, such as deep Q-networks (DQNs), policy gradient methods, continuous control problems, and highly scalable, non-gradient methods. If you want to learn about RL through a practical approach using OpenAI Gym and PyTorch, concise explanations, and the incremental development of topics, then Deep Reinforcement Learning Hands-On, Third Edition, is your ideal companionWhat you will learnStay on the cutting edge with new content on MuZero, RL with human feedback, and LLMsEvaluate RL methods, including cross-entropy, DQN, actor-critic, TRPO, PPO, DDPG, and D4PGImplement RL algorithms using PyTorch and modern RL librariesBuild and train deep Q-networks to solve complex tasks in Atari environmentsSpeed up RL models using algorithmic and engineering approachesLeverage advanced techniques like proximal policy optimization (PPO) for more stable trainingWho this book is forThis book is ideal for machine learning engineers, software engineers, and data scientists looking to learn and apply deep reinforcement learning in practice. It assumes familiarity with Python, calculus, and machine learning concepts. With practical examples and high-level overviews, it’s also suitable for experienced professionals looking to deepen their understanding of advanced deep RL methods and apply them across industries, such as gaming and finance","","9781835882719","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10803977.pdf&bkn=10803976&pdfType=book","","","","","","","","16 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"Salesforce Sales Cloud – An Implementation Handbook: A practical guide from design to deployment for driving success in sales","K. Townsend",NA,Salesforce Sales Cloud – An Implementation Handbook: A practical guide from design to deployment for driving success in sales,"","2024","","","","","Design and build Sales Cloud solutions to solve business challenges with this easy-to-follow handbookKey FeaturesDiscover the full range of capabilities offered by Sales Cloud and how to map them to business processesLearn how to plan and deliver all aspects of a successful Sales Cloud implementationExplore advanced concepts to integrate and extend Sales CloudPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionSalesforce Sales Cloud is a system rich in functionality, addressing many sales business challenges such as sales productivity, forecast visibility, and sales enablement. However, unlocking the full value of the system and getting maximum returns pose a challenge, especially if you’re new to the technology. This implementation handbook goes beyond mere configuration to ensure a successful implementation journey. From laying the groundwork for your project to engaging stakeholders with sales-specific business insights, this book equips you with the knowledge you need to plan and execute. As you progress, you’ll learn how to design a robust data model to support the sales and lead generation process, followed by crafting an intuitive user experience to drive productivity. You’ll then explore crucial post-building aspects such as testing, training, and releasing functionality. Finally, you’ll discover how the solutions’ capability can be expanded by adding and integrating other tools to address typical sales use cases. By the end of this book, you’ll have grasped how to leverage Sales Cloud to solve sales challenges and have gained the confidence to design and implement solutions successfully with the help of real-world use cases.What you will learnFind out how Sales Cloud capabilities solve common sales challengesDetermine the best development methodologiesDesign and build core sales processes, including demand generation and sales productivityImplement best practices for testing and training with accurate dataBuild a release plan by understanding the types of post-go-live supportExplore territory management and model additional processes with Sales CloudUnderstand common system integration use casesHarness the power of AppExchange solutions for salesWho this book is forThis book is for administrators, consultants, and business analysts who want to understand and apply the capabilities of Salesforce Sales Cloud. Whether you’re completely new to Sales Cloud or enhancing existing functionalities within your organization, this handbook is your trusted companion. Business stakeholders responsible for or involved in Sales Cloud implementations will also benefit from this book.","","9781804615188","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10522529.pdf&bkn=10522528&pdfType=book","","","","","","","","8 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"Hands-On Data Analysis with Pandas: A Python data science handbook for data collection, wrangling, analysis, and visualization","S. Molin; K. Jee",NA; NA,"Hands-On Data Analysis with Pandas: A Python data science handbook for data collection, wrangling, analysis, and visualization","","2021","","","","","Get to grips with pandas by working with real datasets and master data discovery, data manipulation, data preparation, and handling data for analytical tasksKey FeaturesPerform efficient data analysis and manipulation tasks using pandas 1.xApply pandas to different real-world domains with the help of step-by-step examplesMake the most of pandas as an effective data exploration toolBook DescriptionExtracting valuable business insights is no longer a ‘nice-to-have’, but an essential skill for anyone who handles data in their enterprise. Hands-On Data Analysis with Pandas is here to help beginners and those who are migrating their skills into data science get up to speed in no time. This book will show you how to analyze your data, get started with machine learning, and work effectively with the Python libraries often used for data science, such as pandas, NumPy, matplotlib, seaborn, and scikit-learn. Using real-world datasets, you will learn how to use the pandas library to perform data wrangling to reshape, clean, and aggregate your data. Then, you will learn how to conduct exploratory data analysis by calculating summary statistics and visualizing the data to find patterns. In the concluding chapters, you will explore some applications of anomaly detection, regression, clustering, and classification using scikit-learn to make predictions based on past data. This updated edition will equip you with the skills you need to use pandas 1.x to efficiently perform various data manipulation tasks, reliably reproduce analyses, and visualize your data for effective decision making – valuable knowledge that can be applied across multiple domains.What you will learnUnderstand how data analysts and scientists gather and analyze dataPerform data analysis and data wrangling using PythonCombine, group, and aggregate data from multiple sourcesCreate data visualizations with pandas, matplotlib, and seabornApply machine learning algorithms to identify patterns and make predictionsUse Python data science libraries to analyze real-world datasetsSolve common data representation and analysis problems using pandasBuild Python scripts, modules, and packages for reusable analysis codeWho this book is forThis book is for data science beginners, data analysts, and Python developers who want to explore each stage of data analysis and scientific computing using a wide range of datasets. Data scientists looking to implement pandas in their machine learning workflow will also find plenty of valuable know-how as they progress. You’ll find it easier to follow along with this book if you have a working knowledge of the Python programming language, but a Python crash-course tutorial is provided in the code bundle for anyone who needs a refresher.","","9781800565913","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163286.pdf&bkn=10163285&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Unity 2021 Cookbook: Over 140 recipes to take your Unity game development skills to the next level","M. Smith; S. Ferns; C. Gregan",NA; NA; NA,Unity 2021 Cookbook: Over 140 recipes to take your Unity game development skills to the next level,"","2021","","","","","Discover the latest features of Unity 2021 and dive deeper into the nuances of professional game development with UnityKey FeaturesDiscover the latest features of Unity 2021 including coverage of AR/VR developmentFollow practical recipes for better 2D and 2D character development with Unity GameKitsLearn powerful techniques and expert best practices in building 3D objects, textures, and materialsBook DescriptionIf you are a Unity developer looking to explore the newest features of Unity 2021 and recipes for advanced challenges, then this fourth edition of Unity Cookbook is here to help you. With this cookbook, you’ll work through a wide variety of recipes that will help you use the essential features of the Unity game engine to their fullest potential. You familiarize yourself with shaders and Shader Graph before exploring animation features to enhance your skills in building games. As you progress, you will gain insights into Unity's latest editor, which will help you in laying out scenes, tweaking existing apps, and building custom tools for augmented reality and virtual reality (AR/VR) experiences. The book will also guide you through many Unity C# gameplay scripting techniques, teaching you how to communicate with database-driven websites and process XML and JSON data files. By the end of this Unity book, you will have gained a comprehensive understanding of Unity game development and built your development skills. The easy-to-follow recipes will earn a permanent place on your bookshelf for reference and help you build better games that stay true to your vision.What you will learnDiscover how to add core game features to your projects with C# scriptingCreate powerful and stylish UI with Unity's UI system, including power bars, radars, and button-driven scene changesWork with essential audio features, including background music and sound effectsDiscover Cinemachine in Unity to intelligently control camera movementsAdd visual effects such as smoke and explosions by creating and customizing particle systemsUnderstand how to build your own Shaders with the Shader Graph toolWho this book is forIf you’re a Unity developer looking for better ways to resolve common recurring problems with recipes, then this book is for you. Programmers dipping their toes into multimedia features for the first time will also find this book useful. Before you get started with this Unity engine book, you’ll need a solid understanding of Unity’s functionality and experience with programming in C#.","","9781839219276","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163644.pdf&bkn=10163643&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Data Science with .NET and Polyglot Notebooks: Programmer's guide to data science using ML.NET, OpenAI, and Semantic Kernel","M. Eland",NA,"Data Science with .NET and Polyglot Notebooks: Programmer's guide to data science using ML.NET, OpenAI, and Semantic Kernel","","2024","","","","","Expand your skillset by learning how to perform data science, machine learning, and generative AI experiments in .NET Interactive notebooks using a variety of languages, including C#, F#, SQL, and PowerShellKey FeaturesConduct a full range of data science experiments with clear explanations from start to finishLearn key concepts in data analytics, machine learning, and AI and apply them to solve real-world problemsAccess all of the code online as a notebook and interactive GitHub CodespacePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionAs the fields of data science, machine learning, and artificial intelligence rapidly evolve, .NET developers are eager to leverage their expertise to dive into these exciting domains but are often unsure of how to do so. Data Science in .NET with Polyglot Notebooks is the practical guide you need to seamlessly bring your .NET skills into the world of analytics and AI. With Microsoft’s .NET platform now robustly supporting machine learning and AI tasks, the introduction of tools such as .NET Interactive kernels and Polyglot Notebooks has opened up a world of possibilities for .NET developers. This book empowers you to harness the full potential of these cutting-edge technologies, guiding you through hands-on experiments that illustrate key concepts and principles. Through a series of interactive notebooks, you’ll not only master technical processes but also discover how to integrate these new skills into your current role or pivot to exciting opportunities in the data science field. By the end of the book, you’ll have acquired the necessary knowledge and confidence to apply cutting-edge data science techniques and deliver impactful solutions within the .NET ecosystem.What you will learnLoad, analyze, and transform data using DataFrames, data visualization, and descriptive statisticsTrain machine learning models with ML.NET for classification and regression tasksCustomize ML.NET model training pipelines with AutoML, transforms, and model trainersApply best practices for deploying models and monitoring their performanceConnect to generative AI models using Polyglot NotebooksChain together complex AI tasks with AI orchestration, RAG, and Semantic KernelCreate interactive online documentation with Mermaid charts and GitHub CodespacesWho this book is forThis book is for experienced C# or F# developers who want to transition into data science and machine learning while leveraging their .NET expertise. It’s ideal for those looking to learn ML.NET and Semantic kernel and extend their .NET skills to data science, machine learning, and Generative AI Workflows.","","9781835882979","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769341.pdf&bkn=10769340&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Distributed Machine Learning with Python: Accelerating model training and serving with distributed systems","G. Wang",NA,Distributed Machine Learning with Python: Accelerating model training and serving with distributed systems,"","2022","","","","","Build and deploy an efficient data processing pipeline for machine learning model training in an elastic, in-parallel model training or multi-tenant cluster and cloudKey FeaturesAccelerate model training and interference with order-of-magnitude time reductionLearn state-of-the-art parallel schemes for both model training and servingA detailed study of bottlenecks at distributed model training and serving stagesBook DescriptionReducing time cost in machine learning leads to a shorter waiting time for model training and a faster model updating cycle. Distributed machine learning enables machine learning practitioners to shorten model training and inference time by orders of magnitude. With the help of this practical guide, you'll be able to put your Python development knowledge to work to get up and running with the implementation of distributed machine learning, including multi-node machine learning systems, in no time. You'll begin by exploring how distributed systems work in the machine learning area and how distributed machine learning is applied to state-of-the-art deep learning models. As you advance, you'll see how to use distributed systems to enhance machine learning model training and serving speed. You'll also get to grips with applying data parallel and model parallel approaches before optimizing the in-parallel model training and serving pipeline in local clusters or cloud environments. By the end of this book, you'll have gained the knowledge and skills needed to build and deploy an efficient data processing pipeline for machine learning model training and inference in a distributed manner.What you will learnDeploy distributed model training and serving pipelinesGet to grips with the advanced features in TensorFlow and PyTorchMitigate system bottlenecks during in-parallel model training and servingDiscover the latest techniques on top of classical parallelism paradigmExplore advanced features in Megatron-LM and Mesh-TensorFlowUse state-of-the-art hardware such as NVLink, NVSwitch, and GPUsWho this book is forThis book is for data scientists, machine learning engineers, and ML practitioners in both academia and industry. A fundamental understanding of machine learning concepts and working knowledge of Python programming is assumed. Prior experience implementing ML/DL models with TensorFlow or PyTorch will be beneficial. You'll find this book useful if you are interested in using distributed systems to boost machine learning model training and serving speed.","","9781801817219","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163214.pdf&bkn=10163213&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Modern Graph Theory Algorithms with Python: Harness the power of graph algorithms and real-world network applications using Python","C. M. Farrelly; F. K. Mutombo; M. Giske",NA; NA; NA,Modern Graph Theory Algorithms with Python: Harness the power of graph algorithms and real-world network applications using Python,"","2024","","","","","Solve challenging and computationally intensive analytics problems by leveraging network science and graph algorithms Key FeaturesLearn how to wrangle different types of datasets and analytics problems into networksLeverage graph theoretic algorithms to analyze data efficientlyApply the skills you gain to solve a variety of problems through case studies in PythonPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionWe are living in the age of big data, and scalable solutions are a necessity. Network science leverages the power of graph theory and flexible data structures to analyze big data at scale. This book guides you through the basics of network science, showing you how to wrangle different types of data (such as spatial and time series data) into network structures. You’ll be introduced to core tools from network science to analyze real-world case studies in Python. As you progress, you’ll find out how to predict fake news spread, track pricing patterns in local markets, forecast stock market crashes, and stop an epidemic spread. Later, you’ll learn about advanced techniques in network science, such as creating and querying graph databases, classifying datasets with graph neural networks (GNNs), and mining educational pathways for insights into student success. Case studies in the book will provide you with end-to-end examples of implementing what you learn in each chapter. By the end of this book, you’ll be well-equipped to wrangle your own datasets into network science problems and scale solutions with Python.What you will learnTransform different data types, such as spatial data, into network formatsExplore common network science tools in PythonDiscover how geometry impacts spreading processes on networksImplement machine learning algorithms on network data featuresBuild and query graph databasesExplore new frontiers in network science such as quantum algorithmsWho this book is forIf you’re a researcher or industry professional analyzing data and are curious about network science approaches to data, this book is for you. To get the most out of the book, basic knowledge of Python, including pandas and NumPy, as well as some experience working with datasets is required. This book is also ideal for anyone interested in network science and learning how graph algorithms are used to solve science and engineering problems. R programmers may also find this book helpful as many algorithms also have R implementations.","","9781805120179","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10559426.pdf&bkn=10559425&pdfType=book","","","","","","","","17 Jun 2024","","","Packt Publishing","Packt Publishing eBooks"
"Building Modern Data Applications Using Databricks Lakehouse: Develop, optimize, and monitor data pipelines on Databricks","W. Girten",NA,"Building Modern Data Applications Using Databricks Lakehouse: Develop, optimize, and monitor data pipelines on Databricks","","2024","","","","","Get up to speed with the Databricks Data Intelligence Platform to build and scale modern data applications, leveraging the latest advancements in data engineeringKey FeaturesLearn how to work with real-time data using Delta Live TablesUnlock insights into the performance of data pipelines using Delta Live TablesApply your knowledge to Unity Catalog for robust data security and governancePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionWith so many tools to choose from in today’s data engineering development stack as well as operational complexity, this often overwhelms data engineers, causing them to spend less time gleaning value from their data and more time maintaining complex data pipelines. Guided by a lead specialist solutions architect at Databricks with 10+ years of experience in data and AI, this book shows you how the Delta Live Tables framework simplifies data pipeline development by allowing you to focus on defining input data sources, transformation logic, and output table destinations. This book gives you an overview of the Delta Lake format, the Databricks Data Intelligence Platform, and the Delta Live Tables framework. It teaches you how to apply data transformations by implementing the Databricks medallion architecture and continuously monitor the data quality of your pipelines. You’ll learn how to handle incoming data using the Databricks Auto Loader feature and automate real-time data processing using Databricks workflows. You’ll master how to recover from runtime errors automatically. By the end of this book, you’ll be able to build a real-time data pipeline from scratch using Delta Live Tables, leverage CI/CD tools to deploy data pipeline changes automatically across deployment environments, and monitor, control, and optimize cloud costs.What you will learnDeploy near-real-time data pipelines in Databricks using Delta Live TablesOrchestrate data pipelines using Databricks workflowsImplement data validation policies and monitor/quarantine bad dataApply slowly changing dimensions (SCD), Type 1 and 2, data to lakehouse tablesSecure data access across different groups and users using Unity CatalogAutomate continuous data pipeline deployment by integrating Git with build tools such as Terraform and Databricks Asset BundlesWho this book is forThis book is for data engineers looking to streamline data ingestion, transformation, and orchestration tasks. Data analysts responsible for managing and processing lakehouse data for analysis, reporting, and visualization will also find this book beneficial. Additionally, DataOps/DevOps engineers will find this book helpful for automating the testing and deployment of data pipelines, optimizing table tasks, and tracking data lineage within the lakehouse. Beginner-level knowledge of Apache Spark and Python is needed to make the most out of this book.","","9781804612873","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10740991.pdf&bkn=10740990&pdfType=book","","","","","","","","1 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Adobe InDesign Masterclass: A comprehensive guide to taking your digital design skills from beginner to professional","M. Jogie; D. R. Chang; F. O. Dartey",NA; NA; NA,Adobe InDesign Masterclass: A comprehensive guide to taking your digital design skills from beginner to professional,"","2024","","","","","Gain valuable insights from an Adobe Certified Expert, Instructor, and Community Professional on designing cross-media content for both print and digital projects with easy-to-follow tutorials and working files in this part-color guideKey FeaturesExplore InDesign by working on bespoke projects covering both print and digital communication designEnhance your learning experience by selecting specific use cases and getting hands-on experience throughout the chaptersSuited for all levels of InDesign users with fundamental to advanced topics based on preferencePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionAdobe InDesign is a powerful multi-page layout publishing tool for creating static and interactive content across print and digital platforms. This book will empower you to develop high-quality publications for digital and online projects. You’ll be guided through the workspace, tools, and trans-disciplinary design projects for various outputs, as well as creating custom award-winning designs. The book is written for creatives with an emphasis on creative outcomes and the tools that support your creative vision. Develop multiple design projects from the ground up to high-quality finished designs. You’ll also learn how to create professional and impactful visual communication pieces. Additionally, you’ll connect data sources like Microsoft Word and Excel to InDesign’s powerful design tools. You’ll then examine the best design practices using features such as Liquid Layout, PDF/X, ePub, and Publish Online. Finally, you’ll explore the latest third party and InDesign AI features and how to use them to fast-track your creative projects. By the end of this Adobe InDesign book, you’ll be able to confidently create different types of communication and design pieces in an efficient workflow.What you will learnMaster InDesign to handle any communication task effortlesslyDiscover how to navigate InDesign's interface and utilize its tools effectivelyDesign modern communication pieces like corporate stationery, menus, eBooks, and interactive documentsBecome proficient in using InDesign's comprehensive featuresUnderstand how to prepare projects for optimal print and digital outputUse advanced features like Liquid Layout, PDF/X, ePub, and Publish Online with artificial intelligence for impactful designsWho this book is forThis Adobe InDesign software book is for layout artists, book designers, graphic designers, creative directors, corporate communication specialists, publishers, art directors, writers, design lecturers, and students who need to create professional communication campaigns. If you’re looking to become proficient in creating high-quality publications for print and screen consumption, this book is for you. You should have some elementary experience with computing to jump right in and start creating standout layouts and designs. ","","9781803241371","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10803979.pdf&bkn=10803978&pdfType=book","","","","","","","","16 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"Thriving in Android Development Using Kotlin: A project-based guide to using the latest Android features for developing production-grade apps","G. S. Rodríguez",NA,Thriving in Android Development Using Kotlin: A project-based guide to using the latest Android features for developing production-grade apps,"","2024","","","","","Build a range of Android applications including a messaging app, a photo editor, and a video streaming platform while learning how to address common real-world issues such as authentication, connecting to synchronous and asynchronous remote sources, rendering complex UIs with Jetpack Compose, and moreKey FeaturesUnderstand complex concepts in a coherent way by solving challenging real-world problems and developing three practical projectsUse the latest features of libraries in Jetpack Compose, Room, CameraX, ExoPlayer, and moreLeverage best practices for UI creation, app structure, data handling, and lifecycle managementPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionWith resources on Android and Kotlin abound, it’s difficult to find content that focuses on resolving common challenges faced by app developers. This book by Gema Socorro Rodríguez – a Google Developer Expert for Android with over 15 years of experience and a proven track record as an effective instructor – is designed to bridge the gap between theory and real-world application. It equips you with the skills to tackle everyday problems in Android development through hands-on projects. Under Gema's expert guidance, you’ll build three sophisticated Android projects. You'll start your development journey by building a WhatsApp-like application, learning how to process asynchronous messages reactively, render them using Jetpack Compose, and advance to creating and uploading a backup of these messages. Next, you’ll channel your creativity into Packtagram, an Instagram-inspired app that offers advanced photo-editing capabilities using the latest CameraX libraries. Your final project will be a Netflix-style app, integrating video playback functionality with ExoPlayer for both foreground and background operations, and implementing device casting features. By the end of this book, you'll have crafted three fully functional, multi-platform projects and gained the confidence to solve the most common challenges in Android development.What you will learnCreate complex UIs with Jetpack ComposeStructure and modularize apps with a focus on further scalingConnect your app to synchronous and asynchronous remote sourcesStore and cache information and manage the lifecycle of this dataExecute periodic tasks using WorkManagerCapture and edit photos and videos using CameraXAuthenticate your users securelyPlay videos in the foreground and background and cast them to other devicesWho this book is forIf you're a mid-level Android engineer, this book is for you as it will not only teach you how to solve issues that occur in real-world apps but also benefit you in your day-to-day work. This book will also help junior engineers who want to get exposed to complex problems and explore best practices to solve them. A basic understanding of Android and Kotlin concepts such as views, activities, lifecycle, and Kotlin coroutines will be useful to get the most out of this book. ","","9781837632770","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769343.pdf&bkn=10769342&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Data Quality in the Age of AI: Building a foundation for AI strategy and data culture","A. Jones",NA,Data Quality in the Age of AI: Building a foundation for AI strategy and data culture,"","2024","","","","","Unlock the power of data with expert insights to enhance data quality, maximizing the potential of AI, and establishing a data-centric cultureKey FeaturesGain a profound understanding of the interplay between data quality and AIExplore strategies to improve data quality with practical implementation and real-world resultsAcquire the skills to measure and evaluate data quality, empowering data-driven decisionsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionAs organizations worldwide seek to revamp their data strategies to leverage AI advancements and benefit from newfound capabilities, data quality emerges as the cornerstone for success. Without high-quality data, even the most advanced AI models falter. Enter Data Quality in the Age of AI, a detailed report that illuminates the crucial role of data quality in shaping effective data strategies. Packed with actionable insights, this report highlights the critical role of data quality in your overall data strategy. It equips teams and organizations with the knowledge and tools to thrive in the evolving AI landscape, serving as a roadmap for harnessing the power of data quality, enabling them to unlock their data's full potential, leading to improved performance, reduced costs, increased revenue, and informed strategic decisions.What you will learnDiscover actionable steps to establish data quality as the foundation of your data cultureEnhance data quality directly at its source with effective strategies and best practicesElevate data quality standards and enhance data literacy within your organizationIdentify and measure data quality within the datasetAdopt a product mindset to address data quality challengesExplore emerging architectural patterns like data mesh and data contractsAssign roles, responsibilities, and incentives for data generatorsGain insights from real-world case studiesWho this book is forThis report is for data leaders and decision-makers, including CTOs, CIOs, CISOs, CPOs, and CEOs responsible for shaping their organization's data strategy to maximize data value, especially those interested in harnessing recent AI advancements.","","9781835088562","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769235.pdf&bkn=10769234&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Mathematical Information Retrieval: Search and Question Answering","R. Zanibbi; B. Mansouri; A. Agarwal",NA; NA; NA,Mathematical Information Retrieval: Search and Question Answering,"","2025","","","","","Mathematical information is essential for technical work, but its creation, interpretation, and search are challenging. To help address these challenges, researchers have developed multimodal search engines and mathematical question answering systems. This monograph begins with a simple framework characterizing the information tasks that people and systems perform as we work to answer math-related questions. The framework is used to organize and relate the other core topics of the monograph, including interactions between people and systems, representing math formulas in sources, and evaluation. The monograph closes by addressing some key questions and presenting directions for future work. This monograph is intended for students, instructors, and researchers interested in systems that help us find and use mathematical information.","","9781638285038","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10858667.pdf&bkn=10858666&pdfType=book","","","","","","","","31 Jan 2025","","","now","Now Foundations and Trends Books"
"Raspberry Pi Pico DIY Workshop: Build exciting projects in home automation, personal health, gardening, and citizen science","S. Yamanoor; S. Yamanoor",NA; NA,"Raspberry Pi Pico DIY Workshop: Build exciting projects in home automation, personal health, gardening, and citizen science","","2022","","","","","Take your first steps with the Raspberry Pi Pico and take on exciting projects using CircuitPython, MicroPython, and PicoKey FeaturesMake the most of the Raspberry Pi Pico—a low-cost microcontroller that is primed for innovationWork with easy-to-follow examples and learn how to interface and program a Raspberry Pi PicoWork on fun projects, right from home automation to building a seven-segment display to tracking air qualityBook DescriptionThe Raspberry Pi Pico is the latest addition to the Raspberry Pi family of products. Introduced by the Raspberry Pi Foundation, based on their RP2040 chip, it is a tiny, fast microcontroller that packs enough punch to power an extensive range of applications. Raspberry Pi Pico DIY Workshop will help you get started with your own Pico and leverage its features to develop innovative products. This book begins with an introduction to the Raspberry Pi Pico, giving you a thorough understanding of the RP2040's peripherals and different development boards for the Pico designed and manufactured by various organizations. You'll explore add-on hardware and programming language options available for the Pico. Next, you'll focus on practical skills, starting with a simple LED blinking project and building up to a giant seven-segment display, while working with application examples such as citizen science displays, digital health, and robots. You'll also work on exciting projects around gardening, building a weather station, tracking air quality, hacking your personal health, and building a robot, along with discovering tips and tricks to give you the confidence needed to make the best use of RP2040. By the end of this Raspberry Pi book, you'll have built a solid foundation in product development using the RP2040, acquired a skillset crucial for embedded device development, and have a robot that you built yourself.What you will learnUnderstand the RP2040's peripherals and apply them in the real worldFind out about the programming languages that can be used to program the RP2040Delve into the applications of serial interfaces available on the PicoDiscover add-on hardware available for the RP2040Explore different development board variants for the Raspberry Pi PicoDiscover tips and tricks for seamless product development with the PicoWho this book is forThis book is for students, teachers, engineers, scientists, artists, and tech enthusiasts who want to develop embedded systems that drive cost-effective automation, IoT, robotics, medical devices, and art projects. If you consider yourself a maker and would like to learn how to use the Raspberry Pi Pico, then this book is for you. Familiarity with Python programming, MicroPython, CircuitPython, embedded hardware, and peripherals is helpful but not mandatory to get the most out of this book.","","9781801810661","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163108.pdf&bkn=10163107&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"5 Looking Again in a Different Way","J. Lepawsky",Memorial University of Newfoundland,Reassembling Rubbish: Worlding Electronic Waste,"","2018","","","93","128","Indeterminacy is the condition of measuring. — Myra Hird (2012, 462) The etymology of the word “fact” is tricky as well as enlightening: it may mean fabricated thus false, or fabricated thus solid. It is the second path we invite you to follow. Cogitamus not cogito. — Bruno Latour (n.d., 1)","","9780262346375","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8544207.pdf&bkn=8544156&pdfType=chapter","","Electronic waste;Recycling;Extrapolation;Global Positioning System;Weight measurement;Solids;Industries","","","","","","13 Dec 2018","","","MIT Press","MIT Press eBook Chapters"
"Databricks Certified Associate Developer for Apache Spark Using Python: The ultimate guide to getting certified in Apache Spark using practical examples with Python","S. Shah; R. Waltermann",NA; NA,Databricks Certified Associate Developer for Apache Spark Using Python: The ultimate guide to getting certified in Apache Spark using practical examples with Python,"","2024","","","","","Learn the concepts and exercises needed to get certified as a Databricks Associate Developer for Apache Spark 3.0 and validate your skills as a Spark expert with an industry-recognized credential Key FeaturesUnderstand the fundamentals of Apache Spark to help you design robust and fast Spark applicationsDelve into various data manipulation components for each phase of your data engineering projectPrepare for the certification exam with sample questions and mock exams, and get closer to your goalPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionWith extensive data being collected every second, computing power cannot keep up with this pace of rapid growth. To make use of all the data, Spark has become a de facto standard for big data processing. Migrating data processing to Spark will not only help you save resources that will allow you to focus on your business, but also enable you to modernize your workloads by leveraging the capabilities of Spark and the modern technology stack for creating new business opportunities. This book is a comprehensive guide that lets you explore the core components of Apache Spark, its architecture, and its optimization. You’ll become familiar with the Spark dataframe API and its components needed for data manipulation. Next, you’ll find out what Spark streaming is and why it’s important for modern data stacks, before learning about machine learning in Spark and its different use cases. What’s more, you’ll discover sample questions at the end of each section along with two mock exams to help you prepare for the certification exam. By the end of this book, you’ll know what to expect in the exam and how to pass it with enough understanding of Spark and its tools. You’ll also be able to apply this knowledge in a real-world setting and take your skillset to the next level.What you will learnCreate and manipulate SQL queries in SparkBuild complex Spark functions using Spark UDFsArchitect big data apps with Spark fundamentals for optimal designApply techniques to manipulate and optimize big data applicationsBuild real-time or near-real-time applications using Spark StreamingWork with Apache Spark for machine learning applicationsWho this book is forThis book is for you if you’re a professional looking to venture into the world of big data and data engineering, a data professional who wants to endorse your knowledge of Spark, or a student. Although working knowledge of Python is required, no prior Spark knowledge is needed. Additionally, experience with Pyspark will be beneficial. ","","9781804616208","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10559432.pdf&bkn=10559431&pdfType=book","","","","","","","","17 Jun 2024","","","Packt Publishing","Packt Publishing eBooks"
"Data Analytics for Marketing: A practical guide to analyzing marketing data using Python","G. Diaz-Bérrio",NA,Data Analytics for Marketing: A practical guide to analyzing marketing data using Python,"","2024","","","","","Conduct data-driven marketing research and analysis with hands-on examples using Python by leveraging open-source tools and libraries Key FeaturesAnalyze marketing data using proper statistical techniquesUse data modeling and analytics to understand customer preferences and enhance strategies without complex mathImplement Python libraries like DoWhy, Pandas, and Prophet in a business setting with examples and use casesPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionMost marketing professionals are familiar with various sources of customer data that promise insights for success. There are extensive sources of data, from customer surveys to digital marketing data. Moreover, there is an increasing variety of tools and techniques to shape data, from small to big data. However, having the right knowledge and understanding the context of how to use data and tools is crucial. In this book, you’ll learn how to give context to your data and turn it into useful information. You’ll understand how and where to use a tool or dataset for a specific question, exploring the ""what and why questions"" to provide real value to your stakeholders. Using Python, this book will delve into the basics of analytics and causal inference. Then, you’ll focus on visualization and presentation, followed by understanding guidelines on how to present and condense large amounts of information into KPIs. After learning how to plan ahead and forecast, you’ll delve into customer analytics and insights. Finally, you’ll measure the effectiveness of your marketing efforts and derive insights for data-driven decision-making. By the end of this book, you’ll understand the tools you need to use on specific datasets to provide context and shape your data, as well as to gain information to boost your marketing efforts.What you will learnUnderstand the basic ideas behind the main statistical models used in marketing analyticsApply the right models and tools to a specific analytical questionDiscover how to conduct causal inference, experimentation, and statistical modeling with PythonImplement common open source Python libraries for specific use cases with immediately applicable codeAnalyze customer lifetime data and generate customer insightsGo through the different stages of analytics, from descriptive to prescriptiveWho this book is forThis book is for data analysts and data scientists working in a marketing team supporting analytics and marketing research, who want to provide better insights that lead to data-driven decision-making. Prior knowledge of Python, data analysis, and statistics is required to get the most out of this book. ","","9781801813839","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10540153.pdf&bkn=10540152&pdfType=book","","","","","","","","28 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"Production-Ready Applied Deep Learning: Learn how to construct and deploy complex models in PyTorch and TensorFlow deep learning frameworks","T. Palczewski; J. (. Lee; L. Mookiah",NA; NA; NA,Production-Ready Applied Deep Learning: Learn how to construct and deploy complex models in PyTorch and TensorFlow deep learning frameworks,"","2022","","","","","Supercharge your skills for developing powerful deep learning models and distributing them at scale efficiently using cloud servicesKey FeaturesUnderstand how to execute a deep learning project effectively using various tools availableLearn how to develop PyTorch and TensorFlow models at scale using Amazon Web ServicesExplore effective solutions to various difficulties that arise from model deploymentBook DescriptionMachine learning engineers, deep learning specialists, and data engineers encounter various problems when moving deep learning models to a production environment. The main objective of this book is to close the gap between theory and applications by providing a thorough explanation of how to transform various models for deployment and efficiently distribute them with a full understanding of the alternatives. First, you will learn how to construct complex deep learning models in PyTorch and TensorFlow. Next, you will acquire the knowledge you need to transform your models from one framework to the other and learn how to tailor them for specific requirements that deployment environments introduce. The book also provides concrete implementations and associated methodologies that will help you apply the knowledge you gain right away. You will get hands-on experience with commonly used deep learning frameworks and popular cloud services designed for data analytics at scale. Additionally, you will get to grips with the authors’ collective knowledge of deploying hundreds of AI-based services at a large scale. By the end of this book, you will have understood how to convert a model developed for proof of concept into a production-ready application optimized for a particular production setting.What you will learnUnderstand how to develop a deep learning model using PyTorch and TensorFlowConvert a proof-of-concept model into a production-ready applicationDiscover how to set up a deep learning pipeline in an efficient way using AWSExplore different ways to compress a model for various deployment requirementsDevelop Android and iOS applications that run deep learning on mobile devicesMonitor a system with a deep learning model in productionChoose the right system architecture for developing and deploying a modelWho this book is forMachine learning engineers, deep learning specialists, and data scientists will find this book helpful in closing the gap between the theory and application with detailed examples. Beginner-level knowledge in machine learning or software engineering will help you grasp the concepts covered in this book easily.","","9781803238050","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163505.pdf&bkn=10163504&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"The Ultimate Zoom Cookbook: Over 100 recipes to enhance and engage communication with Zoom","P. Kelley",NA,The Ultimate Zoom Cookbook: Over 100 recipes to enhance and engage communication with Zoom,"","2024","","","","","Become an expert in meetings, screen share, audio, video, and AI in Zoom by learning advanced techniques and gaining insights from Zoom expert, Patrick Kelley Key FeaturesExplore features such as webinars, attendee engagement techniques, analytics, and reportingLeverage AI to increase effectiveness as a meeting organizer, presenter, and attendeeDiscover advanced content and collaboration tips and tricks to create presentations that are engaging to the audiencePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe last few years have completely changed the way we collaborate and communicate. Whether it’s internally with coworkers at a large enterprise or externally with prospective clients and customers, Zoom has created a new way to interact with people in real-time as well as asynchronously. This cookbook delves into all aspects of Zoom, moving beyond just meetings. You’ll get started by exploring key areas such as audio, video, chat, webinars, advanced reporting, and analytics. Next, you’ll progress from discovering simple Zoom video and audio calls to understanding features such as whiteboards, hardware integration, and phone capabilities. Gradually, you'll discover advanced techniques for effective content presentation, while configuring users and features from the admin portal. The last set of chapters cover advanced features on security and privacy as well as how to make the most of AI Companion to reach out to your audience with better content, clarity, and expertise. By the end of this Zoom book, you’ll be well-versed with the entire Zoom app and all the modalities available.What you will learnHarness Zoom's features and functions beyond just video meetingsUnderstand how to use Zoom for multiple communication modalitiesDiscover advanced techniques for presenting content effectivelyFind out how to configure users and features from the admin portalGet hands on with Zoom phone, chat, email, and calendarConfigure Zoom hardware and software effectivelySecure Zoom with security and privacy techniquesUse AI Companion to work more efficiently and productivelyWho this book is forThis book is for any user looking to get the most out of Zoom’s collaboration or communication tools.","","9781835081396","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10803985.pdf&bkn=10803984&pdfType=book","","","","","","","","16 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"Learning and Experiencing Cryptography with CrypTool and SageMath","B. Esslinger",NA,Learning and Experiencing Cryptography with CrypTool and SageMath,"","2023","","","","","This book provides a broad overview of cryptography and enables cryptography for trying out. It emphasizes the connections between theory and practice, focuses on RSA for introducing number theory and PKI, and links the theory to the most current recommendations from NIST and BSI. The book also enables readers to directly try out the results with existing tools available as open source. It is different from all existing books because it shows very concretely how to execute many procedures with different tools. The target group could be self-learners, pupils and students, but also developers and users in companies. All code written with these open-source tools is available. The appendix describes in detail how to use these tools. The main chapters are independent from one another. At the end of most chapters, you will find references and web links. The sections have been enriched with many footnotes. Within the footnotes you can see where the described functions can be called and tried within the different CrypTool versions, within SageMath or within OpenSSL.","","9781685690182","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10361269.pdf&bkn=10361268&pdfType=book","","","","","","","","15 Dec 2023","","","Artech","Artech Books"
"Conversational AI with Rasa: Build, test, and deploy AI-powered, enterprise-grade virtual assistants and chatbots","X. Kong; G. Wang; A. Nichol",NA; NA; NA,"Conversational AI with Rasa: Build, test, and deploy AI-powered, enterprise-grade virtual assistants and chatbots","","2021","","","","","Create next-level AI assistants and transform how customers communicate with businesses with the power of natural language understanding and dialogue management using RasaKey FeaturesUnderstand the architecture and put the underlying principles of the Rasa framework to practiceLearn how to quickly build different types of chatbots such as task-oriented, FAQ-like, and knowledge graph-based chatbotsExplore best practices for working with Rasa and its debugging and optimizing aspectsBook DescriptionThe Rasa framework enables developers to create industrial-strength chatbots using state-of-the-art natural language processing (NLP) and machine learning technologies quickly, all in open source. Conversational AI with Rasa starts by showing you how the two main components at the heart of Rasa work – Rasa NLU (natural language understanding) and Rasa Core. You'll then learn how to build, configure, train, and serve different types of chatbots from scratch by using the Rasa ecosystem. As you advance, you'll use form-based dialogue management, work with the response selector for chitchat and FAQ-like dialogs, make use of knowledge base actions to answer questions for dynamic queries, and much more. Furthermore, you'll understand how to customize the Rasa framework, use conversation-driven development patterns and tools to develop chatbots, explore what your bot can do, and easily fix any mistakes it makes by using interactive learning. Finally, you'll get to grips with deploying the Rasa system to a production environment with high performance and high scalability and cover best practices for building an efficient and robust chat system. By the end of this book, you'll be able to build and deploy your own chatbots using Rasa, addressing the common pain points encountered in the chatbot life cycle.What you will learnUse the response selector to handle chitchat and FAQsCreate custom actions using the Rasa SDKTrain Rasa to handle complex named entity recognitionBecome skilled at building custom components in the Rasa frameworkValidate and test dialogs end to end in RasaDevelop and refine a chatbot system by using conversation-driven deployment processingUse TensorBoard for tuning to find the best configuration optionsDebug and optimize dialogue systems based on RasaWho this book is forThis book is for NLP professionals as well as machine learning and deep learning practitioners who have knowledge of natural language processing and want to build chatbots with Rasa. Anyone with beginner-level knowledge of NLP and deep learning will be able to get the most out of the book.","","9781801073882","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163114.pdf&bkn=10163113&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Explainable AI Methods for Underwater Mine Warfare","G. J. Richard; J. Habonneau; D. Guériot; J. -M. L. Caillec","Thales DMS, Brest Cedex 3, France; Thales DMS, Brest Cedex 3, France; Institut Mines TELECOM, Brest Cedex 3, France; Institut Mines TELECOM, Brest Cedex 3, France",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"","2024","PP","99","1","19","Artificial Intelligence (AI) has brought new algorithms providing high performance compared to the usual methods. However, the internal behavior of the decision-making process carried out by Neural Networks requires to be finely understood. This questioning has led to the development of the eXplainable Artificial Intelligence (XAI). This is especially true in areas where following AI decisions may have serious consequences, such as underwater mine hunting to increase the AI acceptance. We study the application of XAI methods (backpropagation and perturbation) to the classification (mine vs non-mine) and identification (type of mines) of an object detected by a sonar on the seabed. Although the aim of XAI is to locate relevant features in an image, the classification or identification decisions do not involve the same cognitive process. The main aims of our paper were to verify that the XAI methods, designed for optical images, can be applied to grayscale sonar images (in particular, we explain why backpropagation methods are not suitable for grayscale images, unlike perturbation methods) and whether they are neural network-dependent (two kinds of network have been tested). The features highlighted by XAI methods for the different classes of mines are compared with each other, but also with those involved in the operator decision-making. Three examples of feature extraction are finally discussed in the case of misclassification. Furthermore, the perturbation approach provides the same highlighted areas for both networks, and these areas on which the Neural Networks base their classification can be linked to the explanations given by operators.","2151-1535","","10.1109/JSTARS.2024.3447093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643250","eXplainable AI;mine warfare;sonar images;heat maps;SHAP","Sonar;Fuel processing industries;Artificial neural networks;Shape;Feature extraction;Explainable AI;Task analysis","","","","","CCBYNCND","21 Aug 2024","","","IEEE","IEEE Early Access Articles"
"Confluence of Photonics and Artificial Intelligence","Y. -K. Chen","Coherent Corporation, Santa Clara, CA, USA",IEEE Journal of Selected Topics in Quantum Electronics,"","2025","PP","99","1","15","Over the past decades, significant advances have been made in the fields of photonic technologies, artificial intelligence, and machine learning techniques. Recent AI progress in language models, perception and self-learning capability has had a significant impact on a wide range of communities. The new photonics enhances AI with faster data processing, reduced energy consumption, and improved interconnects, enabling more efficient and powerful AI systems. In this paper, we will highlight the synchroneity of these advances from performance enhancement of AI processing by photonics and advanced photonics with AI processing, leading to new photonic neural networks for future photonic AI processors.","1558-4542","","10.1109/JSTQE.2025.3552430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10930836","optical interconnects;optical switching;photonic inverse design;photonic processing;photonic neural network;photonic computing;artificial intelligence;machine learning","Optical switches;Artificial intelligence;High-speed optical techniques;Optical fiber communication;Optical distortion;Optical interconnections;Photonics;Optical amplifiers;Optical polarization;Training","","","","","IEEE","18 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Deep Discriminative Multi-view Clustering","Z. Chen; X. -J. Wu; T. Xu; H. Li; J. Kittler","School of Computer Science and Technology, Anhui University of Technology, Ma’anshan, China; School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Multi-view clustering based on deep auto-encoder networks has garnered increasing attention and made significant progress in recent years. However, we argue that most existing methods inadequately explore the discriminability while learning clustering assignments, resulting in models struggling to accurately cluster data, particularly those with ambiguous semantics. To address this problem, we propose a novel framework termed deep discriminative multi-view clustering (DDMvC). This framework is designed to further increase the inter-cluster distances by learning a discriminative projection dictionary with global prior information. To begin with, we enhance the reliability of the dictionary atoms by initializing them with class-specific prototypes derived from concatenated global features across multiple views. Subsequently, we iteratively refine the atoms to guarantee their independence from any specific cluster. Simultaneously, we incorporate contrastive learning for the cluster assignments projected by these atoms, striving for inter-view consistent clustering results. Experimental results on benchmark multi-view datasets demonstrate that our framework achieves the state-of-the-art clustering performance.","1558-2205","","10.1109/TCSVT.2025.3541928","Natural Science Foundation of Anhui Provincial Department of Education(grant numbers:2022AH050303); Engineering and Physical Sciences Research Council grant(grant numbers:EP/V002856/1); National Natural Science Foundation of China(grant numbers:62020106012,62106089,62306006,62332008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10884978","Discriminability;dictionary learning;autoencoder network;contrastive learning;multi-view clustering","Dictionaries;Atoms;Contrastive learning;Training;Optimization;Feature extraction;Vectors;Accuracy;Reliability;Prototypes","","","","","IEEE","13 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Causal Fairness Analysis: A Causal Toolkit for Fair Machine Learning","D. Plečko; E. Bareinboim",NA; NA,Causal Fairness Analysis: A Causal Toolkit for Fair Machine Learning,"","2024","","","","","The recent surge of interest in AI systems has raised concerns in moral quarters about their ethical use and whether they can demonstrate fair decision taking processes. Issues of unfairness and discrimination are pervasive when decisions are being made by humans, and are potentially amplified when decisions are made using machines with little transparency, accountability, and fairness. In this monograph, the authors introduce a framework for causal fairness analysis to understand, model, and possibly solve issues of fairness in AI decision-making settings. The authors link the quantification of the disparities present in the observed data with the underlying, often unobserved, collection of causal mechanisms that generate the disparity in the first place, a challenge they call the Fundamental Problem of Causal Fairness Analysis (FPCFA). In order to solve the FPCFA, they study the mapping variations and empirical measures of fairness to structural mechanisms and different units of the population, culminating in the Fairness Map. This monograph presents the first systematic attempt to organize and explain the relationship between various criteria in fairness and studies which causal assumptions are needed for performing causal fairness analysis. The resulting Fairness Cookbook allows anyone to assess the existence of disparate impact and disparate treatment. It is a timely and important introduction to developing future AI systems incorporating inherent fairness and as such will be of wide interest not only to AI system designers, but all who are interested in the wider impact AI will have on society.","","9781638283317","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10418874.pdf&bkn=10418873&pdfType=book","","","","","","","","2 Feb 2024","","","now","Now Foundations and Trends Books"
"Machine Learning on Kubernetes: A practical handbook for building and using a complete open source machine learning platform on Kubernetes","F. Masood; R. Brigoli",NA; NA,Machine Learning on Kubernetes: A practical handbook for building and using a complete open source machine learning platform on Kubernetes,"","2022","","","","","Build a Kubernetes-based self-serving, agile data science and machine learning ecosystem for your organization using reliable and secure open source technologiesKey FeaturesBuild a complete machine learning platform on KubernetesImprove the agility and velocity of your team by adopting the self-service capabilities of the platformReduce time-to-market by automating data pipelines and model training and deploymentBook DescriptionMLOps is an emerging field that aims to bring repeatability, automation, and standardization of the software engineering domain to data science and machine learning engineering. By implementing MLOps with Kubernetes, data scientists, IT professionals, and data engineers can collaborate and build machine learning solutions that deliver business value for their organization. You'll begin by understanding the different components of a machine learning project. Then, you'll design and build a practical end-to-end machine learning project using open source software. As you progress, you'll understand the basics of MLOps and the value it can bring to machine learning projects. You will also gain experience in building, configuring, and using an open source, containerized machine learning platform. In later chapters, you will prepare data, build and deploy machine learning models, and automate workflow tasks using the same platform. Finally, the exercises in this book will help you get hands-on experience in Kubernetes and open source tools, such as JupyterHub, MLflow, and Airflow. By the end of this book, you'll have learned how to effectively build, train, and deploy a machine learning model using the machine learning platform you built.What you will learnUnderstand the different stages of a machine learning projectUse open source software to build a machine learning platform on KubernetesImplement a complete ML project using the machine learning platform presented in this bookImprove on your organization's collaborative journey toward machine learningDiscover how to use the platform as a data engineer, ML engineer, or data scientistFind out how to apply machine learning to solve real business problemsWho this book is forThis book is for data scientists, data engineers, IT platform owners, AI product owners, and data architects who want to build their own platform for ML development. Although this book starts with the basics, a solid understanding of Python and Kubernetes, along with knowledge of the basic concepts of data science and data engineering will help you grasp the topics covered in this book in a better way.","","9781803231655","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162847.pdf&bkn=10162846&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Semantic Image Segmentation: Two Decades of Research","G. Csurka; R. Volpi; B. Chidlovskii",NA; NA; NA,Semantic Image Segmentation: Two Decades of Research,"","2022","","","","","Semantic image segmentation (SiS) plays a fundamental role towards a general understanding of the image content and context, in a broad variety of computer vision applications, thus providing key information for the global understanding of an image. This monograph summarizes two decades of research in the field of SiS, where a literature review of solutions starting from early historical methods is proposed, followed by an overview of more recent deep learning methods, including the latest trend of using transformers. The publication is complemented by presenting particular cases of the weak supervision and side machine learning techniques that can be used to improve the semantic segmentation, such as curriculum, incremental or self-supervised learning. State-of-the-art SiS models rely on a large amount of annotated samples, which are more expensive to obtain than labels for tasks such as image classification. Since unlabeled data is significantly cheaper to obtain, it is not surprising that Unsupervised Domain Adaptation (UDA) reached a broad success within the semantic segmentation community. Therefore, a second core contribution of this monograph is to summarize five years of a rapidly growing field, Domain Adaptation for Semantic Image Segmentation (DASiS), which embraces the importance of semantic segmentation itself and a critical need of adapting segmentation models to new environments. In addition to providing a comprehensive survey on DASiS techniques, newer trends such as multi-domain learning, domain generalization, domain incremental learning, test-time adaptation and source-free domain adaptation are also presented. The publication concludes by describing datasets and benchmarks most widely used in SiS and DASiS and briefly discusses related tasks such as instance and panoptic image segmentation, as well as applications such as medical image segmentation. This monograph should provide researchers across academia and industry with a comprehensive reference guide, and will help them in fostering new research directions in the field.","","9781638280774","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9926015.pdf&bkn=9926014&pdfType=book","","","","","","","","21 Oct 2022","","","now","Now Foundations and Trends Books"
"Graph Machine Learning: Take graph data to the next level by applying machine learning techniques and algorithms","C. Stamile; A. Marzullo; E. Deusebio",NA; NA; NA,Graph Machine Learning: Take graph data to the next level by applying machine learning techniques and algorithms,"","2021","","","","","Build machine learning algorithms using graph data and efficiently exploit topological information within your modelsKey FeaturesImplement machine learning techniques and algorithms in graph dataIdentify the relationship between nodes in order to make better business decisionsApply graph-based machine learning methods to solve real-life problemsBook DescriptionGraph Machine Learning will introduce you to a set of tools used for processing network data and leveraging the power of the relation between entities that can be used for predictive, modeling, and analytics tasks. The first chapters will introduce you to graph theory and graph machine learning, as well as the scope of their potential use. You’ll then learn all you need to know about the main machine learning models for graph representation learning: their purpose, how they work, and how they can be implemented in a wide range of supervised and unsupervised learning applications. You'll build a complete machine learning pipeline, including data processing, model training, and prediction in order to exploit the full potential of graph data. After covering the basics, you’ll be taken through real-world scenarios such as extracting data from social networks, text analytics, and natural language processing (NLP) using graphs and financial transaction systems on graphs. You’ll also learn how to build and scale out data-driven applications for graph analytics to store, query, and process network information, and explore the latest trends on graphs. By the end of this machine learning book, you will have learned essential concepts of graph theory and all the algorithms and techniques used to build successful machine learning applications.What you will learnWrite Python scripts to extract features from graphsDistinguish between the main graph representation learning techniquesLearn how to extract data from social networks, financial transaction systems, for text analysis, and moreImplement the main unsupervised and supervised graph embedding techniquesGet to grips with shallow embedding methods, graph neural networks, graph regularization methods, and moreDeploy and scale out your application seamlesslyWho this book is forThis book is for data scientists, data analysts, graph analysts, and graph professionals who want to leverage the information embedded in the connections and relations between data points to boost their analysis and model performance using machine learning. It will also be useful for machine learning developers or anyone who wants to build ML-driven graph databases. A beginner-level understanding of graph databases and graph data is required, alongside a solid understanding of ML basics. You’ll also need intermediate-level Python programming knowledge to get started with this book.","","9781800206755","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162851.pdf&bkn=10162850&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Bilevel Methods for Image Reconstruction","C. Crockett; J. A. Fessler",NA; NA,Bilevel Methods for Image Reconstruction,"","2022","","","","","Methods for image recovery and reconstruction aim to estimate a good-quality image from noisy, incomplete, or indirect measurements. Such methods are also known as computational imaging. New methods for image reconstruction attempt to lower complexity, decrease data requirements, or improve image quality for a given input data quality. Image reconstruction typically involves optimizing a cost function to recover a vector of unknown variables that agrees with collected measurements and prior assumptions. State-of-the-art image reconstruction methods learn these prior assumptions from training data using various machine learning techniques, such as bilevel methods. This review discusses methods for learning parameters for image reconstruction problems using bilevel formulations, and it lies at the intersection of a specific machine learning method, bilevel, and a specific application, filter learning for image reconstruction. The review discusses multiple perspectives to motivate the use of bilevel methods and to make them more easily accessible to different audiences. Various ways to optimize the bilevel problem are covered, providing pros and cons of the variety of proposed approaches. Finally, an overview of bilevel applications in image reconstruction is provided.","","9781638280033","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9770484.pdf&bkn=9770483&pdfType=book","","","","","","","","10 May 2022","","","now","Now Foundations and Trends Books"
"A Robust Coverless Audio Steganography Based on Differential Privacy Clustering","Y. Feng; L. Xu; X. Lu; G. Zhang; W. Rao","College of Information Science and Technology, Donghua University, Shanghai, China; College of Information Science and Technology, Donghua University, Shanghai, China; College of Information Science and Technology, Donghua University, Shanghai, China; College of Information Science and Technology, Donghua University, Shanghai, China; Tencent Ethereal Audio Lab, Shenzhen, China",IEEE Transactions on Multimedia,"","2025","PP","99","1","16","Conventional audio steganography methods typically require embedding secret information into the carrier, making them vulnerable to steganalysis. To address this issue, we propose a novel coverless audio steganography method that hides information by generating carriers and establishing mapping rules rather than embedding data directly. Our approach leverages a differential privacy clustering algorithm to cluster audio data and select representative audio files, thereby enhancing the security of the steganography. Additionally, we introduce an improved audio feature extraction method that combines traditional Mel-frequency cepstral coefficients (MFCC) with global statistical information, significantly boosting the robustness of the secret information against common audio attacks, particularly time-stretching attacks. Experimental results show that our method achieves a robustness rate of up to 95% against time-stretching and maintains an average security accuracy rate exceeding 97% across various attack scenarios. The proposed method ensures that the audio carrier remains unaltered, thus effectively resisting detection by steganalysis tools. This innovative approach provides a practical and efficient solution for the secure transmission of information in the digital era.","1941-0077","","10.1109/TMM.2025.3543107","National Natural Science Foundation of China(grant numbers:62001100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10891605","Coverless audio steganography;differential privacy clustering;robustness;information security","Steganography;Security;Robustness;Differential privacy;Feature extraction;Privacy;Clustering algorithms;Noise;Sensitivity;Payloads","","","","","IEEE","17 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Neurosymbolic Programming in Scallop: Principles and Practice","Z. Li; J. Huang; J. Liu; M. Naik",NA; NA; NA; NA,Neurosymbolic Programming in Scallop: Principles and Practice,"","2024","","","","","Neurosymbolic programming combines the otherwise complementary worlds of deep learning and symbolic reasoning. It thereby enables more accurate, interpretable, and domain-aware solutions to Artificial Intelligence (AI) tasks. This monograph introduces Scallop, a general-purpose language and compiler toolchain for developing neurosymbolic applications. A Scallop program specifies a suitable decomposition of an AI task’s computation into separate learning and reasoning modules. Learning modules are built using existing machine learning frameworks and range from custom neural models to foundation models for language, vision, and multi-modal data. Reasoning modules are specified in a declarative logic programming language based on Datalog which supports expressive features such as recursion, aggregation, negation, and probabilistic programming over structured relations. Scallop’s compiler enables to automatically train neurosymbolic programs in a data- and compute-efficient manner using an end-to-end differentiable reasoning framework. Scallop also supports features useful for building real-world applications such as user-defined data types, soft logic operations, and foreign interfaces. This monograph demonstrates programming in Scallop for applications that span the domains of image and video processing, natural language processing, planning, and information retrieval in a variety of learning settings such as supervised learning, reinforcement learning, rule learning, contrastive learning, and in-context learning.","","9781638284857","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10848380.pdf&bkn=10848379&pdfType=book","","","","","","","","22 Jan 2025","","","now","Now Foundations and Trends Books"
"Using Perception Cues for Context-Aware Navigation in Dynamic Outdoor Environments","M. Wigness; J. G. Rogers; C. -E. Tsai; C. Mertz; L. Navarro-Serment; J. Oh","DEVCOM Army Research Laboratory, Adelphi, Maryland, USA; DEVCOM Army Research Laboratory, Adelphi, Maryland, USA; The Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA; The Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA; The Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA; The Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA",Field Robotics,"25 Feb 2025","2021","1","","1","33","Continued advancements in robot autonomy have allowed the research community to shift from using robots as tools in the field to deploying robot teammates capable of learning, reasoning, and executing tasks. Autonomous navigation is one necessary capability of a robot teammate that must operate in large field environments. In relatively static environments a simple navigation solution such as obstacle avoidance along the shortest path may suffice; however, as robot teammates are deployed to highly dynamic environments with changing mission requirements, additional environment context may be necessary to ensure safe and reliable navigation. Although recent works in urban autonomous driving have advanced the state-of-the-art in context-aware decision making, the spectrum of behaviors deployed for context-switching is more narrowly focused (by defining constraints specific to operation in structured environments) than what might be required for human-agent teaming field missions. As such, establishing a context-aware intelligent system for dynamic, unstructured environments is still an open problem. We discuss our approach to the integration of several context-aware navigation behaviors on a small unmanned ground vehicle (UGV) and a perception stack that provides cues used to transition between these different learned behaviors. Specifically, we integrate socially compliant, terrain-aware, and covert behaviors in an outdoor navigation scenario where the UGV encounters moving pedestrians, different terrains, and weapon threats. We provide a detailed account of the overall system integration, experiment design, component- and system-level analysis, and lessons learned.","2771-3989","","10.55417/fr.2021001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10878405","context-aware navigation;behavior learning;tactical object detection","Navigation;Robots;Vehicle dynamics;Pedestrians;System integration;Autonomous robots;Semantics;Visual perception;History;Complexity theory","","","","","CCBY","25 Feb 2025","","","FRPS","FRPS Journals"
"9 Meet the Bot Makers","T. Veale; M. Cook","University College Dublin, Belfield; University of Falmouth",Twitterbots: Making Machines that Make Meaning,"","2018","","","275","294","The Victoria and Albert Museum is tucked away in London's South Kensington district, nestled between the glittery lights of Harrods and Knightsbridge on one side and the solemn edifice of the Royal Albert Hall on the other. The area between the museum and the Royal Albert Hall makes up the Albertopolis, an area brought to life in the mid-nineteenth century by a huge wave of investment from Queen Victoria and Prince Albert, following a hugely successful public exhibition (called, imaginatively, the Great Exhibition) that was held in nearby Hyde Park. The profits from the Great Exhibition were set aside for huge investment into this area of London that is now bustling with landmarks, such as the Science Museum, the Natural History Museum, the Royal Albert Hall and Victoria and Albert Museum, the Royal Colleges of Art and Music, Imperial College, and more. All the museums in Albertopolis are free to enter, and the Royal Albert Hall runs countless free or inexpensive events, while Imperial College runs an annual science festival to showcase their research for the public. This is a part of London where art and technology collide with the public in the nicest of ways, so where better to meet and talk about Twitterbots? In April 2016 the Bot Summit, an annual day of talks and thinking about bots, came to Europe for the first time, hosted by the Victoria and Albert Museum.1 The event attracted bot makers from all over the United Kingdom and many from overseas, thanks to a fundraising effort by the bot community to bring speakers from abroad. Any who couldn't make it in person were able to tune in to a live stream, which you can still find and watch online. Its organizer, bot builder Darius Kazemi, was there in person to coordinate the day, at the center of a melting pot for culture and science.","","9780262346436","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8555240.pdf&bkn=8555192&pdfType=chapter","","Art;Twitter;Bot (Internet);Software;Dictionaries;Investment;Tools","","","","","","13 Dec 2018","","","MIT Press","MIT Press eBook Chapters"
"ISO/IEC/IEEE International Standard--Information technology--DevOps--Building reliable and secure systems including application build, package and deployment","",,ISO/IEC/IEEE Std 32675:2022,"8 Sep 2022","2022","","","1","94","Technical principles and processes to build, package, and deploy systems and applications in a reliable and secure way are specified. Establishing effective compliance and information technology (IT) controls is the focus. DevOps principles presented include mission first, customer focus, left-shift, continuous everything, and systems thinking. How stakeholders, including developers and operations staff, can collaborate and communicate effectively is described. The process outcomes and activities herein are aligned with the process model specified in ISO/IEC/IEEE 12207:2017 and ISO/IEC/IEEE 15288:2015.","","978-1-5044-8954-6","10.1109/IEEESTD.2022.9882056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9882056","agile;continuous delivery;continuous deployment;continuous integration;DevOps;IEEE 2675;left-shift","IEEE Standards;ISO Standards;IEC Standards;Information technology;Reliability;Continuous time systems","","","","","","8 Sep 2022","","","IEEE","IEEE Standards"
"Search and Discovery in Personal Email Collections","M. Bendersky; X. Wang; M. Najork; D. Metzler",NA; NA; NA; NA,Search and Discovery in Personal Email Collections,"","2021","","","","","Email has been an essential communication medium for many years and the information accumulated in our mailboxes has become valuable for all of our personal and professional activities. As our mailboxes grow, so does the need for the development of new effective approaches to information finding in this repository. For years, researchers have been developing interfaces, models and algorithms to facilitate search, discovery and organization of email data. In this survey, the authors bring together these diverse research directions by providing both a historical background as well as a comprehensive overview of the recent advances in the field. In particular, they lay out all the components needed in the design of a privacy-centric email search engine. They also go beyond search, presenting recent work on intelligent task assistance in email. Finally, they discuss some emerging trends and future directions in email search and discovery research.","","9781680838398","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9478338.pdf&bkn=9478337&pdfType=book","","","","","","","","9 Jul 2021","","","now","Now Foundations and Trends Books"
"Emotion-Conditioned MusicLM: Enhancing Emotional Resonance in Music Generation","Y. Sun; M. Kuo; X. Wang; W. Li; Q. Bai","School of Engineering, Computer and Mathematical Sciences, Auckland University of Technology, Auckland, New Zealand; School of Engineering, Computer and Mathematical Sciences, Auckland University of Technology, Auckland, New Zealand; Yanbian University, Jilin, China; School of Engineering, Computer and Mathematical Sciences, Auckland University of Technology, Auckland, New Zealand; University of Tasmania, Hobart, Australia",2024 IEEE Congress on Evolutionary Computation (CEC),"8 Aug 2024","2024","","","1","8","Nowadays, most music generation models are limited to accepting conditions from a single modality, whether text-based or neural signal-based. For most users, text represents the most accessible and intuitive input modality. However, this often dilutes the text's emotional characteristics, misaligning the emotional depth in the final music output. This article presents comprehensive research and improvement in the text-conditioned music generation model, named Emotion-Conditioned MusicLM. Building upon the existing text-conditioned music generation model, MusicLM, ECMusicLM is designed to generate music with a deeper emotional resonance while maintaining the high quality of musical output. Our research shows that combining text and emotional elements in music generation leads to the creation of emotionally resonant music. Through the experiments, ECMusicLM showed a notable capability in capturing implicit Valence-Arousal features from text prompts, significantly enhancing the emotional depth of the generated music. This study not only pushes the boundaries of AI in artistic creation but also opens avenues for future research in multi-modal emotional synthesis.","","979-8-3503-0836-5","10.1109/CEC60901.2024.10612075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10612075","","Buildings;Music;Evolutionary computation;Resonance;Multiple signal classification;Artificial intelligence","","","","27","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"An Introduction to Deep Reinforcement Learning","V. François-Lavet; P. Henderson; R. Islam; M. G. Bellemare; J. Pineau",NA; NA; NA; NA; NA,An Introduction to Deep Reinforcement Learning,"","2018","","","","","Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has recently been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This book provides the reader with a starting point for understanding the topic. Although written at a research level it provides a comprehensive and accessible introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. Written by recognized experts, this book is an important introduction to Deep Reinforcement Learning for practitioners, researchers and students alike.","","9781680835397","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8585412.pdf&bkn=8585411&pdfType=book","","","","","","","","24 Dec 2018","","","now","Now Foundations and Trends Books"
"Real-time distance field acceleration based free-viewpoint video synthesis for large sports fields","Y. Dai; J. Li; Y. Jiang; H. Qin; B. Liang; S. Hong; H. Pan; T. Yang","School of Telecommunications Engineering, Xidian University, Xi'an 710071, China; School of Telecommunications Engineering, Xidian University, Xi'an 710071, China; School of Telecommunications Engineering, Xidian University, Xi'an 710071, China; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, SAIIP, the School of Computer Science, Northwestern Polytechnical University, Xi'an 710129, China; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, SAIIP, the School of Computer Science, Northwestern Polytechnical University, Xi'an 710129, China; School of Telecommunications Engineering, Xidian University, Xi'an 710071, China; School of Telecommunications Engineering, Xidian University, Xi'an 710071, China; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, SAIIP, the School of Computer Science, Northwestern Polytechnical University, Xi'an 710129, China",Computational Visual Media,"20 Feb 2025","2024","10","2","331","353","Free-viewpoint video allows the user to view objects from any virtual perspective, creating an immersive visual experience. This technology enhances the interactivity and freedom of multimedia performances. However, many free-viewpoint video synthesis methods hardly satisfy the requirement to work in real time with high precision, particularly for sports fields having large areas and numerous moving objects. To address these issues, we propose a free-viewpoint video synthesis method based on distance field acceleration. The central idea is to fuse multi-view distance field information and use it to adjust the search step size adaptively. Adaptive step size search is used in two ways: for fast estimation of multi-object three-dimensional surfaces, and synthetic view rendering based on global occlusion judgement. We have implemented our ideas using parallel computing for interactive display, using CUDA and OpenGL frameworks, and have used real-world and simulated experimental datasets for evaluation. The results show that the proposed method can render free-viewpoint videos with multiple objects on large sports fields at 25 fps. Furthermore, the visual quality of our synthetic novel viewpoint images exceeds that of state-of-the-art neural-rendering-based methods.","2096-0662","","10.1007/s41095-022-0323-3","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897651","free-viewpoint video;view synthesis;camera array;distance field;sports video","Rendering (computer graphics);Three-dimensional displays;Cameras;Streaming media;Sports;Training;Real-time systems;Search problems;Image reconstruction;Solid modeling","","","","","","20 Feb 2025","","","TUP","TUP Journals"
"Journey to Become a Google Cloud Machine Learning Engineer: Build the mind and hand of a Google Certified ML professional","L. Song",NA,Journey to Become a Google Cloud Machine Learning Engineer: Build the mind and hand of a Google Certified ML professional,"","2022","","","","","Prepare for the GCP ML certification exam along with exploring cloud computing and machine learning concepts and gaining Google Cloud ML skillsKey FeaturesA comprehensive yet easy-to-follow Google Cloud machine learning study guideExplore full-spectrum and step-by-step practice examples to develop hands-on skillsRead through and learn from in-depth discussions of Google ML certification exam questionsBook DescriptionThis book aims to provide a study guide to learn and master machine learning in Google Cloud: to build a broad and strong knowledge base, train hands-on skills, and get certified as a Google Cloud Machine Learning Engineer. The book is for someone who has the basic Google Cloud Platform (GCP) knowledge and skills, and basic Python programming skills, and wants to learn machine learning in GCP to take their next step toward becoming a Google Cloud Certified Machine Learning professional. The book starts by laying the foundations of Google Cloud Platform and Python programming, followed the by building blocks of machine learning, then focusing on machine learning in Google Cloud, and finally ends the studying for the Google Cloud Machine Learning certification by integrating all the knowledge and skills together. The book is based on the graduate courses the author has been teaching at the University of Texas at Dallas. When going through the chapters, the reader is expected to study the concepts, complete the exercises, understand and practice the labs in the appendices, and study each exam question thoroughly. Then, at the end of the learning journey, you can expect to harvest the knowledge, skills, and a certificate.What you will learnProvision Google Cloud services related to data science and machine learningProgram with the Python programming language and data science librariesUnderstand machine learning concepts and model development processesExplore deep learning concepts and neural networksBuild, train, and deploy ML models with Google BigQuery ML, Keras, and Google Cloud Vertex AIDiscover the Google Cloud ML Application Programming Interface (API)Prepare to achieve Google Cloud Professional Machine Learning Engineer certificationWho this book is forAnyone from the cloud computing, data analytics, and machine learning domains, such as cloud engineers, data scientists, data engineers, ML practitioners, and engineers, will be able to acquire the knowledge and skills and achieve the Google Cloud professional ML Engineer certification with this study guide. Basic knowledge of Google Cloud Platform and Python programming is required to get the most out of this book.","","9781803239415","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162706.pdf&bkn=10162705&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Modern Time Series Forecasting with Python: Industry-ready machine learning and deep learning time series analysis with PyTorch and pandas","M. Joseph; J. Tackes; C. Bergmeir",NA; NA; NA,Modern Time Series Forecasting with Python: Industry-ready machine learning and deep learning time series analysis with PyTorch and pandas,"","2024","","","","","Learn traditional and cutting-edge machine learning (ML) and deep learning techniques and best practices for time series forecasting, including global forecasting models, conformal prediction, and transformer architecturesKey FeaturesApply ML and global models to improve forecasting accuracy through practical examplesEnhance your time series toolkit by using deep learning models, including RNNs, transformers, and N-BEATSLearn probabilistic forecasting with conformal prediction, Monte Carlo dropout, and quantile regressionsPurchase of the print or Kindle book includes a free eBook in PDF formatBook DescriptionPredicting the future, whether it's market trends, energy demand, or website traffic, has never been more crucial. This practical, hands-on guide empowers you to build and deploy powerful time series forecasting models. Whether you’re working with traditional statistical methods or cutting-edge deep learning architectures, this book provides structured learning and best practices for both. Starting with the basics, this data science book introduces fundamental time series concepts, such as ARIMA and exponential smoothing, before gradually progressing to advanced topics, such as machine learning for time series, deep neural networks, and transformers. As part of your fundamentals training, you’ll learn preprocessing, feature engineering, and model evaluation. As you progress, you’ll also explore global forecasting models, ensemble methods, and probabilistic forecasting techniques. This new edition goes deeper into transformer architectures and probabilistic forecasting, including new content on the latest time series models, conformal prediction, and hierarchical forecasting. Whether you seek advanced deep learning insights or specialized architecture implementations, this edition provides practical strategies and new content to elevate your forecasting skills.What you will learnBuild machine learning models for regression-based time series forecastingApply powerful feature engineering techniques to enhance prediction accuracyTackle common challenges like non-stationarity and seasonalityCombine multiple forecasts using ensembling and stacking for superior resultsExplore cutting-edge advancements in probabilistic forecasting and handle intermittent or sparse time seriesEvaluate and validate your forecasts using best practices and statistical metricsWho this book is forThis book is ideal for data scientists, financial analysts, quantitative analysts, machine learning engineers, and researchers who need to model time-dependent data across industries, such as finance, energy, meteorology, risk analysis, and retail. Whether you are a professional looking to apply cutting-edge models to real-world problems or a student aiming to build a strong foundation in time series analysis and forecasting, this book will provide the tools and techniques you need. Familiarity with Python and basic machine learning concepts is recommended.","","9781835883198","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769099.pdf&bkn=10769098&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Multiple Attention Mechanism for Camera-Radar Fusion Object Detection","W. Wang; Y. Wang; J. Zhang; W. Zhang","School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nangjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nangjing, China; Nanjing Les Information Technology Co. Ltd, Nangjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nangjing, China",IEEE Transactions on Vehicular Technology,"","2025","PP","99","1","14","The attention mechanism has gained significant prominence in visual-based object detection due to its outstanding efficiency and adaptability. This paper addresses the challenges of current Attention Mechanisms (AM) in traffic object detection tasks utilizing Camera-Radar Fusion (CRF) technology. A novel Multiple Attention Mechanism for Camera-Radar Fusion Object Detection (MAM-CRF) is proposed. A new network architecture is designed specifically for CRF networks to seamlessly integrate with existing vision-based AM modules. Additionally, an innovative Azimuth Attention Mechanism (AAM) module is developed, combining visual and radar data to enhance the network's focus on azimuthal features. A data scrambling technique is also introduced to reduce over-reliance on individual sensors in fusion-based methods. In the absence of a publicly available dataset tailored to this task, the Camera-Radar University of Washington (CRUW) dataset was selected and modified. Furthermore, a new dataset, the Camera-Radar Nanjing University of Science and Technology Version 1.0 (CRNJUST-v1.0), was created to validate the proposed methodology. Results demonstrate that MAM-CRF improves performance by over 2% with only a marginal complexity increase of 5%. The method also exhibits robust performance under severe data disturbances. Lightweight and portable, MAM-CRF is well-suited for a wide range of CRF applications.","1939-9359","","10.1109/TVT.2025.3531379","National Natural Science Foundation of China(grant numbers:71971116); Frontier Technologies R&D Program of Jiangsu(grant numbers:BF2024052); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10845197","Attention mechanism;camera and radar fusion;computer vision;object detection;pattern recognition;signal processing","Radar;Radar imaging;Object detection;Cameras;Radar detection;Azimuth;Attention mechanisms;Feature extraction;Radio frequency;Active appearance model","","","","","IEEE","17 Jan 2025","","","IEEE","IEEE Early Access Articles"
"Cross-graph Knowledge Exchange for Personalized Response Generation in Dialogue Systems","Y. Dong; K. Qin; P. Ke; S. Liang; G. Luo","National Key Laboratory of Intelligent Collaborative Computing, China; National Key Laboratory of Intelligent Collaborative Computing, China; National Key Laboratory of Intelligent Collaborative Computing, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Intelligent Collaborative Computing, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Intelligent Collaborative Computing, China",IEEE Internet of Things Journal,"","2025","PP","99","1","1","Recent advancements in language models have greatly improved dialogue systems, but they still face challenges in generating personalized responses that are consistent with the user’s persona and dialogue context. Existing approaches typically model dialogue context and persona information together in a unified manner, but they lack fine-grained differentiation between the two, leading to inaccurate user modeling. This misalignment hinders the ability of dialogue systems to produce personalized responses. In this work, we propose CKE (Cross-Graph Knowledge Exchange), a novel algorithm designed to enhance personalized response generation in dialogue systems. CKE constructs separate dialogue user graphs for each party in the dialogue, representing both their dialogue context and persona information. These graphs are then utilized to perform cross-graph structured knowledge aggregation, where the aggregation is under supervised by both its own and the other party’s persona and dialogue context, providing richer, more accurate representations. Furthermore, CKE introduces a hybrid prompt template that combines both discrete and continuous elements, improving the language model’s ability to leverage graph-structured information. The experimental results demonstrate that CKE significantly outperforms existing baseline methods in generating more coherent, contextually appropriate, and personalized responses in dialogue systems.","2327-4662","","10.1109/JIOT.2025.3541258","Noncommunicable Chronic Diseases-National Science and Technology Major Project(grant numbers:2023ZD0501806); Fundamental Research Funds for the Central Universities(grant numbers:ZYGX2023K010); National Natural Science Foundation of China(grant numbers:62176046,62406057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10884697","Personalized Dialogue Generation;Knowledge Graph;Dialogue User Graph;Graph Neural Network","Context modeling;Data models;Computational modeling;Training;Real-time systems;Industrial Internet of Things;Fans;Encoding;Electronic mail;Semantics","","","","","IEEE","13 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Mastering OpenStack: Implement the latest techniques for designing and deploying an operational, production-ready private cloud","O. Khedher",NA,"Mastering OpenStack: Implement the latest techniques for designing and deploying an operational, production-ready private cloud","","2024","","","","","Design and manage a powerful OpenStack cloud with practical insights from real-world examplesKey FeaturesSimplify the architecture complexity of the OpenStack ecosystem with new container and networking optionsApply best practices to operate and manage large OpenStack deployments with confidenceDesign and implement hybrid cloud setups using OpenStack and public cloudsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionOpenStack provides flexibility and control for designing and deploying robust and scalable cloud infrastructures, which has led to it having one of the largest open source communities in the cloud market. This book delves deep into the OpenStack architecture, dissecting each component to guide you in architecting your cloud with precision. From essential components to cutting-edge services, this book offers a step-by step approach, ensuring you grasp the fundamentals before exploring the latest advancements. This updated edition guides you through the deployment process, integrating secure best practices inspired by the DevSecOps philosophy. You’ll also explore the Antelope release, covering new services such as container management and software-defined networking (SDN). The book outlines best practices for running and managing fault-tolerant, secure, monitored, and high-performing setups. In the last part, it navigates the convergence of public and private clouds, covering hybrid models through use cases of managing Kubernetes-based applications in OpenStack private and public clouds. By the end of the book, you’ll be well versed in the latest OpenStack advancements, ready to lead your organization on a successful cloud journey.What you will learnExplore the latest design patterns in the OpenStack ecosystemImplement DevSecOps practices for agile and secure deployment managementEnsure resilience, fault tolerance, and performance in your cloud setupStay up to date with OpenStack networking and storage advancementsMaster operational best practices for managing a large-scale cloud setupDiscover logging and monitoring options for your cloudGet acquainted with new services such as SDN and containersUnderstand how to extend OpenStack's capabilities through a hybrid modelWho this book is forThis book is for OpenStack administrators, cloud and enterprise architects, and system and DevOps engineers looking to launch a private cloud with OpenStack. If you’re a cloud advisor, consultant, or evangelist, you’ll benefit from the expert insights, and if you’re a software developer or system operator aiming to accelerate your development cycle and agility, this book will help you bridge the gap between these roles. Basic knowledge of OpenStack, along with a prior understanding of systems, virtualization, and networking, is recommended.","","9781835466858","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769353.pdf&bkn=10769352&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Community Detection and Stochastic Block Models","E. Abbe",NA,Community Detection and Stochastic Block Models,"","2018","","","","","The field of community detection has been expanding greatly since the 1980s, with a remarkable diversity of models and algorithms developed in different communities like machine learning, computer science, network science, social science, and statistical physics. Various fundamental questions remain nonetheless unsettled, such as: Are there really communities? Algorithms may output community structures, but are these meaningful or artefacts? Can we always extract the communities when they are present; fully, partially? And what is a good benchmark to measure the performance of algorithms, and how good are the current algorithms? This monograph describes recent developments aiming at answering these questions in the context of block models. Addressing the issues from an information-theoretic view-point, the author gives a comprehensive description of the historical and recent work that has led to key new concepts in the various recovery requirements for community detection. The monograph provides a compact introduction to community detection, which enables the reader to apply these techniques in applications such as understanding sociological behavior, protein to protein interactions; gene expressions; recommendation systems; medical prognosis; DNA 3D folding; image segmentation, natural language processing, product-customer segmentation, webpage sorting, and many more.","","9781680834772","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8384201.pdf&bkn=8384200&pdfType=book","","","","","","","","14 Jun 2018","","","now","Now Foundations and Trends Books"
"ReFrESH – Relation-preserving Feedback-reliant Enhancement of Subjective Content Descriptions","M. Bender; T. Braun; R. Möller; M. Gehrke","Institute of Information Systems Ratzeburger Allee 160, University of Lübeck, Lübeck; Computer Science Department Einsteinstr. 62, University of Münster, Münster; Institute of Information Systems Ratzeburger Allee 160, University of Lübeck, Lübeck; Institute of Information Systems Ratzeburger Allee 160, University of Lübeck, Lübeck",2024 IEEE 18th International Conference on Semantic Computing (ICSC),"22 Mar 2024","2024","","","17","24","An agent providing an information retrieval service may work with a corpus of text documents. The documents in the corpus may contain annotations such as Subjective Content Descriptions (SCD)—additional data associated with different sentences of the documents. Each SCD is associated with multiple sentences of the corpus and has relations among each other. The agent uses the SCDs to create its answers in response to user supplied queries. However, a user of the agent may not be the creator of the SCDs for the corpus. Hence, answers may be considered faulty by an agent’s user, because the SCDs may not exactly match the perceptions of an agent’s user. A naive and very costly approach would be to ask each user to completely create all the SCD themselves. To circumvent this, this paper presents ReFrESH, an approach for Relation-preserving Feedback-reliant Enhancement of SCDs by Humans. An agent’s user can give feedback about faulty answers to the agent. This feedback is then used by ReFrESH to update the SCDs incrementally. Using ReFrESH, SCDs can be refreshed with feedback by humans and it allows users to build even better SCDs for their needs.","2472-9671","979-8-3503-8535-9","10.1109/ICSC59802.2024.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10475610","Subjective Content Descriptions (SCDs);Text Annotation;Information Retrieval Agent;Incorporate Human Feedback;Incremental Model Updates","Annotations;Fitting;Semantics;Information retrieval","","","","13","IEEE","22 Mar 2024","","","IEEE","IEEE Conferences"
"Current and Evolving Applications to Video and Imaging","D. Minoli; B. Occhiogrosso","Stevens Institute of Technology, AT&T, Red Bank, NJ; NA",AI Applications to Communications and Information Technologies: The Role of Ultra Deep Neural Networks,"","2024","","","173","256","This chapter explores machine learning techniques for imaging and computer vision (CV). It focuses on the use of artificial intelligence to interpret or enhance images in support of applications such as classification, detection, CV, face recognition, surveillance, situational awareness, and medical imaging. Neural networks (NNs), deep neural networks (DNNs), and ultra deep NNs play important roles in automatic processing of large bodies of data, especially in the video/CV arena. There are various types of NNs including feed‐forward networks and convolutional neural networks (CNNs). A CNN is a DNN with a convolutional structure. CNNs have become prevalent in the CV field in recent years: they are now often used for vision and image recognition applications. Convolution has applications that include signal processing, image processing, CV, among others. In imaging applications, convolutions are used for extracting shapes and curves in an image.","","9781394190027","10.1002/9781394190034.ch4","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10320248.pdf&bkn=10320140&pdfType=chapter","","Convolution;Artificial neural networks;Convolutional neural networks;Feature extraction;Image color analysis;Computational modeling;Training","","","","","","16 Nov 2023","","","IEEE","Wiley-IEEE Press eBook Chapters"
"A Holistic Review on Detection of Malicious Browser Extensions and Links using Deep Learning","R. A. K; T. Zonta; M. Sathiyanarayanan","Curtin University, Malaysia; University of Western Santa Catarina, Santa Catarina, Brazil; MIT Square, London, UK",2024 IEEE 3rd International Conference on AI in Cybersecurity (ICAIC),"16 Feb 2024","2024","","","1","6","The growth of the Internet has aroused people’s attention toward network security. A secure network environment is fundamental for the expeditious and impeccable development of the Internet. The majority of internet-based tasks can be completed with the help of a web browser. Although many web applications add browser extensions to improve their functionality, some of these extensions are malicious and can access sensitive data without the user’s knowledge. Browser extensions with malicious intent present a growing security concern and have quickly become one of the most prevalent methods used to compromise Internet security. This is largely due to their widespread usage and the extensive privileges they possess. After being installed, these malicious extensions are executed and make an attempt to compromise the victim’s browser. This makes them particularly elusive and challenging to combat. It is crucial to promptly develop an effective strategy to address the threats posed by these extensions. A comprehensive review of the research on browser extension vulnerabilities is presented in this paper. The role of malicious links in web browser extensions are examined for several attacks. Detection of malicious browser extension on various aspects are represented namely Intrusion malicious web browser extensions detection using Intrusion detection, Machine learning based detection methods and Deep learning based techniques to mitigate malicious web browser extensions are examined. This study investigates the critical function of malicious detection in protecting web browsers, looking at the changing threats and risk-reduction tactics. A robust cybersecurity frameworks can be created that not only respond to known threats but also anticipate and thwart the strategies of future cyber adversaries by realizing the significance of proactive detection. Thus this survey provides a detailed comparison of various solutions for malicious browser extension.","","979-8-3503-8185-6","10.1109/ICAIC60265.2024.10433842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10433842","Cyber-attacks;Machine learning;Malicious Browser extension;and Malicious Uniform Resource Locator (URL)","Surveys;Deep learning;Uniform resource locators;Intrusion detection;Network security;Browsers;Task analysis","","","","50","IEEE","16 Feb 2024","","","IEEE","IEEE Conferences"
"The TensorFlow Workshop: A hands-on guide to building deep learning models from scratch using real-world datasets","M. Moocarme; A. So; A. Maddalone",NA; NA; NA,The TensorFlow Workshop: A hands-on guide to building deep learning models from scratch using real-world datasets,"","2021","","","","","Get started with TensorFlow fundamentals to build and train deep learning models with real-world data, practical exercises, and challenging activitiesKey FeaturesUnderstand the fundamentals of tensors, neural networks, and deep learningDiscover how to implement and fine-tune deep learning models for real-world datasetsBuild your experience and confidence with hands-on exercises and activitiesBook DescriptionGetting to grips with tensors, deep learning, and neural networks can be intimidating and confusing for anyone, no matter their experience level. The breadth of information out there, often written at a very high level and aimed at advanced practitioners, can make getting started even more challenging. If this sounds familiar to you, The TensorFlow Workshop is here to help. Combining clear explanations, realistic examples, and plenty of hands-on practice, it’ll quickly get you up and running. You’ll start off with the basics – learning how to load data into TensorFlow, perform tensor operations, and utilize common optimizers and activation functions. As you progress, you’ll experiment with different TensorFlow development tools, including TensorBoard, TensorFlow Hub, and Google Colab, before moving on to solve regression and classification problems with sequential models. Building on this solid foundation, you’ll learn how to tune models and work with different types of neural network, getting hands-on with real-world deep learning applications such as text encoding, temperature forecasting, image augmentation, and audio processing. By the end of this deep learning book, you’ll have the skills, knowledge, and confidence to tackle your own ambitious deep learning projects with TensorFlow.What you will learnGet to grips with TensorFlow’s mathematical operationsPre-process a wide variety of tabular, sequential, and image dataUnderstand the purpose and usage of different deep learning layersPerform hyperparameter-tuning to prevent overfitting of training dataUse pre-trained models to speed up the development of learning modelsGenerate new data based on existing patterns using generative modelsWho this book is forThis TensorFlow book is for anyone who wants to develop their understanding of deep learning and get started building neural networks with TensorFlow. Basic knowledge of Python programming and its libraries, as well as a general understanding of the fundamentals of data science and machine learning, will help you grasp the topics covered in this book more easily.","","9781800200227","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163587.pdf&bkn=10163586&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Lessons from the Pandemic for Healthcare Operations","",,Lessons from the Pandemic for Healthcare Operations,"","2025","","","","","Lessons from the Pandemic for Healthcare Operations delves into the lessons learned from the COVID-19 pandemic that can be applied to the post-pandemic world to enhance efficiency, equity, and fairness in healthcare operations. It emphasizes the importance of preparedness in combating future pandemics or public health disasters, regardless of when or where they may occur. This work offers a unique perspective through which to view the evolving outlines of healthcare delivery, policy, and research. This is illustrated using several real-world experiences, empirical studies, and forward-looking insights. The contributions fall under three broad themes: the management of policies and funding in healthcare, the role of data and data-driven research, and accessible healthcare services during and after the pandemic. ","","9781638284659","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10925599.pdf&bkn=10925598&pdfType=book","","","","","","","","14 Mar 2025","","","now","Now Foundations and Trends Books"
"AWS Cloud Projects: Strengthen your AWS skills through practical projects, from websites to advanced AI applications","I. Pinto; P. Santos",NA; NA,"AWS Cloud Projects: Strengthen your AWS skills through practical projects, from websites to advanced AI applications","","2024","","","","","Gain a deeper understanding of AWS services by building eight real-world projectsKey FeaturesGain practical skills in architecting, deploying, and managing applications on AWS from seasoned expertsGet hands-on experience by building different architectures in an easy-to-follow mannerUnderstand the purpose of different aspects in AWS, and how to make the most of themPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionTired of resumes that get lost in the pile? This book is your roadmap to creating an in-demand AWS portfolio that grabs attention and gets you hired. This comprehensive guide unlocks the vast potential of AWS for developers of all levels. Inside, you'll find invaluable guidance for crafting stunning websites with S3, CloudFront, and Route53. You’ll build robust and scalable applications, such as recipe-sharing platforms, using DynamoDB and Elastic Load Balancing. For streamlined efficiency, the book will teach you how to develop serverless architectures with AWS Lambda and Cognito. Gradually, you’ll infuse your projects with artificial intelligence by creating a photo analyzer powered by Amazon Rekognition. You’ll also automate complex workflows for seamless content translation using Translate, CodePipeline, and CodeBuild. Later, you’ll construct intelligent virtual assistants with Amazon Lex and Bedrock to answer web development queries. The book will also show you how to visualize your data with insightful dashboards built using Athena, Glue, and QuickSight. By the end of this book, you’ll be ready to take your projects to the next level and succeed in the dynamic world of cloud computing.What you will learnDevelop a professional CV website and gain familiarity with the core aspects of AWSBuild a recipe-sharing application using AWS's serverless toolkitLeverage AWS AI services to create a photo friendliness analyzer for professional profilesImplement a CI/CD pipeline to automate content translation across languagesDevelop a web development Q&A chatbot powered by cutting-edge LLMsBuild a business intelligence application to analyze website clickstream data and understand user behavior with AWSWho this book is forIf you’re a student who wants to start your career in cloud computing or a professional with experience in other technical areas like software development who wants to embrace a new professional path or complement your technical skills in cloud computing, this book is for you. A background in computer science or engineering and basic programming skills is recommended. All the projects in the book have theoretical explanations of the services used and do not assume any previous AWS knowledge.","","9781835889299","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769208.pdf&bkn=10769207&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Mastering AWS Security: Strengthen your cloud environment using AWS security features coupled with proven strategies","L. Mathieu",NA,Mastering AWS Security: Strengthen your cloud environment using AWS security features coupled with proven strategies,"","2024","","","","","Explore the depths of AWS security and learn how to design, implement, and maintain a secure cloud environment using state-of-the-art AWS technology Key FeaturesDive into AWS security concepts and technologies that can be applied for diverse use casesDesign and deploy secure AWS environments based on modern architectural principlesElevate your AWS security expertise with advanced techniques for automation and continuous improvementPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionIf you’re trying to navigate the complex world of AWS security and fortify your organizational cloud environment, then this book is for you. Written by an accomplished cybersecurity and AWS cloud consultant, Mastering AWS Security will help you understand and master the complexities of AWS security. This book offers an in-depth and practical exploration of AWS security concepts, features, and services, focusing on how they apply to modern cloud-based application environments. As you progress, you’ll gain a thorough introduction to the art of security automation and DevSecOps. You’ll learn how to automate security tasks, integrate security into your development process, and maintain a high level of security as your applications evolve and scale. Emphasizing continuous monitoring and improvement, this book will teach you how to set up monitoring systems, interpret security data, and make informed decisions to enhance your security over time. Through real-world case studies, you’ll learn how to tackle the challenges and find solutions for securing AWS environments. By the end of this book, you’ll confidently secure your AWS environments, and stay up to date with the latest security trends and updates in the AWS ecosystem.What you will learnDiscover AWS IAM, access control models, and the principle of least privilegeGet to grips with VPC network security strategies and tools to protect and isolate your critical assetsLeverage and orchestrate AWS security services tailored to your environmentImplement encryption and data protection best practices in key AWS servicesExplore best practices to secure microservices and serverless architectures on AWSImplement security strategies for multi-tenant architecturesMaster the art of security automation and DevSecOps toolingWho this book is forThis comprehensive guide is for cloud architects, engineers, DevOps professionals, and AWS enthusiasts. Cybersecurity professionals who want to learn AWS security to protect their applications, data, and infrastructure from threats, ensure compliance with regulations, and build trust with customers, will also find this book useful. ","","9781805121718","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10522547.pdf&bkn=10522546&pdfType=book","","","","","","","","8 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"A Comprehensive Review of Modern Object Segmentation Approaches","Y. Wang; U. Ahsan; H. Li; M. Hagen",NA; NA; NA; NA,A Comprehensive Review of Modern Object Segmentation Approaches,"","2022","","","","","Automated visual recognition tasks such as image classification, image captioning, object detection and image segmentation are essential for image and video processing. Of these, image segmentation is the task of associating pixels in an image with their respective object class labels. It has a wide range of applications within many industries, including healthcare, transportation, robotics, fashion, home improvement, and tourism. In this monograph, both traditional and modern object segmentation approaches are investigated, comparing their strengths, weaknesses, and utilities. The main focus is on the deep learning-based techniques for the two most widely solved segmentation tasks: Semantic Segmentation and Instance Segmentation. A wide range of deep learning-based segmentation techniques developed in recent years are examined. Various themes emerge from these techniques that push machines to their limits, and often deviate from human perception principles. In addition, an overview of the widely used benchmark datasets for each of these techniques, along with the respective evaluation metrics to measure the models’ performances, are presented. Potential future research directions conclude the monograph. This monograph serves as a good introduction to the automated visual recognition task of image segmentation and is intended for students and professionals.","","9781638280712","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9913376.pdf&bkn=9913355&pdfType=book","","","","","","","","10 Oct 2022","","","now","Now Foundations and Trends Books"
"STHVC: Spatial-Temporal Hybrid Video Compression for UAV-assisted IoV Systems","L. Chen; J. Deng; X. Zeng; L. Liu; Y. Wu; J. Hu; Q. Sun; Z. Shi; C. Zhuo","Zhejiang University, Hangzhou, China; University of Pittsburgh, Pittsburgh, USA; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; University of Pittsburgh, Pittsburgh, USA; University of Pittsburgh, Pittsburgh, USA; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Recent rapid advancements in intelligent vehicular systems and deep learning techniques have led to the emergence of diverse applications utilizing high-quality automotive videos in the Internet-of-Vehicles (IoV), often assisted by unmanned aerial vehicles (UAVs). These applications aim to provide convenience and security for users. However, transmitting automotive videos with high-quality and low-bit-rate poses a challenge due to the inherent lossiness of traditional compression codecs in current UAV-assisted IoV systems, thereby affecting the performance of subsequent tasks. To address this, we propose a spatial-temporal hybrid video compression framework (STHVC), which integrates Space-Time Super-Resolution (STSR) with conventional codecs to enhance the compression efficiency on automotive videos. In our hybrid design, the encoder generates a low-frame-rate and low-resolution version of the source video, which is then compressed using a traditional codec. During the decoding stage, an effective STSR network is developed to increase both the resolution and the frame rate, and mitigate compression artifacts for automotive videos simultaneously. Additionally, we introduce a rectified intermediate flow estimation technique (RecIFE) within the proposed STSR network to address the challenge of noisy and inaccurate motions during the compression pipeline. Extensive experiments on various benchmark datasets demonstrate that our approach achieves bit-rate reductions of 29.97% compared to H.265 (slow) and 31.27% compared to H.266, while also exhibiting superior restoration performance compared to other state-of-the-art learning-based approaches.","1558-2205","","10.1109/TCSVT.2025.3550726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10924199","Video Compression;Internet-of-Vehicles (IoV);Space-time Super-resolution;Deep Learning;Unmanned Aerial Vehicle (UAV)","Video compression;Automotive engineering;Superresolution;Decoding;Autonomous aerial vehicles;Servers;Encoding;Deep learning;Circuits and systems;Video codecs","","","","","IEEE","12 Mar 2025","","","IEEE","IEEE Early Access Articles"
"1 Plasmonic Sensors for Applications in Liquid Biopsies","U. L. Jong; J. S. Sang","Department of Chemical and Biological Engineering, Korea University, Korea; Department of Chemical and Biological Engineering, Korea University, Korea",Cutting-edge Technologies in Biological Sensing and Analysis,"","2023","","","1","32","Advanced technologies have been transforming the ways we carry out biological studies as well as deliver healthcare. While micro- and nano-fabrication have provided miniaturized sensors and systems with better sensitivity and selectivity,; innovations in flexible electronics, biomaterials and telecommunications have helped in enabling novel biomedical devices, reducing cost, bringing convenience and establishing mobile-health (m-Health), and personalized- and tele-medicine. Further, the recent rise of the internet of things (IoTs) and machine learning-based approaches has paved the avenue for those biomedical systems to become popular and widely accepted by our society. In this context, we edit this book aiming to cover a broad field of novel technologies used in biological assessment and analysis for humans, animal models and in vitro platforms, in both health monitoring and biological studies. Technical topics discussed in the book include: • Biosensing systems and biomedical techniques • Imaging techniques and systems • Biosignal analysis • Animal models used in biological research","","9788770225854","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10266965.pdf&bkn=10266846&pdfType=chapter","","","","","","","","28 Sep 2023","","","River Publishers","River eBook Chapters"
"Time Series Analysis with Python Cookbook: Practical recipes for exploratory data analysis, data preparation, forecasting, and model evaluation","T. A. Atwan",NA,"Time Series Analysis with Python Cookbook: Practical recipes for exploratory data analysis, data preparation, forecasting, and model evaluation","","2022","","","","","Perform time series analysis and forecasting confidently with this Python code bank and reference manualKey FeaturesExplore forecasting and anomaly detection techniques using statistical, machine learning, and deep learning algorithmsLearn different techniques for evaluating, diagnosing, and optimizing your modelsWork with a variety of complex data with trends, multiple seasonal patterns, and irregularitiesBook DescriptionTime series data is everywhere, available at a high frequency and volume. It is complex and can contain noise, irregularities, and multiple patterns, making it crucial to be well-versed with the techniques covered in this book for data preparation, analysis, and forecasting. This book covers practical techniques for working with time series data, starting with ingesting time series data from various sources and formats, whether in private cloud storage, relational databases, non-relational databases, or specialized time series databases such as InfluxDB. Next, you’ll learn strategies for handling missing data, dealing with time zones and custom business days, and detecting anomalies using intuitive statistical methods, followed by more advanced unsupervised ML models. The book will also explore forecasting using classical statistical models such as Holt-Winters, SARIMA, and VAR. The recipes will present practical techniques for handling non-stationary data, using power transforms, ACF and PACF plots, and decomposing time series data with multiple seasonal patterns. Later, you’ll work with ML and DL models using TensorFlow and PyTorch. Finally, you’ll learn how to evaluate, compare, optimize models, and more using the recipes covered in the book.What you will learnUnderstand what makes time series data different from other dataApply various imputation and interpolation strategies for missing dataImplement different models for univariate and multivariate time seriesUse different deep learning libraries such as TensorFlow, Keras, and PyTorchPlot interactive time series visualizations using hvPlotExplore state-space models and the unobserved components model (UCM)Detect anomalies using statistical and machine learning methodsForecast complex time series with multiple seasonal patternsWho this book is forThis book is for data analysts, business analysts, data scientists, data engineers, or Python developers who want practical Python recipes for time series analysis and forecasting techniques. Fundamental knowledge of Python programming is required. Although having a basic math and statistics background will be beneficial, it is not necessary. Prior experience working with time series data to solve business problems will also help you to better utilize and apply the different recipes in this book.","","9781801071260","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163088.pdf&bkn=10163087&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Non Expensive Mobile Robot for Sensor Based Research on Artificial Cognition","V. E. Arriola Ríos","Mathematics Department, Faculty of Science (UNAM), Mexico City, México",2024 XXVI Robotics Mexican Congress (COMRob),"12 Dec 2024","2024","","","47","52","Research on Embodied Cognition constantly faces two difficulties related to hardware: either robotic platforms are too expensive for the University budget and/or robots were designed for industrial purposes, favoring fine control and reproducibility at the cost of restricted customization for sensor readings and movement control. In order to fully explore new venues for machine learning algorithms linking the pathways from sensation to cognition, back to acting, the physical architecture of the robot must be open and its programming easily altered without putting humans or robots at risk. In the search for a friendly robotic environment for research on cognition, a simple architecture is presented here, with a cost around $10,000 MX or $600 US.","","979-8-3315-3339-7","10.1109/COMRob64055.2024.10777443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10777443","Embodied cognition;Artificial Cognition;Machine Learning;Robot Design;Artificial Intelligence;Cognitive Machines","Costs;Educational robots;Service robots;Robot sensing systems;Cognition;Teamwork;Robots;Programming profession;Wireless fidelity;Standards","","","","12","IEEE","12 Dec 2024","","","IEEE","IEEE Conferences"
"3D face recognition: A comprehensive survey in 2022","Y. Jing; X. Lu; S. Gao","The School of Information Technology, Deakin University, Waurn Ponds, VIC, Australia; The School of Information Technology, Deakin University, Waurn Ponds, VIC, Australia; The School of Information Technology, Deakin University, Waurn Ponds, VIC, Australia",Computational Visual Media,"20 Feb 2025","2023","9","4","657","685","In the past ten years, research on face recognition has shifted to using 3D facial surfaces, as 3D geometric information provides more discriminative features. This comprehensive survey reviews 3D face recognition techniques developed in the past decade, both conventional methods and deep learning methods. These methods are evaluated with detailed descriptions of selected representative works. Their advantages and disadvantages are summarized in terms of accuracy, complexity, and robustness to facial variations (expression, pose, occlusion, etc.). A review of 3D face databases is also provided, and a discussion of future research challenges and directions of the topic.","2096-0662","","10.1007/s41095-022-0317-1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897726","3D face recognition;3D face databases;deep learning;local features;global features","Face recognition;Three-dimensional displays;Databases;Feature extraction;Surveys;Lasers;Reviews;Lighting;Cameras;Accuracy","","","","","","20 Feb 2025","","","TUP","TUP Journals"
"The Midjourney Expedition: Generate creative images from text prompts and seamlessly integrate them into your workflow","M. Barreto",NA,The Midjourney Expedition: Generate creative images from text prompts and seamlessly integrate them into your workflow,"","2024","","","","","Harness the power of Midjourney to create impactful and memorable artistic outputs and gain a distinctive edge in your professional endeavorsKey FeaturesMaster Midjourney prompting with the help of practical examples from an experienced communication and web design specialistExplore Midjourney's capabilities to create visually stunning art without prior design knowledgeGain practical insights into how to strategically apply AI-generated art in your workPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionLike various other fields, AI offers boundless possibilities when it comes to art. Midjourney is one of the leading AI art creation tools that can assist you in your artistic ideas, regardless of your technical skill level. Written by an accomplished communication and web design specialist, The Midjourney Expedition is your guide to harnessing the power of AI in your creative journey. With this guide, you’ll explore the extensive features of Midjourney and start creating compelling AI-generated art with ease. The first set of chapters will teach you how to set up and use Discord for personalized and seamless art creation, with a dedicated section that will help you understand the different versions of Midjourney and their capabilities. As you progress, you’ll hone your prompt engineering skills, and eventually learn how to leverage the power of complex prompts. You’ll also learn how Midjourney-generated images can be integrated into a multitude of workflows and domains through real-life case studies. In the last set of chapters, you’ll get to grips with real-world applications of Midjourney for storytelling, creating moodboards, and more. By the end of this book, you’ll not only be proficient in using Midjourney, but also understand how to strategically apply AI-generated art in your projects.What you will learnNavigate and master Midjourney's extensive features for AI art creationApply practical techniques to create visually stunning AI-generated artworkAccelerate your creative process to produce captivating visual contentUnderstand and master essential parameters to enhance your creationsCreate consistent characters for storytellingIncorporate AI-generated art into various work contextsWho this book is forThe Midjourney Expedition is for creative individuals who are looking to visually express their ideas through the power of AI. While this book will certainly benefit designers, it's equally valuable for marketing professionals, brand strategists, content creators, media managers, and entrepreneurs. Those responsible for creating compelling visual content to represent a brand, product, or concept will also find this book useful. Basic knowledge of web user interfaces will be helpful, but not required. ","","9781835089187","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769286.pdf&bkn=10769285&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Amazon DynamoDB - The Definitive Guide: Explore enterprise-ready, serverless NoSQL with predictable, scalable performance","A. Dhingra; M. Mackay",NA; NA,"Amazon DynamoDB - The Definitive Guide: Explore enterprise-ready, serverless NoSQL with predictable, scalable performance","","2024","","","","","Harness the potential and scalability of DynamoDB to effortlessly construct resilient, low-latency databases Key FeaturesDiscover how DynamoDB works behind the scenes to make the most of its featuresLearn how to keep latency and costs minimal even when scaling upIntegrate DynamoDB with other AWS services to create a full data analytics systemPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThis book will help you master Amazon DynamoDB, the fully managed, serverless, NoSQL database service designed for high performance at any scale. Authored by Aman Dhingra, senior DynamoDB specialist solutions architect at AWS, and Mike Mackay, former senior NoSQL specialist solutions architect at AWS, this guide draws on their expertise to equip you with the knowledge and skills needed to harness DynamoDB's full potential. This book not only introduces you to DynamoDB's core features and real-world applications, but also provides in-depth guidance on transitioning from traditional relational databases to the NoSQL world. You'll learn essential data modeling techniques, such as vertical partitioning, and explore the nuances of DynamoDB's indexing capabilities, capacity modes, and consistency models. The chapters also help you gain a solid understanding of advanced topics such as enhanced analytical patterns, implementing caching with DynamoDB Accelerator (DAX), and integrating DynamoDB with other AWS services to optimize your data strategies. By the end of this book, you’ll be able to design, build, and deliver low-latency, high-throughput DynamoDB solutions, driving new levels of efficiency and performance for your applications.What you will learnMaster key-value data modeling in DynamoDB for efficiencyTransition from RDBMSs to NoSQL with optimized strategiesImplement read consistency and ACID transactions effectivelyExplore vertical partitioning for specific data access patternsOptimize data retrieval using secondary indexes in DynamoDBManage capacity modes, backup strategies, and core componentsEnhance DynamoDB with caching, analytics, and global tablesEvaluate and design your DynamoDB migration strategyWho this book is forThis book is for software architects designing scalable systems, developers optimizing performance with DynamoDB, and engineering managers guiding decision-making. Data engineers will learn to integrate DynamoDB into workflows, while product owners will explore its innovative capabilities. DBAs transitioning to NoSQL will find valuable insights on DynamoDB and RDBMS integration. Basic knowledge of software engineering, Python, and cloud computing is helpful. Hands-on AWS or DynamoDB experience is beneficial but not required. ","","9781803248325","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10803997.pdf&bkn=10803996&pdfType=book","","","","","","","","16 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"StreetSurfGS: Scalable Urban Street Surface Reconstruction with Planar-based Gaussian Splatting","X. Cui; W. Ye; Y. Wang; G. Zhang; W. Zhou; T. He; H. Li","Department of Electrical Engineering and Information Science, University of Science and Technology of China, Hefei, China; State Key Laboratory of CAD and CG, Zhejiang University, Hangzhou, China; Shanghai AI Laboratory, Shanghai, China; State Key Laboratory of CAD and CG, Zhejiang University, Hangzhou, China; Department of Electrical Engineering and Information Science, University of Science and Technology of China, Hefei, China; Shanghai AI Laboratory, Shanghai, China; Department of Electrical Engineering and Information Science, University of Science and Technology of China, Hefei, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","Reconstructing urban street scenes is crucial due to its vital role in applications such as autonomous driving and urban planning. These scenes are characterized by long, narrow camera trajectories, occlusion, complex object relationships, and sparse data across multiple scales. Despite recent advancements, existing surface reconstruction methods, which are primarily designed for object-centric scenarios, struggle to adapt effectively to the unique characteristics of street scenes. To address this challenge, we introduce StreetSurfGS, the first method to employ Gaussian Splatting specifically tailored for scalable urban street scene surface reconstruction. StreetSurfGS utilizes a planar-based octree representation and segmented training to reduce memory costs, accommodate unique camera characteristics, and improve scalability. Additionally, to mitigate depth inaccuracies caused by object overlap, we propose a guided smoothing strategy within regularization to eliminate inaccurate boundary points and outliers. Furthermore, to address sparse views and multi-scale challenges, we use a dual-step matching strategy that leverages adjacent and long-term information. Extensive experiments validate the efficacy of StreetSurfGS in both novel view synthesis and surface reconstruction.","1558-2205","","10.1109/TCSVT.2025.3551719","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10929011","Surface Reconstruction;Gaussian Splatting;Urban Street Reconstruction","Surface reconstruction;Image reconstruction;Three-dimensional displays;Cameras;Training;Rendering (computer graphics);Scalability;Octrees;Neural radiance field;Circuits and systems","","","","","IEEE","17 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Video Summarization Overview","M. Otani; Y. Song; Y. Wang",NA; NA; NA,Video Summarization Overview,"","2022","","","","","The widespread use of the internet and affordable video capturing devices has dramatically changed the landscape of video creation and consumption. In particular, user-created videos are more prevalent than ever with the evolution of video streaming services and social networks. The rapid growth of video creation necessitates advanced technologies that enable efficient consumption of desired video content. The scenarios include enhancing user experience for viewers on video streaming services, enabling quick video browsing for video creators who need to go through a massive amount of video rushes, and for security teams who need to monitor surveillance videos. With the broad growth of video capturing devices and applications on the web, it is more demanding to provide desired video content for users efficiently. Video summarization facilitates quickly grasping video content by creating a compact summary of videos. Much effort has been devoted to automatic video summarization, and various problem settings and approaches have been proposed. This monograph provides an overview of this field, and covers early studies as well as recent approaches which take advantage of deep learning techniques. Video summarization approaches and their underlying concepts are described, and benchmarks and evaluations are included. Evaluation techniques in prior work in this field are addressed, and the pros and cons of the evaluation protocols are detailed. The monograph concludes with current and open challenges in this field. This monograph is a useful reference for students and professionals who are active in, or wish to enter into the field of Video Summarization.","","9781638280798","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9919110.pdf&bkn=9919109&pdfType=book","","","","","","","","14 Oct 2022","","","now","Now Foundations and Trends Books"
"Multimodal Foundation Models: From Specialists to General-Purpose Assistants","C. Li; Z. Gan; Z. Yang; J. Yang; L. Li; L. Wang; J. Gao",NA; NA; NA; NA; NA; NA; NA,Multimodal Foundation Models: From Specialists to General-Purpose Assistants,"","2024","","","","","This monograph presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from specialist models to general-purpose assistants. The focus encompasses five core topics, categorized into two classes; (i) a survey of well-established research areas: multimodal foundation models pre-trained for specific purposes, including two topics – methods of learning vision backbones for visual understanding and text-to-image generation; (ii) recent advances in exploratory, open research areas: multimodal foundation models that aim to play the role of general-purpose assistants, including three topics – unified vision models inspired by large language models (LLMs), end-to-end training of multimodal LLMs, and chaining multimodal tools with LLMs. The target audience of the monograph is researchers, graduate students, and professionals in computer vision and vision-language multimodal communities who are eager to learn the basics and recent advances in multimodal foundation models.","","9781638283379","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10522588.pdf&bkn=10522587&pdfType=book","","","","","","","","8 May 2024","","","now","Now Foundations and Trends Books"
