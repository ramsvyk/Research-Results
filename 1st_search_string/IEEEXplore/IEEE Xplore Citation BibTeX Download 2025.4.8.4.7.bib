@ARTICLE{10184011,
  author={Kim, Baekgyu and Kang, Eunsuk},
  journal={IEEE Access}, 
  title={Toward Large-Scale Test for Certifying Autonomous Driving Software in Collaborative Virtual Environment}, 
  year={2023},
  volume={11},
  number={},
  pages={72641-72654},
  abstract={Virtual simulation environments are widely used to test autonomous driving software by creating highly complex driving scenarios that are non-trivial to set up in a physical environment. However, the current practice of using the virtual test still does not fully utilize its potential to build a much larger scale test. We propose a perspective and research vision to build a large-scale test architecture in which participants collaboratively construct, execute and analyze complex test scenarios at scale in the virtual world. In particular, the architectural concept is built on the existing concept of the Collaborative Virtual Environment (CVE) that has been successfully applied in other domains, such as entertainment or military training applications. The proposed domain-specific architectural requirements extend the CVE to include the following necessary properties - selective sharing and collaboration - to test autonomous driving software. In addition, the test architectural concept is explained as to how a large number of participants interact with each other collaboratively to build and execute diverse test scenarios at scale. Finally, we explain the new research directions to make this test architectural concept realized for testing autonomous driving software.},
  keywords={Virtual environments;Autonomous vehicles;Testing;Collaboration;Software systems;Road traffic;Virtual environments;Vehicle safety;Autonomous driving;Software testing;Automation;Certification;collaborative virtual environment;driving scenario selection;software safety;testing autonomous driving software;test automation;test scalability},
  doi={10.1109/ACCESS.2023.3295500},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10658223,
  author={Liu, Yixin and Fan, Chenrui and Dai, Yutong and Chen, Xun and Zhou, Pan and Sun, Lichao},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={MetaCloak: Preventing Unauthorized Subject-Driven Text-to-Image Diffusion-Based Synthesis via Meta-Learning}, 
  year={2024},
  volume={},
  number={},
  pages={24219-24228},
  abstract={Text-to-image diffusion models allow seamless generation of personalized images from scant reference photos. Yet, these tools, in the wrong hands, can fabricate misleading or harmful content, endangering individuals. To address this problem, existing poisoning-based approaches perturb user images in an imperceptible way to render them “unlearnable” from malicious uses. We identify two limitations of these defending approaches: i) sub-optimal due to the hand-crafted heuristics for solving the intractable bilevel optimization and ii) lack of robustness against simple data transformations like Gaussian filtering. To solve these challenges, we propose MetaCloak, which solves the bi-level poisoning problem with a meta-learning framework with an additional transformation sampling process to craft transferable and robust perturbation. Specifically, we employ a pool of surrogate diffusion models to craft transferable and model-agnostic perturbation. Furthermore, by incorporating an additional transformation process, we design a simple denoising-error maximization loss that is sufficient for causing transformation-robust semantic distortion and degradation in a personalized generation. Extensive experiments on the VGGFace2 and CelebA-HQ datasets show that MetaCloak outperforms existing approaches. Notably, MetaCloak can successfully fool online training services like Replicate, in a black-box manner, demonstrating the effectiveness of Meta Cloak in real-world scenarios. Our code is available at https://github.com/liuyixin-louis/MetaCloak.},
  keywords={Metalearning;Training;Filtering;Perturbation methods;Semantics;Text to image;Diffusion models;DeepFake Defense;Unlearnable Example;Adversarial ML;Unauthorized Exploitation;Imperceptible Perturbation},
  doi={10.1109/CVPR52733.2024.02286},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10912769,
  author={Liu, Mingyi and Wu, Gensheng and Xu, Hanchuan and Wang, Jian and Xu, Xiaofei and Wang, Zhongjie},
  journal={IEEE Transactions on Services Computing}, 
  title={AdaFlow: Learning and Utilizing Workflows for Enhanced Service Recommendation in Dynamic Environments}, 
  year={2025},
  volume={},
  number={},
  pages={1-14},
  abstract={Service provisioning represents a nuanced form of recommendation, offering a bundle of services (APIs) tailored to the specifics needs of an application (mashup) as defined by the developer, significantly easing development efforts. Unlike standard product recommendations, service recommendations face unique challenges, including cold-start, long-tail phenomena, constraints, dynamic environments, and workflows. While the first four issues have seen some resolution in the literature, the workflow mining and integration among services remains underexplored. In this paper, we focus on this gap by introducing AdaFlow, a model designed to understand and leverage service workflows within mashups, identifying viable service patterns for recommendations. AdaFlow employs a Graph Neural Network (GNN)-based framework, AdaptiveNN, to capture and learn service interactions. This learned workflow knowledge feeds into a dynamic GNN, enhancing service evolution representations that inform our recommendation process. Moreover, AdaFlow exhibits superior performance in managing dynamic and imbalanced scenarios. Our code is publicized on GitHub: https://github.com/HIT-ICES/AdaFlow},
  keywords={Mashups;Computational modeling;Graph neural networks;Training;Semantics;Noise;Heavily-tailed distribution;Feature extraction;Data mining;Collaborative filtering;Mashup creation;Service Recommendation;Graph Neural Network;Workflow},
  doi={10.1109/TSC.2025.3547219},
  ISSN={1939-1374},
  month={},}@ARTICLE{10843373,
  author={Zhang, Xinyue and Zhou, Jiahuan and Yan, Luxin and Zhong, Sheng and Zou, Xu},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Hunt Camouflaged Objects via Revealing Mutation Regions}, 
  year={2025},
  volume={20},
  number={},
  pages={1836-1851},
  abstract={Due to the high similarity between hidden objects and the surrounding background, camouflaged object detection (COD) remains a challenge. While many recently proposed methods have shown remarkable performance, most of them begin object perception by indiscriminately considering every pixel of the image. However, these early-stage region-insensitive perception methods still struggle to resist background interference, potentially missing subtle pixel changes by not prioritizing potential camouflaged areas initially. Fortunately, we reveal that the availability of an accurate mutation map can significantly enhance camouflaged discrimination ability. To this end, we propose MRNet (Mutation Region Network). MRNet initially generates a mutation map that identifies potential mutation regions exhibiting subtle pixel changes. The generation method involves amplifying and differing pixel changes based on the position and corresponding values of pixels. Subsequently, the selective expansion search operation utilizes the mutation map to extract the mapped graph, effectively reducing interference from background pixels that are distant from the mutation regions. Finally, decoding the mapped graph generates precise masks. Furthermore, we have created the largest test dataset with known categories to advance community research. Extensive experiments conducted on three widely used datasets and our proposed dataset show that MRNet surpasses other methods with superior performance. Source code is publicly available at https://github.com/XinyueZhangHust/MRNet},
  keywords={Feature extraction;Image edge detection;Transformers;Search problems;Interference;Image segmentation;Data mining;Convolution;Training;Object recognition;Camouflaged object detection;mutation map;selective expansion search operation},
  doi={10.1109/TIFS.2025.3530703},
  ISSN={1556-6021},
  month={},}@ARTICLE{10804638,
  author={He, Qiaoping},
  journal={IEEE Internet of Things Journal}, 
  title={Unsupervised Cross-Domain Deep-Fused Feature Descriptor for Efficient Image Retrieval}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  abstract={Deep feature-based methods show advantages in image retrieval, yet their robustness and generalization warrant further investigation. Toward this end, we propose a robust unsupervised Cross-domain Deep-fused Feature Descriptor (CDFD) method for efficient image retrieval. It analyzes deep features from a frequency domain perspective, employing Discrete Wavelet Transform (DWT) and Discrete Cosine Transform (DCT) to enhance its robustness and distinguishability. Specifically, we begin with a distinct perspective and introduce DWT to identify low-frequency and high-frequency components in deep features. A multi-trait spatial fusion strategy is proposed to integrate various features and strengthen low-frequency information. It can highlight the target region in the image and suppress the background clutters by introducing a low-pass filter, thus improving the discriminating ability of the feature representation. To enhance the robustness of the features, a cross-domain convergence scheme is proposed, which enables the rational integration of spatial and frequency-domain information by utilizing DCT. In addition, a computationally simple decrement query expansion technique is introduced. Using it with CDFD can effectively improve retrieval performance. Extensive comparative experiments on several benchmark datasets demonstrate that our CDFD method is robust and effective in image retrieval and outperforms several unsupervised state-of-the-art methods. The Source code is published at https://github.com/sevenjava/CDFD.},
  keywords={Image retrieval;Robustness;Discrete wavelet transforms;Computational modeling;Transforms;Feature extraction;Frequency-domain analysis;Vectors;Discrete cosine transforms;Computer vision;Deep features;Discrete wavelet transform;Discrete cosine transform;Image retrieval;Cross-domain deep-fused feature descriptor},
  doi={10.1109/JIOT.2024.3519175},
  ISSN={2327-4662},
  month={},}@ARTICLE{10623180,
  author={Brilhador, Anderson and da Silva, Rodrigo Tchalski and Modinez-Junior, Carlos Roberto and de Almeida Spadafora, Gabriel and Lopes, Heitor Silvério and Lazzaretti, André Eugênio},
  journal={IEEE Access}, 
  title={Open-Set Tattoo Semantic Segmentation}, 
  year={2024},
  volume={12},
  number={},
  pages={107181-107200},
  abstract={Tattoos can serve as an essential source of biometric information for public security, aiding in identifying suspects and victims. In order to automate tattoo classification, tasks like classification require more detailed image content analysis, such as semantic segmentation. However, a dataset with appropriate semantic segmentation annotations is currently lacking. Also, there are countless ways to categorize tattoo classes, and many are not directly categorizable, either because they belong to a specific artistic trait or characterize an object with previously undefined semantics. An effective way to overcome these limitations is to build recognition systems based on open-set assumptions. Nevertheless, state-of-the-art open set approaches are not directly applicable in tattoo semantic segmentation, mainly due to the significant class imbalance (predominant background). To the best of our knowledge, this paper is the first to explore semantic segmentation in closed and open-set scenarios for tattoos. In this sense, this paper presents two key contributions: (i) a novel large-margin loss function and generalized open-set classifier approach and (ii) an open-set tattoo semantic segmentation dataset with a publicly accessible test set, enabling comparisons and future research in this area. The proposed approach outperforms other methods, achieving 0.8013 of AUROC, 0.6318 of Macro F1, 0.4900 of mIoU, and notably 0.2753 of IoU for the unknown class, demonstrating the feasibility of this approach for automatic tattoo analysis. The paper also highlights key limitations and open research areas in this challenging field. Dataset and codes are available at https://github.com/Brilhador/tssd2023.},
  keywords={Semantic segmentation;Semantics;Flowering plants;Biometrics (access control);Annotations;Task analysis;Complexity theory;Open-world;open-set;semantic segmentation;large-margin learning;tattoo classification},
  doi={10.1109/ACCESS.2024.3438557},
  ISSN={2169-3536},
  month={},}@ARTICLE{10938537,
  author={Bezerra, Jose Fabio Ribeiro and Kozierkiewicz, Adrianna and Pietranik, Marcin},
  journal={IEEE Access}, 
  title={A Novel Approach for Tweet Similarity in a Context-Aware Fake News Detection Model}, 
  year={2025},
  volume={13},
  number={},
  pages={57043-57061},
  abstract={In today’s information-driven world, identifying fake news is more critical than ever. The spread of false information threatens societal stability, eroding trust in institutions and fostering community polarization. While numerous algorithms and methods have been developed to detect fake news, their effectiveness varies depending on the context and data used. Recent advancements, particularly those leveraging transformer-based models like BERT, have significantly improved detection performance, especially in multimodal environments. Social media platforms are the primary focus of fake news research, as they serve as fertile grounds for the unchecked spread of misinformation. Their rapid dissemination capabilities and global reach make them ideal channels for propagating disinformation. Addressing these challenges requires innovative approaches and robust solutions. In response to these essential issues, this study presents a formal framework for detecting fake news using a multilayered approach. The proposed three-tiered model includes the topic, social, and context layers, providing a comprehensive approach to identifying fake news. One of the critical tasks within the topic layer involves assessing the similarity between two messages. To achieve this, we developed a novel method for evaluating the similarity of two tweets by extending a state-of-the-art model. Our approach leverages FastText vectors, cosine similarity, and word positions within sentences to improve accuracy. For verification, we utilized the STSBenchmark dataset and employed correlation values to measure our model’s quality. We conducted a rigorous 50-fold evaluation of the validation data. Ultimately, our model achieved a superior correlation value (median 0.673528) compared to another leading model, which had previously delivered the best results in the field. This paper presents one of the important elements of a comprehensive tool for detecting fake news, with some components already published in other works by the authors or planned for future development (e.g. multimodal content processing or multi-lingual similarity).},
  keywords={Fake news;Social networking (online);Feature extraction;Data models;Context modeling;Vectors;Visualization;Videos;Neural networks;Electronic mail;Context;fake news;natural language processing;tweet similarity},
  doi={10.1109/ACCESS.2025.3554540},
  ISSN={2169-3536},
  month={},}@ARTICLE{10854210,
  author={Wijngaard, Gijs and Formisano, Elia and Esposito, Michele and Dumontier, Michel},
  journal={IEEE Access}, 
  title={Audio-Language Datasets of Scenes and Events: A Survey}, 
  year={2025},
  volume={13},
  number={},
  pages={20328-20360},
  abstract={Audio-language models (ALMs) generate linguistic descriptions of sound-producing events and scenes. Advances in dataset creation and computational power have led to significant progress in this domain. This paper surveys 69 datasets used to train ALMs, covering research up to September 2024 (https://github.com/GLJS/audio-datasets). The survey provides a comprehensive analysis of dataset origins, audio and linguistic characteristics, and use cases. Key sources include YouTube-based datasets such as AudioSet, with over two million samples, and community platforms such as Freesound, with over one million samples. The survey evaluates acoustic and linguistic variability across datasets through principal component analysis of audio and text embeddings. The survey also analyzes data leakage through CLAP embeddings, and examines sound category distributions to identify imbalances. Finally, the survey identifies key challenges in developing large, diverse datasets to enhance ALM performance, including dataset overlap, biases, accessibility barriers, and the predominance of English-language content, while highlighting specific areas requiring attention: multilingual dataset development, specialized domain coverage and improved dataset accessibility.},
  keywords={Surveys;Data models;Training;Gold;Decoding;Contrastive learning;Transformers;Source separation;Web sites;Video on demand;Audio-to-language learning;language-to-audio learning;audio-language datasets;review},
  doi={10.1109/ACCESS.2025.3534621},
  ISSN={2169-3536},
  month={},}@INBOOK{10502009,
  author={Bayuk, Jennifer L.},
  booktitle={Stepping Through Cybersecurity Risk Management: A Systems Thinking Approach}, 
  title={Threats}, 
  year={2024},
  volume={},
  number={},
  pages={17-45},
  abstract={The mainstay declares that threats embolden adversaries to exploit vulnerabilities that expose assets that enable their objectives. That is the basic idea behind a cyber threat. The most important thing to know about cybersecurity threats is that the actors who enact them may be dangerous adversaries. The second most important thing to know is that there is an interaction between an adversary and its target whether or not the target chooses to actively participate. In the threat actor landscape, actors with diverse objectives often find kindred spirits in organized crime syndicates that are similar to legitimate technology businesses. Threat actors spend considerable time and energy in reconnaissance to identify vulnerabilities in their target's systems. In cyber security, potential attacks are the aggregated set of all publicly documented cyber attacks to date.},
  keywords={Organizations;Computer security;Matched filters;Sun;Social implications of technology;Malware;NIST},
  doi={10.1002/9781394213986.ch2},
  ISSN={},
  publisher={Wiley},
  isbn={9781394213962},
  url={https://ieeexplore.ieee.org/document/10502009},}@ARTICLE{10854802,
  author={Shang, Lanyu and Zhang, Yang and Deng, Yawen and Wang, Dong},
  journal={IEEE Transactions on Big Data}, 
  title={MultiTec: A Data-Driven Multimodal Short Video Detection Framework for Healthcare Misinformation on TikTok}, 
  year={2025},
  volume={},
  number={},
  pages={1-18},
  abstract={With the prevalence of social media and short video sharing platforms (e.g., TikTok, YouTube Shorts), the proliferation of healthcare misinformation has become a widespread and concerning issue that threatens public health and undermines trust in mass media. This paper focuses on an important problem of detecting multimodal healthcare misinformation in short videos on TikTok. Our objective is to accurately identify misleading healthcare information that is jointly conveyed by the visual, audio, and textual content within the TikTok short videos. Three critical challenges exist in solving our problem: i) how to effectively extract information from distractive and manipulated visual content in short videos? ii) How to efficiently identify the interrelation of the heterogeneous visual and speech content in short videos? iii) How to accurately capture the complex dependency of the densely connected sequential content in short videos? To address the above challenges, we develop MultiTec, a multimodal detector that explicitly explores the audio and visual content in short videos to investigate both the sequential relation of video elements and their inter-modality dependencies to jointly detect misinformation in healthcare videos on TikTok. To the best of our knowledge, MultiTec is the first modality-aware dual-attentive short video detection model for multimodal healthcare misinformation on TikTok. We evaluate MultiTec on two real-world healthcare video datasets collected from TikTok. Evaluation results show that MultiTec achieves substantial performance gains compared to state-of-the-art baselines in accurately detecting misleading healthcare short videos.},
  keywords={Fake news;Medical services;Web sites;Video on demand;Visualization;COVID-19;Vaccines;Semantics;Feature extraction;Social networking (online);COVID-19;healthcare misinformation;misinformation detection;multimodal fusion;short video;TikTok},
  doi={10.1109/TBDATA.2025.3533919},
  ISSN={2332-7790},
  month={},}@INPROCEEDINGS{10522431,
  author={Mahajan, Renu and Kaur, Parneet},
  booktitle={2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)}, 
  title={Robustness in Deep Neural Network: A Focus on Software Tampering and Counter Measures}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Over the past several years, there has been a proliferation of “Deep Neural Network” (DNN) models that have been constructed as well as implemented. These models require safeguarding against potential tampering by malevolent individuals. This paper aims to explore the significance of “recoverable, self-embedding fragile watermarking approach for deep neural network DNN” models in order to safeguard the integrity of the models. This system possesses the capacity to not only identify and locate the altered parameter blocks in the framework, but also to accurately recover the compromised values. The verified data and recovery data are derived through a comprehensive analysis of the specific attributes of the DNN model that requires safeguarding. These data are then embedded into the model using a reference sharing mechanism, without compromising its original functionality. This enables the recovery of the model parameters even when subjected to various levels of tampering.},
  keywords={Measurement uncertainty;Artificial neural networks;Watermarking;Manuals;Market research;Software;Data models;DNN;Watermarks;robustness;tampering},
  doi={10.1109/ICRITO61523.2024.10522431},
  ISSN={2769-2884},
  month={March},}@INPROCEEDINGS{10764862,
  author={Muttillo, Vittoriano and Di Sipio, Claudio and Rubei, Riccardo and Berardinelli, Luca and Dehghani, MohammadHadi},
  booktitle={2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Towards Synthetic Trace Generation of Modeling Operations using In-Context Learning Approach}, 
  year={2024},
  volume={},
  number={},
  pages={619-630},
  abstract={Producing accurate software models is crucial in model-driven software engineering (MDE). However, modeling complex systems is an error-prone task that requires deep application domain knowledge. In the past decade, several automated techniques have been proposed to support academic and industrial practitioners by providing relevant modeling operations. Nevertheless, those techniques require a huge amount of training data that cannot be available due to several factors, e.g., privacy issues. The advent of large language models (LLMs) can support the generation of synthetic data although state-of-the-art approaches are not yet supporting the generation of modeling operations. To fill the gap, we propose a conceptual framework that combines modeling event logs, intelligent modeling assistants, and the generation of modeling operations using LLMs. In particular, the architecture comprises modeling components that help the designer specify the system, record its operation within a graphical modeling environment, and automatically recommend relevant operations. In addition, we generate a completely new dataset of modeling events by telling on the most prominent LLMs currently available. As a proof of concept, we instantiate the proposed framework using a set of existing modeling tools employed in industrial use cases within different European projects. To assess the proposed methodology, we first evaluate the capability of the examined LLMs to generate realistic modeling operations by relying on well-founded distance metrics. Then, we evaluate the recommended operations by considering real-world industrial modeling artifacts. Our findings demonstrate that LLMs can generate modeling events even though the overall accuracy is higher when considering human-based operations. In this respect, we see generative AI tools as an alternative when the modeling operations are not available to train traditional IMAs specifically conceived to support industrial practitioners.},
  keywords={Data privacy;Accuracy;Large language models;Training data;Data models;Software;Regulation;Context modeling;Software engineering;Synthetic data},
  doi={},
  ISSN={2643-1572},
  month={Oct},}@ARTICLE{10797720,
  author={},
  journal={IEEE P3119/D7, December 2024}, 
  title={IEEE Draft Standard for the Procurement of Artificial Intelligence and Automated Decision Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-192},
  abstract={The standard helps procurement teams reduce risks in artificial intelligence systems (AIS) by using tailored risk management practices when purchasing AIS. Specific process steps for AIS problem definition, solicitation preparation, vendor and solution evaluation, contract negotiation, and contract monitoring are described. Risk management methodologies that augment customary activities and tasks performed during the procurement life cycle are provided including identifying, analyzing, evaluating, prioritizing, mitigating, and controlling unique AIS risks that can detract from unique AIS benefits. Practical AIS procurement tools and metrics and how procurement teams can use and apply them are also provided. The standard focuses explicitly on AIS risks (when compared to traditional, non-AIS technology) and is designed to address the purchase of commercial AI products and services procured using a formal contract or contract framework. NOTE–AI and ADS are referred to as artificial intelligence systems (AIS) for simplicity.},
  keywords={IEEE Standards;Artificial intelligence;Risk management;Sociotechnical systems;Human factors;Decision making;Computer security;artificial intelligence;automated decision systems;data;data governance;systems;responsible procurement;public interest;procurement;technology;solicitation;tender;impact assessment;due diligence;risks;harms;responsible AI;sociotechnical;high-risk;acquisition;human rights;commercial AI products and services},
  doi={},
  ISSN={},
  month={Dec},}@BOOK{9453355,
  author={Renn, Jürgen},
  booktitle={The Evolution of Knowledge: Rethinking Science for the Anthropocene},
  year={2020},
  volume={},
  number={},
  pages={},
  abstract={A fundamentally new approach to the history of science and technologyThis book presents a new way of thinking about the history of science and technology, one that offers a grand narrative of human history in which knowledge serves as a critical factor of cultural evolution. Jürgen Renn examines the role of knowledge in global transformations going back to the dawn of civilization while providing vital perspectives on the complex challenges confronting us today in the Anthropocene—this new geological epoch shaped by humankind.Renn reframes the history of science and technology within a much broader history of knowledge, analyzing key episodes such as the evolution of writing, the emergence of science in the ancient world, the Scientific Revolution of early modernity, the globalization of knowledge, industrialization, and the profound transformations wrought by modern science. He investigates the evolution of knowledge using an array of disciplines and methods, from cognitive science and experimental psychology to earth science and evolutionary biology. The result is an entirely new framework for understanding structural changes in systems of knowledge—and a bold new approach to the history and philosophy of science.Written by one of today's preeminent historians of science, The Evolution of Knowledge features discussions of historiographical themes, a glossary of key terms, and practical insights on global issues ranging from climate change to digital capitalism. This incisive book also serves as an invaluable introduction to the history of knowledge.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691185675},
  url={https://ieeexplore.ieee.org/document/9453355},}@ARTICLE{10845850,
  author={Shi, Yuhong and Liu, Jianyi and Sun, Lihang and Zheng, Xinhu},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={E2BA: Environment Exploration and Backtracking Agent for Visual Language Object Navigation}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Robot navigation in an unknown environment is a challenging task, due to the lack of spatial awareness and semantic understanding of the environment. Previous works predominantly relied on prior scene knowledge and semantic information, lacking generalization and transferability. This paper proposes an environment exploration and backtracking agent (E2BA) for visual language object navigation, which leverages the rich semantic prior knowledge and commonsense reasoning of large language models (LLMs) to explore the environment and find the object. By fusing LLM scores and spatial geometric costs using particle filters, we select a redefined optimal frontier as sub-goal for environment exploration. To avoid redundant exploration and paths, we design a backtracking discriminator to evaluate the state of the agent and determine the timing of backtracking triggering through a double-level cascade mechanism. Additionally, we design a random instruction fuzzy semantic guessing task to verify the application diversity of this method. Comprehensive experiments on the Habitat-Matterport 3D dataset show that our method achieves a success rate of 0.704, which is higher than the existing baseline method. This study explores the potential application of LLMs in environment exploration without the need for additional training and semantic supplementation.},
  keywords={Navigation;Visualization;Semantics;Backtracking;Robots;Planning;Costs;Commonsense reasoning;Circuits and systems;Training;Vision language object navigation;environment exploration;backtracking;large language models},
  doi={10.1109/TCSVT.2025.3531410},
  ISSN={1558-2205},
  month={},}@ARTICLE{10059007,
  author={Hua, Guang and Teoh, Andrew Beng Jin and Xiang, Yong and Jiang, Hao},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Unambiguous and High-Fidelity Backdoor Watermarking for Deep Neural Networks}, 
  year={2024},
  volume={35},
  number={8},
  pages={11204-11217},
  abstract={The unprecedented success of deep learning could not be achieved without the synergy of big data, computing power, and human knowledge, among which none is free. This calls for the copyright protection of deep neural networks (DNNs), which has been tackled via DNN watermarking. Due to the special structure of DNNs, backdoor watermarks have been one of the popular solutions. In this article, we first present a big picture of DNN watermarking scenarios with rigorous definitions unifying the black- and white-box concepts across watermark embedding, attack, and verification phases. Then, from the perspective of data diversity, especially adversarial and open set examples overlooked in the existing works, we rigorously reveal the vulnerability of backdoor watermarks against black-box ambiguity attacks. To solve this problem, we propose an unambiguous backdoor watermarking scheme via the design of deterministically dependent trigger samples and labels, showing that the cost of ambiguity attacks will increase from the existing linear complexity to exponential complexity. Furthermore, noting that the existing definition of backdoor fidelity is solely concerned with classification accuracy, we propose to more rigorously evaluate fidelity via examining training data feature distributions and decision boundaries before and after backdoor embedding. Incorporating the proposed prototype guided regularizer (PGR) and fine-tune all layers (FTAL) strategy, we show that backdoor fidelity can be substantially improved. Experimental results using two versions of the basic ResNet18, advanced wide residual network (WRN28_10) and EfficientNet-B0, on MNIST, CIFAR-10, CIFAR-100, and FOOD-101 classification tasks, respectively, illustrate the advantages of the proposed method.},
  keywords={Watermarking;Training;Deep learning;Closed box;Training data;Neural networks;Computational modeling;Backdoor watermarking;black-box ambiguity attack;high-fidelity deep neural network (DNN) watermarking;neural network watermarking;unambiguous verification},
  doi={10.1109/TNNLS.2023.3250210},
  ISSN={2162-2388},
  month={Aug},}@ARTICLE{10041909,
  author={Ghosh, Subhadip and Zaboli, Aydin and Hong, Junho and Kwon, Jaerock},
  journal={IEEE Access}, 
  title={An Integrated Approach of Threat Analysis for Autonomous Vehicles Perception System}, 
  year={2023},
  volume={11},
  number={},
  pages={14752-14777},
  abstract={Automated vehicles are a revolutionary step in mobility, providing a safe and convenient riding experience while keeping the human-driving task minimal to none. Therefore, these intelligent vehicles are equipped with sophisticated perception sensors (e.g., cameras and radars), high-performance computers, artificial intelligence (AI)-driven algorithms, and connectivity with other internet-of-things (IoT) devices. This makes autonomous vehicles (AVs) a special kind of cyber-physical system (CPS) that is moving at speed in highly interactive and dynamic environments (e.g., public roads). Thus, AV is a potential target for cyber attackers to weaponize, compromising safety and mobility on the road. The first step in addressing this problem is to have a robust threat modeling framework that can address the evolving cyber-physical threats, especially to AV applications. In this regard, two areas are studied in this paper: the common practice of threat modeling in automotive and the ISO/SAE 21434 standard, and sensors and machine learning (ML) algorithms for AV perception systems and potential cyber-physical attacks. A comparative threat analysis for an AV perception system with the ISO/SAE 21434 standard and a system-theoretic process analysis for security (STPA-Sec) approach is also demonstrated in this paper. Based on the analysis, this paper proposes a robust threat analysis and risk assessment framework with mathematical modeling to identify cyber-physical threats to AV perception systems that are critical for the driving behaviors and complex interactions of AVs in their operational design domain.},
  keywords={Threat modeling;Computer security;Sensors;Automotive engineering;Connected vehicles;Cyber-physical systems;Autonomous systems;Threat modeling;Threat modeling;cyber-physical system;autonomous and connected vehicles;perception;object classification;cyber-security},
  doi={10.1109/ACCESS.2023.3243906},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10667520,
  author={Tu, Lan and Lv, Shuojun and Shi, Hui},
  booktitle={2024 5th International Conference on Information Science, Parallel and Distributed Systems (ISPDS)}, 
  title={Research on the Application of Natural Language Processing Technology in Medicine Patent Retrieval}, 
  year={2024},
  volume={},
  number={},
  pages={176-181},
  abstract={In view of the high professionalism and complexity of medicine patent text, traditional patent retrieval mainly relies on manual operation and simple text Processing methods, this paper aims to use Natural Language Processing (NLP) technology to efficiently obtain valuable information in medicine patent retrieval. To transform unstructured text into meaningful information. Through in-depth research on the characteristics and structure of medicine patent text, a new structured feature extraction algorithm of patent text is proposed, and a patent retrieval system based on NLP is developed, which can carry out instant structured processing, and can automatically identify and translate text into corresponding patented medicines, and nearly 300 medicine patent abstracts are tested by experiment. The system performance is good. The system achieves the expected effect.},
  keywords={Training;Patents;Adaptation models;Transfer learning;Transforms;Search problems;Natural language processing;medicine patent retrieval;Natural language processing technology;Patent text structuring;Feature extraction algorithm},
  doi={10.1109/ISPDS62779.2024.10667520},
  ISSN={},
  month={May},}@INPROCEEDINGS{10885734,
  author={Chaudhuri, Arunava and Shukla, Shubhi and Bhattacharya, Sarani and Mukhopadhyay, Debdeep},
  booktitle={2025 17th International Conference on COMmunication Systems and NETworks (COMSNETS)}, 
  title={Secured and Privacy-Preserving GPU-Based Machine Learning Inference in Trusted Execution Environment: A Comprehensive Survey}, 
  year={2025},
  volume={},
  number={},
  pages={207-216},
  abstract={With the rapid advancement of machine learning (ML) models and their widespread application across various sectors such as intrusion detection, medical diagnosis, natural language processing, and autonomous driving, these technologies have achieved remarkable success. However, this progress has also raised significant concerns about ensuring the security of ML models and protecting both private training data and model outputs from getting exposed in a shared cloud environment. To address these challenges, researchers have proposed various methodologies to create privacy-preserving, secure, and trustworthy model execution environments to prevent adversarial attacks. This study provides a comprehensive review of Trusted Execution Environment (TEE) implementations across different hardware accelerators. It also offers an overview of modern techniques for preserving privacy and security in execution environments, while identifying critical research gaps that require attention. In essence, this survey is an important resource for researchers, providing insights into recent methodologies and guiding them to focus on pressing research challenges.},
  keywords={Surveys;Deep learning;Training;Privacy;Data privacy;Training data;Data models;Stability analysis;Security;Hardware acceleration;security;privacy;trusted execution environment;GPU;deep learning},
  doi={10.1109/COMSNETS63942.2025.10885734},
  ISSN={2155-2509},
  month={Jan},}@ARTICLE{9924181,
  author={Battah, Ammar and Madine, Mohammad and Yaqoob, Ibrar and Salah, Khaled and Hasan, Haya R. and Jayaraman, Raja},
  journal={IEEE Access}, 
  title={Blockchain and NFTs for Trusted Ownership, Trading, and Access of AI Models}, 
  year={2022},
  volume={10},
  number={},
  pages={112230-112249},
  abstract={The demand for high-quality Artificial Intelligence (AI) models is ever-increasing in this digital era. However, most of the existing methods leveraged for managing the ownership, trading, and access of AI models fall short of providing traceability, transparency, audit, security, and trustful features. In this paper, we propose a solution based on blockchain and Non-fungible Tokens (NFTs) to manage ownership rights and exchange of AI models in a transparent, traceable, auditable, secure, and trustworthy manner. Smart contracts are employed to enforce ownership, ease of access, and exchange policies for the unique NFT linked to an AI model. We use decentralized storage of the InterPlanetary File System (IPFS) and proxy re-encryption oracles to securely fetch, store, and share data related to AI models. We present algorithms along with their implementation, testing, and validation details. The proposed solution is evaluated using cost and security analyses to show its affordability and resiliency against security threats and attacks. All smart contract codes are made publicly available on GitHub.},
  keywords={Artificial intelligence;Data models;Collaboration;Security;Computational modeling;Smart contracts;Data privacy;Blockchains;Nonfungible tokens;Blockchain;decentralized storage;non-fungible tokens (NFTs);oracles;provenance;proxy re-encryption;smart contracts},
  doi={10.1109/ACCESS.2022.3215660},
  ISSN={2169-3536},
  month={},}@ARTICLE{10781408,
  author={Afrin, Sadia and Roksana, Shobnom and Akram, Riad},
  journal={IEEE Access}, 
  title={AI-Enhanced Robotic Process Automation: A Review of Intelligent Automation Innovations}, 
  year={2025},
  volume={13},
  number={},
  pages={173-197},
  abstract={The rapid technological growth in recent decades due to the integration of robust technologies and automation have led to the rise of digital services and the emergence of Industry 4.0. This paper explores the concept and potential of AI-powered intelligent automation based on the synergistic use of Robotic Process Automation (RPA) and Artificial Intelligence (AI) to enhance organizational and business processes across various sectors. RPA automates routine, rules-based tasks, thereby allowing human workers to engage in more innovative activities. When integrated with AI, RPA systems gain the capacity to analyze data, identify patterns, classify information and forecast which leads to significant improvement in accuracy and productivity. This literature review investigates the current state of RPA and AI integration while highlighting its applications in different sectors such as manufacturing, agriculture, healthcare, finance, and retail. Along with discussing the drawbacks and restrictions, such as technological issues and moral dilemmas, this paper also discusses the advantages of this integration, which include decreased costs, increased output, and simplified operations. By leveraging AI techniques such as classification, text mining of neural network, RPA technologies optimize business operations and advance Industry 4.0. This study also illustrates the challenges and limitations of this integration such as technical difficulties and ethical considerations. The aim of this review is to provide a comprehensive understanding of the synergistic potential of RPA and AI while offering insights into their contribution in shaping the future of intelligent automation.},
  keywords={Artificial intelligence;Automation;Business;Companies;Industries;Robot kinematics;Recording;Productivity;Ethics;Analytical models;Artificial intelligence (AI);business process;intelligent process automation (IPA);robotic process automation (RPA)},
  doi={10.1109/ACCESS.2024.3513279},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10510680,
  author={Zaman, Safaa},
  booktitle={7th IET Smart Cities Symposium (SCS 2023)}, 
  title={ChatGPT security risks and solutions}, 
  year={2023},
  volume={2023},
  number={},
  pages={378-387},
  abstract={As artificial intelligence (AI) technologies become more popular in enterprise environments, special chatbot technologies such as ChatGPT are gaining popularity due to their abilities, services, and benefits. However, these chatbots suffer from significant security risks and vulnerabilities that need to be addressed, such as confidentiality weaknesses, authentication issues, chatbot breaches, data poisoning, and others. This article identifies the potential ChatGPT risks and threats and proposes solutions and remedies to address the security issues of the chatbots risks, such as calling for policy changes, proposing training, equipping the teams with AI technology and tools, implementing strong access controls, using secure servers and storage systems, limiting access to systems, supporting a well-defined incident response process, and calling for government oversight to ensure AI usage doesn't become detrimental to cybersecurity efforts. Ending with future directions and recommendations to protect the use of this technology and guide future research and practice in this field.},
  keywords={},
  doi={10.1049/icp.2024.0955},
  ISSN={},
  month={Dec},}@ARTICLE{9000519,
  author={Oulasvirta, Antti and Dayama, Niraj Ramesh and Shiripour, Morteza and John, Maximilian and Karrenbauer, Andreas},
  journal={Proceedings of the IEEE}, 
  title={Combinatorial Optimization of Graphical User Interface Designs}, 
  year={2020},
  volume={108},
  number={3},
  pages={434-464},
  abstract={The graphical user interface (GUI) has become the prime means for interacting with computing systems. It leverages human perceptual and motor capabilities for elementary tasks such as command exploration and invocation, information search, and multitasking. For designing a GUI, numerous interconnected decisions must be made such that the outcome strikes a balance between human factors and technical objectives. Normally, design choices are specified manually and coded within the software by professional designers and developers. This article surveys combinatorial optimization as a flexible and powerful tool for computational generation and adaptation of GUIs. As recently as 15 years ago, applications were limited to keyboards and widget layouts. The obstacle has been the mathematical definition of design tasks, on the one hand, and the lack of objective functions that capture essential aspects of human behavior, on the other. This article presents definitions of layout design problems as integer programming tasks, a coherent formalism that permits identification of problem types, analysis of their complexity, and exploitation of known algorithmic solutions. It then surveys advances in formulating evaluative functions for common design-goal foci such as user performance and experience. The convergence of these two advances has expanded the range of solvable problems. Approaches to practical deployment are outlined with a wide spectrum of applications. This article concludes by discussing the position of this application area within optimization and human-computer interaction research and outlines challenges for future work.},
  keywords={Grahical user interfaces;Task analysis;Linear programming;Multitasking;Human factors;Adversarial machine learning;Learning systems;Human computer interaction;Human factors;Computer applications;Combinatorial optimization;computational design;graphical user interfaces (GUIs);human–computer interaction (HCI);integer programming;interactive optimization;meta-heuristic optimization},
  doi={10.1109/JPROC.2020.2969687},
  ISSN={1558-2256},
  month={March},}@INPROCEEDINGS{10744490,
  author={Yang, Jing and Xi, Runping and Lai, Yingxin and Lin, Xun and Yu, Zitong},
  booktitle={2024 IEEE International Joint Conference on Biometrics (IJCB)}, 
  title={DDAP: Dual-Domain Anti-Personalization against Text-to-Image Diffusion Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={Diffusion-based personalized visual content generation technologies have achieved significant breakthroughs, allowing for the creation of specific objects by just learning from a few reference photos. However, when misused to fabricate fake news or unsettling content targeting individuals, these technologies could cause considerable societal harm. To address this problem, current methods generate adversarial samples by adversarially maximizing the training loss, thereby disrupting the output of any personalized generation model trained with these samples. However, the existing methods fail to achieve effective defense and maintain stealthiness, as they overlook the intrinsic properties of diffusion models. In this paper, we introduce a novel Dual-Domain Anti-Personalization framework (DDAP). Specifically, we have developed Spatial Perturbation Learning (SPL) by exploiting the fixed and perturbation-sensitive nature of the image encoder in personalized generation. Subsequently, we have designed a Frequency Perturbation Learning (FPL) method that utilizes the characteristics of diffusion models in the frequency domain. The SPL disrupts the overall texture of the generated images, while the FPL focuses on image details. By alternating between these two methods, we construct the DDAP framework, effectively harnessing the strengths of both domains. To further enhance the visual quality of the adversarial samples, we design a localization module to accurately capture attentive areas while ensuring the effectiveness of the attack and avoiding unnecessary disturbances in the background. Extensive experiments on facial benchmarks have shown that the proposed DDAP enhances the disruption of personalized generation models while also maintaining high quality in adversarial samples, making it more effective in protecting privacy in practical applications.},
  keywords={Location awareness;Training;Biometrics;Visualization;Privacy;Frequency-domain analysis;Perturbation methods;Text to image;Diffusion models;Fake news},
  doi={10.1109/IJCB62174.2024.10744490},
  ISSN={2474-9699},
  month={Sep.},}@BOOK{9453333,
  author={Vertesi, Janet and Ribes, David and Forlano, Laura and Camus, Alexandre and Vinck, Dominique and Ribes, David and Calvillo, Nerea and Rosner, Daniela K. and Dunbar-Hester, Christina and Kerasidou, Xaroula (Charalampia) and Stark, Luke and Couture, Stéphane and Jackson, Steven J. and Chan, Anita Say and Hawthorne, Camilla A. and Ilten, Carla and McInerney, Paul-Brian and Nemer, David and Chirumamilla, Padma and Poster, Winifred R. and Sawyer, Steve and Erickson, Ingrid and Jarrahi, Mohammad Hossein and Singh, Ranjit and Hesselbein, Chris and Price, Jessica and Lynch, Michael and Parmiggiani, Elena and Monteiro, Eric and Allhutter, Doris and Winthereik, Brit Ross and Maguire, James and Watts, Laura and DiSalvo, Carl and Vertesi, Janet and Latzko-Toth, Guillaume and Söderberg, Johan and Millerand, Florence and Jones, Steve and Seaver, Nick and Cohn, Marisa Leavitt and Loukissas, Yanni and Llach, Daniel Cardoso and Munk, Anders Kristian and Meunier, Axel and Venturini, Tommaso and Salamanca, Juan and Jacomy, Mathieu},
  booktitle={digitalSTS: A Field Guide for Science & Technology Studies},
  year={2019},
  volume={},
  number={},
  pages={},
  abstract={New perspectives on digital scholarship that speak to today's computational realities Scholars across the humanities, social sciences, and information sciences are grappling with how best to study virtual environments, use computational tools in their research, and engage audiences with their results. Classic work in science and technology studies (STS) has played a central role in how these fields analyze digital technologies, but many of its key examples do not speak to today’s computational realities. This groundbreaking collection brings together a world-class group of contributors to refresh the canon for contemporary digital scholarship.In twenty-five pioneering and incisive essays, this unique digital field guide offers innovative new approaches to digital scholarship, the design of digital tools and objects, and the deployment of critically grounded technologies for analysis and discovery. Contributors cover a broad range of topics, including software development, hackathons, digitized objects, diversity in the tech sector, and distributed scientific collaborations. They discuss methodological considerations of social networks and data analysis, design projects that can translate STS concepts into durable scientific work, and much more.Featuring a concise introduction by Janet Vertesi and David Ribes and accompanied by an interactive microsite, this book provides new perspectives on digital scholarship that will shape the agenda for tomorrow’s generation of STS researchers and practitioners.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691190600},
  url={https://ieeexplore.ieee.org/document/9453333},}@INPROCEEDINGS{10397662,
  author={Jha, Ravi Shankar and Sahoo, Priti Ranjan and Mahapatra, Biplab and Mohapatra, Alivarani and Rajni},
  booktitle={2023 6th International Conference on Contemporary Computing and Informatics (IC3I)}, 
  title={Transforming the Insurance Landscape: How ChatGPT's Proficiency Empowers the Industry}, 
  year={2023},
  volume={6},
  number={},
  pages={1265-1270},
  abstract={The release of the ChatGPT by OpenAI in 2022 generated interest in academia and industry, with mixed reactions to its capabilities and ethical concerns. Numerous studies have explored its accuracy and adaptability in several industries, such as healthcare, education, finance, and atmospheric science. Still, its applicability in the insurance industry has yet to be comprehensively analyzed. The insurance industry protects financially against risks like property damage, accidents, and health issues. Insurance companies offer policies to individuals and businesses; policyholders pay premiums to cover potential losses. The industry relies on efficient communication, accurate information processing, and risk assessment to effectively underwrite policies and manage claims. In summary, the ChatGPT has the potential to become a game-changing tool in the Insurance industry, driving innovation in customer interactions, process automation, and risk management. As the technology continues to evolve, its' potential to reshape insurers' operations and customer service in the digital era is immense.},
  keywords={Industries;Data integrity;Insurance;Machine learning;Chatbots;Risk management;Faces;ChatGPT;Insurance;Innovation;Artificial Intelligence;Chatbot;Regulation;Data;OpenAI;Large Language Model},
  doi={10.1109/IC3I59117.2023.10397662},
  ISSN={},
  month={Sep.},}@ARTICLE{10601235,
  author={Chen, Ying-Lin and Sacchi, Sara and Dey, Bappaditya and Blanco, Victor and Halder, Sandip and Leray, Philippe and Gendt, Stefan De},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Exploring Machine Learning for Semiconductor Process Optimization: A Systematic Review}, 
  year={2024},
  volume={5},
  number={12},
  pages={5969-5989},
  abstract={As machine learning (ML) continues to find applications, extensive research is currently underway across various domains. This study examines the current methodologies of ML being investigated to optimize semiconductor manufacturing processes. Our research involved searching the SPIE Digital Library, IEEE Xplore, and ArXiv databases, identifying 58 publications in the field of ML-based semiconductor process optimization. These investigations employ ML techniques such as feature extraction, feature selection, and neural network architecture are analyzed using different algorithms. These models find applications in advanced process control, virtual metrology, and quality control, critical aspects in semiconductor manufacturing for enhancing throughput and reducing production costs. We categorize the articles based on the methods and applications employed, summarizing the primary findings. Furthermore, we discuss the general conclusion of several studies. Overall, the reviewed literature suggests that ML-based semiconductor manufacturing is rapidly gaining popularity and advancing at a swift pace.},
  keywords={Machine learning;Process control;Metrology;Data models;Semiconductor device manufacture;Reviews;Semiconductor process modeling;Advanced process control (APC);artificial intelligence (AI);chemical mechanical polishing (CMP);deep learning;etching;lithography;machine learning (ML);neural networks (NNs);predictive metrology (PM);root cause analysis (RCA);scatterometry;semiconductor manufacturing;semiconductor process optimization;thin film;virtual metrology (VM)},
  doi={10.1109/TAI.2024.3429479},
  ISSN={2691-4581},
  month={Dec},}@ARTICLE{10900388,
  author={Bian, Yiming and Somani, Arun K.},
  journal={IEEE Access}, 
  title={CQS-Attention: Scaling Up the Standard Attention Computation for Infinitely Long Sequences}, 
  year={2025},
  volume={13},
  number={},
  pages={35527-35538},
  abstract={Transformer models suffer from unaffordable high memory consumption when the sequence is long and standard self-attention is utilized. We developed a sequence parallelism scheme called CQS-Attention that can break the limit of sequence length. A long sequence is divided into multiple overlapping subsequences. The attention of each subsequence is independently computed and gathered as the final exact attention of the original long sequence. CQS-Attention is a fork-join parallel model comprising three components: Scheduler, Workers, and Tiler. The Scheduler equally partitions computation responsibility in a completely mutually exclusive manner and ensures the local subsequence length is minimum. Each worker independently computes the standard attention of the assigned subsequence and transfers local results to the Tiler, which produces the final attention. CQS-Attention makes attention computation embarrassingly parallel. Hence, it enjoys great performance regarding single-device memory and computation time consumption, mathematical stability and scalability. More importantly, it is fully compatible with all state-of-the-art attention optimizations. Our code and supplementary information (SI) are available at https://github.com/CQS-Attention/CQS_Attention.},
  keywords={Standards;Parallel processing;Memory management;Transformers;Indexes;Lower bound;Stability analysis;Scalability;Reproducibility of results;Proposals;Attention computation;cyclic quorum sets;parallel algorithm;transformer},
  doi={10.1109/ACCESS.2025.3544550},
  ISSN={2169-3536},
  month={},}@ARTICLE{10648646,
  author={Zhang, Chengyang and Zhang, Yong and Shao, Qitan and Feng, Jiangtao and Li, Bo and Lv, Yisheng and Piao, Xinglin and Yin, Baocai},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={BjTT: A Large-Scale Multimodal Dataset for Traffic Prediction}, 
  year={2024},
  volume={25},
  number={11},
  pages={18992-19003},
  abstract={Traffic prediction plays a significant role in Intelligent Transportation Systems (ITS). Although many datasets have been introduced to support the study of traffic prediction, most of them only provide time-series traffic data. However, urban transportation systems are always susceptible to various factors, including unusual weather and traffic accidents. Therefore, relying solely on historical data for traffic prediction greatly limits the accuracy of the prediction. In this paper, we introduce Beijing Text-Traffic (BjTT), a large-scale multimodal dataset for traffic prediction. BjTT comprises over 32,000 time-series traffic records, capturing velocity and congestion levels on more than 1,200 roads within the 5th ring area of Beijing. Meanwhile, each piece of traffic data is coupled with a text describing the traffic system (including time, location, and events). We detail the data collection and processing procedures and present a statistical analysis of the BjTT dataset. Furthermore, we conduct comprehensive experiments on the dataset with state-of-the-art traffic prediction methods and text-guided generative models, which reveal the unique characteristics of the BjTT. The dataset is available at https://github.com/ChyaZhang/BjTT.},
  keywords={Roads;Social networking (online);Data collection;Blogs;Meteorology;Traffic control;Predictive models;Large scale integration;Intelligent transportation systems;Traffic prediction;large-scale;new dataset},
  doi={10.1109/TITS.2024.3440650},
  ISSN={1558-0016},
  month={Nov},}@ARTICLE{10342565,
  author={Ojeda, Alejandro and Kreutz-Delgado, Kenneth and Mishra, Jyoti},
  journal={Neural Computation}, 
  title={Bridging M/EEG Source Imaging and Independent Component Analysis Frameworks Using Biologically Inspired Sparsity Priors}, 
  year={2021},
  volume={33},
  number={9},
  pages={2408-2438},
  abstract={Electromagnetic source imaging (ESI) and independent component analysis (ICA) are two popular and apparently dissimilar frameworks for M/EEG analysis. This letter shows that the two frameworks can be linked by choosing biologically inspired source sparsity priors. We demonstrate that ESI carried out by the sparse Bayesian learning (SBL) algorithm yields source configurations composed of a few active regions that are also maximally independent from one another. In addition, we extend the standard SBL approach to source imaging in two important directions. First, we augment the generative model of M/EEG to include artifactual sources. Second, we modify SBL to allow for efficient model inversion with sequential data. We refer to this new algorithm as recursive SBL (RSBL), a source estimation filter with potential for online and offline imaging applications. We use simulated data to verify that RSBL can accurately estimate and demix cortical and artifactual sources under different noise conditions. Finally, we show that on real error-related EEG data, RSBL can yield single-trial source estimates in agreement with the experimental literature. Overall, by demonstrating that ESI can produce maximally independent sources while simultaneously localizing them in cortical space, we bridge the gap between the ESI and ICA frameworks for M/EEG analysis.},
  keywords={},
  doi={10.1162/neco_a_01415},
  ISSN={0899-7667},
  month={Aug},}@ARTICLE{10314044,
  author={Cano-Marin, Enrique and Sánchez-Alonso, Salvador and Mora-Cantallops, Marçal},
  journal={IEEE Transactions on Engineering Management}, 
  title={Unleashing Competitive Intelligence: News Mining Analysis on Technology Trends and Digital Health Driving Healthcare Innovation}, 
  year={2024},
  volume={71},
  number={},
  pages={12311-12325},
  abstract={In the rapidly evolving digital health landscape, technology plays a pivotal role in transforming the healthcare industry. With the exponential growth of data, uncovering valuable insights has become a daunting task. In today's data-driven world, healthcare businesses must leverage emerging technologies to stay informed about trends in their field. This research article presents a novel approach to deriving business insights in digital health enabled by technology, including artificial intelligence, and other cutting-edge advancements. We propose a methodology that utilizes news mining techniques and the global data on events, location, and tone database as the primary data source. By employing natural language processing, we developed a practical way of extracting relevant insights from vast amounts of public data. We implemented named-entity recognition (NER) enriched with the DBpedia knowledge base and relationship extraction. In addition, we leveraged graph analytics to identify and analyze the most significant concept relationships within the text corpus and their evolution in time. By integrating these advanced techniques, healthcare businesses can extract actionable insights from public datasets, empowering them to stay abreast of emerging trends and advancements in digital health, such as telehealth, precision medicine, or medical imaging.},
  keywords={Medical services;Data mining;Electronic healthcare;Market research;Telemedicine;Databases;Digital health;emerging technologies;entity extraction;graph analytics;healthcare;innovation;natural language processing (NLP);news mining},
  doi={10.1109/TEM.2023.3326233},
  ISSN={1558-0040},
  month={},}@ARTICLE{8527529,
  author={Musumeci, Francesco and Rottondi, Cristina and Nag, Avishek and Macaluso, Irene and Zibar, Darko and Ruffini, Marco and Tornatore, Massimo},
  journal={IEEE Communications Surveys & Tutorials}, 
  title={An Overview on Application of Machine Learning Techniques in Optical Networks}, 
  year={2019},
  volume={21},
  number={2},
  pages={1383-1408},
  abstract={Today's telecommunication networks have become sources of enormous amounts of widely heterogeneous data. This information can be retrieved from network traffic traces, network alarms, signal quality indicators, users' behavioral data, etc. Advanced mathematical tools are required to extract meaningful information from these data and take decisions pertaining to the proper functioning of the networks from the network-generated data. Among these mathematical tools, machine learning (ML) is regarded as one of the most promising methodological approaches to perform network-data analysis and enable automated network self-configuration and fault management. The adoption of ML techniques in the field of optical communication networks is motivated by the unprecedented growth of network complexity faced by optical networks in the last few years. Such complexity increase is due to the introduction of a huge number of adjustable and interdependent system parameters (e.g., routing configurations, modulation format, symbol rate, coding schemes, etc.) that are enabled by the usage of coherent transmission/reception technologies, advanced digital signal processing, and compensation of nonlinear effects in optical fiber propagation. In this paper we provide an overview of the application of ML to optical communications and networking. We classify and survey relevant literature dealing with the topic, and we also provide an introductory tutorial on ML for researchers and practitioners interested in this field. Although a good number of research papers have recently appeared, the application of ML to optical networks is still in its infancy: to stimulate further work in this area, we conclude this paper proposing new possible research directions.},
  keywords={Optical fiber networks;Adaptive optics;Nonlinear optics;Artificial neural networks;Machine learning;Optical modulation;Machine learning;data analytics;optical communications and networking;neural networks;bit error rate;optical signal-to-noise ratio;network monitoring},
  doi={10.1109/COMST.2018.2880039},
  ISSN={1553-877X},
  month={Secondquarter},}@ARTICLE{9398613,
  author={},
  journal={Ethically Aligned Design - A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems}, 
  title={Ethically Aligned Design - A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems}, 
  year={2019},
  volume={},
  number={},
  pages={1-294},
  abstract={As the use and impact of autonomous and intelligent systems (A/IS) become pervasive, we need to establish societal and policy guidelines in order for such systems to remain human-centric, serving humanity’s values and ethical principles. These systems must be developed and should operate in a way that is beneficial to people and the environment, beyond simply reaching functional goals and addressing technical problems. This approach will foster the heightened level of trust between people and technology that is needed for its fruitful use in our daily lives.},
  keywords={autonomous and intelligent systems;A/IS;human-centric;intelligent technical systems;Ethics of Autonomous and Intelligent Systems;Ethically Aligned Design;EAD1e;Personal Data and Individual Agency},
  doi={},
  ISSN={},
  month={March},}@INBOOK{9394882,
  author={Bown, Oliver},
  booktitle={Beyond the Creative Species: Making Machines That Make Art and Music}, 
  title={8 Speculative Futures}, 
  year={2021},
  volume={},
  number={},
  pages={295-322},
  abstract={So fast moving is this area, and so muddied by various forms of hype and emotional responses, that speculating about where we might end up even in a couple of years, let alone a decade or so, is a risky activity. But speculation is not prediction, and certainly not prophecy. It serves to point to and examine different possible futures, to allow discussion of where we want to go, what might need to be done to get there, and what we might want to look out for. I draw on the topics and examples discussed so far to analyze such possibilities and to frame the most prominent themes appearing in the emerging future of computational creativity. Most of the trends imagined below are already apparent and have already been identified in the preceding chapters, and are merely being extrapolated given what we can see the technology capable of, framed from the perspective of distributed, networked creativity. I divide this concluding chapter into three areas: industry, covering the nature of creative production and the use of commercially developed computationally creative systems; society, covering the impact on cultural production and social behavior; and lastly, the most speculative and philosophical topic, the question of how machine creativity may cause us to reflect on our status as intelligent and creative beings.},
  keywords={},
  doi={},
  ISSN={},
  publisher={MIT Press},
  isbn={9780262361750},
  url={https://ieeexplore.ieee.org/document/9394882},}@INBOOK{9226791,
  author={Petrenko, Sergei},
  booktitle={Cyber Security Innovation for the Digital Economy: A Case Study of the Russian Federation}, 
  title={1 Relevance of Cyber Security Innovations}, 
  year={2018},
  volume={},
  number={},
  pages={7-162},
  abstract={Cyber Security Innovation for the Digital Economy considers possible solutions to the relatively new scientific-technical problem of developing innovative solutions in the field of cyber security for the Digital Economy. The solutions proposed are based on the results of exploratory studies conducted by the author in the areas of Big Data acquisition, cognitive information technologies (cogno-technologies), new methods of analytical verification of digital ecosystems on the basis of similarity invariants and dimensions, and “computational cognitivism,” involving a number of existing models and methods. In practice, this successfully allowed the creation of new entities - the required safe and trusted digital ecosystems - on the basis of the development of digital and cyber security technologies, and the resulting changes in their behavioral preferences. Here, the ecosystem is understood as a certain system of organizations, created around a certain Technological Platform that use its services to make the best offers to customers and access to them to meet the ultimate needs of clients - legal entities and individuals. The basis of such ecosystems is a certain technological platform, created on advanced innovative developments, including the open interfaces and code, machine learning, cloud technologies, Big Data collection and processing, artificial intelligence technologies, etc. The mentioned Technological Platform allows creating the best offer for the client both from own goods and services and from the offers of external service providers in real time. This book contains four chapters devoted to the following subjects: • Relevance of the given scientific-technical problems in the cybersecurity of Digital Economy • Determination of the limiting capabilities • Possible scientific and technical solutions • Organization of perspective research studies in the area of Digital Economy cyber security in Russia.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770220217},
  url={https://ieeexplore.ieee.org/document/9226791},}@ARTICLE{10155147,
  author={Shahriar, Sakib and Allana, Sonal and Hazratifard, Seyed Mehdi and Dara, Rozita},
  journal={IEEE Access}, 
  title={A Survey of Privacy Risks and Mitigation Strategies in the Artificial Intelligence Life Cycle}, 
  year={2023},
  volume={11},
  number={},
  pages={61829-61854},
  abstract={Over the decades, Artificial Intelligence (AI) and machine learning has become a transformative solution in many sectors, services, and technology platforms in a wide range of applications, such as in smart healthcare, financial, political, and surveillance systems. In such applications, a large amount of data is generated about diverse aspects of our life. Although utilizing AI in real-world applications provides numerous opportunities for societies and industries, it raises concerns regarding data privacy. Data used in an AI system are cleaned, integrated, and processed throughout the AI life cycle. Each of these stages can introduce unique threats to individual’s privacy and have an impact on ethical processing and protection of data. In this paper, we examine privacy risks in different phases of the AI life cycle and review the existing privacy-enhancing solutions. We introduce four different categories of privacy risk, including (i) risk of identification, (ii) risk of making an inaccurate decision, (iii) risk of non-transparency in AI systems, and (iv) risk of non-compliance with privacy regulations and best practices. We then examined the potential privacy risks in each AI life cycle phase, evaluated concerns, and reviewed privacy-enhancing technologies, requirements, and process solutions to countermeasure these risks. We also reviewed some of the existing privacy protection policies and the need for compliance with available privacy regulations in AI-based systems. The main contribution of this survey is examining privacy challenges and solutions, including technology, process, and privacy legislation in the entire AI life cycle. In each phase of the AI life cycle, open challenges have been identified.},
  keywords={Artificial intelligence;Privacy;Data privacy;Surveys;Regulation;Machine learning;Security;Artificial intelligence;machine learning;AI life cycle;privacy risk;privacy legislation;privacy enhancing solutions},
  doi={10.1109/ACCESS.2023.3287195},
  ISSN={2169-3536},
  month={},}@ARTICLE{9447833,
  author={Rodríguez, Eva and Otero, Beatriz and Gutiérrez, Norma and Canal, Ramon},
  journal={IEEE Communications Surveys & Tutorials}, 
  title={A Survey of Deep Learning Techniques for Cybersecurity in Mobile Networks}, 
  year={2021},
  volume={23},
  number={3},
  pages={1920-1955},
  abstract={The widespread use of mobile devices, as well as the increasing popularity of mobile services has raised serious cybersecurity challenges. In the last years, the number of cyberattacks has grown dramatically, as well as their complexity. Traditional cybersecurity systems have failed to detect complex attacks, unknown malware, and they do not guarantee the preservation of user privacy. Consequently, cybersecurity systems have embraced Deep Learning (DL) models as they provide efficient detection of novel attacks and better accuracy. This paper presents a comprehensive survey of recent cybersecurity works that use DL in mobile and wireless networks. It covers all cybersecurity aspects: infrastructure threads and attacks, software attacks and privacy preservation. First, we provide a detailed overview of DL techniques applied, or with potential applications, to cybersecurity. Then, we review cybersecurity works based on DL. For each cybersecurity threat or attack, we discuss the challenges for using DL methods. For each contribution, we review the implementation details and the performance of the solution. In a nutshell, this paper constitutes the first survey that provides a complete review of the DL methods for cybersecurity. Given the analysis performed, we identify the most effective DL methods for the different threats and attacks.},
  keywords={Computer security;Malware;Security;Privacy;Computer crime;Software;Computer architecture;Cyberattacks;deep learning;machine learning;mobile networking;privacy;security;wireless networking},
  doi={10.1109/COMST.2021.3086296},
  ISSN={1553-877X},
  month={thirdquarter},}@ARTICLE{10145423,
  author={Yan, Tianyi and Wang, Gongshu and Liu, Tiantian and Li, Guoqi and Wang, Changming and Funahashi, Shintaro and Suo, Dingjie and Pei, Guangying},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title={Effects of Microstate Dynamic Brain Network Disruption in Different Stages of Schizophrenia}, 
  year={2023},
  volume={31},
  number={},
  pages={2688-2697},
  abstract={Schizophrenia is a heterogeneous mental disorder with unknown etiology or pathological characteristics. Microstate analysis of the electroencephalogram (EEG) signal has shown significant potential value for clinical research. Importantly, significant changes in microstate-specific parameters have been extensively reported; however, these studies have ignored the information interactions within the microstate network in different stages of schizophrenia. Based on recent findings, since rich information about the functional organization of the brain can be revealed by functional connectivity dynamics, we use the first-order autoregressive model to construct the functional connectivity of intra- and intermicrostate networks to identify information interactions among microstate networks. We demonstrate that, beyond abnormal parameters, disrupted organization of the microstate networks plays a crucial role in different stages of the disease by 128-channel EEG data collected from individuals with first-episode schizophrenia, ultrahigh-risk, familial high-risk, and healthy controls. According to the characteristics of the microstates of patients at different stages, the parameters of microstate class A are reduced, those of class C are increased, and the transitions from intra- to intermicrostate functional connectivity are gradually disrupted. Furthermore, decreased integration of intermicrostate information might lead to cognitive deficits in individuals with schizophrenia and those in high-risk states. Taken together, these findings illustrate that the dynamic functional connectivity of intra- and intermicrostate networks captures more components of disease pathophysiology. Our work sheds new light on the characterization of dynamic functional brain networks based on EEG signals and provides a new interpretation of aberrant brain function in different stages of schizophrenia from the perspective of microstates.},
  keywords={Electroencephalography;Mental disorders;Time series analysis;Diseases;Brain modeling;Electrodes;Pathology;Dynamic brain network;information interaction;microstates;resting-state EEG;schizophrenia stages},
  doi={10.1109/TNSRE.2023.3283708},
  ISSN={1558-0210},
  month={},}@ARTICLE{10272298,
  author={Alam, Gulzar and McChesney, Ian and Nicholl, Peter and Rafferty, Joseph},
  journal={IEEE Sensors Journal}, 
  title={Open Datasets in Human Activity Recognition Research—Issues and Challenges: A Review}, 
  year={2023},
  volume={23},
  number={22},
  pages={26952-26980},
  abstract={Huge amounts of data are generated with the emergence of new sensor technologies. Human activity recognition (HAR) datasets are generated from cameras, such as video or still images, capturing human behavior through sensors such as gyroscopes, Bluetooth, sound sensors, and accelerometers. These generated data sources are collected by the researchers and formed into open datasets. However, these datasets often show issues during dataset construction, sharing, and searching, which could produce further challenges for the reuse of the data by others. The main objective of this research is to explore the current issues and challenges faced by researchers in the HAR domain. A detail literature review was conducted to extract information from the published literature. Similarly, a questionnaire survey was sent to selected researchers having expertise in the HAR domain, who work with open datasets. The main issues and challenges were identified and classified into a hierarchical structure. This research will help HAR researchers to be aware of the current issues and challenges in the field of HAR open datasets. It will help to promote important attributes applicable to many open datasets, such as privacy, anonymity, platform maintenance, datasets’ descriptions, metadata, environmental conditions, resources, and training, while constructing and sharing new datasets.},
  keywords={Open data;Sensors;Human activity recognition;Surveys;Bibliographies;Medical services;Feature extraction;Artificial intelligence (AI);dataset quality;datasets’ issues and challenges;human activity recognition (HAR);open dataset lifecycle},
  doi={10.1109/JSEN.2023.3317645},
  ISSN={1558-1748},
  month={Nov},}@ARTICLE{10942367,
  author={Saka, Tarini and Vaniea, Kami and Kökciyan, Nadin},
  journal={IEEE Access}, 
  title={SoK: Grouping Spam and Phishing Email Threats for Smarter Security}, 
  year={2025},
  volume={13},
  number={},
  pages={54938-54959},
  abstract={Emails are a vital form of communication, owing to their open nature, which allows any individual to send emails to anyone else without centralized monitoring. While this has facilitated the widespread adoption of email, it has also inadvertently facilitated malicious activities, such as spam and phishing attacks, which pose a serious threat to the security of organizations worldwide. The volume of such emails is growing at an alarming rate, leading to security researchers finding new ways to protect their organizations. To develop effective protection, it’s essential to identify commonalities among emails, such as whether they originate from the same attacker, contain similar wording, or promote nearly identical products. The commonalities used in research to group emails can vary significantly. While the range of research is laudable, the absence of consistent language, datasets, and features can make understanding the results and limitations of this field very challenging. In this systematic literature survey, we looked at 23 research articles on grouping spam and phishing emails, focusing on two foundational aspects (definition of a group and use case) and four methodological aspects (dataset, input features, clustering or grouping algorithms, and evaluation strategies). We propose three definitions of “campaign” representing how researchers approach the groupings: source-based, scam-based, and response-based. Furthermore, we discuss the various features and algorithms that have been utilized in relation to the goals of the researchers and highlight the key takeaways and recommendations for future work.},
  keywords={Phishing;Unsolicited e-mail;Security;Surveys;Botnet;Market research;Malware;Machine learning;Vectors;Systematic literature review;Phishing;spam;email security;campaigns;social engineering;email grouping;literature review},
  doi={10.1109/ACCESS.2025.3555157},
  ISSN={2169-3536},
  month={},}@ARTICLE{10630697,
  author={},
  journal={A Transdisciplinary Framework for Effective and Reliable Continuum of Care}, 
  title={A Transdisciplinary Framework for Effective and Reliable Continuum of Care}, 
  year={2024},
  volume={},
  number={},
  pages={1-37},
  abstract={There is a critical need to identify electronic health records without compromising the privacy and confidentiality of patient data. This white paper discusses important possible parameters associated with electronic health records that could be used for this identification. The core objective of this white paper is to set a stage for the development of IEEE standards associated with the identification of electronic health records, thereby discussing the feasibility and potential stakeholders for such standards.},
  keywords={confidentiality;electronic health record;Industry Connections;privacy standard development;white paper},
  doi={},
  ISSN={},
  month={Aug},}@BOOK{10614692,
  author={},
  booktitle={Sourcebook in the Mathematics of Ancient Greece and the Eastern Mediterranean},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={An invaluable reference book on the mathematics of Greek antiquityEuclid, Archimedes, and Apollonius are familiar names to many of us, and their contributions have shaped mathematical practice up to modern times. Yet the mathematical activity of Greek antiquity extended far beyond their achievements and was furthered by diverse individuals in different contexts. Sourcebook in the Mathematics of Ancient Greece and the Eastern Mediterranean brings together an extensive collection of primary source materials that document the extraordinary breadth of mathematical ideas developed in the Eastern Mediterranean from 500 BCE to 500 CE, a millennium in which Greek cultural influence spanned the ancient world.Weaving together ancient commentaries with the works themselves, Victor Katz and Clemency Montelle present a wealth of newly translated texts along with sources difficult to find elsewhere, from writings by the great mathematical thinkers of Greek antiquity to those by practitioners who used mathematics in everyday life. This comprehensive and wide-ranging sourcebook includes lesser-known authors who made critical contributions, sometimes in languages other than Greek, as well as accounts of technical instrumentation, papyri by anonymous authors designed for teaching purposes, and evidence of hand computations and numerical tables.An essential resource for anyone interested in the mathematical achievements of this remarkable intellectual culture, Sourcebook in the Mathematics of Ancient Greece and the Eastern Mediterranean encompasses disciplines that illustrate the important role of mathematics in ancient Greek society more broadly, from astronomy, music, and optics to philosophy, literature, and theater.},
  keywords={Clemency Montelle;Greek mathematics;math;mathematical papyri;Proposition;Ptolemy;Sourcebook in Greek Mathematics;theoretical mathematics;Victor J. Katz;Euclid;Archimedes;Socrates;Greek;Apollonius;Pappus;Plato;Heron;Pythagorean;Glaucon;Eutocius;Archytas;Arabic;Diophantus;Conics;Theaetetus;Chord;Proclus;Aristarchus;Almagest;Eudoxus;Latin;Nicomachus;Antikythera;Mechanism;Egyptian;Eratosthenes;Boethius;Menelaus;Pythagorean Theorem;Hippocrates;Antikythera Mechanism;Strepsiades;Simplicius;Euclidean;Columella;Disciple;Meton;Western World;Great Books;Eudemus;Diocles;Tarentum;Theon;Conon;Peisthetaerus;Protarchus;Vitruvius;Geminus;Syene;Nicomedes;Sphaerica;Metrica;Museum;Venus;Thule;Mesopotamian;Sourcebook;Hypsicles;Theodorus;Polybius},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691257686},
  url={https://ieeexplore.ieee.org/document/10614692},}@INPROCEEDINGS{9667085,
  author={Vepakomma, Praneeth and Singh, Abhishek and Zhang, Emily and Gupta, Otkrist and Raskar, Ramesh},
  booktitle={2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)}, 
  title={NoPeek-Infer: Preventing face reconstruction attacks in distributed inference after on-premise training}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={For models trained on-premise but deployed in a distributed fashion across multiple entities, we demonstrate that minimizing distance correlation between sensitive data such as faces and intermediary representations enables prediction while preventing reconstruction attacks. Leakage (measured using distance correlation between input and intermediate representations) is the risk associated with the reconstruction of raw face data from intermediary representations that are communicated in a distributed setting. We demonstrate on face datasets that our method is resilient to reconstruction attacks during distributed inference while maintaining information required to sustain good classification accuracy. We share modular code for performing NoPeek-Infer at http://tiny.cc/nopeek along with corresponding trained models for benchmarking attack techniques.},
  keywords={Training;Privacy;Correlation;Computational modeling;Distributed databases;Machine learning;Predictive models},
  doi={10.1109/FG52635.2021.9667085},
  ISSN={},
  month={Dec},}@ARTICLE{10559590,
  author={Yoshida, Masatomo and Namura, Haruto and Okuda, Masahiro},
  journal={IEEE Access}, 
  title={Adversarial Examples for Image Cropping: Gradient-Based and Bayesian-Optimized Approaches for Effective Adversarial Attack}, 
  year={2024},
  volume={12},
  number={},
  pages={86541-86552},
  abstract={In this study, we propose novel approaches for generating adversarial examples targeting machine learning-based image cropping systems. Image cropping is crucial for meeting display space restrictions and highlighting content’s interest areas. However, existing image cropping systems often miss user-intended areas, have necessities to remove inherent biases in light of AI fairness, or might expose users to legal risks. To address these issues, our paper introduces approaches for effectively creating adversarial examples in both black-box and white-box settings. In the white-box approach, we utilize gradient-based perturbations focusing on the model’s blurring layer and targeting effective areas. For the black-box approach, even for models where gradient information is unavailable, we levered pixel attacks with Bayesian optimization and patch attacks to effectively narrow the search space. We also introduce a novel quantitative evaluation method for image cropping by measuring shifts in gaze saliency map peak values, reflecting a typical scenario with social network services. Our results suggest that our approaches not only outperform existing methods but also exhibit the potential to be an effective solution to the problems even with models on actual platforms.},
  keywords={Glass box;Computational modeling;Perturbation methods;Predictive models;Closed box;Gaussian noise;Data models;Adversarial machine learning;Object detection;Social networking (online);Adversarial examples;image cropping;object detection;saliency map;Twitter},
  doi={10.1109/ACCESS.2024.3415356},
  ISSN={2169-3536},
  month={},}@ARTICLE{10902484,
  author={Wang, Jilong and Hou, Saihui and Guo, Xianda and Huang, Yan and Huang, Yongzhen and Zhang, Tianzhu and Wang, Liang},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={GaitC3I: Robust Cross-Covariate Gait Recognition via Causal Intervention}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Cross-covariate gait recognition aims to analyze a pedestrian’s gait to extract an identity representation that is invariant across varying covariates. However, prevailing methods that have achieved good results on controlled in-the-lab datasets often perform poorly on realistic datasets. In this work, we find a significant cause is that the widely used pairwise metric learning paradigm cannot correctly handle the relationship between samples from different covariate conditions. Even worse, it may yield harmful signals that inadvertently mislead models to focus on covariate-related features, particularly when covariate distributions vary across subjects. To address this issue, we propose a Cross-Covariate Causal Intervention (GaitC3I) framework, a unified causality-inspired approach aimed at enhancing the robustness of gait recognition across diverse conditions. Specifically, our method consists of two parts: (i) an effective causal intervention metric learning paradigm based on backdoor adjustment, which strategically mitigates spurious correlations induced by covariates, thus ensuring a more invariant gait representation; and (ii) an annotation-free selection strategy that progressively matches each positive sample with negative samples from similar covariate conditions at various granularities. We demonstrate the effectiveness of our GaitC3I through extensive evaluation on six popular gait datasets–Gait3D, GREW, OUMVLP, CASIA-B, CCPG, and CCGR–achieving substantial improvements. Our method not only outperforms existing state-of-the-art models but also provides a systematic solution to remove the spurious correlations in gait recognition.},
  keywords={Gait recognition;Measurement;Correlation;Feature extraction;Visualization;Data mining;Clothing;Training;Annotations;Electronic mail;Identification of persons;biometrics;gait recognition;debiasing;causal inference;backdoor adjustment},
  doi={10.1109/TCSVT.2025.3545210},
  ISSN={1558-2205},
  month={},}@BOOK{10803968,
  author={Cronin, Irena},
  booktitle={Decoding Large Language Models: An exhaustive guide to understanding, implementing, and optimizing LLMs for NLP applications},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Explore the architecture, development, and deployment strategies of large language models to unlock their full potentialKey FeaturesGain in-depth insight into LLMs, from architecture through to deploymentLearn through practical insights into real-world case studies and optimization techniquesGet a detailed overview of the AI landscape to tackle a wide variety of AI and NLP challengesPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionEver wondered how large language models (LLMs) work and how they're shaping the future of artificial intelligence? Written by a renowned author and AI, AR, and data expert, Decoding Large Language Models is a combination of deep technical insights and practical use cases that not only demystifies complex AI concepts, but also guides you through the implementation and optimization of LLMs for real-world applications. You’ll learn about the structure of LLMs, how they're developed, and how to utilize them in various ways. The chapters will help you explore strategies for improving these models and testing them to ensure effective deployment. Packed with real-life examples, this book covers ethical considerations, offering a balanced perspective on their societal impact. You’ll be able to leverage and fine-tune LLMs for optimal performance with the help of detailed explanations. You’ll also master techniques for training, deploying, and scaling models to be able to overcome complex data challenges with confidence and precision. This book will prepare you for future challenges in the ever-evolving fields of AI and NLP. By the end of this book, you’ll have gained a solid understanding of the architecture, development, applications, and ethical use of LLMs and be up to date with emerging trends, such as GPT-5.What you will learnExplore the architecture and components of contemporary LLMsExamine how LLMs reach decisions and navigate their decision-making processImplement and oversee LLMs effectively within your organizationMaster dataset preparation and the training process for LLMsHone your skills in fine-tuning LLMs for targeted NLP tasksFormulate strategies for the thorough testing and evaluation of LLMsDiscover the challenges associated with deploying LLMs in production environmentsDevelop effective strategies for integrating LLMs into existing systemsWho this book is forIf you’re a technical leader working in NLP, an AI researcher, or a software developer interested in building AI-powered applications, this book is for you. To get the most out of this book, you should have a foundational understanding of machine learning principles; proficiency in a programming language such as Python; knowledge of algebra and statistics; and familiarity with natural language processing basics.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835081808},
  url={https://ieeexplore.ieee.org/document/10803968},}@ARTICLE{10897423,
  author={Le, Thuc Trinh and Almansa, Andrés and Gousseau, Yann and Masnou, Simon},
  journal={Computational Visual Media}, 
  title={Object removal from complex videos using a few annotations}, 
  year={2019},
  volume={5},
  number={3},
  pages={267-291},
  abstract={We present a system for the removal of objects from videos. As input, the system only needs a user to draw a few strokes on the first frame, roughly delimiting the objects to be removed. To the best of our knowledge, this is the first system allowing the semi-automatic removal of objects from videos with complex backgrounds. The key steps of our system are the following: after initialization, segmentation masks are first refined and then automatically propagated through the video. Missing regions are then synthesized using video inpainting techniques. Our system can deal with multiple, possibly crossing objects, with complex motions, and with dynamic textures. This results in a computational tool that can alleviate tedious manual operations for editing high-quality videos.},
  keywords={Videos;Object segmentation;Annotations;Accuracy;Motion segmentation;Image reconstruction;Dynamics;Proposals;Adaptive optics;Semantic segmentation;object removal;object segmentation;object tracking;video inpainting;video completion},
  doi={10.1007/s41095-019-0145-0},
  ISSN={2096-0662},
  month={Sep.},}@BOOK{9519685,
  author={Mattern, Shannon},
  booktitle={A City Is Not a Computer: Other Urban Intelligences},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={A bold reassessment of "smart cities" that reveals what is lost when we conceive of our urban spaces as computersComputational models of urbanism—smart cities that use data-driven planning and algorithmic administration—promise to deliver new urban efficiencies and conveniences. Yet these models limit our understanding of what we can know about a city. A City Is Not a Computer reveals how cities encompass myriad forms of local and indigenous intelligences and knowledge institutions, arguing that these resources are a vital supplement and corrective to increasingly prevalent algorithmic models.Shannon Mattern begins by examining the ethical and ontological implications of urban technologies and computational models, discussing how they shape and in many cases profoundly limit our engagement with cities. She looks at the methods and underlying assumptions of data-driven urbanism, and demonstrates how the "city-as-computer" metaphor, which undergirds much of today's urban policy and design, reduces place-based knowledge to information processing. Mattern then imagines how we might sustain institutions and infrastructures that constitute more diverse, open, inclusive urban forms. She shows how the public library functions as a steward of urban intelligence, and describes the scales of upkeep needed to sustain a city's many moving parts, from spinning hard drives to bridge repairs.Incorporating insights from urban studies, data science, and media and information studies, A City Is Not a Computer offers a visionary new approach to urban planning and design.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691226750},
  url={https://ieeexplore.ieee.org/document/9519685},}@INPROCEEDINGS{10101148,
  author={Malviya, Utsav Kumar and Chauhan, Sanjay Pratap Singh},
  booktitle={2023 2nd International Conference for Innovation in Technology (INOCON)}, 
  title={Multiple Agents based Disaster Prediction for Public Environments using Data Mining Techniques}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={Real-time data on natural disasters are collected, explained, analysed, predicted, and shown in the disaster management system. The development of GIS-based informational understanding has been documented (GIS). Using GIS and geographic data mining, the disaster management approach can pinpoint the epicentre of an occurrence and direct relief workers along the safest possible paths to the scene. The precise geological state and geographical placement of many areas makes them vulnerable to a wide range of natural disasters, including earthquakes, floods, land debris, landslides, cloud bursts, and human casualties. An efficient real-time system for predicting natural occurrences and locations is necessary to minimise damages and suffering. This research presents a unique methodology for predicting the location of disasters using density-based spatiotemporal clustering and global positioning system data. Before implementing clustering and feature selection, the process of data cleansing removes redundant, irrelevant, and inconsistent information from the news databases based on natural events. Areas prone to natural disasters like earthquakes, floods, landslides, and so on will be culled using a spatiotemporal clustering technique. The clustered data is then sorted by terms associated with natural catastrophes, and features are selected accordingly. In order to aid event detectors and location estimators, extracted features are supplied to a decision tree, which then categorises the data into both positive and negative classes.},
  keywords={Landslides;Earthquakes;Disaster management;Feature extraction;Real-time systems;Spatiotemporal phenomena;Data mining;Geograpic data mining;Disasters;Detectors and Location estimators},
  doi={10.1109/INOCON57975.2023.10101148},
  ISSN={},
  month={March},}@ARTICLE{9755165,
  author={Lai, Joel Weijia and Cheong, Kang Hao},
  journal={IEEE Access}, 
  title={Educational Opportunities and Challenges in Augmented Reality: Featuring Implementations in Physics Education}, 
  year={2022},
  volume={10},
  number={},
  pages={43143-43158},
  abstract={This review paper provides the conceptualization and development of augmented reality (AR) environment for education by featuring implementations in physics education. The use of AR creates an environment designed to fully incorporate next-generation AR-aided notes, virtual laboratory and interactive problem-based learning with real-time automated generation of application-centric scenarios. This can be carried out via the fusion and technologizing of pre-existing teaching materials (such as books and notes) using AR and be mobile device friendly to fully leverage on learning beyond classrooms. Such a method is proposed to give students the access to resources anytime, anywhere without the spatial and temporal restrictions of synchronous-learning. This review discusses the advances of AR as an important tool in physics education, identify potential challenges and envisions the future by surveying recent trends and reviews. We provide perspective on practical AR implementation and evaluation for educators and school administrator, and potential academic advances through physics education research for researchers.},
  keywords={Physics education;Research and development;Visualization;Educational courses;Mobile handsets;Augmented reality;Augmented reality;immersive technology;education development;physics education research},
  doi={10.1109/ACCESS.2022.3166478},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9885649,
  author={J, Karthika and Senthilselvi, A.},
  booktitle={2022 3rd International Conference on Electronics and Sustainable Communication Systems (ICESC)}, 
  title={Credit Card Fraud Detection based on Ensemble Machine Learning Classifiers}, 
  year={2022},
  volume={},
  number={},
  pages={1604-1610},
  abstract={Credit card is considered as one of the most popular paying methods for online and regular purchases, due to the advancement in communication and electronic commerce systems. Thus, the fraud associated with these transactions increased significantly. The great utilization of electronic payment is highly affected by this fraudulent transactions, which requires urgent detection to solve this issue. Therefore, effective and efficient approaches to detect fraud in credit card transactions are needed. To catch the fraudulent transaction, a good fitting model is needed, hence researchers recommends the use of various Machine Learning (ML) techniques, because of its beneficial characteristics. The main aim of the research work is to implement an ensemble based ML techniques for Credit Card Fraud Detection (CCFD). The strength of our model is a combination of the forces of the three subsystems; Recursive Feature Elimination (RFE), CCFD's using ensemble classifiers, and Synthetic Minority Oversampling (S MOTE) to deal with the problem of unbalanced data to identify the most effective prediction features. The proposed model run typical tests on two real databases of public credit card transactions, including fraudulent and official ones. Based on the comparison of other ML methods, the extra tree classifier has performed better and achieved high efficiencies such as 96% of accuracy and 57.95% of F1-measure.},
  keywords={Adaptation models;Fitting;Predictive models;Credit cards;Feature extraction;Data models;Fraud;Credit Card Fraud Detection;Synthetic Minority Oversampling;Imbalance Dataset;Machine Learning;Recursive Feature Elimination},
  doi={10.1109/ICESC54411.2022.9885649},
  ISSN={},
  month={Aug},}@ARTICLE{10942377,
  author={Oun, Ahmed and Wince, Kaden and Cheng, Xiangyi},
  journal={IEEE Access}, 
  title={The Role of Artificial Intelligence in Boosting Cybersecurity and Trusted Embedded Systems Performance: A Systematic Review on Current and Future Trends}, 
  year={2025},
  volume={13},
  number={},
  pages={55258-55276},
  abstract={As technology becomes increasingly interconnected, ensuring the security of cyber and embedded systems is critical due to escalating vulnerabilities and sophisticated cyber threats. Researchers are exploring artificial intelligence (AI) to improve security mechanisms, yet there is a lack of a comprehensive technical, AI-focused analysis detailing the integration of AI into existing security hardware and frameworks. To address this gap, this article systematically reviews 63 articles on AI in cybersecurity and trusted embedded systems. The reviewed articles are categorized into four application domains: 1) Intrusion Detection and Prevention (IDPS), 2) Malware Detection, 3) Industrial Control and Cyber-Physical Systems (CPS) and 4) Distributed Denial-of-Service (DDoS) Detection and Prevention. We investigated current trends in integrating AI into security domains by summarizing the hardware used, the AI methodologies adopted, and the statistical distribution by publication year and region. The key findings of our review indicate that AI significantly enhances security measures by enabling capabilities such as detection, classification, feature selection, data privacy preservation, model combination, data generation, output interpretation, optimization, and adaptation. In addition, the benefits and challenges identified in these studies provide insight into the future potential of AI integration in security. Suggested directions for future work include improving generalization and scalability, exploring continuous or real-time monitoring, and improving AI model performance. This analysis serves as a foundation for advancing AI applications in the effective securing of cyber and embedded systems effectively.},
  keywords={Artificial intelligence;Security;Embedded systems;Hardware;Computer crime;Internet of Things;Deep learning;Systematic literature review;Market research;Malware;Security;cybersecurity;embedded system;artificial intelligence;machine learning;deep learning;review;systematic review},
  doi={10.1109/ACCESS.2025.3554739},
  ISSN={2169-3536},
  month={},}@INBOOK{10830638,
  author={Kumar, Akshi},
  booktitle={Language Intelligence: Expanding Frontiers in Natural Language Processing}, 
  title={Natural Language Processing for Affective, Psychological, and Content Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={159-221},
  abstract={Summary <p>This chapter investigates NLP's role in analyzing affective and psychological aspects of language, focusing on sentiment analysis, emotion recognition, and psychometric NLP. It also delves into specific content analysis tasks like sarcasm detection, humor recognition, and identifying distress indicators such as depression and anxiety. These applications highlight the capability of NLP to interpret human emotions and psychological states from textual data.</p>},
  keywords={Sentiment analysis;Transfer learning;Analytical models;Transformers;Neural networks;Bayes methods;Zero shot learning;Reviews;Psychology;Organizations},
  doi={10.1002/9781394297290.ch7},
  ISSN={},
  publisher={IEEE},
  isbn={9781394297283},
  url={https://ieeexplore.ieee.org/document/10830638},}@INBOOK{9933568,
  author={},
  booktitle={Artificial Intelligence and Blockchain in Digital Forensics}, 
  title={1 Digital Forensics Meets AI: A Game-changer for the 4th Industrial Revolution}, 
  year={2022},
  volume={},
  number={},
  pages={1-20},
  abstract={Digital forensics is the science of detecting evidence from digital media like a computer, smart phone, server, or network. It provides the forensic team with the most beneficial methods to solve confused digital-related cases. AI and blockchain can be applied to solve online predatory chat cases and photo forensics cases, provide network service evidence, custody of digital files in forensic medicine, and identify roots of data scavenging. The increased use of PCs and extensive use of internet access, has meant easy availability of hacking tools. Over the past two decades, improvements in the information technology landscape have made the collection, preservation, and analysis of digital evidence extremely important. The traditional tools for solving cybercrimes and preparing court cases are making investigations difficult. We can use AI and blockchain design frameworks to make the digital forensic process efficient and straightforward. AI features help determine the contents of a picture, detect spam email messages and recognize swatches of hard drives that could contain suspicious files. Blockchain-based lawful evidence management schemes can supervise the entire evidence flow of all of the court data. This book can provide a wide-ranging overview of how AI and blockchain can be used to solve problems in digital forensics using advanced tools and applications available on the market.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770226875},
  url={https://ieeexplore.ieee.org/document/9933568},}@BOOK{9453261,
  author={Newman, William},
  booktitle={Newton the Alchemist: Science, Enigma, and the Quest for Nature's "Secret Fire"},
  year={2019},
  volume={},
  number={},
  pages={},
  abstract={A book that finally demystifies Newton’s experiments in alchemyWhen Isaac Newton’s alchemical papers surfaced at a Sotheby’s auction in 1936, the quantity and seeming incoherence of the manuscripts were shocking. No longer the exemplar of Enlightenment rationality, the legendary physicist suddenly became “the last of the magicians.” Newton the Alchemist unlocks the secrets of Newton’s alchemical quest, providing a radically new understanding of the uncommon genius who probed nature at its deepest levels in pursuit of empirical knowledge.In this evocative and superbly written book, William Newman blends in-depth analysis of newly available texts with laboratory replications of Newton’s actual experiments in alchemy. He does not justify Newton’s alchemical research as part of a religious search for God in the physical world, nor does he argue that Newton studied alchemy to learn about gravitational attraction. Newman traces the evolution of Newton’s alchemical ideas and practices over a span of more than three decades, showing how they proved fruitful in diverse scientific fields. A precise experimenter in the realm of “chymistry,” Newton put the riddles of alchemy to the test in his lab. He also used ideas drawn from the alchemical texts to great effect in his optical experimentation. In his hands, alchemy was a tool for attaining the material benefits associated with the philosopher’s stone and an instrument for acquiring scientific knowledge of the most sophisticated kind.Newton the Alchemist provides rare insights into a man who was neither Enlightenment rationalist nor irrational magus, but rather an alchemist who sought through experiment and empiricism to alter nature at its very heart.},
  keywords={Isaac Newton;nature;alchemy;chymistry;optical experimentation;scientific knowledge;Enlightenment;empiricism;reason;chymical research;secret fire;natural world;chymical studies;experimental philosophy;philosophers' stone;adept;exegesis;biblical prophecy;aurific art;ancient mythology;metals;mining;Eirenaeus Philalethes;Nicolas Flamel;education;Free Grammar School;Trinity College;Treatise of Chymistry;Robert Boyle;Benedictine Basilius Valentinus;optical research;optics;color theory;treatise;Humores minerales;Of Natures obvious laws & processes in vegetation;sea salt;niter;vegetability;Michael Sendivogius;antimony;lead;Philalethes;florilegium;Keynes 35;Johann de Monte–Snyders;alchemist;Johann de Monte-Snyders;Sendivogius;Keynes MS 58;Ramon Lull;Epistola ad Theodorum Mundanum;Opera;Nicolas Fatio de Duillier;experimental notebooks;chymical laboratory;CU Add. 3973;sophic sal ammoniac;antimonial sublimate;copper vitriol;sal ammoniac;our Venus;scientific collaboration;Three Mysterious Fires;Keynes 58;caduceus of Mercury;scythe of Saturn;Praxis;alchemical text;florilegium style;laboratory notebooks;chymist;Captain Hylliard;William Yworth;lorilegia;refraction theory;sulfur;Opticks;Hypothesis of Light;chymistry of light;refractive power;color;phlogiston theory;sophic mercury;gold;alchemists;chrysopoeia},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691185033},
  url={https://ieeexplore.ieee.org/document/9453261},}@ARTICLE{10945359,
  author={Ahmad, Waqas and Khan, Hikmat Ullah and Alarfaj, Fawaz Khaled and Alreshoodi, Mohammed},
  journal={IEEE Access}, 
  title={Aspect-Base Sentiment Analysis: A Comprehensive Review and Open Research Challenges}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={The social web provides a facility for common people to share their views, comments, feedback, and experiences on various social media platforms. Due to these platforms now communication has become easier and it has provided us with opportunities to use social media channels for various businesses. The survival of an e-commerce business highly relies on customers’ opinions or feedback extensively articulated on internet-based social media platforms or social networking sites. Eventually, analysis of these public opinions from these platforms to identify and demonstrate the cumulative meaningful information is the prime objective of Sentiment Analysis (SA). Summarization of this informative knowledge is advantageous for companies, organizations, and industrialist analysts to improve the quality of their products or services. In this scenario, Aspect-Based Sentiment Analysis (ABSA) has proven to be a powerful companion for companies, organizations, and producers to specify the consumers’ attitudes and opinions towards products and brands’ impressive features. Various efforts have contributed to aspect extraction and sentiment classification over the last few decades. In this review study, we first focus on two diverse research tasks, aspect extraction, and aspect sentiment analysis and then we present a comprehensive review of existing studies in various classifications such as lexicon-based, graph data, topic models, machine learning, and deep learning. This diverse analysis provides pros and cons for various research approaches and comparative analysis. We also discuss various sources and details of datasets, which are used in this research study. We also present the research gaps including open research challenges as a guide for future researchers in the form of future research. Moreover, we also present the bibliometric analysis of the aspect-based sentiment analysis.},
  keywords={Reviews;Feature extraction;Social networking (online);Sentiment analysis;Data mining;Surveys;Industries;Electronic commerce;Deep learning;Companies;Aspect Extraction;Deep learning;Explicit Aspect;Implicit Aspect;Machine learning;Sentiment Analysis},
  doi={10.1109/ACCESS.2025.3555744},
  ISSN={2169-3536},
  month={},}@ARTICLE{10942343,
  author={Alsuhaibani, Muath and Pourramezan Fard, Ali and Sun, Jian and Far Poor, Farida and Pressman, Peter S. and Mahoor, Mohammad H.},
  journal={IEEE Access}, 
  title={A Review of Machine Learning Approaches for Non-Invasive Cognitive Impairment Detection}, 
  year={2025},
  volume={13},
  number={},
  pages={56355-56384},
  abstract={This review paper explores recent advances in machine learning approaches with emphasis on deep learning techniques for non-invasive cognitive impairment detection. We examine various non-invasive indicators of cognitive decline, including speech and language, facial, and motoric mobility. The paper provides an overview of relevant datasets, feature-extracting techniques, and deep-learning architectures applied to this domain. The paper comprises 60 peer-reviewed papers that mainly utilize deep learning models to detect cognitive impairment conditions. We have analyzed the performance of different methods across modalities and observed that speech and language-based methods generally achieved the highest detection performance. Studies combining acoustic and linguistic features tended to outperform those using a single modality. Facial analysis methods showed promise for visual modalities but were less extensively studied. Most papers focused on binary classification (impaired vs. non-impaired), with fewer addressing multi-class or regression tasks. Transfer learning and pre-trained language models emerged as popular and effective techniques, especially for linguistic analysis. Despite significant progress, several challenges remain, including data standardization and accessibility, model explainability, longitudinal analysis limitations, and clinical adaptation. Lastly, we propose future research directions, such as investigating language-agnostic speech analysis methods, developing multi-modal diagnostic systems, and addressing ethical considerations in AI-assisted healthcare. By synthesizing current trends and identifying key obstacles, this review aims to guide further development of deep learning-based cognitive impairment detection systems to improve early diagnosis and ultimately patient outcomes.},
  keywords={Deep learning;Reviews;Acoustics;Linguistics;Feature extraction;Positron emission tomography;Magnetic resonance imaging;Learning systems;Data models;Alzheimer's disease;Alzheimer’s disease;cognitive impairment detection;deep learning models},
  doi={10.1109/ACCESS.2025.3555176},
  ISSN={2169-3536},
  month={},}@INBOOK{10320143,
  author={Minoli, Daniel and Occhiogrosso, Benedict},
  booktitle={AI Applications to Communications and Information Technologies: The Role of Ultra Deep Neural Networks}, 
  title={Current and Evolving Applications to Network Cybersecurity}, 
  year={2024},
  volume={},
  number={},
  pages={347-405},
  abstract={This chapter focuses on cybersecurity challenges in the information and communications technology (ICT, also known as IT) arena and on machine learning (ML) methods to address some of these critical concerns. ML has many applications in cybersecurity including identifying network cyber threats and enhancing host antivirus software. Cybersecurity concerns deal with at least four environments: ICT corporate networks, intranets, computing resources/databases and websites; (home) personal devices including laptops, smart phones and intelligent appliances; distributed Internet of Things sensors and devices; and service provider networks of all types, also including cloud&#x2010;based services. Security Policy and Defense in Depth are two important elements to deal with corporate cybersecurity risks. A blockchain is a cryptographically&#x2010;linked list of blocks created by nodes, where each block has a header, the relevant transaction data to be protected, and relevant security metadata.},
  keywords={Security;Data breach;Computer security;Information and communication technology;Grippers;Computer viruses;Computer hacking},
  doi={10.1002/9781394190034.ch6},
  ISSN={},
  publisher={IEEE},
  isbn={9781394190027},
  url={https://ieeexplore.ieee.org/document/10320143},}@BOOK{10769242,
  author={Makrehchi, Masoud},
  booktitle={Efficient Algorithm Design: Unlock the power of algorithms to optimize computer programming},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Master advanced algorithm design techniques to tackle complex programming challenges and optimize application performanceKey FeaturesDevelop advanced algorithm design skills to solve modern computational problemsLearn state-of-the-art techniques to deepen your understanding of complex algorithmsApply your skills to real-world scenarios, enhancing your expertise in today's tech landscapePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionEfficient Algorithm Design redefines algorithms, tracing the evolution of computer science as a discipline bridging natural science and mathematics. Author Masoud Makrehchi, PhD, with his extensive experience in delivering publications and presentations, explores the duality of computers as mortal hardware and immortal algorithms. The book guides you through essential aspects of algorithm design and analysis, including proving correctness and the importance of repetition and loops. This groundwork sets the stage for exploring algorithm complexity, with practical exercises in design and analysis using sorting and search as examples. Each chapter delves into critical topics such as recursion and dynamic programming, reinforced with practical examples and exercises that link theory with real-world applications. What sets this book apart is its focus on the practical application of algorithm design and analysis, equipping you to solve real programming challenges effectively. By the end of this book, you’ll have a deep understanding of algorithmic foundations and gain proficiency in designing efficient algorithms, empowering you to develop more robust and optimized software solutions. What you will learnGain skills in advanced algorithm design for better problem-solvingUnderstand algorithm correctness and complexity for robust softwareApply theoretical concepts to real-world scenarios for practical solutionsMaster sorting and search algorithms, understanding their synergyExplore recursion and recurrence for complex algorithmic structuresLeverage dynamic programming to optimize algorithmsGrasp the impact of data structures on algorithm efficiency and designWho this book is forIf you’re a software engineer, computer scientist, or a student in a related field looking to deepen your understanding of algorithm design and analysis, this book is tailored for you. A foundation in programming and a grasp of basic mathematical concepts is recommended. It's an ideal resource for those already familiar with the basics of algorithms who want to explore more advanced topics. Data scientists and AI developers will find this book invaluable for enhancing their algorithmic approaches in practical applications.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835886830},
  url={https://ieeexplore.ieee.org/document/10769242},}@BOOK{10803998,
  author={Pinto, Rohan},
  booktitle={Decentralized Identity Explained: Embrace decentralization for a more secure and empowering digital experience},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Delve into the cutting-edge trends of decentralized identities, blockchains, and other digital identity management technologies and leverage them to craft seamless digital experiences for both your customers and employees Key FeaturesExplore decentralized identities and blockchain technology in depthGain practical insights for leveraging advanced digital identity management tools, frameworks, and solutionsDiscover best practices for integrating decentralized identity solutions into existing systemsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionLooking forward to mastering digital identity? This book will help you get to grips with complete frameworks, tools, and strategies for safeguarding personal data, securing online transactions, and ensuring trust in digital interactions in today's cybersecurity landscape. Decentralized Identity Explained delves into the evolution of digital identities, from their historical roots to the present landscape and future trajectories, exploring crucial concepts such as IAM, the significance of trust anchors and sources of truth, and emerging trends such as SSI and DIDs. Additionally, you’ll gain insights into the intricate relationships between trust and risk, the importance of informed consent, and the evolving role of biometrics in enhancing security within distributed identity management systems. Through detailed discussions on protocols, standards, and authentication mechanisms, this book equips you with the knowledge and tools needed to navigate the complexities of digital identity management in both current and future cybersecurity landscapes. By the end of this book, you’ll have a detailed understanding of digital identity management and best practices to implement secure and efficient digital identity frameworks, enhancing both organizational security and user experiences in the digital realm.What you will learnUnderstand the need for security, privacy, and user-centric methodsGet up to speed with the IAM security frameworkExplore the crucial role of sources of truth in identity data verificationDiscover best practices for implementing access control listsGain insights into the fundamentals of informed consentDelve into SSI and understand why it mattersExplore identity verification methods such as knowledge-based and biometricWho this book is forThis book is for cybersecurity professionals and IAM engineers/architects who want to learn how decentralized identity helps to improve security and privacy and how to leverage it as a trust framework for identity management. },
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781804614549},
  url={https://ieeexplore.ieee.org/document/10803998},}@BOOK{10172381,
  author={Shulman, James L.},
  booktitle={The Synthetic University: How Higher Education Can Benefit from Shared Solutions and Save Itself},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={A bold, collaborative vision for combatting the ever-rising cost of collegeUS colleges and universities have long been the envy of the world. Institutional autonomy has fostered creativity among faculty, students, and staff. But this autonomy means that colleges tend to create their own solutions for every need. As a result, higher education suffers from costly redundancies that drive tuitions ever upward, putting higher education, essential to the fabric of the country, at risk. Instead of wishful thinking about collaboration or miraculous subsidies, The Synthetic University describes intermediary organizations that can provide innovative, cost-effective solutions.Offering answers to challenges jointly faced by thousands of institutions, James Shulman lays out a compelling new vision of how to reduce spending while enabling schools to maintain their particular contributions. He explains why colleges are so resistant to change and presents illuminating case studies of mission-driven and market-supported entrepreneurial organizations—such as the student tracking infrastructure of the National Student Clearinghouse or the ambitious effort of classics professors to create a shared transinstitutional department. Mixing theory with lessons drawn from his own experience, he demonstrates how to finance and implement the organizations that can synthesize much-needed solutions.A road map for sustained institutional change, The Synthetic University shows how to overcome colleges’ do-it-yourself impulses, avoid the threat of disruption, and preserve the institutions that we need to conduct basic research, foster innovation, and prepare diverse students to lead meaningful and productive lives.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691237626},
  url={https://ieeexplore.ieee.org/document/10172381},}@BOOK{9453373,
  author={},
  booktitle={The Best Writing on Mathematics 2018},
  year={2019},
  volume={},
  number={},
  pages={},
  abstract={The year’s finest mathematical writing from around the worldThis annual anthology brings together the year’s finest mathematics writing from around the world. Featuring promising new voices alongside some of the foremost names in the field, The Best Writing on Mathematics 2018 makes available to a wide audience many pieces not easily found anywhere else—and you don’t need to be a mathematician to enjoy them. These essays delve into the history, philosophy, teaching, and everyday aspects of math, offering surprising insights into its nature, meaning, and practice—and taking readers behind the scenes of today’s hottest mathematical debates.James Grime shows how to build subtly mischievous dice for playing slightly unfair games and Michael Barany traces how our appreciation of the societal importance of mathematics has developed since World War II. In other essays, Francis Su extolls the inherent values of learning, doing, and sharing mathematics, and Margaret Wertheim takes us on a mathematical exploration of the mind and the world—with glimpses at science, philosophy, music, art, and even crocheting. And there’s much, much more.In addition to presenting the year’s most memorable math writing, this must-have anthology includes an introduction by the editor and a bibliography of other notable pieces on mathematics.This is a must-read for anyone interested in where math has taken us—and where it is headed.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691188720},
  url={https://ieeexplore.ieee.org/document/9453373},}@BOOK{10540164,
  author={Aravilli, Srinivasa Rao and Hamilton, Sam},
  booktitle={Privacy-Preserving Machine Learning: A use-case-driven approach to building and protecting ML pipelines from privacy and security threats},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Gain hands-on experience in data privacy and privacy-preserving machine learning with open-source ML frameworks, while exploring techniques and algorithms to protect sensitive data from privacy breaches Key FeaturesUnderstand machine learning privacy risks and employ machine learning algorithms to safeguard data against breachesDevelop and deploy privacy-preserving ML pipelines using open-source frameworksGain insights into confidential computing and its role in countering memory-based data attacksPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionPrivacy regulations are evolving each year and compliance with privacy regulations is mandatory for every enterprise. Machine learning engineers are required to not only analyze large amounts of data to gain crucial insights, but also comply with privacy regulations to protect sensitive data. This may seem quite challenging considering the large volume of data involved and lack of in-depth expertise in privacy-preserving machine learning. This book delves into data privacy, machine learning privacy threats, and real-world cases of privacy-preserving machine learning, as well as open-source frameworks for implementation. You’ll be guided through developing anti-money laundering solutions via federated learning and differential privacy. Dedicated sections also address data in-memory attacks and strategies for safeguarding data and ML models. The book concludes by discussing the necessity of confidential computation, privacy-preserving machine learning benchmarks, and cutting-edge research. By the end of this machine learning book, you’ll be well-versed in privacy-preserving machine learning and know how to effectively protect data from threats and attacks in the real world.What you will learnStudy data privacy, threats, and attacks across different machine learning phasesExplore Uber and Apple cases for applying differential privacy and enhancing data securityDiscover IID and non-IID data sets as well as data categoriesUse open-source tools for federated learning (FL) and explore FL algorithms and benchmarksUnderstand secure multiparty computation with PSI for large dataGet up to speed with confidential computation and find out how it helps data in memory attacksWho this book is forThis book is for data scientists, machine learning engineers, and privacy engineers who have working knowledge of mathematics as well as basic knowledge in any one of the ML frameworks (TensorFlow, PyTorch, or scikit-learn). },
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781800564220},
  url={https://ieeexplore.ieee.org/document/10540164},}@ARTICLE{8739153,
  author={},
  journal={IEEE P1647/D3, April 2019}, 
  title={IEEE Approved Standard for the Functional Verification Language e}, 
  year={2019},
  volume={},
  number={},
  pages={1-652},
  abstract={The e functional verification language is an application-specific programming language, aimed at automating the task of verifying a hardware or software design with respect to its specification. Verification environments written in e provide a model of the environment in which the design is expected to function, including the kinds of erroneous conditions the design needs to withstand. A typical verification environment is capable of generating user-controlled test inputs with statistically interesting characteristics. Such an environment can check the validity of the design responses. Functional coverage metrics are used to control the verification effort and gauge the quality of the design. e verification environments can be used throughout the design cycle, from a high-level architectural model to a fully realized system. A definition of the e language syntax and semantics and how tool developers and verification engineers should use them are contained in this standard.},
  keywords={IEEE Standards;Functional programming;Formal verification;assertion;concurrent programming;constraint;dynamic verification;functional coverage;functional verification;IEEE 1647;simulation;temporal logic;test generation},
  doi={},
  ISSN={},
  month={June},}@ARTICLE{10412092,
  author={Estrada, Daniel},
  journal={Journal of Social Computing}, 
  title={AIdeal: Sentience and Ideology}, 
  year={2023},
  volume={4},
  number={4},
  pages={275-325},
  abstract={This paper addresses a set of ideological tensions involving the classification of agential kinds, which I see as the methodological and conceptual core of the sentience discourse. Specifically, I consider ideals involved in the classification of biological and artifactual kinds, and ideals related to agency, identity, and value. These ideals frame the background against which sentience in Artificial Intelligence (AI) is theorized and debated, a framework I call the AIdeal. To make this framework explicit, I review the historical discourse on sentience as it appears in ancient, early modern, and the 20th century philosophy, paying special attention to how these ideals are projected onto artificial agents. I argue that tensions among these ideals create conditions where artificial sentience is both necessary and impossible, resulting in a crisis of ideology. Moving past this crisis does not require a satisfying resolution among competing ideals, but instead requires a shift in focus to the material conditions and actual practices in which these ideals operate. Following Charles Mills, I sketch a nonideal approach to AI and artificial sentience that seeks to loosen the grip of ideology on the discourse. Specifically, I propose a notion of participation that deflates the sentience discourse in AI and shifts focus to the material conditions in which sociotechnical networks operate.},
  keywords={Social computing;Philosophical considerations;Biology;Artificial intelligence;sentience;agency;artifacts;artificial intelligence;ideology;nonideal theory;natural kinds;participation},
  doi={10.23919/JSC.2023.0029},
  ISSN={2688-5255},
  month={December},}@BOOK{10745302,
  author={Adjaoute, Akli},
  booktitle={Inside AI: Over 150 billion purchases per year use this author’s AI},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Separate AI truth from AI hype, and learn how to put this powerful technology to work. In Inside AI AI professor and entrepreneur Dr. Akli Adjaoute puts AI in perspective, with informed insights from 30 years spent in the field. His book lays out a pragmatic blueprint that every leader can utilize to drive innovation with artificial intelligence. In Inside AI you’ll learn how to:  Gain insight into diverse AI techniques and methodologies Learn from both successful and failed AI applications Identify the capabilities and limitations of AI systems Understand successful and failed uses of AI in business See where human cognition still exceeds AI Bust common myths like AI’s threat to jobs and civilization Manage AI projects effectively  Inside AI takes you on a journey through artificial intelligence, from AI’s origins in traditional expert systems all the way to deep learning and Large Language Models. There’s no hype here—you’ll get the grounded, evidence-based insights that are vital for making strategic decisions and preparing your business for the future.},
  keywords={modern;DL;deep learning;diverse;LLMs;large language models;strategic;future;business;hype-free;pragmatic;techniques;methodologies;future},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781633437722},
  url={https://ieeexplore.ieee.org/document/10745302},}@ARTICLE{9294026,
  author={Liu, Ximeng and Xie, Lehui and Wang, Yaopeng and Zou, Jian and Xiong, Jinbo and Ying, Zuobin and Vasilakos, Athanasios V.},
  journal={IEEE Access}, 
  title={Privacy and Security Issues in Deep Learning: A Survey}, 
  year={2021},
  volume={9},
  number={},
  pages={4566-4593},
  abstract={Deep Learning (DL) algorithms based on artificial neural networks have achieved remarkable success and are being extensively applied in a variety of application domains, ranging from image classification, automatic driving, natural language processing to medical diagnosis, credit risk assessment, intrusion detection. However, the privacy and security issues of DL have been revealed that the DL model can be stolen or reverse engineered, sensitive training data can be inferred, even a recognizable face image of the victim can be recovered. Besides, the recent works have found that the DL model is vulnerable to adversarial examples perturbed by imperceptible noised, which can lead the DL model to predict wrongly with high confidence. In this paper, we first briefly introduces the four types of attacks and privacy-preserving techniques in DL. We then review and summarize the attack and defense methods associated with DL privacy and security in recent years. To demonstrate that security threats really exist in the real world, we also reviewed the adversarial attacks under the physical condition. Finally, we discuss current challenges and open problems regarding privacy and security issues in DL.},
  keywords={Security;Computational modeling;Privacy;Data models;Training;Training data;Face recognition;Deep learning;DL privacy;DL security;model extraction attack;model inversion attack;adversarial attack;poisoning attack;adversarial defense;privacy-preserving},
  doi={10.1109/ACCESS.2020.3045078},
  ISSN={2169-3536},
  month={},}@BOOK{10001749,
  author={Coluccia, Angelo},
  booktitle={Adaptive Radar Detection: Model-Based, Data-Driven and Hybrid Approaches},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={This book shows you how to adopt data-driven techniques for the problem of radar detection, both per se and in combination with model-based approaches. In particular, the focus is on space-time adaptive target detection against a background of interference consisting of clutter, possible jammers, and noise. It is a handy, concise reference for many classic (model-based) adaptive radar detection schemes as well as the most popular machine learning techniques (including deep neural networks) and helps you identify suitable data-driven approaches for radar detection and the main related issues. You’ll learn how data-driven tools relate to, and can be coupled or hybridized with, traditional adaptive detection statistics; understand fundamental concepts, schemes, and algorithms from statistical learning, classification, and neural networks domains. The book also walks you through how these concepts and schemes have been adapted for the problem of radar detection in the literature and provides you with a methodological guide for the design, illustrating different possible strategies. You’ll be equipped to develop a unified view, under which you can exploit the new possibilities of the data-driven approach even using simulated data. This book is an excellent resource for Radar professionals and industrial researchers, postgraduate students in electrical engineering and the academic community.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781630819019},
  url={https://ieeexplore.ieee.org/document/10001749},}@ARTICLE{8906261,
  author={},
  journal={IEEE Std 1647-2019 (Revision of IEEE Std 1647-2016) - Redline}, 
  title={IEEE Standard for the Functional Verification Language e - Redline}, 
  year={2019},
  volume={},
  number={},
  pages={1-981},
  abstract={The e functional verification language is an application-specific programming language, aimed at automating the task of verifying a hardware or software design with respect to its specification. Verification environments written in e provide a model of the environment in which the design is expected to function, including the kinds of erroneous conditions the design needs to withstand. A typical verification environment is capable of generating user-controlled test inputs with statistically interesting characteristics. Such an environment can check the validity of the design responses. Functional coverage metrics are used to control the verification effort and gauge the quality of the design. e verification environments can be used throughout the design cycle, from a high-level architectural model to a fully realized system. A definition of the e language syntax and semantics and how tool developers and verification engineers should use them are contained in this standard.},
  keywords={IEEE Standards;Concurrent programming;System verification;Simulation;Test generation;assertion;concurrent programming;constraint;dynamic verification;functional coverage;functional verification;IEEE 1647;simulation;temporal logic;test generation},
  doi={},
  ISSN={},
  month={Aug},}@BOOK{10614669,
  author={Ding, Jeffrey},
  booktitle={Technology and the Rise of Great Powers: How Diffusion Shapes Economic Competition},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={A novel theory of how technological revolutions affect the rise and fall of great powersWhen scholars and policymakers consider how technological advances affect the rise and fall of great powers, they draw on theories that center the moment of innovation—the eureka moment that sparks astonishing technological feats. In this book, Jeffrey Ding offers a different explanation of how technological revolutions affect competition among great powers. Rather than focusing on which state first introduced major innovations, he investigates why some states were more successful than others at adapting and embracing new technologies at scale. Drawing on historical case studies of past industrial revolutions as well as statistical analysis, Ding develops a theory that emphasizes institutional adaptations oriented around diffusing technological advances throughout the entire economy.Examining Britain’s rise to preeminence in the First Industrial Revolution, America and Germany’s overtaking of Britain in the Second Industrial Revolution, and Japan’s challenge to America’s technological dominance in the Third Industrial Revolution (also known as the “information revolution”), Ding illuminates the pathway by which these technological revolutions influenced the global distribution of power and explores the generalizability of his theory beyond the given set of great powers. His findings bear directly on current concerns about how emerging technologies such as AI could influence the US-China power balance.},
  keywords={artificial intelligence;China;economic productivity;engineering;general-purpose technology;great powers;human capital;industrial revolutions;Jeffrey Ding;leading sectors;political economy;rising powers;Technological;technological competition;Technology and the Rise of Great Powers: How Diffusion Shapes Economic Competition;United States;Power;Economic;Sectors;Industrial;Growth;Innovation;Diffusion;Productivity;Mechanism;Industries;Technologies;Gpts;Economy;Revolution;GPT diffusion;Transition;Period;Trajectory;Technology;Power transition;Infrastructure;Institutional;Chemical;Skill infrastructure;Leadership;Economic power;Machine;Technological change;Century;Data;Country;Steam;GPT skill infrastructure;LS mechanism;Analysis;Education;Iron;Computerization;Industrial revolution;Breakthroughs;Technical;Military;Economic power transition;Production;Software;Productivity growth;Machine tools;Steam engine;Scholars;GPT mechanism;Engineers;Cotton;Software engineering;Research},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691260372},
  url={https://ieeexplore.ieee.org/document/10614669},}@INPROCEEDINGS{9423393,
  author={Birhane, Abeba and Prabhu, Vinay Uday},
  booktitle={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Large image datasets: A pyrrhic win for computer vision?}, 
  year={2021},
  volume={},
  number={},
  pages={1536-1546},
  abstract={In this paper we investigate problematic practices and consequences of large scale vision datasets (LSVDs). We examine broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an example, we perform a cross-sectional model-based quantitative census covering factors such as age, gender, NSFW content scoring, class- wise accuracy, human-cardinality-analysis, and the semanticity of the image class information in order to statistically investigate the extent and subtleties of ethical transgressions. We then use the census to help hand-curate a look-up-table of images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of verifiably pornographic: shot in a non-consensual setting (up-skirt), beach voyeuristic, and exposed private parts. We survey the landscape of harm and threats both the society at large and individuals face due to uncritical and ill-considered dataset curation practices. We then propose possible courses of correction and critique their pros and cons. We have duly open-sourced all of the code and the census meta-datasets generated in this endeavor for the computer vision community to build on. By unveiling the severity of the threats, our hope is to motivate the constitution of mandatory Institutional Review Boards (IRB) for large scale dataset curation.},
  keywords={Computer vision;Conferences;IEEE Constitution;Faces},
  doi={10.1109/WACV48630.2021.00158},
  ISSN={2642-9381},
  month={Jan},}@ARTICLE{9248607,
  author={Peng, Yifan and Tang, Yuxing and Lee, Sungwon and Zhu, Yingying and Summers, Ronald M. and Lu, Zhiyong},
  journal={IEEE Transactions on Big Data}, 
  title={COVID-19-CT-CXR: A Freely Accessible and Weakly Labeled Chest X-Ray and CT Image Collection on COVID-19 From Biomedical Literature}, 
  year={2021},
  volume={7},
  number={1},
  pages={3-12},
  abstract={The latest threat to global health is the COVID-19 outbreak. Although there exist large datasets of chest X-rays (CXR) and computed tomography (CT) scans, few COVID-19 image collections are currently available due to patient privacy. At the same time, there is a rapid growth of COVID-19-relevant articles in the biomedical literature, including those that report findings on radiographs. Here, we present COVID-19-CT-CXR, a public database of COVID-19 CXR and CT images, which are automatically extracted from COVID-19-relevant articles from the PubMed Central Open Access (PMC-OA) Subset. We extracted figures, associated captions, and relevant figure descriptions in the article and separated compound figures into subfigures. Because a large portion of figures in COVID-19 articles are not CXR or CT, we designed a deep-learning model to distinguish them from other figure types and to classify them accordingly. The final database includes 1,327 CT and 263 CXR images (as of May 9, 2020) with their relevant text. To demonstrate the utility of COVID-19-CT-CXR, we conducted four case studies. (1) We show that COVID-19-CT-CXR, when used as additional training data, is able to contribute to improved deep-learning (DL) performance for the classification of COVID-19 and non-COVID-19 CT. (2) We collected CT images of influenza, another common infectious respiratory illness that may present similarly to COVID-19, and fine-tuned a baseline deep neural network to distinguish a diagnosis of COVID-19, influenza, or normal or other types of diseases on CT. (3) We fine-tuned an unsupervised one-class classifier from non-COVID-19 CXR and performed anomaly detection to detect COVID-19 CXR. (4) From text-mined captions and figure descriptions, we compared 15 clinical symptoms and 20 clinical findings of COVID-19 versus those of influenza to demonstrate the disease differences in the scientific publications. Our database is unique, as the figures are retrieved along with relevant text with fine-grained descriptions, and it can be extended easily in the future. We believe that our work is complementary to existing resources and hope that it will contribute to medical image analysis of the COVID-19 pandemic. The dataset, code, and DL models are publicly available at https://github.com/ncbi-nlp/COVID-19-CT-CXR.},
  keywords={COVID-19;Computed tomography;Databases;Biomedical imaging;X-ray imaging;Compounds;COVID-19;chest X-ray;CT},
  doi={10.1109/TBDATA.2020.3035935},
  ISSN={2332-7790},
  month={March},}@INPROCEEDINGS{10658372,
  author={Zhang, Xingguang and Chimitt, Nicholas and Chi, Yiheng and Mao, Zhiyuan and Chan, Stanley H.},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Spatio-Temporal Turbulence Mitigation: A Translational Perspective}, 
  year={2024},
  volume={},
  number={},
  pages={2889-2899},
  abstract={Recovering images distorted by atmospheric turbulence is a challenging inverse problem due to the stochastic nature of turbulence. Although numerous turbulence mitigation (TM) algorithms have been proposed, their efficiency and generalization to real-world dynamic scenarios remain severely limited. Building upon the intuitions of classical TM algorithms, we present the Deep Atmospheric TUrbulence Mitigation network (DATUM). DATUM aims to overcome major challenges when transitioning from classical to deep learning approaches. By carefully integrating the merits of classical multi-frame TM methods into a deep network structure, we demonstrate that DATUM can efficiently perform long-range temporal aggregation using a recurrent fashion, while deformable attention and temporal-channel attention seamlessly facilitate pixel registration and lucky imaging. With additional supervision, tilt and blur degradation can be Jointly mitigated. These inductive biases empower DATUM to significantly outperform existing methods while delivering a tenfold increase in processing speed. A large-scale training dataset, ATSyn, is presented as a co-invention to enable the generalization to real turbulence. Our code and datasets are available at http://xg416.github.io/DATUM},
  keywords={Degradation;Deep learning;Training;Prevention and mitigation;Heuristic algorithms;Government;Stochastic processes;turbulence mitigation;video restoration;synthetic dataset},
  doi={10.1109/CVPR52733.2024.00279},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10656262,
  author={Lyu, Mengyao and Yang, Yuhong and Hong, Haiwen and Chen, Hui and Jin, Xuan and He, Yuan and Xue, Hui and Han, Jungong and Ding, Guiguang},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={One-dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications}, 
  year={2024},
  volume={},
  number={},
  pages={7559-7568},
  abstract={The prevalent use of commercial and open-source diffusion models (DMs) for text-to-image generation prompts risk mitigation to prevent undesired behaviors. Existing concept erasing methods in academia are all based on full parameter or specification-based fine-tuning, from which we observe the following issues: 1) Generation alteration towards erosion: Parameter drift during target elimination causes alterations and potential deformations across all generations, even eroding other concepts at varying degrees, which is more evident with multi-concept erased; 2) Transfer in-ability & deployment inefficiency: Previous model-specific erasure impedes the flexible combination of concepts and the training-free transfer towards other models, resulting in linear cost growth as the deployment scenarios increase. To achieve non-invasive, precise, customizable, and transferable elimination, we ground our erasing framework on one-dimensional adapters to erase multiple concepts from most DMs at once across versatile erasing applications. The concept-SemiPermeable structure is injected as a Membrane (SPM) into any DM to learn targeted erasing, and mean-time the alteration and erosion phenomenon is effectively mitigated via a novel Latent Anchoring fine-tuning strategy. Once obtained, SPMs can be flexibly combined and plug-and-play for other DMs without specific re-tuning, enabling timely and efficient adaptation to diverse scenarios. During generation, our Facilitated Transport mechanism dynamically regulates the permeability of each SPM to re-spond to different input prompts, further minimizing the impact on other concepts. Quantitative and qualitative results across ~40 concepts, 7 DMs and 4 erasing applications have demonstrated the superior erasing of SPM. Our code and pre-tuned SPMs are available on the project page https:/lyumengyao.github.io/projects/spm.},
  keywords={Deformable models;Adaptation models;Costs;Deformation;Text to image;Diffusion models;Permeability;Diffusion Models;Concept Erasing},
  doi={10.1109/CVPR52733.2024.00722},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{9412849,
  author={Koker, T.E. and Chintapalli, S.S. and Wang, S. and Talbot, B.A. and Wainstock, D. and Cicconet, M. and Walsh, M.C.},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={On Identification and Retrieval of Near-Duplicate Biological Images: a New Dataset and Protocol}, 
  year={2021},
  volume={},
  number={},
  pages={3114-3121},
  abstract={Manipulation and re-use of images in scientific publications is a growing issue, not only for biomedical publishers, but also for the research community in general. In this work we introduce BINDER - Bio-Image Near-Duplicate Examples Repository, a novel dataset to help researchers develop, train, and test models to detect same-source biomedical images. BINDER contains 7,490 unique image patches for model training, 1,821 same-size patch duplicates for validation and testing, and 868 different-size image/patch pairs for image retrieval validation and testing. Except for the training set, patches already contain manipulations including rotation, translation, scale, perspective transform, contrast adjustment and/or compression artifacts. We further use the dataset to demonstrate how novel adaptations of existing image retrieval and metric learning models can be applied to achieve high-accuracy inference results, creating a baseline for future work. In aggregate, we thus present a supervised protocol for near-duplicate image identification and retrieval without any “real-world” training example. Our dataset and source code are available at hms-idac.github.io/BINDER.},
  keywords={Training;Measurement;Adaptation models;Protocols;Biological system modeling;Image retrieval;Transforms},
  doi={10.1109/ICPR48806.2021.9412849},
  ISSN={1051-4651},
  month={Jan},}@ARTICLE{10897653,
  author={Liu, Yihao and Zhao, Hengyuan and Chan, Kelvin C. K. and Wang, Xintao and Loy, Chen Change and Qiao, Yu and Dong, Chao},
  journal={Computational Visual Media}, 
  title={Temporally consistent video colorization with deep feature propagation and self-regularization learning}, 
  year={2024},
  volume={10},
  number={2},
  pages={375-395},
  abstract={Video colorization is a challenging and highly ill-posed problem. Although recent years have witnessed remarkable progress in single image colorization, there is relatively less research effort on video colorization, and existing methods always suffer from severe flickering artifacts (temporal inconsistency) or unsatisfactory colorization. We address this problem from a new perspective, by jointly considering colorization and temporal consistency in a unified framework. Specifically, we propose a novel temporally consistent video colorization (TCVC) framework. TCVC effectively propagates frame-level deep features in a bidirectional way to enhance the temporal consistency of colorization. Furthermore, TCVC introduces a self-regularization learning (SRL) scheme to minimize the differences in predictions obtained using different time steps. SRL does not require any ground-truth color videos for training and can further improve temporal consistency. Experiments demonstrate that our method can not only provide visually pleasing colorized video, but also with clearly better temporal consistency than state-of-the-art methods. A video demo is provided at https://www.youtube.com/watch?v=c7dczMs-olE, while code is available at https://github.com/lyh-18/TCVC-Temporally-Consistent-Video-Colorization.},
  keywords={Image color analysis;Feature extraction;Gray-scale;Art;Training;Spatiotemporal phenomena;Indexes;Data mining;Coherence;Visualization;video colorization;temporal consistency;feature propagation;self-regularization},
  doi={10.1007/s41095-023-0342-8},
  ISSN={2096-0662},
  month={April},}@ARTICLE{10876033,
  author={Borges, Paulo V. K. and Peynot, Thierry and Liang, Sisi and Arain, Bilal and Wildie, Matthew and Minareci, Melih G. and Lichman, Serge and Samvedi, Garima and Sa, Inkyu and Hudson, Nicolas and Milford, Michael and Moghadam, Peyman and Corke, Peter},
  journal={Field Robotics}, 
  title={A Survey on Terrain Traversability Analysis for Autonomous Ground Vehicles: Methods, Sensors, and Challenges}, 
  year={2022},
  volume={2},
  number={},
  pages={1567-1627},
  abstract={Understanding the terrain in the upcoming path of a ground robot is one of the most challenging problems in field robotics. Terrain and traversability analysis is a multidisciplinary field combining robotics with image and signal processing, feature extraction, machine learning, three-dimensional (3D) mapping, and 3D geometry. Application scenarios range from autonomous vehicles on urban networks to agriculture, defence, exploration, mining, and search and rescue. Given the broad set of techniques available and the fast progress in this area, in this paper we organize and survey the corresponding literature, define unambiguous key terms, and discuss links among fundamental building blocks ranging from terrain classification to traversability regression. The advantages and the drawbacks of the methods are critically discussed, providing a comprehensive coverage of key aspects, including open code, available datasets for experimentation and comparisons, and important open research issues.},
  keywords={Robots;Robot sensing systems;Navigation;Surveys;Costs;Reviews;Taxonomy;Sensor phenomena and characterization;Machine learning;Laser radar;perception;obstacle avoidance;terrestrial robotics;navigation},
  doi={10.55417/fr.2022049},
  ISSN={2771-3989},
  month={July},}@ARTICLE{10318045,
  author={Nolazco-Flores, Juan Arturo and Guerrero-Galván, Ana Verónica and Del-Valle-Soto, Carolina and Garcia-Perera, Leibny Paola},
  journal={IEEE Access}, 
  title={Genre Classification of Books on Spanish}, 
  year={2023},
  volume={11},
  number={},
  pages={132878-132892},
  abstract={Genre categorization of published titles is a common practice in publishing houses, libraries, and bookstores, as well as a fundamental element of editorial marketing. However, assigning subject codes to each title proves to be an arduous task for both publishers and data aggregators. The problem with automatic genre categorization is that some publishers use more than 200 categories, making it a highly complex task. Moreover, even though these publishers based their categorization on standards, they ofthen alter the names of these standards as they consider to be too technical. In this paper, we proposed Thema-based categorization as a tool to facilitate editors’ work by advancing the categorization process, allowing them to focus on finer category granularity. This categorization has four key features: first, it clusters the most important categories for Latin American publishers. Second, it stops grouping when the number of thematic categories remains practical for the purposes of the publishing business. Third, we assign names to these categories that resonate with Latin American stakeholders. Finally, the number of categories is optimized to provide reasonable classification performance. We worked on the description of books in Spanish of two publishers, and mapped them to this proposed categorization. This allowed us to created a database for train a model to automate categorization. After conducting our analysis, we determined that 26 thematic categories were an appropriate number that fulfilled the three features mentioned earlier. However, we recognized that classifying into 26 categories was still a complex task, so to overcome this challenge, we decided to augment data by back-translating it into Spanish using the translation function,  $T_{l}^{S}\left ({T_{S}^{l}\left ({S }\right) }\right)$ , where  $T_{S}^{l}\left ({S }\right)$  is the translation function from Spanish,  $s$ , to language,  $l$ ;  $T_{l}^{s}\left ({l }\right)$  is the translation function from language,  $l$ , to Spanish,  $S$ , and  $T_{S}^{l}\left ({S }\right)$  and  $T_{l}^{S}\left ({l }\right)$  are not-invertible functions. Experimental results, obtained using 5-fold cross-validation, were approximately 57%, 57%, 63.38%, and 65.26% for the F1-score of Support Vector Machine (SVM), Logist Regression (LR), BERT, and RoBERTa models, respectively. We utilized the F1-score metric because our categories were not perfectly balanced. The results achieved by RoBERTa outperform those reported in the literature. Furthermore, these results are built upon the foundation of the Thema standard for categorizing book genres. Additionally, the categories have been specifically designed to align with the preferences and needs of Latin American publishers.},
  keywords={Standards;Task analysis;Natural language processing;Databases;Training;Feature extraction;Support vector machines;Text categorization;Information analysis;Publishing;Genre categorization;text classification;book categorization;BERT;RoBERTa},
  doi={10.1109/ACCESS.2023.3332997},
  ISSN={2169-3536},
  month={},}@ARTICLE{10786260,
  author={Wang, Yong and Jiang, Lijun and Du, Zilong and Li, Bo and Yang, Wenming},
  journal={IEEE Internet of Things Journal}, 
  title={CSBNet: Leveraging Edge Intelligence for Multi-Granularity Low-Light Image Enhancement}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  abstract={Low-light conditions constantly restrict the performance of IoT image sensors, thereby impacting image quality and the precision of visual data analysis. The emerging edge intelligence is crucial for low-light image enhancement in improving image quality and data support reliability for IoT systems, which in turn fosters the intelligence and automation progress of the IoT. The enhancement of low-light images necessitates the restoration of both contextual information and spatial details, maintaining the semantic content of the original image and the point-to-point correspondence between inputs and outputs. However, existing methods predominantly concentrate on one aspect, either contextual information or spatial details, making it difficult to simultaneously balance both. To overcome this challenge, we introduce a novel two-branch network, the Context-Space Balance Network (CSBNet), tailored for low-light image enhancement. It comprises a Contextual Information Recovery Network (CIRNet), which adeptly extracts contextual information from multi-scale low-light images, and a Spatial Information Recovery Network (SIRNet), which is designed to preserve spatial details at the original resolution. We also implement a Context-Space Feature Fusion (CSFF) module to seamlessly integrate contextual information with spatial details. Qualitative and quantitative experimental results demonstrate that our CSBNet can better handle many quality degradation types in lowlight images compared with state-of-the-art solutions, outperforming them by PSNR=24.64dB, SSIM=0.867, and LPIPS=0.063 on the benchmark LOL dataset. The source code of CSBNet is available at https://github.com/Loong161/CSBNet.},
  keywords={Internet of Things;Image enhancement;Image edge detection;Lighting;Image quality;Image restoration;Sensors;Noise;Image sensors;Gray-scale;Edge Intelligence;Low-light Image Enhancement;Internet of Things (IoT);Contextual Information;Spatial Details;Context-Space Feature Fusion},
  doi={10.1109/JIOT.2024.3513545},
  ISSN={2327-4662},
  month={},}@ARTICLE{10884889,
  author={Wang, Tao and Wen, Wenying and Xiao, Xiangli and Hua, Zhongyun and Zhang, Yushu and Fang, Yuming},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Beyond Privacy: Generating Privacy-Preserving Faces Supporting Robust Image Authentication}, 
  year={2025},
  volume={20},
  number={},
  pages={2564-2576},
  abstract={The prevalence of face capturing along with the advancement of face recognition poses a potential threat to individual privacy. To protect privacy, plenty of methods have been proposed to change identity in the face, thus blocking malicious face recognition. However, these methods fail to satisfy authentication requirements for special application scenarios, e.g., face authentication in surveillance capture. In this paper, we propose a novel face privacy protection model, which supports robust image authentication via information-conditional identity transformation. Specifically, we first introduce a basic face manipulation model (FMM), which can preserve identity-irrelevant attributes when manipulating identity. Based on FMM, we further design a lightweight protector called AIDPro, outputting a transformed identity which is different from the original one and is embedded a message presenting authentication information. Benefiting from the semantic robustness, our model does not require noise layers to achieve accurate message extraction after various image distortions. In addition, the message can be the condition to guide the identity transformation for privacy protection, which avoids extra resource consumption from supporting image authentication. Extensive experimental results demonstrate our model has comparable privacy protection performance, superior attribute preservation performance, and robust authentication performance especially in JPEG compression and screen shooting. Our code is available at https://github.com/daizigege/AIDPro.},
  keywords={Faces;Privacy;Authentication;Protection;Noise;Robustness;Surveillance;Perturbation methods;Face recognition;Data privacy;Image privacy;face protection;information hiding;robustness},
  doi={10.1109/TIFS.2025.3541859},
  ISSN={1556-6021},
  month={},}@ARTICLE{10935664,
  author={Xie, Chengxing and Zhang, Xiaoming and Li, Linze and Fu, Yuqian and Gong, Biao and Li, Tianrui and Zhang, Kai},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={MAT: Multi-Range Attention Transformer for Efficient Image Super-Resolution}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Image super-resolution (SR) has significantly advanced through the adoption of Transformer architectures. However, conventional techniques aimed at enlarging the self-attention window to capture broader contexts come with inherent drawbacks, especially the significantly increased computational demands. Moreover, the feature perception within a fixed-size window of existing models restricts the effective receptive field (ERF) and the intermediate feature diversity. We demonstrate that a flexible integration of attention across diverse spatial extents can yield significant performance enhancements. In line with this insight, we introduce Multi-Range Attention Transformer (MAT) for SR tasks. MAT leverages the computational advantages inherent in dilation operation, in conjunction with self-attention mechanism, to facilitate both multi-range attention (MA) and sparse multi-range attention (SMA), enabling efficient capture of both regional and sparse global features. Combined with local feature extraction, MAT adeptly capture dependencies across various spatial ranges, improving the diversity and efficacy of its feature representations. We also introduce the MSConvStar module, which augments the model’s ability for multi-range representation learning. Comprehensive experiments show that our MAT exhibits superior performance to existing state-of-the-art SR models with remarkable efficiency (∼ 3.3× faster than SRFormer-light). The codes are available at https://github.com/stella-von/MAT.},
  keywords={Feature extraction;Transformers;Image reconstruction;Convolution;Circuits and systems;Computer architecture;Windows;Training;Superresolution;Computational complexity;Transformer;image super-resolution;multi-range attention;efficient},
  doi={10.1109/TCSVT.2025.3553135},
  ISSN={1558-2205},
  month={},}@ARTICLE{10836835,
  author={Jing, Hao and Wang, Anhong and Zhao, Lijun and Yang, Yakun and Bu, Donghan and Zhang, Jing and Zhang, Yifan and Hou, Junhui},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Boosting 3D Object Detection with Semantic-Aware Multi-Branch Framework}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={In autonomous driving, LiDAR sensors are vital for acquiring 3D point clouds, providing reliable geometric information. However, traditional sampling methods of preprocessing often ignore semantic features, leading to detail loss and ground point interference in 3D object detection. To address this, we propose a multi-branch two-stage 3D object detection framework using a Semantic-aware Multi-branch Sampling (SMS) module and multi-view consistency constraints. The SMS module includes random sampling, Density Equalization Sampling (DES) for enhancing distant objects, and Ground Abandonment Sampling (GAS) to focus on non-ground points. The sampled multi-view points are processed through a Consistent KeyPoint Selection (CKPS) module to generate consistent keypoint masks for efficient proposal sampling. The first-stage detector uses multi-branch parallel learning with multi-view consistency loss for feature aggregation, while the second-stage detector fuses multi-view data through a Multi-View Fusion Pooling (MVFP) module to precisely predict 3D objects. The experimental results on the KITTI dataset and Waymo Open Dataset show that our method achieves excellent detection performance improvement for a variety of backbones, especially for low-performance backbones with simple network structures. The code will be publicly available at https://github.com/HaoJing-SX/SMS.},
  keywords={Three-dimensional displays;Semantics;Feature extraction;Proposals;Point cloud compression;Object detection;Detectors;Data preprocessing;Sampling methods;Interference;Point Clouds;3D Object Detection;Sampling Method;Preprocessing},
  doi={10.1109/TCSVT.2025.3527997},
  ISSN={1558-2205},
  month={},}@ARTICLE{10884900,
  author={Chen, Junxi and Wu, Guangxing and Li, Hongxiang and Chen, Jiankang and Zhang, Wentao and Zheng, Weishi and Wang, Ruixuan},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={DAT: Dual-branch Adapter-Tuning for Few-shot Recognition}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Parameter-Efficient Fine-Tuning methods based on vision-language models (such as CLIP) for few-shot learning have recently received considerable attention. However, previous works only fine-tune either the image or text branch, breaking the alignment of the original two branches, meanwhile fine-tuning both branches of the CLIP would inevitably introduce more trainable parameters and likely cause more severe over-fitting due to the limited training data. In this study, we propose a novel Dual-branch Adapter-Tuning framework (DAT), which collaboratively trains the visual adapter and textual adapter added to the two branches of the original CLIP with multiple consistency constraints. By effectively utilizing the semantically detailed class-specific prompts and outputs of the original CLIP to guide the fine-tuning of both branches, our method gains exceptional adaptation ability to the downstream few-shot learning tasks and alleviates the over-fitting issue, meanwhile maximally preserving the generalization ability of the original CLIP model. Our proposed framework has achieved superior performance on diverse datasets under various few-shot learning settings compared to the existing approaches. The source code is available at https://github.com/SandyXi/DAT.},
  keywords={Visualization;Few shot learning;Training;Adaptation models;Circuits and systems;Tuning;Training data;Accuracy;Semantics;Metalearning;vision-language model;parameter-efficient fine-tuning;few-shot learning},
  doi={10.1109/TCSVT.2025.3541960},
  ISSN={1558-2205},
  month={},}@ARTICLE{10830588,
  author={Xu, Jialang and Wang, Jiacheng and Yu, Lequan and Stoyanov, Danail and Jin, Yueming and Mazomenos, Evangelos B.},
  journal={IEEE Transactions on Biomedical Engineering}, 
  title={Personalizing Federated Instrument Segmentation With Visual Trait Priors in Robotic Surgery}, 
  year={2025},
  volume={},
  number={},
  pages={1-11},
  abstract={Personalized federated learning (PFL) for surgical instrument segmentation (SIS) is a promising approach. It enables multiple clinical sites to collaboratively train a series of models in privacy, with each model tailored to the individual distribution of each site. Existing PFL methods rarely consider the personalization of multi-headed self-attention, and do not account for appearance diversity and instrument shape similarity, both inherent in surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait priors for SIS, incorporating global-personalized disentanglement (GPD), appearance-regulation personalized enhancement (APE), and shape-similarity global enhancement (SGE), to boost SIS performance in each site. GPD represents the first attempt at head- wise assignment for multi-headed self-attention personalization. To preserve the unique appearance representation of each site and gradually leverage the inter-site difference, APE introduces appearance regulation and provides customized layer- wise aggregation solutions via hypernetworks for each site's personalized parameters. The mutual shape information of instruments is maintained and shared via SGE, which enhances the cross-style shape consistency on the image level and computes the shape-similarity contribution of each site on the prediction level for updating the global parameters. PFedSIS outperforms state-of-the-art methods with +1.51% Dice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding code and models are available at https://github.com/wzjialang/PFedSIS.},
  keywords={Instruments;Shape;Surgery;Training;Servers;Federated learning;Data models;Biomedical engineering;Visualization;Regulation;Personalized federated learning;multi-headed self-attention;hypernetwork;appearance regulation;shape similarity},
  doi={10.1109/TBME.2025.3526667},
  ISSN={1558-2531},
  month={},}@BOOK{10745330,
  author={Lo Duca, Angelica},
  booktitle={Data Storytelling with Altair and AI},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Great data presentations tell a story. Learn how to organize, visualize, and present data using Python, generative AI, and the cutting-edge Altair data visualization toolkit. Take the fast track to amazing data presentations! Data Storytelling with Altair and AI introduces a stack of useful tools and tried-and-tested methodologies that will rapidly increase your productivity, streamline the visualization process, and leave your audience inspired. In Data Storytelling with Altair and AI you’ll discover:  Using Python Altair for data visualization Using Generative AI tools for data storytelling The main concepts of data storytelling Building data stories with the DIKW pyramid approach Transforming raw data into a data story  Data Storytelling with Altair and AI teaches you how to turn raw data into effective, insightful data stories. You’ll learn exactly what goes into an effective data story, then combine your Python data skills with the Altair library and AI tools to rapidly create amazing visualizations. Your bosses and decision-makers will love your new presentations—and you’ll love how quick Generative AI makes the whole process!},
  keywords={DIKW pyramid;information;knowledge;wisdom;visualization;Python;Copilot;ChatGPT;Vega;DALL-E;intuitive;prompt;transform;chart;presentation;encodings},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781633437920},
  url={https://ieeexplore.ieee.org/document/10745330},}@ARTICLE{10877920,
  author={Hu, Ke and He, Yudong and Li, Yuan and Zhao, Jiayu and Chen, Song and Kang, Yi},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={EI2Det: Edge-Guided Illumination-Aware Interactive Learning for Visible-Infrared Object Detection}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={The complementary characteristics of visible (VIS) and infrared (IR) modalities play a crucial role in scene perception for autonomous driving, especially under poor lighting conditions. However, effectively leveraging the complementary information from visible and infrared images to further enhance perception performance remains a challenging task. These challenges stem from the difficulty of adaptively balancing the contributions of visible and infrared information under dynamic illumination conditions, the reliance on static fusion strategies that fail to fully utilize cross-modal complementarities, and the limitations of existing datasets in terms of diverse scenes, fine-grained illumination annotations, and high imaging quality. To address the challenges, we propose an Edge-guided Illumination-aware Interactive learning-based Detector (EI2Det). It includes three novel modules. The cross-modal interaction module uses visible-priority and infrared-priority multi-head cross-attention mechanisms to refine inter-modality and intra-modality feature representations, improving the model’s robustness and adaptability. The illumination-aware weighting module predicts illumination intensity levels to dynamically adjust the contributions of visible and infrared features, ensuring effective fusion under various lighting conditions. The edge-guided fusion module leverages critical edge information to guide the detector’s attention to object boundaries, significantly enhancing its localization capability. Additionally, we introduce a Multi-modality Full-time dataset for Autonomous Driving (MFAD), featuring 12,370 image pairs with fine-grained annotations of illumination intensity, covering diverse driving scenarios and weather conditions. Extensive experiments on the public M3FD, KAIST, FLIR, LLVIP, and our MFAD datasets demonstrate superior performance and generalization ability of our approach. The code and dataset will be available at https://github.com/hukefy/EI2Det.},
  keywords={Lighting;Object detection;Image edge detection;Feature extraction;Autonomous vehicles;Meteorology;Image fusion;Clouds;Circuits and systems;Training;Visible and infrared;object detection;dataset and benchmark;autonomous driving},
  doi={10.1109/TCSVT.2025.3539625},
  ISSN={1558-2205},
  month={},}@ARTICLE{10946169,
  author={Qin, Yixin and Zhao, Lei and Gao, Lianli and Zhang, Haonan and Zeng, Pengpeng and Shen, Heng Tao},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Temporal-guided Mixture-of-Experts for Zero-Shot Video Question Answering}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Video Question Answering (VideoQA) is a challenging task in the vision-language field. Due to the time-consuming and labor-intensive labeling process of the question-answer pairs, fully supervised methods are no longer suitable for the current increasing demand for data. This has led to the rise of zero-shot VideoQA, and some works propose to adapt large language models (LLMs) to assist zero-shot learning. Despite recent progress, the inadequacy of LLMs in comprehending temporal information in videos and the neglect of temporal differences, e.g., the different dynamic changes between scenes or objects, remain insufficiently addressed by existing attempts in zero-shot VideoQA. In light of these challenges, a novel Temporal-guided Mixture-of-Experts Network (T-MoENet) for zero-shot video question answering is proposed in this paper. Specifically, we apply a temporal module to imbue language models with the capacity to perceive temporal information. Then a temporal-guided mixture-of-experts module is proposed to further learn the temporal differences presented in different videos. It enables the model to effectively improve the capacity of generalization. Our proposed method achieves state-of-the-art performance on multiple zero-shot VideoQA benchmarks, notably improving accuracy by 5.6% on TGIF-FrameQA and 2.3% on MSRVTT-QA while remaining competitive with other methods in the fully supervised setting. The codes and models developed in this study will be made publicly available at https://github.com/qyx1121/T-MoENet.},
  keywords={Feature extraction;Question answering (information retrieval);Adaptation models;Visualization;Zero shot learning;Training;Routing;Heat maps;Cognition;Data models;Video Question Answering;Zero-shot Learning;Mixture-of-Experts;Vision-Language},
  doi={10.1109/TCSVT.2025.3556422},
  ISSN={1558-2205},
  month={},}@ARTICLE{10948459,
  author={Chen, Wangxing and Sang, Haifeng and Zhao, Zishan},
  journal={IEEE Internet of Things Journal}, 
  title={PCHGCN: Physically Constrained Higher-order Graph Convolutional Network for Pedestrian Trajectory Prediction}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Pedestrian trajectory prediction is crucial in fields such as autonomous driving and robot navigation, but it remains challenging due to the complex social interactions among pedestrians. Previous trajectory prediction methods are limited by redundant interactions that make it difficult to accurately model higher-order social relations and capture the indirect effects of surrounding pedestrians. To address these challenges, we propose a physically constrained higher-order graph convolutional network (PCHGCN) for pedestrian trajectory prediction. PCHGCN first constructs a spatial graph and adopts the attention mechanism to obtain an attention score matrix representing the social interactions of pedestrians. Then, the higher-order social interactions are obtained through multiple matrix power operations of the higher-order graph module. To eliminate redundant interactions in higher-order social interactions, we design a physical constraint module to constrain the modeling of higher-order social interactions according to physical characteristics of distance, field of view, and distance transformation rate between pedestrians. Additionally, we design a gated fusion module that assigns weights to higher-order social interactions, ensuring the rational utilization of different degrees of social relations. Finally, we introduce the last frame velocity as physical prior knowledge to dynamically correct the predicted trajectory of the constant velocity model to improve the accuracy and diversity of future trajectory prediction. Experiments on ETH, UCY, and SDD datasets demonstrate that our method outperforms existing methods, achieving superior prediction performance with only 15.3K model parameters. The code is publicly available at https://github.com/Chenwangxing/PCHGCN-Master.},
  keywords={Pedestrians;Trajectory;Predictive models;Accuracy;Logic gates;Convolution;Adaptation models;Attention mechanisms;Transformers;Navigation;pedestrian trajectory prediction;higher-order social relations;physical constrain;gated fusion;physical prior knowledge},
  doi={10.1109/JIOT.2025.3556839},
  ISSN={2327-4662},
  month={},}@BOOK{10172361,
  author={Sword, Helen and Marsh, Selina Tusitala},
  booktitle={Writing with Pleasure},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={An essential guide to cultivating joy in your professional and personal writingWriting should be a pleasurable challenge, not a painful chore. Writing with Pleasure empowers academic, professional, and creative writers to reframe their negative emotions about writing and reclaim their positive ones. By learning how to cast light on the shadows, you will soon find yourself bringing passion and pleasure to everything you write.Acclaimed international writing expert Helen Sword invites you to step into your “WriteSPACE”—a space of pleasurable writing that is socially balanced, physically engaged, aesthetically nourishing, creatively challenging, and emotionally uplifting. Sword weaves together cutting-edge findings in the sciences and social sciences with compelling narratives gathered from nearly six hundred faculty members and graduate students from across the disciplines and around the world. She provides research-based principles, hands-on strategies, and creative “pleasure prompts” designed to help you ramp up your productivity and enhance the personal rewards of your writing practice. Whether you’re writing a scholarly article, an administrative email, or a love letter, this book will inspire you to find delight in even the most mundane writing tasks and a richer, deeper pleasure in those you already enjoy.Exuberantly illustrated by prizewinning graphic memoirist Selina Tusitala Marsh, Writing with Pleasure is an indispensable resource for academics, students, professionals, and anyone for whom writing has come to feel like a burden rather than a joy.},
  keywords={Writing with Pleasure;helen sword;Selina Tusitala Marsh;essential guide;professional writing;learning how to write;how to write;personal writing;skills for scholars;princeton university press;creative writing;compelling narratives;scholarly article;an administrative email;a love letter;illustrated by prizewinning graphic memoirist;WriteSPACE;pleasurable writing;creatively challenging;research-based principles;hands-on strategies;writing prompts;writing skills;learn to write;make writing fun;how to make writing fun;creative space;be a better writer;books about writing},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9780691229416},
  url={https://ieeexplore.ieee.org/document/10172361},}@INPROCEEDINGS{10152832,
  author={Ning, Bin and Liu, Fang and Liu, Zhixiong},
  booktitle={2023 26th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
  title={Creativity Support in AI Co-creative Tools: Current Research, Challenges and Opportunities}, 
  year={2023},
  volume={},
  number={},
  pages={5-10},
  abstract={Artificial Intelligence technology-driven Creativity Support Tools (AI-CSTs) provide specific field capability support for human creative activities. In this paper, we compare and analyze the current situation and trend of AI-CSTs design space in four aspects: creative stage, support form, support technology, and role diversity. Through a coding study and comparative analysis of 50 AI-CSTs cases, we discuss the impact of AI-CSTs on traditional workflows, the boundaries of AI-CSTs as co-creators, and how to treat AI errors, which provides insights for future AI-CSTs design. We summarize the collaboration framework in AI-CSTs. Finally, this paper also studies the information technology requirements and challenges of AI-CSTs research, which provides a new perspective to understanding the landscape of AI-CSTs.},
  keywords={Systematics;Federated learning;Collaboration;Market research;Encoding;Information technology;Creativity;Creativity Support Tools;Computational Creativity;Artificial Intelligence;Collaboration framework},
  doi={10.1109/CSCWD57460.2023.10152832},
  ISSN={2768-1904},
  month={May},}@INBOOK{9226728,
  author={Riguzzi, Fabrizio},
  booktitle={Foundations of Probabilistic Logic Programming: Languages, Semantics, Inference and Learning}, 
  title={Foundations of Probabilistic Logic Programming: Languages, Semantics, Inference and Learning}, 
  year={2020},
  volume={},
  number={},
  pages={i-xxxv},
  abstract={Probabilistic Logic Programming extends Logic Programming by enabling the representation of uncertain information by means of probability theory. Probabilistic Logic Programming is at the intersection of two wider research fields: the integration of logic and probability and Probabilistic Programming. Logic enables the representation of complex relations among entities while probability theory is useful for model uncertainty over attributes and relations. Combining the two is a very active field of study. Probabilistic Programming extends programming languages with probabilistic primitives that can be used to write complex probabilistic models. Algorithms for the inference and learning tasks are then provided automatically by the system. Probabilistic Logic programming is at the same time a logic language, with its knowledge representation capabilities, and a Turing complete language, with its computation capabilities, thus providing the best of both worlds. Since its birth, the field of Probabilistic Logic Programming has seen a steady increase of activity, with many proposals for languages and algorithms for inference and learning. Foundations of Probabilistic Logic Programming aims at providing an overview of the field with a special emphasis on languages under the Distribution Semantics, one of the most influential approaches. The book presents the main ideas for semantics, inference, and learning and highlights connections between the methods. Many examples of the book include a link to a page of the web application http://cplint.eu where the code can be run online.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770220170},
  url={https://ieeexplore.ieee.org/document/9226728},}@ARTICLE{10938259,
  author={Pan, Jialun and Zhao, Zhanzhan and Han, Dongkun},
  journal={IEEE Transactions on Learning Technologies}, 
  title={Academic Performance Prediction Using Machine Learning Approaches: A Survey}, 
  year={2025},
  volume={},
  number={},
  pages={1-18},
  abstract={Properly predicting students' academic performance is crucial for elevating educational outcomes in various disciplines. Through precise performance prediction, schools can quickly pinpoint students facing challenges and provide customized educational materials suited to their specific learning needs. The reliance on teachers' experience to predict students' academic performance has proven to be less accurate and efficient than desired. Consequently, the past decade has witnessed a marked surge in employing machine learning and data mining techniques to forecast students' performance. However, the academic community has yet to agree on the most effective algorithm for predicting academic outcomes. Nonetheless, conducting an analysis and comparison of the existing algorithms in this field remains meaningful. Furthermore, recommendations for selecting an appropriate algorithm will be provided to interested researchers and educators based on their specific requirements. This paper reviews the state-of-the-art literature on academic performance predictions using machine-learning approaches in recent years. It details the variables analyzed, the algorithms implemented, the datasets utilized, and the evaluation metrics applied to assess model efficacy. What makes this work different is that relevant surveys in the past 10 years are also analyzed and compared, highlighting their contributions and review methods. Additionally, we compared the accuracy of various machine learning models using popular open-access datasets and determined the best-performing algorithms among them. Our dataset and source codes are released for future algorithm comparisons and evaluations in this community.},
  keywords={Reviews;Prediction algorithms;Surveys;Machine learning;Data mining;Machine learning algorithms;Random forests;Artificial intelligence;Accuracy;Logistic regression;Survey;Performance prediction;Learning Analytics;AI in Education;Machine learning;Students' performance;Supervised learning;Neural networks},
  doi={10.1109/TLT.2025.3554174},
  ISSN={1939-1382},
  month={},}@ARTICLE{10771963,
  author={Lai, Zengyuan and Yang, Jiarui and Xia, Songpengcheng and Wu, Qi and Sun, Zhen and Yu, Wenxian and Pei, Ling},
  journal={IEEE Internet of Things Journal}, 
  title={SMART: Scene-Motion-Aware Human Action Recognition Framework for Mental Disorder Group}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  abstract={Patients with mental disorders often exhibit risky abnormal actions, such as climbing walls or hitting windows, necessitating intelligent video behavior monitoring for smart healthcare with the rising Internet of Things (IoT) technology. However, the development of vision-based Human Action Recognition (HAR) for these actions is hindered by the lack of specialized algorithms and datasets. In this paper, we innovatively propose to build a vision-based HAR dataset including abnormal actions often occurring in the mental disorder group and then introduce a novel Scene-Motion-aware Action Recognition Technology framework, named SMART, consisting of two technical modules. First, we propose a scene perception module to extract human motion trajectory and human-scene interaction features, which introduces additional scene information for a supplementary semantic representation of the above actions. Second, the multi-stage fusion module fuses the skeleton motion, motion trajectory, and human-scene interaction features, enhancing the semantic association between the skeleton motion and the above supplementary representation, thus generating a comprehensive representation with both human motion and scene information. The effectiveness of our proposed method has been validated on our self-collected HAR dataset (MentalHAD), achieving 94.9% and 93.1% accuracy in un-seen subjects and scenes and outperforming state-of-the-art approaches by 6.5% and 13.2%, respectively. The demonstrated subject-and scene-generalizability makes it possible for SMART’s migration to practical deployment in smart healthcare systems for mental disorder patients in medical settings. The code and dataset will be released publicly for further research: https://github.com/Inowlzy/SMART.git.},
  keywords={Feature extraction;Skeleton;Semantics;Mental disorders;Internet of Things;Trajectory;Data mining;Medical services;Human activity recognition;Pipelines;Action Recognition;Mental Disorders;Multi-stage Fusion;Scene Semantic Understanding;Smart Healthcare},
  doi={10.1109/JIOT.2024.3509458},
  ISSN={2327-4662},
  month={},}@ARTICLE{10819292,
  author={Tamir, Tariku Sinshaw and Hua, Xijin and Jiang, Jingchao and Leng, Jiewu and Xiong, Gang and Shen, Zhen and Liu, Qiang},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Data-Driven and Physics-Assisted Machine Learning Approach for Warpage Classification and Process Parameter Optimization in a 3-D-Printed BeltClip}, 
  year={2024},
  volume={},
  number={},
  pages={1-16},
  abstract={3-D printing, or additive manufacturing (AM), leverages 3-D computer-aided design models and numerical control to produce objects layer-by-layer, playing a key role in Industry 4.0 and Industry 5.0. Despite its potential to revolutionize manufacturing by creating complex structures more efficiently and cost-effectively, 3-D printing still faces quality issues due to a lack of sufficient data, resulting in improper process parameter settings and poor analyzability. This work introduces a data-driven and physics-assisted machine learning (DP-ML) approach for a 3-D-printed BeltClip object, integrating finite element analysis (FEA) and physics-informed machine learning (PIML). The proposed DP-ML framework provides a cost-effective and time-efficient data collection method using Digimat-AM and a warpage classification algorithm. The data collection begins with obtaining the STereoLithography (STL) file of the BeltClip object from Thingiverse and slicing it in Ultimaker©, Cura, considering process parameters such as infill amount, toolpath pattern, layer height, print speed, and extrusion temperature. The resulting G-code file is then input into Digimat-AM for further parameter setting and analysis. In Digimat-AM, glass fiber-filled and unfilled material types are set, undergoing the virtual 3-D printing process, followed by a warpage analysis of the printed BeltClip. The collected 3-D printing data is used to build ML models—deep neural network (DNN), decision tree (DT), support vector machine (SVM), logistic regression (LR), and random forest. The DNN contains three architectures—DNN-1, DNN-2, and DNN-3. Based on the metrics of precision, recall, F1-score, and accuracy, DNN-3 outperforms the others and is chosen for the warpage classification algorithm. The presented DP-ML approach is compared with the state-of-the-art methods and shows a promising capability to predicting warpage, optimizing process parameters, and improving the overall quality and efficiency of a 3-D-printed BeltClip.},
  keywords={Three-dimensional printing;Solid modeling;Manufacturing;Data collection;Monitoring;Artificial neural networks;Data models;Computational modeling;Support vector machines;Design automation;3-D printing;data-driven;digital manufacturing;optimal process parameters;physics-assisted machine learning;warpage analysis},
  doi={10.1109/TCSS.2024.3512614},
  ISSN={2329-924X},
  month={},}@BOOK{9098746,
  author={Porche, Isaac},
  booktitle={Cyberwarfare: An Introduction to Information-Age Conflict},
  year={2019},
  volume={},
  number={},
  pages={},
  abstract={Conflict in cyberspace is becoming more prevalent in all public and private sectors and is of concern on many levels. As a result, knowledge of the topic is becoming essential across most disciplines. This book reviews and explains the technologies that underlie offensive and defensive cyber operations, which are practiced by a range of cyber actors including state actors, criminal enterprises, activists, and individuals. It explains the processes and technologies that enable the full spectrum of cyber operations. Readers will learn how to use basic tools for cyber security and pen-testing, and also be able to quantitatively assess cyber risk to systems and environments and discern and categorize malicious activity. The book provides key concepts of information age conflict technical basics/fundamentals needed to understand more specific remedies and activities associated with all aspects of cyber operations. It explains techniques associated with offensive cyber operations, with careful distinctions made between cyber ISR, cyber exploitation, and cyber attack. It explores defensive cyber operations and includes case studies that provide practical information, making this book useful for both novice and advanced information warfare practitioners.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781630815783},
  url={https://ieeexplore.ieee.org/document/9098746},}@ARTICLE{10879537,
  author={Lin, Ancheng and Xiang, Yusheng and Li, Jun and Prasad, Mukesh},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Dynamic Appearance Particle Neural Radiance Field}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Neural Radiance Fields (NeRFs) have shown great potential in modeling 3D scenes. Dynamic NeRFs extend this model by capturing time-varying elements, typically using deformation fields. The existing dynamic NeRFs employ a similar Eulerian representation for both light radiance and deformation fields. This leads to a close coupling of appearance and motion and lacks a physical interpretation. In this work, we propose Dynamic Appearance Particle Neural Radiance Field (DAP-NeRF), which introduces particle-based representation to model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists of the superposition of a static field and a dynamic field. The dynamic field is quantized as a collection of appearance particles, which carries the visual information of a small dynamic element in the scene and is equipped with a motion model. All components, including the static field, the visual features and the motion models of particles, are learned from monocular videos without any prior geometric knowledge of the scene. We develop an efficient computational framework for the particle-based model. We also construct a new dataset to evaluate motion modeling. Experimental results show that DAP-NeRF is an effective technique to capture not only the appearance but also the physically meaningful motions in a 3D dynamic scene. Code is available at: https://github.com/Cenbylin/DAP-NeRF.},
  keywords={Neural radiance field;Dynamics;Three-dimensional displays;Deformation;Solid modeling;Deformable models;Vehicle dynamics;Computational modeling;Visualization;Rendering (computer graphics);Neural Radiance Field;Dynamic Scene modeling;3D Reconstruction;View Synthesis},
  doi={10.1109/TCSVT.2025.3540792},
  ISSN={1558-2205},
  month={},}@ARTICLE{10884991,
  author={Liu, Shanghuan and Gai, Shaoyan and Da, Feipeng and Waris, Fazal},
  journal={Computational Visual Media}, 
  title={Geometry-aware 3D pose transfer using transformer autoencoder}, 
  year={2024},
  volume={10},
  number={6},
  pages={1063-1078},
  abstract={3D pose transfer over unorganized point clouds is a challenging generation task, which transfers a source's pose to a target shape and keeps the target's identity. Recent deep models have learned deformations and used the target's identity as a style to modulate the combined features of two shapes or the aligned vertices of the source shape. However, all operations in these models are point-wise and independent and ignore the geometric information on the surface and structure of the input shapes. This disadvantage severely limits the generation and generalization capabilities. In this study, we propose a geometry-aware method based on a novel transformer autoencoder to solve this problem. An efficient self-attention mechanism, that is, cross-covariance attention, was utilized across our framework to perceive the correlations between points at different distances. Specifically, the transformer encoder extracts the target shape's local geometry details for identity attributes and the source shape's global geometry structure for pose information. Our transformer decoder efficiently learns deformations and recovers identity properties by fusing and decoding the extracted features in a geometry attentional manner, which does not require corresponding information or modulation steps. The experiments demonstrated that the geometry-aware method achieved state-of-the-art performance in a 3D pose transfer task. The implementation code and data are available at https://github.com/SEULSH/Geometry-Aware-3D-Pose-Transfer-Using-Transformer-Autoencoder.},
  keywords={Shape;Transformers;Three-dimensional displays;Deformation;Autoencoders;Decoding;Feature extraction;Solid modeling;Deformable models;Topology;3D pose transfer;geometry-aware;transformer autoencoder;cross-covariance attention},
  doi={10.1007/s41095-023-0379-8},
  ISSN={2096-0662},
  month={Dec},}@ARTICLE{9314114,
  author={Jha, Debesh and Smedsrud, Pia H. and Johansen, Dag and de Lange, Thomas and Johansen, Håvard D. and Halvorsen, Pål and Riegler, Michael A.},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={A Comprehensive Study on Colorectal Polyp Segmentation With ResUNet++, Conditional Random Field and Test-Time Augmentation}, 
  year={2021},
  volume={25},
  number={6},
  pages={2029-2040},
  abstract={Colonoscopy is considered the gold standard for detection of colorectal cancer and its precursors. Existing examination methods are, however, hampered by high overall miss-rate, and many abnormalities are left undetected. Computer-Aided Diagnosis systems based on advanced machine learning algorithms are touted as a game-changer that can identify regions in the colon overlooked by the physicians during endoscopic examinations, and help detect and characterize lesions. In previous work, we have proposed the ResUNet++ architecture and demonstrated that it produces more efficient results compared with its counterparts U-Net and ResUNet. In this paper, we demonstrate that further improvements to the overall prediction performance of the ResUNet++ architecture can be achieved by using Conditional Random Field (CRF) and Test-Time Augmentation (TTA). We have performed extensive evaluations and validated the improvements using six publicly available datasets: Kvasir-SEG, CVC-ClinicDB, CVC-ColonDB, ETIS-Larib Polyp DB, ASU-Mayo Clinic Colonoscopy Video Database, and CVC-VideoClinicDB. Moreover, we compare our proposed architecture and resulting model with other state-of-the-art methods. To explore the generalization capability of ResUNet++ on different publicly available polyp datasets, so that it could be used in a real-world setting, we performed an extensive cross-dataset evaluation. The experimental results show that applying CRF and TTA improves the performance on various polyp segmentation datasets both on the same dataset and cross-dataset. To check the model's performance on difficult to detect polyps, we selected, with the help of an expert gastroenterologist, 196 sessile or flat polyps that are less than ten millimeters in size. This additional data has been made available as a subset of Kvasir-SEG. Our approaches showed good results for flat or sessile and smaller polyps, which are known to be one of the major reasons for high polyp miss-rates. This is one of the significant strengths of our work and indicates that our methods should be investigated further for use in clinical practice.},
  keywords={Image segmentation;Colonoscopy;Cancer;Hospitals;Computer architecture;Training;Task analysis;Colonoscopy;polyp segmentation;ResUNet++;conditional random field;test-time augmentation;generalization},
  doi={10.1109/JBHI.2021.3049304},
  ISSN={2168-2208},
  month={June},}@ARTICLE{10261199,
  author={Qiu, Jianing and Li, Lin and Sun, Jiankai and Peng, Jiachuan and Shi, Peilun and Zhang, Ruiyang and Dong, Yinzhao and Lam, Kyle and Lo, Frank P.-W. and Xiao, Bo and Yuan, Wu and Wang, Ningli and Xu, Dong and Lo, Benny},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Large AI Models in Health Informatics: Applications, Challenges, and the Future}, 
  year={2023},
  volume={27},
  number={12},
  pages={6074-6087},
  abstract={Large AI models, or foundation models, are models recently emerging with massive scales both parameter-wise and data-wise, the magnitudes of which can reach beyond billions. Once pretrained, large AI models demonstrate impressive performance in various downstream tasks. A prime example is ChatGPT, whose capability has compelled people's imagination about the far-reaching influence that large AI models can have and their potential to transform different domains of our lives. In health informatics, the advent of large AI models has brought new paradigms for the design of methodologies. The scale of multi-modal data in the biomedical and health domain has been ever-expanding especially since the community embraced the era of deep learning, which provides the ground to develop, validate, and advance large AI models for breakthroughs in health-related areas. This article presents a comprehensive review of large AI models, from background to their applications. We identify seven key sectors in which large AI models are applicable and might have substantial influence, including: 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics. We examine their challenges, followed by a critical discussion about potential future directions and pitfalls of large AI models in transforming the field of health informatics.},
  keywords={Data models;Artificial intelligence;Biological system modeling;Task analysis;Bioinformatics;Medical diagnostic imaging;Chatbots;Artificial intelligence;bioinformatics;biomedicine;deep learning;foundation model;health informatics;healthcare;medical imaging},
  doi={10.1109/JBHI.2023.3316750},
  ISSN={2168-2208},
  month={Dec},}@ARTICLE{9738624,
  author={Tabatabaee Malazi, Hadi and Chaudhry, Saqib Rasool and Kazmi, Aqeel and Palade, Andrei and Cabrera, Christian and White, Gary and Clarke, Siobhán},
  journal={IEEE Access}, 
  title={Dynamic Service Placement in Multi-Access Edge Computing: A Systematic Literature Review}, 
  year={2022},
  volume={10},
  number={},
  pages={32639-32688},
  abstract={The advent of new cloud-based applications such as mixed reality, online gaming, autonomous driving, and healthcare has introduced infrastructure management challenges to the underlying service network. Multi-access edge computing (MEC) extends the cloud computing paradigm and leverages servers near end-users at the network edge to provide a cloud-like environment. The optimum placement of services on edge servers plays a crucial role in the performance of such service-based applications. Dynamic service placement problem addresses the adaptive configuration of application services at edge servers to facilitate end-users and those devices that need to offload computation tasks. While reported approaches in the literature shed light on this problem from a particular perspective, a panoramic study of this problem reveals the research gaps in the big picture. This paper introduces the dynamic service placement problem and outline its relations with other problems such as task scheduling, resource management, and caching at the edge. We also present a systematic literature review of existing dynamic service placement methods for MEC environments from networking, middleware, applications, and evaluation perspectives. In the first step, we review different MEC architectures and their enabling technologies from a networking point of view. We also introduce different cache deployment solutions in network architectures and discuss their design considerations. The second step investigates dynamic service placement methods from a middleware viewpoint. We review different service packaging technologies and discuss their trade-offs. We also survey the methods and identify eight research directions that researchers follow. Our study categorises the research objectives into six main classes, proposing a taxonomy of design objectives for the dynamic service placement problem. We also investigate the reported methods and devise a solutions taxonomy comprising six criteria. In the third step, we concentrate on the application layer and introduce the applications that can take advantage of dynamic service placement. The fourth step investigates evaluation environments used to validate the solutions, including simulators and testbeds. We introduce real-world datasets such as edge server locations, mobility traces, and service requests used to evaluate the methods. We compile a list of open issues and challenges categorised by various viewpoints in the last step.},
  keywords={Cloud computing;Servers;Vehicle dynamics;Resource management;Wireless fidelity;Taxonomy;Task analysis;Mobile edge computing;decentralised cloud;MEC server;service caching;service offloading;computational offloading;service deployment;resource management;service orchestration},
  doi={10.1109/ACCESS.2022.3160738},
  ISSN={2169-3536},
  month={},}
